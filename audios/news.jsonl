{"query": "AI", "id": "2505.16647v1", "url": "http://arxiv.org/abs/2505.16647v1", "title": "Point, Detect, Count: Multi-Task Medical Image Understanding with Instruction-Tuned Vision-Language Models", "summary": "We investigate fine-tuning Vision-Language Models (VLMs) for multi-task\nmedical image understanding, focusing on detection, localization, and counting\nof findings in medical images. Our objective is to evaluate whether\ninstruction-tuned VLMs can simultaneously improve these tasks, with the goal of\nenhancing diagnostic accuracy and efficiency. Using MedMultiPoints, a\nmultimodal dataset with annotations from endoscopy (polyps and instruments) and\nmicroscopy (sperm cells), we reformulate each task into instruction-based\nprompts suitable for vision-language reasoning. We fine-tune\nQwen2.5-VL-7B-Instruct using Low-Rank Adaptation (LoRA) across multiple task\ncombinations. Results show that multi-task training improves robustness and\naccuracy. For example, it reduces the Count Mean Absolute Error (MAE) and\nincreases Matching Accuracy in the Counting + Pointing task. However,\ntrade-offs emerge, such as more zero-case point predictions, indicating reduced\nreliability in edge cases despite overall performance gains. Our study\nhighlights the potential of adapting general-purpose VLMs to specialized\nmedical tasks via prompt-driven fine-tuning. This approach mirrors clinical\nworkflows, where radiologists simultaneously localize, count, and describe\nfindings - demonstrating how VLMs can learn composite diagnostic reasoning\npatterns. The model produces interpretable, structured outputs, offering a\npromising step toward explainable and versatile medical AI. Code, model\nweights, and scripts will be released for reproducibility at\nhttps://github.com/simula/PointDetectCount.", "authors": ["Sushant Gautam", "Michael A. Riegler", "Pål Halvorsen"], "published_date": "2025-05-22", "title_zh": "指點、偵測、計數：利用指令調整的視覺語言模型實現多任務醫療影像理解", "summary_zh": "本研究探索如何微調視覺語言模型（VLMs）以處理多任務醫療影像理解，特別關注醫療影像中病灶的偵測、定位和計數。目標是評估經過指令調整的VLMs是否能同時改善這些任務，進而提升診斷準確性和效率。研究利用MedMultiPoints這個多模態資料集，包含內視鏡（瘜肉和器械）和顯微鏡（精子細胞）的標註，將每個任務重新設計成基於指令的提示，以適用於視覺語言推理。研究使用低秩適應（LoRA）在多個任務組合中微調Qwen2.5-VL-7B-Instruct模型。結果表明，多任務訓練提高了模型的穩健性和準確性，例如，在計數+指向任務中，降低了計數平均絕對誤差（MAE），並提高了匹配準確性。然而，也出現了一些權衡，例如更多的零案例點預測，表明在邊緣情況下可靠性降低，儘管整體性能有所提升。研究強調了通過提示驅動的微調，將通用VLMs適應於專門醫療任務的潛力。這種方法模擬了臨床工作流程，放射科醫生同時定位、計數和描述病灶，展示了VLMs如何學習複合診斷推理模式。模型產生可解釋的結構化輸出，為可解釋和多功能的醫療AI提供了一個有希望的方向。", "applications": ["**AI輔助病理切片分析：** 醫生可以利用該模型快速且準確地計數病理切片中的癌細胞，協助判斷癌症分期和預後。", "**內視鏡檢查即時輔助：** 在內視鏡檢查過程中，模型可以即時標記並計數瘜肉，幫助醫生更全面地檢查並減少漏診。", "**精液分析自動化：** 自動計數精子數量和評估精子品質，提高精液分析的效率和客觀性，協助不孕症的診斷和治療。"], "pitch": "這項研究開發的醫療影像多任務理解模型，能精準執行指點、偵測、計數等關鍵任務，模擬醫師診斷流程，大幅提升醫療效率及準確性。其商業價值體現在：\n\n1.  **診斷輔助工具：** 可整合至現有醫療影像系統，提供AI輔助診斷，降低誤診率，提高診斷效率，減少醫師工作負擔。\n2.  **精準醫療應用：** 提供精準計數和定位功能，能用於精準醫療方案的開發，如針對特定癌細胞數量和位置的標靶治療。\n3.  **遠程醫療與篩檢：** 透過雲端部署，可在資源匱乏地區提供遠程診斷服務，並進行大規模篩檢，及早發現潛在疾病。同時，模型的可解釋性與結構化輸出，有利於醫療人員理解判斷依據，進而提升對AI診斷的信任感，促進臨床採用。初期可鎖定病理分析、內視鏡檢查等高價值應用場景，透過SaaS模式或軟體授權獲利，並與醫療設備商、醫院等建立戰略合作關係，搶占市場先機。", "audio": "docs/audios/2505.16647v1.mp3", "timestamp": "2025-05-24T20:18:25.101514"}
{"query": "Foundation Model", "id": "2505.15132v1", "url": "http://arxiv.org/abs/2505.15132v1", "title": "Multicrossmodal Automated Agent for Integrating Diverse Materials Science Data", "summary": "We introduce a multicrossmodal LLM-agent framework motivated by the growing\nvolume and diversity of materials-science data ranging from high-resolution\nmicroscopy and dynamic simulation videos to tabular experiment logs and\nsprawling literature archives. While recent AI efforts have accelerated\nindividual tasks such as property prediction or image classification, they\ntypically treat each modality in isolation, leaving rich cross-modal\ncorrelations unexplored and forcing researchers to perform laborious manual\nintegration. Moreover, existing multimodal foundation models often require\nexpensive retraining or fine-tuning on domain data, and current multi-agent\nsystems in materials informatics address only narrow subtasks. To overcome\nthese obstacles, we design a coordinated team of specialized LLM agents, each\nequipped with domain-adapted prompts and plugins that project their outputs\ninto a shared embedding space. A dynamic gating mechanism then weights and\nmerges these insights, enabling unified reasoning over heterogeneous inputs\nwithout ever modifying the underlying LLM weights. We validate our approach on\nchallenging case studies and demonstrate substantial gains in retrieval\naccuracy (85%), captioning fidelity, and integrated coverage (35%) compared to\nsingle-modality and zero-shot baselines. Our work paves the way for AI digital\nresearchers capable of bridging data silos and accelerating the\nmaterials-discovery cycle. The code is available at\nhttps://github.com/adibgpt/Multicrossmodal-Autonomous-Materials-Science-Agent.", "authors": ["Adib Bazgir", "Rama chandra Praneeth Madugula", "Yuwen Zhang"], "published_date": "2025-05-21", "title_zh": "多模態交叉自動化代理，用於整合多樣化的材料科學數據", "summary_zh": "這篇論文提出了一個多模態交叉的LLM代理框架，旨在解決材料科學領域數據量龐大且種類繁多的問題。這些數據涵蓋高解析度顯微鏡圖像、動態模擬影片、實驗紀錄表格以及大量的文獻檔案。現有AI雖然在個別任務上表現出色，但往往忽略了不同數據模態之間的關聯性，需要研究人員手動整合。這個框架透過協調一個由專門LLM代理組成的團隊，每個代理都配備了針對特定領域的提示詞和插件，將其輸出投影到一個共享的嵌入空間。然後，透過動態閘控機制加權並合併這些見解，實現對異質輸入的統一推理，而無需修改底層LLM的權重。實驗證明，與單模態和零樣本基線相比，該方法在檢索準確性、圖像描述保真度和綜合覆蓋率方面都取得了顯著提升。此研究為能夠橋接數據孤島並加速材料發現週期的AI數位研究人員鋪平了道路。", "applications": ["**新材料設計加速:** 工程師可上傳材料顯微鏡圖像、化學成分和模擬數據，AI自動找出關聯性，加速設計具有特定性能（如高強度、耐腐蝕）的新材料，應用於汽車、航空航天等領域。", "**失效分析與預測:** 根據設備的使用記錄（例如：溫度、壓力）和材料檢測數據（例如：裂紋圖像、微量元素分析），AI能分析材料的失效原因，並預測未來可能發生的失效，協助設備維護人員進行預防性維護。", "**科研文獻自動整理與知識發現:** 研究人員可將大量材料科學文獻導入系統，AI自動提取關鍵信息、建立知識圖譜，幫助研究人員快速掌握領域動態、發現研究空白，進而提出新的研究方向。"], "pitch": "我們正在構建一個AI驅動的材料科學研究平台，它能整合來自不同來源、不同格式的數據，並利用先進的LLM技術進行深度分析，打破數據孤島，加速新材料的發現和應用。我們的核心競爭力在於多模態數據的無縫整合和AI推理能力的強大。市場機會巨大，材料科學是各行各業的基石。通過我們的平台，企業可以顯著縮短研發週期、降低成本、並創造更高性能的產品。我們尋求投資者，共同打造材料科學的未來。", "audio": "docs/audios/2505.15132v1.mp3", "timestamp": "2025-05-24T20:18:43.899310"}
{"query": "Diffusion Model", "id": "2505.16275v1", "url": "http://arxiv.org/abs/2505.16275v1", "title": "Semiparametric Bernstein-von Mises theorems for reversible diffusions", "summary": "We establish a general semiparametric Bernstein-von Mises theorem for\nBayesian nonparametric priors based on continuous observations in a periodic\nreversible multidimensional diffusion model. We consider a wide range of\nfunctionals satisfying an approximate linearization condition, including\nseveral nonlinear functionals of the invariant measure. Our result is applied\nto Gaussian and Besov-Laplace priors, showing these can perform efficient\nsemiparametric inference and thus justifying the corresponding Bayesian\napproach to uncertainty quantification. Our theoretical results are illustrated\nvia numerical simulations.", "authors": ["Matteo Giordano", "Kolyan Ray"], "published_date": "2025-05-22", "title_zh": "可逆擴散模型的半參數 Bernstein-von Mises 定理", "summary_zh": "本研究針對週期性可逆多維擴散模型中，基於連續觀測的貝葉斯非參數先驗，建立了廣義半參數 Bernstein-von Mises 定理。 我們考慮了滿足近似線性化條件的廣泛函數，包括不變測度的幾個非線性函數。 我們的結果應用於高斯和 Besov-Laplace 先驗，表明這些先驗可以執行有效的半參數推斷，從而證明了不確定性量化的貝葉斯方法的合理性。 我們的理論結果通過數值模擬進行了說明。", "applications": ["**金融風險評估：** 透過觀察股票價格的波動，推斷隱藏的市場風險因子，並更精準地預測未來風險。", "**氣候模型校準：** 利用連續氣溫數據，估計氣候模型的參數，並量化預測的不確定性，提高氣候變遷預測的準確度。", "**醫療影像分析：** 分析連續的醫療影像序列（如MRI），估計腫瘤的生長速率和治療效果，並提供個性化的治療方案。"], "pitch": "這項研究提供了一種更精準、更可靠的不確定性量化方法，尤其適用於處理複雜且具有時間依賴性的數據。 它的核心價值在於能更有效地提取隱藏在連續數據中的信息，並提供更準確的預測和風險評估。 這在金融、氣候科學和醫療等領域具有巨大的潛力，能協助決策者做出更明智的選擇。 例如，在金融領域，它可以幫助開發更有效的風險管理工具；在氣候科學領域，它可以幫助更準確地預測氣候變遷的影響；在醫療領域，它可以幫助醫生制定更個性化的治療方案。 透過將這項研究成果商業化，我們可以開發出更先進的分析工具和預測模型，為各行業帶來顯著的效率提升和價值創造。", "audio": "docs/audios/2505.16275v1.mp3", "timestamp": "2025-05-24T20:18:57.819960"}
{"query": "AI", "id": "2505.16630v1", "url": "http://arxiv.org/abs/2505.16630v1", "title": "SoccerChat: Integrating Multimodal Data for Enhanced Soccer Game Understanding", "summary": "The integration of artificial intelligence in sports analytics has\ntransformed soccer video understanding, enabling real-time, automated insights\ninto complex game dynamics. Traditional approaches rely on isolated data\nstreams, limiting their effectiveness in capturing the full context of a match.\nTo address this, we introduce SoccerChat, a multimodal conversational AI\nframework that integrates visual and textual data for enhanced soccer video\ncomprehension. Leveraging the extensive SoccerNet dataset, enriched with jersey\ncolor annotations and automatic speech recognition (ASR) transcripts,\nSoccerChat is fine-tuned on a structured video instruction dataset to\nfacilitate accurate game understanding, event classification, and referee\ndecision making. We benchmark SoccerChat on action classification and referee\ndecision-making tasks, demonstrating its performance in general soccer event\ncomprehension while maintaining competitive accuracy in referee decision\nmaking. Our findings highlight the importance of multimodal integration in\nadvancing soccer analytics, paving the way for more interactive and explainable\nAI-driven sports analysis. https://github.com/simula/SoccerChat", "authors": ["Sushant Gautam", "Cise Midoglu", "Vajira Thambawita", "Michael A. Riegler", "Pål Halvorsen", "Mubarak Shah"], "published_date": "2025-05-22", "title_zh": "足球聊天：整合多模態數據以增強足球比賽理解", "summary_zh": "本研究提出一個名為「足球聊天 (SoccerChat)」的多模態對話式AI框架，透過整合視覺數據（比賽畫面、球衣顏色）與文字數據（語音轉錄稿），來增強對足球比賽的理解。 SoccerChat 利用 SoccerNet 資料集進行訓練，能夠更精準地理解比賽，進行事件分類，並輔助裁判決策。實驗結果顯示，整合多模態數據能顯著提升足球分析的準確性和解釋性，為更互動、更易於理解的AI驅動體育分析開闢了道路。", "applications": ["**AI足球解說員：** SoccerChat 可以作為AI驅動的足球解說員，即時分析比賽事件，提供更深入、更全面的解說，甚至能回答觀眾提出的問題，提供互動式體驗。", "**球隊戰術分析工具：** 教練團隊可以利用 SoccerChat 分析比賽錄像，快速定位關鍵事件，分析球員表現、戰術執行情況，從而制定更有效的戰術策略。", "**足球遊戲AI助手：** 將 SoccerChat 整合到足球遊戲中，可以提升遊戲的真實感和沉浸感。 AI助手可以根據比賽情況提供建議、分析戰術，甚至模擬真實的球員互動。"], "pitch": "SoccerChat 解決了傳統足球分析過於依賴單一數據源的問題，透過整合視覺和文字數據，提供更全面、更深入的比賽理解。其商業價值在於：\n\n*   **提升現有體育數據產品的競爭力：** SoccerChat 可作為核心引擎，賦能現有的體育數據分析平台、直播平台，提供更具洞察力的分析服務，吸引更多用戶。\n*   **開創新的商業模式：** SoccerChat 可以開發成面向不同用戶群體的訂閱服務，例如為專業球隊提供深度戰術分析，為球迷提供互動式解說體驗。\n*   **授權與合作：** 可以將 SoccerChat 技術授權給體育遊戲開發商、虛擬現實公司，共同打造更具沉浸感的體育娛樂產品。\n\n總而言之，SoccerChat 具有強大的技術優勢和廣闊的應用前景，有望成為體育科技領域的一顆新星，帶來可觀的商業回報。", "audio": "audios/2505.16630v1.mp3", "timestamp": "2025-05-24T20:33:52.757575"}
{"query": "Foundation Model", "id": "2505.15116v1", "url": "http://arxiv.org/abs/2505.15116v1", "title": "Graph Foundation Models: A Comprehensive Survey", "summary": "Graph-structured data pervades domains such as social networks, biological\nsystems, knowledge graphs, and recommender systems. While foundation models\nhave transformed natural language processing, vision, and multimodal learning\nthrough large-scale pretraining and generalization, extending these\ncapabilities to graphs -- characterized by non-Euclidean structures and complex\nrelational semantics -- poses unique challenges and opens new opportunities. To\nthis end, Graph Foundation Models (GFMs) aim to bring scalable, general-purpose\nintelligence to structured data, enabling broad transfer across graph-centric\ntasks and domains. This survey provides a comprehensive overview of GFMs,\nunifying diverse efforts under a modular framework comprising three key\ncomponents: backbone architectures, pretraining strategies, and adaptation\nmechanisms. We categorize GFMs by their generalization scope -- universal,\ntask-specific, and domain-specific -- and review representative methods, key\ninnovations, and theoretical insights within each category. Beyond methodology,\nwe examine theoretical foundations including transferability and emergent\ncapabilities, and highlight key challenges such as structural alignment,\nheterogeneity, scalability, and evaluation. Positioned at the intersection of\ngraph learning and general-purpose AI, GFMs are poised to become foundational\ninfrastructure for open-ended reasoning over structured data. This survey\nconsolidates current progress and outlines future directions to guide research\nin this rapidly evolving field. Resources are available at\nhttps://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs.", "authors": ["Zehong Wang", "Zheyuan Liu", "Tianyi Ma", "Jiazheng Li", "Zheyuan Zhang", "Xingbo Fu", "Yiyang Li", "Zhengqing Yuan", "Wei Song", "Yijun Ma", "Qingkai Zeng", "Xiusi Chen", "Jianan Zhao", "Jundong Li", "Meng Jiang", "Pietro Lio", "Nitesh Chawla", "Chuxu Zhang", "Yanfang Ye"], "published_date": "2025-05-21", "title_zh": "圖神經網路基石模型：一份綜合綜述", "summary_zh": "這篇論文綜述了圖神經網路基石模型(GFMs)的最新發展。GFMs旨在將大規模、通用的智慧帶入結構化數據，從而實現跨圖形任務和領域的廣泛遷移學習。論文將GFMs分為通用、特定任務和特定領域三類，並分析了其架構、預訓練策略和適應機制。文章也探討了可遷移性、湧現能力等理論基礎，並指出了結構對齊、異質性、可擴展性和評估等挑戰。GFMs有望成為基於結構化數據進行開放式推理的基礎設施。", "applications": ["**更精準的推薦系統：** GFMs能更好地理解使用者間的關係和商品的關聯性，進而推薦更符合個人喜好的產品或服務，例如：影片、音樂、商品等。", "**更有效的藥物發現：** GFMs能分析複雜的生物網路，預測藥物與目標蛋白的相互作用，加速新藥開發流程。", "**更強大的金融風險管理：** GFMs能識別複雜的金融交易網路中的潛在詐欺行為和洗錢活動，提升金融系統的安全性。"], "pitch": "各位投資人，我們正在構建基於圖神經網路的下一代AI基石模型。想像一下，有一個AI能像理解人類語言一樣理解各種關係型數據，從社交網絡到生物系統，甚至金融市場。我們的GFMs能大幅提升各行業的效率和準確性，從而帶來顛覆性的創新。我們將提供開源的基石模型，並提供基於模型的API和工具，方便各行業快速採用和定制。這不僅僅是一個技術突破，更是一個巨大的商業機會，我們正在構建一個全新的AI基礎設施，搶佔市場先機，回報絕對超乎您的想像！", "audio": "audios/2505.15116v1.mp3", "timestamp": "2025-05-24T20:34:06.938955"}
{"query": "Diffusion Model", "id": "2505.16239v1", "url": "http://arxiv.org/abs/2505.16239v1", "title": "DOVE: Efficient One-Step Diffusion Model for Real-World Video Super-Resolution", "summary": "Diffusion models have demonstrated promising performance in real-world video\nsuper-resolution (VSR). However, the dozens of sampling steps they require,\nmake inference extremely slow. Sampling acceleration techniques, particularly\nsingle-step, provide a potential solution. Nonetheless, achieving one step in\nVSR remains challenging, due to the high training overhead on video data and\nstringent fidelity demands. To tackle the above issues, we propose DOVE, an\nefficient one-step diffusion model for real-world VSR. DOVE is obtained by\nfine-tuning a pretrained video diffusion model (*i.e.*, CogVideoX). To\neffectively train DOVE, we introduce the latent-pixel training strategy. The\nstrategy employs a two-stage scheme to gradually adapt the model to the video\nsuper-resolution task. Meanwhile, we design a video processing pipeline to\nconstruct a high-quality dataset tailored for VSR, termed HQ-VSR. Fine-tuning\non this dataset further enhances the restoration capability of DOVE. Extensive\nexperiments show that DOVE exhibits comparable or superior performance to\nmulti-step diffusion-based VSR methods. It also offers outstanding inference\nefficiency, achieving up to a **28$\\times$** speed-up over existing methods\nsuch as MGLD-VSR. Code is available at: https://github.com/zhengchen1999/DOVE.", "authors": ["Zheng Chen", "Zichen Zou", "Kewei Zhang", "Xiongfei Su", "Xin Yuan", "Yong Guo", "Yulun Zhang"], "published_date": "2025-05-22", "title_zh": "DOVE：適用於真實世界影片超解析度的有效率單步擴散模型", "summary_zh": "這篇論文提出一個名為DOVE的單步擴散模型，旨在加速真實世界影片超解析度(VSR)的處理速度。傳統的擴散模型雖然效果好，但需要大量運算步驟，導致速度慢。DOVE透過微調預訓練的影片擴散模型CogVideoX，並引入潛在像素訓練策略以及高品質VSR專用資料集HQ-VSR，大幅提升訓練效率和還原品質。實驗結果顯示，DOVE在效能上可與多步擴散模型媲美，並且速度提升高達28倍。", "applications": ["舊影片修復：利用DOVE快速將老舊影片或低解析度影片提升至高畫質，讓回憶更加清晰。", "即時視訊會議：在網路不穩定的情況下，利用DOVE即時提升視訊解析度，提供更清晰的畫面，改善溝通品質。", "監視系統升級：將低解析度的監視器影像透過DOVE提升解析度，協助辨識更多細節，提升安全性。"], "pitch": "DOVE解決了影片超解析度領域的一個關鍵痛點：運算速度。 傳統擴散模型雖然效果好，但運算量巨大，難以應用在需要即時性的場景。 DOVE的單步設計，將速度提升了28倍，使其在諸如舊影片修復、即時視訊會議、監視系統等領域具有廣泛的商業價值。 其核心優勢在於演算法本身的效率提升，而非依賴更強大的硬體。 這意味著更低的部署成本和更廣泛的市場潛力。 我們相信，DOVE將徹底改變影片超解析度領域，並為相關應用帶來革命性的發展。", "audio": "audios/2505.16239v1.mp3", "timestamp": "2025-05-24T20:34:22.197954"}
{"query": "AI", "id": "2505.16619v1", "url": "http://arxiv.org/abs/2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "published_date": "2025-05-22", "title_zh": "開放且永續的AI：生命科學領域的挑戰、機會與未來發展", "summary_zh": "這篇論文探討了AI在生命科學領域的快速發展所帶來的機遇與挑戰。雖然AI極大地提升了生物資訊的解讀能力，但也加劇了研究結果的可重複性、可重用性以及環境永續性等問題，進而影響了對AI研究成果的信任。論文針對這些問題，提出了基於開放且永續AI (OSAI) 的實用建議，並將其與AI生態系統的300多個組成部分進行了映射，旨在協助研究人員更容易找到相關AI資源，實現可持續、可重用且透明的AI。總之，本文旨在為未來AI政策制定和結構化路徑的發展提供指導。", "applications": ["**藥物研發加速器：** AI模型若能確保數據透明、方法可重複，藥廠就能更快速驗證藥物靶點的有效性，降低研發成本和時間。", "**精準醫療診斷：** 基於開放AI的診斷模型，讓醫生可以更信任AI的診斷結果，提升診斷效率和準確性，為患者提供個性化的治療方案。", "**生態環境監測與保護：** 利用開放且可追溯的AI模型分析生物多樣性數據，更有效預測生態風險，制定可持續的環境保護策略。"], "pitch": "我們正在解決生命科學領域AI應用中的信任危機，打造一個開放、永續且透明的AI平台。目前生命科學研究對AI的投入巨大，但可重複性和可驗證性不足導致資源浪費和信任缺失。我們的平台將提供一套標準化的AI模型開發流程和資源庫，讓研究人員能更輕鬆地找到、使用和驗證AI工具。這將極大地降低藥物研發成本、提升精準醫療水平，並促進生態環境保護。我們預計，透過提升AI的可信度與效率，將能吸引更多投資進入生命科學領域，並在未來的AI驅動的醫療革命中佔據領先地位。我們的商業模式將包括訂閱制、諮詢服務和數據共享平台，為生命科學領域的AI應用帶來可持續的價值。", "audio": "audios/2505.16619v1.mp3", "timestamp": "2025-05-24T21:56:03.004955"}
{"query": "Foundation Model", "id": "2505.14975v1", "url": "http://arxiv.org/abs/2505.14975v1", "title": "Flattening Hierarchies with Policy Bootstrapping", "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "authors": ["John L. Zhou", "Jonathan C. Kao"], "published_date": "2025-05-20", "title_zh": "利用策略自舉展平層級結構", "summary_zh": "離線目標條件強化學習(GCRL)有望透過大型無獎勵軌跡數據集預訓練通用策略，類似於用於訓練電腦視覺和自然語言處理基礎模型的自監督目標。然而，由於稀疏獎勵和折扣的結合，使得原始動作相對於遙遠目標的比較優勢模糊不清，因此將GCRL擴展到更長的時間範圍仍然具有挑戰性。分層強化學習方法在長期的目標達成任務中取得了強勁的經驗結果，但它們對模組化、特定時間尺度的策略和子目標生成方法的依賴，引入了額外的複雜性，並阻礙了擴展到高維目標空間。在本研究中，我們提出了一種算法，通過基於優勢加權重要性抽樣在子目標條件策略上進行自舉，來訓練扁平（非分層）目標條件策略。我們的研究方法消除了對（子）目標空間生成模型的需求，我們發現這是擴展到大型狀態空間中高維控制的關鍵。我們進一步表明，現有的分層和基於自舉的方法對應於我們推導中的特定設計選擇。在一系列全面的基於狀態和基於像素的運動和操作基準測試中，我們的研究方法與最先進的離線GCRL算法相匹配或超過，並可擴展到先前方法失敗的複雜、長期任務。", "applications": ["**智慧家庭機器人控制：** 教導機器人執行複雜的家務，例如準備餐點、清潔房間，甚至是照顧寵物，無需人工編寫每個步驟，而是透過預先蒐集的數據讓機器人自己學習。想像一下，你只需要說「幫我煮晚餐」，機器人就能自己搞定從冰箱拿食材到烹飪的所有步驟。", "**自動駕駛長途規劃：** 讓自動駕駛汽車學習如何在複雜的城市或郊區環境中規劃長途路線，克服突發狀況，例如繞過施工路段、避開交通堵塞，並在沒有明確指令的情況下自主完成旅程。", "**工業機器手臂高精度操作：** 訓練工業機器手臂在複雜的組裝線上執行高精度的操作，例如焊接、噴漆或組裝精密零件，降低人工干預，提高生產效率和產品品質。"], "pitch": "想像一下，我們正在打造一個AI的「瑞士刀」，一個能解決各種複雜任務的通用智能體。我們的新算法通過策略自舉，克服了傳統分層強化學習的局限，能夠在高維環境下訓練出高效的策略。這意味著我們可以訓練機器人執行以前認為不可能的複雜任務，而無需耗時的手動編程。我們的潛在市場巨大，涵蓋智慧家庭、自動駕駛、製造業等領域。我們相信，這項技術將徹底改變人機互動的方式，並為各行業帶來巨大的效率提升。我們正在尋找投資者，共同將這項革命性的技術推向市場，成為通用人工智能領域的領導者。", "audio": "audios/2505.14975v1.mp3", "timestamp": "2025-05-24T21:56:32.033865"}
{"query": "Diffusion Model", "id": "2505.16174v1", "url": "http://arxiv.org/abs/2505.16174v1", "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "authors": ["Ping Liu", "Chi Zhang"], "published_date": "2025-05-22", "title_zh": "抹除還是休眠？透過可逆性重新思考概念抹除", "summary_zh": "這篇論文探討現有的 AI 影像生成模型概念抹除技術是否真的有效。研究發現，這些技術可能只是表面上的抑制，並沒有完全消除模型生成特定概念的能力。通過測試兩種代表性的抹除方法，發現被「抹除」的概念只需經過輕微的調整就能夠重新出現。這表明，現有的方法只能抑制潛在的生成表徵，而無法完全消除它們。因此，需要更深入的干預措施和更嚴格的評估標準，才能確保從生成模型中真正且不可逆地移除概念。", "applications": ["**內容審核與安全：** 在AI生成內容中，確保特定敏感或有害概念（例如暴力、仇恨言論）能夠被真正抹除，避免模型生成不當內容，保障使用者安全。", "**客製化與品牌保護：** 允許用戶移除不需要的風格或元素，客製化生成結果，同時企業可以確保其品牌形象不被AI模型不當生成和使用，維護品牌權益。", "**藝術創作與風格控制：** 藝術家或設計師可以控制AI生成作品的風格，例如移除特定藝術家的風格，避免侵權風險，或移除不符合主題的元素，專注於想要表達的意象。"], "pitch": "現今AI生成模型存在概念抹除不徹底的問題，導致潛在的安全風險和商業限制。我們的研究揭示了現有技術的不足，並為開發更有效的概念抹除方法奠定了基礎。這項技術的商業價值在於：(1) **建立更安全、可控的AI生成平台，吸引更多使用者和企業客戶。** (2) **提供進階的內容審核和品牌保護工具，解決企業在AI生成內容方面的合規性需求。** (3) **催生更具創意的AI藝術工具，賦能藝術家和設計師，創造獨一無二的作品。** 我們相信，掌握真正有效的概念抹除技術，將在蓬勃發展的AI生成市場中佔據領先地位，並開創新的商業模式和應用場景。", "audio": "audios/2505.16174v1.mp3", "timestamp": "2025-05-24T21:56:51.701931"}
