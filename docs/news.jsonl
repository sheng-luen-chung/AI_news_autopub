{"query": "AI", "id": "2505.17021v1", "url": "http://arxiv.org/abs/2505.17021v1", "title": "ARB: A Comprehensive Arabic Multimodal Reasoning Benchmark", "summary": "As Large Multimodal Models (LMMs) become more capable, there is growing\ninterest in evaluating their reasoning processes alongside their final outputs.\nHowever, most benchmarks remain focused on English, overlooking languages with\nrich linguistic and cultural contexts, such as Arabic. To address this gap, we\nintroduce the Comprehensive Arabic Multimodal Reasoning Benchmark (ARB), the\nfirst benchmark designed to evaluate step-by-step reasoning in Arabic across\nboth textual and visual modalities. ARB spans 11 diverse domains, including\nvisual reasoning, document understanding, OCR, scientific analysis, and\ncultural interpretation. It comprises 1,356 multimodal samples paired with\n5,119 human-curated reasoning steps and corresponding actions. We evaluated 12\nstate-of-the-art open- and closed-source LMMs and found persistent challenges\nin coherence, faithfulness, and cultural grounding. ARB offers a structured\nframework for diagnosing multimodal reasoning in underrepresented languages and\nmarks a critical step toward inclusive, transparent, and culturally aware AI\nsystems. We release the benchmark, rubric, and evaluation suit to support\nfuture research and reproducibility. Code available at:\nhttps://github.com/mbzuai-oryx/ARB", "authors": ["Sara Ghaboura", "Ketan More", "Wafa Alghallabi", "Omkar Thawakar", "Jorma Laaksonen", "Hisham Cholakkal", "Salman Khan", "Rao Muhammad Anwer"], "published_date": "2025-05-22", "title_zh": "ARB：一個全面的阿拉伯語多模態推理基準", "summary_zh": "大型多模態模型(LMMs)越來越強大，評估它們的推理過程也變得更加重要。但現有基準主要集中在英語，忽略了像阿拉伯語這樣具有豐富語言和文化背景的語言。為了解決這個問題，我們推出了全面的阿拉伯語多模態推理基準(ARB)，這是第一個旨在評估阿拉伯語文本和視覺模態中逐步推理的基準。ARB涵蓋11個不同的領域，包括視覺推理、文檔理解、OCR、科學分析和文化詮釋。它包含1,356個多模態樣本，並配有5,119個人工策劃的推理步驟和相應的操作。我們評估了12個最先進的開源和閉源LMMs，發現它們在連貫性、忠實性和文化基礎方面存在持續的挑戰。ARB提供了一個結構化的框架，用於診斷代表性不足語言中的多模態推理，並標誌著邁向包容、透明和具有文化意識的人工智能系統的關鍵一步。我們發布了基準、評分標準和評估套件，以支持未來的研究和可重複性。", "applications": ["**智慧教育：** 將教材圖片和文字結合，讓阿拉伯語學生在學習歷史或科學概念時，能透過模型的逐步推理，更深入理解知識點之間的關聯，例如分析古蘭經中的文本與圖像，輔助宗教研究。", "**智能客服：** 針對阿拉伯語用戶的圖文諮詢，模型可以理解用戶需求並逐步推理，提供更精確的產品建議或問題解答。例如，用戶上傳房屋照片並描述需求，模型可以分析照片風格和文字描述，推薦符合的室內設計方案。", "**新聞分析：** 模型可以分析阿拉伯語新聞文章中的文本和圖片，識別關鍵事件和人物，並根據阿拉伯文化背景進行深入解讀，例如分析政治漫畫中的隱喻和象徵意義。"], "pitch": "ARB基準是阿拉伯語多模態AI的敲門磚。現今LMMs在阿拉伯語理解和推理方面存在顯著差距，這意味著巨大的市場機會。ARB提供了一個客觀的評估標準和豐富的數據集，能加速相關技術的開發和商業化。想像一下，一個能深度理解阿拉伯文化的AI，在智慧教育、內容審核、商業洞察等領域都有著巨大潛力。我們正在打造一個充滿文化智慧的AI生態系統，早期投資將帶來豐厚回報。", "audio": "audios/2505.17021v1.mp3", "timestamp": "2025-05-24T15:19:40.770897"}
{"query": "Foundation Model", "id": "2505.16982v1", "url": "http://arxiv.org/abs/2505.16982v1", "title": "Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine", "summary": "Large Language Models (LLMs) show promise in biomedicine but lack true causal\nunderstanding, relying instead on correlations. This paper envisions causal LLM\nagents that integrate multimodal data (text, images, genomics, etc.) and\nperform intervention-based reasoning to infer cause-and-effect. Addressing this\nrequires overcoming key challenges: designing safe, controllable agentic\nframeworks; developing rigorous benchmarks for causal evaluation; integrating\nheterogeneous data sources; and synergistically combining LLMs with structured\nknowledge (KGs) and formal causal inference tools. Such agents could unlock\ntransformative opportunities, including accelerating drug discovery through\nautomated hypothesis generation and simulation, enabling personalized medicine\nthrough patient-specific causal models. This research agenda aims to foster\ninterdisciplinary efforts, bridging causal concepts and foundation models to\ndevelop reliable AI partners for biomedical progress.", "authors": ["Adib Bazgir", "Amir Habibdoust Lafmajani", "Yuwen Zhang"], "published_date": "2025-05-22", "title_zh": "超越關聯性：邁向生物醫學領域的因果大型語言模型代理", "summary_zh": "本文探討如何讓大型語言模型（LLMs）在生物醫學領域具備真正的因果理解能力，而不僅僅依賴關聯性。我們設想一種因果LLM代理，能夠整合多模態數據（文字、圖像、基因組等），並進行基於介入的推理，以推斷因果關係。實現這一目標需要克服諸多挑戰，包括設計安全可控的代理框架、開發嚴格的因果評估基準、整合異質數據源，以及將LLM與結構化知識（知識圖譜）和形式化的因果推斷工具相結合。這種代理有潛力加速藥物發現、實現個人化醫療等變革性應用。本研究旨在促進跨學科合作，結合因果概念和基礎模型，為生物醫學進展開發可靠的AI夥伴。", "applications": ["**精準醫療診斷助手:** 醫生可利用AI分析病人的基因數據、病歷、影像等多模態數據，快速找出疾病的真正病因，制定更有效的治療方案。", "**新藥研發加速器:** AI能夠自動生成藥物作用機制的假設，並模擬藥物在人體內的反應，大幅縮短新藥研發週期，降低研發成本。", "**公共衛生政策模擬器:** 政府可利用AI模擬不同公共衛生政策（例如疫苗接種計劃）對疾病傳播的影響，預測潛在風險，制定更科學合理的政策。"], "pitch": "各位投資人，我們正處於生物醫學AI的黃金時代！現有LLM主要依賴關聯性分析，缺乏真正的因果理解，這限制了它們在生物醫學領域的應用。我們的團隊正在開發基於因果推理的LLM代理，這將是下一代AI的關鍵突破。想像一下，一個能夠預測藥物副作用、診斷罕見疾病、甚至預測公共衛生危機的AI。我們相信，這項技術將徹底改變藥物研發、臨床診斷和公共衛生管理，創造巨大的市場價值。我們不僅提供了一個AI工具，更提供了一個生物醫學領域的變革引擎，等待您的投資來驅動它！", "audio": "audios/2505.16982v1.mp3", "timestamp": "2025-05-24T15:19:59.989021"}
{"query": "Diffusion Model", "id": "2505.17013v1", "url": "http://arxiv.org/abs/2505.17013v1", "title": "When Are Concepts Erased From Diffusion Models?", "summary": "Concept erasure, the ability to selectively prevent a model from generating\nspecific concepts, has attracted growing interest, with various approaches\nemerging to address the challenge. However, it remains unclear how thoroughly\nthese methods erase the target concept. We begin by proposing two conceptual\nmodels for the erasure mechanism in diffusion models: (i) reducing the\nlikelihood of generating the target concept, and (ii) interfering with the\nmodel's internal guidance mechanisms. To thoroughly assess whether a concept\nhas been truly erased from the model, we introduce a suite of independent\nevaluations. Our evaluation framework includes adversarial attacks, novel\nprobing techniques, and analysis of the model's alternative generations in\nplace of the erased concept. Our results shed light on the tension between\nminimizing side effects and maintaining robustness to adversarial prompts.\nBroadly, our work underlines the importance of comprehensive evaluation for\nerasure in diffusion models.", "authors": ["Kevin Lu", "Nicky Kriplani", "Rohit Gandikota", "Minh Pham", "David Bau", "Chinmay Hegde", "Niv Cohen"], "published_date": "2025-05-22", "title_zh": "擴散模型中的概念何時被抹除？", "summary_zh": "概念抹除，也就是選擇性地阻止模型生成特定概念的能力，越來越受到關注。雖然出現了許多方法來應對這個挑戰，但這些方法究竟能多徹底地抹除目標概念仍然不明確。本研究提出了兩個關於擴散模型中抹除機制的概念模型：（一）降低生成目標概念的可能性，以及（二）干擾模型內部的引導機制。為了徹底評估一個概念是否真正從模型中抹除，我們引入了一系列獨立的評估方法，包括對抗攻擊、新穎的探測技術，以及對模型在抹除概念後生成替代方案的分析。我們的結果揭示了最小化副作用和保持對抗性提示的魯棒性之間的權衡。總體而言，我們的工作強調了對擴散模型中的抹除進行全面評估的重要性。", "applications": ["**內容審查與合規性：** 應用於防止生成涉及敏感主題、仇恨言論或侵犯版權的內容，例如自動過濾掉生成式 AI 中的不雅圖片或言論。", "**藝術風格遷移與商業品牌保護：** 允許用戶在模仿特定藝術家風格的同時，避免產生與該藝術家關聯的特定圖像或主題，保護品牌形象，防止AI生成不符品牌調性的內容。", "**數據增強與合成數據安全性：** 在生成用於訓練模型的合成數據時，可以抹除敏感資訊，例如人臉特徵、個人身份資料，保護隱私，同時保留數據的整體結構和特性。"], "pitch": "我們正在開發一種更可靠、可控的概念抹除技術，用於擴散模型。現有技術在抹除特定概念時存在副作用和容易被對抗攻擊繞過的問題。我們的研究揭示了抹除機制的本質，並提供了一套全面的評估框架。這使我們能夠構建更安全、更可定制的生成式AI。商業價值體現在多個方面：首先，它可以幫助企業更好地遵守內容合規法規，降低法律風險。其次，可以保護品牌形象，防止AI生成不符品牌調性的內容。最後，在數據增強和合成數據領域，可以安全地生成大量訓練數據，加速AI模型的開發和部署。我們將打造一個API平台，提供高效、穩定的概念抹除服務，目標是成為生成式AI安全領域的領導者。", "audio": "audios/2505.17013v1.mp3", "timestamp": "2025-05-24T15:20:24.507568"}
{"query": "AI", "id": "2505.17021v1", "url": "http://arxiv.org/abs/2505.17021v1", "title": "ARB: A Comprehensive Arabic Multimodal Reasoning Benchmark", "summary": "As Large Multimodal Models (LMMs) become more capable, there is growing\ninterest in evaluating their reasoning processes alongside their final outputs.\nHowever, most benchmarks remain focused on English, overlooking languages with\nrich linguistic and cultural contexts, such as Arabic. To address this gap, we\nintroduce the Comprehensive Arabic Multimodal Reasoning Benchmark (ARB), the\nfirst benchmark designed to evaluate step-by-step reasoning in Arabic across\nboth textual and visual modalities. ARB spans 11 diverse domains, including\nvisual reasoning, document understanding, OCR, scientific analysis, and\ncultural interpretation. It comprises 1,356 multimodal samples paired with\n5,119 human-curated reasoning steps and corresponding actions. We evaluated 12\nstate-of-the-art open- and closed-source LMMs and found persistent challenges\nin coherence, faithfulness, and cultural grounding. ARB offers a structured\nframework for diagnosing multimodal reasoning in underrepresented languages and\nmarks a critical step toward inclusive, transparent, and culturally aware AI\nsystems. We release the benchmark, rubric, and evaluation suit to support\nfuture research and reproducibility. Code available at:\nhttps://github.com/mbzuai-oryx/ARB", "authors": ["Sara Ghaboura", "Ketan More", "Wafa Alghallabi", "Omkar Thawakar", "Jorma Laaksonen", "Hisham Cholakkal", "Salman Khan", "Rao Muhammad Anwer"], "published_date": "2025-05-22", "title_zh": "ARB：一個全面的阿拉伯語多模態推理基準", "summary_zh": "論文介紹了一個新的基準測試工具 ARB，用於評估大型多模態模型（LMMs）在阿拉伯語環境下的推理能力。現有的基準測試主要集中在英語，忽略了阿拉伯語等具有豐富語言和文化背景的語言。ARB 涵蓋了視覺推理、文檔理解、OCR、科學分析和文化詮釋等多個領域，包含1356個多模態樣本和5119個人工整理的推理步驟。研究團隊評估了12個最先進的LMMs，發現它們在連貫性、忠實性和文化基礎方面仍存在挑戰。ARB 提供了一個結構化的框架，用於診斷在代表性不足的語言中的多模態推理，並朝著包容性、透明性和具有文化意識的人工智慧系統邁出了重要一步。該基準、評分標準和評估套件已公開發布。", "applications": ["**智能客服：** 應用於理解阿拉伯語客戶的語音和圖像問題，提供更準確和文化的相關的回應，例如：識別客戶拍攝的水果照片，並用阿拉伯語回答關於水果營養和產地等問題。", "**文化遺產保護：** 幫助解讀阿拉伯語古籍文獻和文物圖片，促進文化遺產的數位化和研究。 例如，分析古蘭經手稿的圖片，並自動提取重要段落，進行翻譯和註釋。", "**教育應用：** 用於開發阿拉伯語的互動式學習工具，提供多模態的教學內容，例如：根據學生提交的阿拉伯語作文圖片，自動評估其語法、文法和風格，並提供個性化建議。"], "pitch": "ARB 作為首個專注於阿拉伯語多模態推理的基準，解決了市場上對於非英語環境下 AI 評估的巨大缺口。 我們將提供一套經過驗證的工具，幫助企業和研究機構開發更智能、更符合文化背景的阿拉伯語 AI 應用。 數據集的稀缺性和專業性構成天然壁壘，有利於佔領市場先機。 潛在的商業模式包括：基準測試服務、AI模型評估和優化諮詢、特定領域的阿拉伯語AI解決方案（如智能客服、教育、文化遺產保護等）。 隨著中東和北非地區AI市場的快速增長，ARB具有巨大的商業價值。", "audio": "audios/2505.17021v1.mp3", "timestamp": "2025-05-24T15:38:43.259354"}
{"query": "Foundation Model", "id": "2505.16982v1", "url": "http://arxiv.org/abs/2505.16982v1", "title": "Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine", "summary": "Large Language Models (LLMs) show promise in biomedicine but lack true causal\nunderstanding, relying instead on correlations. This paper envisions causal LLM\nagents that integrate multimodal data (text, images, genomics, etc.) and\nperform intervention-based reasoning to infer cause-and-effect. Addressing this\nrequires overcoming key challenges: designing safe, controllable agentic\nframeworks; developing rigorous benchmarks for causal evaluation; integrating\nheterogeneous data sources; and synergistically combining LLMs with structured\nknowledge (KGs) and formal causal inference tools. Such agents could unlock\ntransformative opportunities, including accelerating drug discovery through\nautomated hypothesis generation and simulation, enabling personalized medicine\nthrough patient-specific causal models. This research agenda aims to foster\ninterdisciplinary efforts, bridging causal concepts and foundation models to\ndevelop reliable AI partners for biomedical progress.", "authors": ["Adib Bazgir", "Amir Habibdoust Lafmajani", "Yuwen Zhang"], "published_date": "2025-05-22", "title_zh": "超越關聯性：朝向生物醫學領域的因果大型語言模型代理", "summary_zh": "這篇論文探討了如何讓大型語言模型（LLMs）在生物醫學領域具備真正的因果理解能力，而不僅僅是依賴關聯性。它提出了因果LLM代理的概念，這些代理能整合多模態數據（文字、圖片、基因組等），並進行基於干預的推理來推斷因果關係。開發這種代理需要克服一些挑戰，例如設計安全可控的代理框架、建立嚴謹的因果評估基準、整合異質數據源，以及將LLMs與結構化知識（知識圖譜）和正式的因果推斷工具結合。這樣的代理有潛力帶來變革性的機會，例如加速藥物發現和實現個性化醫療。", "applications": ["**個性化藥物反應預測：** 根據病患的基因組、病史、生活習慣等多維度資料，預測他們對不同藥物的反應，避免無效治療或不良反應。", "**新型療法探索：** 通過模擬不同治療方案對疾病進程的影響，找出潛在的藥物靶點或干預策略，加速新藥研發和療法開發。", "**臨床決策輔助：** 協助醫生評估病患病情，提供更精確的診斷建議，並根據因果關係模型，預測不同治療方案的可能結果，輔助決策。"], "pitch": "我們正在開發新一代的AI藥物研發平台，核心是具備因果推理能力的大型語言模型。相較於目前僅能分析關聯性的AI，我們的技術能更準確地預測藥物效果，大幅縮短研發時程，降低失敗風險，並實現精準醫療。 我們預計能顯著提升藥物開發的成功率，為製藥公司帶來數十億美元的潛在收益。 這項技術的商業價值在於大幅降低研發成本，加速藥物上市，並且為患者提供更有效的治療方案。 想像一下，一個AI能精準預測病患對特定藥物的反應，避免無效治療，這將對醫療產業帶來革命性的影響。", "audio": "audios/2505.16982v1.mp3", "timestamp": "2025-05-24T15:38:58.221274"}
{"query": "Diffusion Model", "id": "2505.17013v1", "url": "http://arxiv.org/abs/2505.17013v1", "title": "When Are Concepts Erased From Diffusion Models?", "summary": "Concept erasure, the ability to selectively prevent a model from generating\nspecific concepts, has attracted growing interest, with various approaches\nemerging to address the challenge. However, it remains unclear how thoroughly\nthese methods erase the target concept. We begin by proposing two conceptual\nmodels for the erasure mechanism in diffusion models: (i) reducing the\nlikelihood of generating the target concept, and (ii) interfering with the\nmodel's internal guidance mechanisms. To thoroughly assess whether a concept\nhas been truly erased from the model, we introduce a suite of independent\nevaluations. Our evaluation framework includes adversarial attacks, novel\nprobing techniques, and analysis of the model's alternative generations in\nplace of the erased concept. Our results shed light on the tension between\nminimizing side effects and maintaining robustness to adversarial prompts.\nBroadly, our work underlines the importance of comprehensive evaluation for\nerasure in diffusion models.", "authors": ["Kevin Lu", "Nicky Kriplani", "Rohit Gandikota", "Minh Pham", "David Bau", "Chinmay Hegde", "Niv Cohen"], "published_date": "2025-05-22", "title_zh": "擴散模型中的概念何時會被抹除？", "summary_zh": "概念抹除，也就是選擇性地阻止模型生成特定概念的能力，正受到越來越多的關注。儘管出現了各種方法來應對這一挑戰，但這些方法究竟能多徹底地抹除目標概念仍然不明朗。本研究提出了兩個關於擴散模型中抹除機制的概念模型：（一）降低生成目標概念的可能性，以及（二）干擾模型內部的引導機制。為了全面評估一個概念是否真正從模型中被抹除，我們引入了一套獨立的評估方法，包括對抗性攻擊、新型探測技術，以及分析模型在抹除概念後的替代生成結果。研究結果揭示了最小化副作用和維持對抗性提示的穩健性之間的緊張關係。總體而言，我們的工作強調了在擴散模型中進行概念抹除時，全面評估的重要性。", "applications": ["**內容安全過濾：** 針對仇恨言論、暴力內容等敏感詞彙和圖像，從AI生成內容中徹底抹除，避免生成不當內容。", "**個性化產品設計：** 在設計過程中抹除特定的設計風格或品牌元素，例如，在生成Logo時避免與現有品牌過於相似，確保原創性。", "**醫療影像匿名化：** 在醫療影像數據集中，抹除人臉、紋身等個人識別信息，同時保留醫療診斷所需的關鍵特徵，保護患者隱私。"], "pitch": "各位投資人，想像一下，AI創作內容充斥著版權爭議、仇恨言論和侵犯隱私的風險。我們團隊的研究突破性地解決了這個問題，開發出能精準抹除AI模型中特定概念的技術。這不僅能確保內容安全合規，還能賦予AI更強大的可控性，催生出個性化定製和數據隱私保護的新應用。從內容審核到設計創新，再到醫療數據安全，我們的技術擁有廣闊的市場前景。我們需要您的投資，將這項技術商業化，引領下一代安全、可信賴的AI內容創作，成為AI時代的守門人，佔據市場領導地位。", "audio": "audios/2505.17013v1.mp3", "timestamp": "2025-05-24T15:39:20.981673"}
{"query": "AI", "id": "2505.17021v1", "url": "http://arxiv.org/abs/2505.17021v1", "title": "ARB: A Comprehensive Arabic Multimodal Reasoning Benchmark", "summary": "As Large Multimodal Models (LMMs) become more capable, there is growing\ninterest in evaluating their reasoning processes alongside their final outputs.\nHowever, most benchmarks remain focused on English, overlooking languages with\nrich linguistic and cultural contexts, such as Arabic. To address this gap, we\nintroduce the Comprehensive Arabic Multimodal Reasoning Benchmark (ARB), the\nfirst benchmark designed to evaluate step-by-step reasoning in Arabic across\nboth textual and visual modalities. ARB spans 11 diverse domains, including\nvisual reasoning, document understanding, OCR, scientific analysis, and\ncultural interpretation. It comprises 1,356 multimodal samples paired with\n5,119 human-curated reasoning steps and corresponding actions. We evaluated 12\nstate-of-the-art open- and closed-source LMMs and found persistent challenges\nin coherence, faithfulness, and cultural grounding. ARB offers a structured\nframework for diagnosing multimodal reasoning in underrepresented languages and\nmarks a critical step toward inclusive, transparent, and culturally aware AI\nsystems. We release the benchmark, rubric, and evaluation suit to support\nfuture research and reproducibility. Code available at:\nhttps://github.com/mbzuai-oryx/ARB", "authors": ["Sara Ghaboura", "Ketan More", "Wafa Alghallabi", "Omkar Thawakar", "Jorma Laaksonen", "Hisham Cholakkal", "Salman Khan", "Rao Muhammad Anwer"], "published_date": "2025-05-22", "title_zh": "ARB：一個全面的阿拉伯語多模態推理基準", "summary_zh": "大型多模態模型（LMMs）越來越強大，評估其推理過程變得重要。但現有基準主要針對英語，忽略了阿拉伯語等具有豐富語言和文化背景的語言。為此，我們推出了全面的阿拉伯語多模態推理基準（ARB），這是第一個旨在評估阿拉伯語中跨文本和視覺模態逐步推理的基準。ARB涵蓋11個不同的領域，包括視覺推理、文檔理解、OCR、科學分析和文化詮釋。它包含1,356個多模態樣本，配有5,119個人工整理的推理步驟和相應的操作。我們評估了12個最先進的開源和閉源LMM，發現它們在連貫性、忠實性和文化基礎方面仍存在挑戰。ARB提供了一個結構化的框架，用於診斷代表性不足語言中的多模態推理，標誌著邁向包容性、透明和具有文化意識的AI系統的關鍵一步。我們發布了基準、評分標準和評估套件，以支持未來的研究和可重複性。", "applications": ["**自動阿拉伯語文檔審閱與摘要:** 自動理解並總結阿拉伯語的法律、科學或歷史文檔，並能解釋其背後的邏輯和文化含義，例如審核伊斯蘭教法相關合約。", "**視覺內容的阿拉伯語文化適配:** 能夠理解圖像或影片中的阿拉伯語文化元素，並根據目標受眾進行適當的修改或翻譯，例如將西方廣告翻譯成阿拉伯語時，確保符合當地文化規範。", "**阿拉伯語教材輔助教學:** 分析阿拉伯語教材中的圖片、文字，並提供逐步的解釋和推理，幫助學生更好地理解和掌握知識，例如解釋古蘭經或阿拉伯文學作品中的隱喻和文化背景。"], "pitch": "ARB是第一個針對阿拉伯語多模態推理的綜合性基準，解決了AI模型在理解阿拉伯語文化和上下文方面的巨大缺口。目前市場上缺乏有效的阿拉伯語AI解決方案，而ARB的出現將加速相關技術的發展，為企業提供在阿拉伯語市場開展業務的關鍵工具。投資ARB的相關研究和應用，將有助於開發出更智能、更可靠、更符合文化需求的AI產品，並在快速增長的阿拉伯語市場中獲得巨大的商業價值。我們正處於構建下一個十億用戶AI的風口浪尖，而阿拉伯語AI將是其中的重要一環。", "audio": "audios/2505.17021v1.mp3", "timestamp": "2025-05-24T16:01:24.302023"}
{"query": "Foundation Model", "id": "2505.16982v1", "url": "http://arxiv.org/abs/2505.16982v1", "title": "Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine", "summary": "Large Language Models (LLMs) show promise in biomedicine but lack true causal\nunderstanding, relying instead on correlations. This paper envisions causal LLM\nagents that integrate multimodal data (text, images, genomics, etc.) and\nperform intervention-based reasoning to infer cause-and-effect. Addressing this\nrequires overcoming key challenges: designing safe, controllable agentic\nframeworks; developing rigorous benchmarks for causal evaluation; integrating\nheterogeneous data sources; and synergistically combining LLMs with structured\nknowledge (KGs) and formal causal inference tools. Such agents could unlock\ntransformative opportunities, including accelerating drug discovery through\nautomated hypothesis generation and simulation, enabling personalized medicine\nthrough patient-specific causal models. This research agenda aims to foster\ninterdisciplinary efforts, bridging causal concepts and foundation models to\ndevelop reliable AI partners for biomedical progress.", "authors": ["Adib Bazgir", "Amir Habibdoust Lafmajani", "Yuwen Zhang"], "published_date": "2025-05-22", "title_zh": "超越相關性：邁向生物醫學領域的因果大型語言模型代理", "summary_zh": "目前的大型語言模型雖然在生物醫學領域展現潛力，但缺乏真正的因果理解，主要依賴相關性。本文提出一個願景，設想具備因果理解能力的大型語言模型代理，它們能整合多模態數據（文本、圖像、基因組等），並執行基於干預的推理，以推斷因果關係。要實現這一目標，需要克服關鍵挑戰：設計安全、可控的代理框架；開發嚴格的因果評估基準；整合異質數據源；以及將大型語言模型與結構化知識（知識圖譜）和形式化的因果推理工具協同結合。這樣的代理可以釋放變革性的機會，包括通過自動化的假設生成和模擬加速藥物發現，並通過針對患者的因果模型實現個性化醫療。此研究議程旨在促進跨學科合作，將因果概念和基礎模型結合起來，從而開發出可靠的AI合作夥伴，推動生物醫學的進步。", "applications": ["**加速藥物發現：** AI 代理可以自動生成藥物開發的假設，並模擬藥物在不同生理環境下的作用，大幅縮短研發週期。", "**個性化醫療方案：** 根據患者的基因、生活習慣、病史等多維數據，建立個性化的因果模型，預測治療效果，制定更精準的醫療方案。", "**公共衛生政策模擬：** 在疫情爆發等緊急情況下，模擬不同干預措施（例如封鎖、疫苗接種）對疫情傳播的影響，為決策者提供科學依據。"], "pitch": "我們正在構建生物醫學領域的下一代AI引擎：因果大型語言模型代理。與傳統依賴相關性的AI不同，我們的技術能真正理解因果關係，從而做出更準確的預測和更合理的決策。想像一下，一種AI能夠自動發現新藥靶點，為每位患者量身定制治療方案，並在公共衛生危機中提供最佳干預策略。這不僅能大幅降低藥物研發成本，還能顯著提高治療效果，並為公共衛生安全提供強有力的保障。我們擁有一支跨學科的頂尖團隊，正在攻克關鍵技術挑戰，並已取得初步成果。 我們相信，這項技術具有巨大的市場潛力，將徹底改變生物醫學領域，為投資者帶來豐厚的回報。現在加入我們，一起開創生物醫學AI的未來！", "audio": "audios/2505.16982v1.mp3", "timestamp": "2025-05-24T16:01:42.349579"}
{"query": "Diffusion Model", "id": "2505.17013v1", "url": "http://arxiv.org/abs/2505.17013v1", "title": "When Are Concepts Erased From Diffusion Models?", "summary": "Concept erasure, the ability to selectively prevent a model from generating\nspecific concepts, has attracted growing interest, with various approaches\nemerging to address the challenge. However, it remains unclear how thoroughly\nthese methods erase the target concept. We begin by proposing two conceptual\nmodels for the erasure mechanism in diffusion models: (i) reducing the\nlikelihood of generating the target concept, and (ii) interfering with the\nmodel's internal guidance mechanisms. To thoroughly assess whether a concept\nhas been truly erased from the model, we introduce a suite of independent\nevaluations. Our evaluation framework includes adversarial attacks, novel\nprobing techniques, and analysis of the model's alternative generations in\nplace of the erased concept. Our results shed light on the tension between\nminimizing side effects and maintaining robustness to adversarial prompts.\nBroadly, our work underlines the importance of comprehensive evaluation for\nerasure in diffusion models.", "authors": ["Kevin Lu", "Nicky Kriplani", "Rohit Gandikota", "Minh Pham", "David Bau", "Chinmay Hegde", "Niv Cohen"], "published_date": "2025-05-22", "title_zh": "擴散模型中的概念何時被抹除？", "summary_zh": "概念抹除，也就是選擇性地阻止模型生成特定概念的能力，引起了越來越多的關注，並且出現了各種解決這個挑戰的方法。然而，這些方法抹除目標概念的徹底程度仍然不清楚。我們首先提出了兩個擴散模型中抹除機制的概念模型：(i) 降低生成目標概念的可能性，以及 (ii) 干擾模型內部的引導機制。為了徹底評估一個概念是否真正從模型中抹除，我們引入了一套獨立的評估方法。我們的評估框架包括對抗性攻擊、新穎的探測技術，以及對模型在抹除概念後產生的替代生成的分析。我們的結果揭示了最小化副作用和保持對對抗性提示的魯棒性之間的張力。總體而言，我們的研究強調了對擴散模型中的抹除進行全面評估的重要性。", "applications": ["**兒童安全內容生成：** 可以移除模型生成兒童不宜的內容，例如暴力、性暗示等，確保產出的影像適合兒童觀看，打造更安全的內容生態。", "**保護智慧財產權：** 移除模型生成涉及特定品牌或角色的內容，避免侵權行為，協助企業保護自己的智慧財產權。", "**醫療影像處理：** 在醫療影像中移除可能洩漏病人隱私的個人資訊或敏感區域，例如臉部、紋身等，在保護病人隱私的同時，也能進行影像分析和模型訓練。"], "pitch": "我們正在解決AI安全和道德的根本問題：如何可靠地從AI模型中移除特定概念，防止其產生有害或不當內容。現有的「概念抹除」方法效果不一，我們提供了更嚴格的評估標準和技術，確保抹除的徹底性。這項技術的商業價值在於：1. **合規性與信任：** 協助企業符合日益嚴格的AI監管要求，建立使用者對AI內容的信任。2. **降低風險：** 減少AI模型產生有害內容造成的法律責任和聲譽損害。3. **差異化競爭：** 為內容生成平台提供更安全、更可靠的服務，在市場上脫穎而出。我們正在尋求投資，以擴展我們的評估框架，開發更有效的抹除技術，並將其商業化，成為AI安全領域的領導者。", "audio": "audios/2505.17013v1.mp3", "timestamp": "2025-05-24T16:01:59.997130"}
{"query": "AI", "id": "2505.17021v1", "url": "http://arxiv.org/abs/2505.17021v1", "title": "ARB: A Comprehensive Arabic Multimodal Reasoning Benchmark", "summary": "As Large Multimodal Models (LMMs) become more capable, there is growing\ninterest in evaluating their reasoning processes alongside their final outputs.\nHowever, most benchmarks remain focused on English, overlooking languages with\nrich linguistic and cultural contexts, such as Arabic. To address this gap, we\nintroduce the Comprehensive Arabic Multimodal Reasoning Benchmark (ARB), the\nfirst benchmark designed to evaluate step-by-step reasoning in Arabic across\nboth textual and visual modalities. ARB spans 11 diverse domains, including\nvisual reasoning, document understanding, OCR, scientific analysis, and\ncultural interpretation. It comprises 1,356 multimodal samples paired with\n5,119 human-curated reasoning steps and corresponding actions. We evaluated 12\nstate-of-the-art open- and closed-source LMMs and found persistent challenges\nin coherence, faithfulness, and cultural grounding. ARB offers a structured\nframework for diagnosing multimodal reasoning in underrepresented languages and\nmarks a critical step toward inclusive, transparent, and culturally aware AI\nsystems. We release the benchmark, rubric, and evaluation suit to support\nfuture research and reproducibility. Code available at:\nhttps://github.com/mbzuai-oryx/ARB", "authors": ["Sara Ghaboura", "Ketan More", "Wafa Alghallabi", "Omkar Thawakar", "Jorma Laaksonen", "Hisham Cholakkal", "Salman Khan", "Rao Muhammad Anwer"], "published_date": "2025-05-22", "title_zh": "ARB：一個全面的阿拉伯語多模態推理基準測試", "summary_zh": "本論文提出了一個名為 ARB 的全新基準測試，專門用於評估大型多模態模型 (LMM) 在阿拉伯語環境下的推理能力。現有基準測試大多集中在英語，忽略了像阿拉伯語這樣具有豐富語言和文化背景的語言。ARB 涵蓋視覺推理、文檔理解、光學字元識別、科學分析和文化詮釋等11個不同領域，包含1356個多模態樣本和5119個人工策劃的推理步驟。研究團隊評估了12個最先進的 LMM，發現它們在連貫性、忠實性和文化基礎方面仍存在挑戰。ARB 提供了一個結構化的框架，用於診斷在代表性不足的語言中的多模態推理能力，並標誌著邁向包容性、透明性和具有文化意識的 AI 系統的關鍵一步。研究團隊公開了基準測試、評分標準和評估工具，以支持未來的研究和可再現性。", "applications": ["**智能阿拉伯語教學系統：** 利用 ARB 訓練的 LMM 可以根據學生提供的文本和圖像，提供個性化的學習反饋，並解釋其中涉及的阿拉伯文化元素，從而提升學習效率和趣味性。", "**多語種文檔處理與翻譯：** 應用於大規模阿拉伯語文檔的自動翻譯，不僅能準確翻譯文字，還能理解圖像和圖表，並根據阿拉伯文化背景進行調整，確保翻譯結果更符合當地習慣。", "**智慧城市中的文化遺產保護：** 通過分析歷史圖片、文檔和地理數據，LMM 可以自動識別和保護阿拉伯世界的文化遺址，並生成互動式展覽內容，讓遊客更深入地了解當地歷史和文化。"], "pitch": "ARB 填補了大型多模態模型在阿拉伯語領域的評估空白，解決了目前 AI 模型缺乏文化理解和推理能力的問題。這個基準測試對於開發能夠有效處理阿拉伯語環境下文本和圖像的 AI 系統至關重要。基於 ARB 的技術具有廣泛的商業價值，例如提升阿拉伯語客戶服務的質量、加速阿拉伯語文檔的數位化進程，以及促進阿拉伯文化遺產的保護和傳播。通過投資基於 ARB 基準開發的 AI 解決方案，我們可以搶佔阿拉伯語多模態 AI 市場的先機，並在快速增長的阿拉伯數字經濟中獲得可觀的回報。", "audio": "audios/2505.17021v1.mp3", "timestamp": "2025-05-24T16:05:27.675474"}
{"query": "Foundation Model", "id": "2505.16982v1", "url": "http://arxiv.org/abs/2505.16982v1", "title": "Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine", "summary": "Large Language Models (LLMs) show promise in biomedicine but lack true causal\nunderstanding, relying instead on correlations. This paper envisions causal LLM\nagents that integrate multimodal data (text, images, genomics, etc.) and\nperform intervention-based reasoning to infer cause-and-effect. Addressing this\nrequires overcoming key challenges: designing safe, controllable agentic\nframeworks; developing rigorous benchmarks for causal evaluation; integrating\nheterogeneous data sources; and synergistically combining LLMs with structured\nknowledge (KGs) and formal causal inference tools. Such agents could unlock\ntransformative opportunities, including accelerating drug discovery through\nautomated hypothesis generation and simulation, enabling personalized medicine\nthrough patient-specific causal models. This research agenda aims to foster\ninterdisciplinary efforts, bridging causal concepts and foundation models to\ndevelop reliable AI partners for biomedical progress.", "authors": ["Adib Bazgir", "Amir Habibdoust Lafmajani", "Yuwen Zhang"], "published_date": "2025-05-22", "title_zh": "超越相關性：邁向生物醫學領域的因果大型語言模型代理", "summary_zh": "這篇論文探討如何讓大型語言模型（LLMs）在生物醫學領域具備真正的因果理解能力，而不僅僅是仰賴相關性。作者設想一種因果LLM代理，它能整合多模態數據（文本、圖像、基因組數據等），並通過基於干預的推理來推斷因果關係。要實現這一目標，需要克服安全可控的代理框架設計、嚴格的因果評估基準開發、異構數據源整合，以及將LLMs與結構化知識（知識圖譜）和形式因果推理工具協同結合等挑戰。這種代理有望加速藥物發現、實現個性化醫療，並成為生物醫學研究中可靠的AI夥伴。", "applications": ["**藥物開發加速器：** 因果LLM代理可以自動生成和模擬藥物作用假設，大幅縮短藥物研發周期。", "**精準醫療專家：** 根據患者的個性化數據，構建個體化的因果模型，幫助醫生制定更有效的治療方案。", "**疾病風險預測工具：** 整合基因組、生活習慣等多維度數據，預測個體患病風險，提前進行預防干預。"], "pitch": "我們正在構建下一代生物醫學AI，它不僅僅是信息彙總工具，而是具備因果推理能力的智能代理。這將徹底改變藥物研發、個性化醫療和疾病預防等領域。試想一下，一個能夠自動發現新藥靶點、為每位患者量身定制治療方案的AI助手。我們需要資金來克服技術挑戰，建立可靠、安全的因果LLM代理，並将其商業化。回報將是巨大的：更快的藥物研發速度、更精準的醫療方案和更健康的社會。這不僅僅是一個投資，更是一次對未來醫療的佈局。", "audio": "audios/2505.16982v1.mp3", "timestamp": "2025-05-24T16:05:44.781587"}
{"query": "Diffusion Model", "id": "2505.17013v1", "url": "http://arxiv.org/abs/2505.17013v1", "title": "When Are Concepts Erased From Diffusion Models?", "summary": "Concept erasure, the ability to selectively prevent a model from generating\nspecific concepts, has attracted growing interest, with various approaches\nemerging to address the challenge. However, it remains unclear how thoroughly\nthese methods erase the target concept. We begin by proposing two conceptual\nmodels for the erasure mechanism in diffusion models: (i) reducing the\nlikelihood of generating the target concept, and (ii) interfering with the\nmodel's internal guidance mechanisms. To thoroughly assess whether a concept\nhas been truly erased from the model, we introduce a suite of independent\nevaluations. Our evaluation framework includes adversarial attacks, novel\nprobing techniques, and analysis of the model's alternative generations in\nplace of the erased concept. Our results shed light on the tension between\nminimizing side effects and maintaining robustness to adversarial prompts.\nBroadly, our work underlines the importance of comprehensive evaluation for\nerasure in diffusion models.", "authors": ["Kevin Lu", "Nicky Kriplani", "Rohit Gandikota", "Minh Pham", "David Bau", "Chinmay Hegde", "Niv Cohen"], "published_date": "2025-05-22", "title_zh": "擴散模型中的概念何時被抹除？", "summary_zh": "概念抹除，也就是選擇性地阻止模型生成特定概念的能力，正引起越來越多的關注，並出現了各種方法來應對這一挑戰。然而，這些方法究竟能多徹底地抹除目標概念，仍然不明朗。 本文首先針對擴散模型中的抹除機制提出了兩個概念模型：（i）降低生成目標概念的可能性，以及（ii）干擾模型的內部引導機制。 為了徹底評估一個概念是否真正從模型中被抹除，我們引入了一套獨立的評估方法。 我們的評估框架包括對抗性攻擊、新穎的探測技術，以及對模型在抹除概念後產生的替代性生成的分析。 我們的研究結果揭示了最小化副作用和保持對抗性提示的穩健性之間的權衡。 總體而言，我們的研究強調了對擴散模型中抹除效果進行全面評估的重要性。", "applications": ["**內容審查與保護：** 防止生成涉及暴力、仇恨言論或不適當內容的圖像，例如自動過濾掉兒童不宜的圖像或避免生成特定政治人物的虛假圖片。", "**個人化體驗優化：** 根據用戶偏好，動態過濾掉用戶不喜歡或觸發負面情緒的概念，例如在遊戲中移除特定敵人類型，或在廣告中排除特定品牌。", "**設計與創作控制：** 在設計過程中，移除設計師不希望出現的元素或風格，例如在建築設計中避免特定建築風格，或在產品設計中排除特定材料。"], "pitch": "我們解決的是AI生成內容領域的核心信任問題：如何有效控制模型的生成內容，避免產生有害、不適當或不符合需求的結果。我們的研究揭示了現有概念抹除方法的局限性，並提供了更全面的評估框架。這使得我們能夠開發更有效、更安全的AI生成工具，應用於內容審查、個人化體驗和創意設計等領域。想像一下，一個能夠完全受控的AI內容生成引擎，它能夠在滿足用戶需求的同時，有效避免產生有害內容，這將極大地提升AI在各行業的應用價值，並為我們帶來巨大的商業機會。", "audio": "audios/2505.17013v1.mp3", "timestamp": "2025-05-24T16:06:09.636884"}
{"query": "AI", "id": "2505.17021v1", "url": "http://arxiv.org/abs/2505.17021v1", "title": "ARB: A Comprehensive Arabic Multimodal Reasoning Benchmark", "summary": "As Large Multimodal Models (LMMs) become more capable, there is growing\ninterest in evaluating their reasoning processes alongside their final outputs.\nHowever, most benchmarks remain focused on English, overlooking languages with\nrich linguistic and cultural contexts, such as Arabic. To address this gap, we\nintroduce the Comprehensive Arabic Multimodal Reasoning Benchmark (ARB), the\nfirst benchmark designed to evaluate step-by-step reasoning in Arabic across\nboth textual and visual modalities. ARB spans 11 diverse domains, including\nvisual reasoning, document understanding, OCR, scientific analysis, and\ncultural interpretation. It comprises 1,356 multimodal samples paired with\n5,119 human-curated reasoning steps and corresponding actions. We evaluated 12\nstate-of-the-art open- and closed-source LMMs and found persistent challenges\nin coherence, faithfulness, and cultural grounding. ARB offers a structured\nframework for diagnosing multimodal reasoning in underrepresented languages and\nmarks a critical step toward inclusive, transparent, and culturally aware AI\nsystems. We release the benchmark, rubric, and evaluation suit to support\nfuture research and reproducibility. Code available at:\nhttps://github.com/mbzuai-oryx/ARB", "authors": ["Sara Ghaboura", "Ketan More", "Wafa Alghallabi", "Omkar Thawakar", "Jorma Laaksonen", "Hisham Cholakkal", "Salman Khan", "Rao Muhammad Anwer"], "published_date": "2025-05-22", "title_zh": "ARB：一個全面的阿拉伯語多模態推理基準測試", "summary_zh": "大型多模態模型(LMM)越來越強大，評估其推理過程也變得重要。然而，大多數基準測試都集中在英語上，忽略了像阿拉伯語這樣具有豐富語言和文化背景的語言。因此，我們推出了一個全面的阿拉伯語多模態推理基準測試(ARB)，旨在評估阿拉伯語中跨文本和視覺模態的逐步推理。ARB涵蓋11個不同的領域，包括視覺推理、文檔理解、OCR、科學分析和文化解釋。它包含1,356個多模態樣本，並配有5,119個人工策劃的推理步驟和相應的動作。我們評估了12個最先進的開源和閉源LMM，發現它們在連貫性、忠實性和文化基礎方面仍然存在挑戰。ARB為診斷代表性不足的語言中的多模態推理提供了一個結構化的框架，並標誌著邁向包容、透明和具有文化意識的AI系統的關鍵一步。我們發布了基準、評分標準和評估套件，以支持未來的研究和可重複性。程式碼可在以下網址取得：https://github.com/mbzuai-oryx/ARB", "applications": ["**自動化阿拉伯語文獻分析：** 用於分析大量的阿拉伯語歷史文獻或科學論文，自動提取關鍵信息、識別論點，並理解其中的文化背景，例如分析古代手稿或現代阿拉伯科學文獻。", "**智能客服與教育：** 在阿拉伯語地區提供更準確、更貼近文化的智能客服，解決客戶問題，或者作為阿拉伯語學習工具，幫助學生理解複雜的文本和視覺信息，並解答與阿拉伯文化相關的問題。", "**內容審核與本地化：** 自動審核阿拉伯語社交媒體內容，識別仇恨言論、錯誤信息或不當內容，並確保內容符合當地文化和法律規範。此外，還可以提升英文內容翻譯成阿拉伯語的品質，確保翻譯結果在文化上準確且易於理解。"], "pitch": "ARB基準測試解決了目前AI模型在阿拉伯語多模態推理能力上的空白，擁有巨大的商業潛力。首先，它為開發更智能、更貼近阿拉伯文化的AI應用奠定了基礎，有助於打開中東和北非市場。其次，ARB可以加速各行各業的AI落地，例如文獻分析、客戶服務、教育和內容審核等，提升效率和降低成本。最後，ARB的數據集和評估工具本身就具有商業價值，可以授權給企業和研究機構使用，加速AI模型的開發和優化。我們相信，ARB將成為阿拉伯語AI領域的黃金標準，並為投資者帶來豐厚的回報。", "audio": "audios/2505.17021v1.mp3", "timestamp": "2025-05-24T16:15:03.736340"}
{"query": "Foundation Model", "id": "2505.16982v1", "url": "http://arxiv.org/abs/2505.16982v1", "title": "Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine", "summary": "Large Language Models (LLMs) show promise in biomedicine but lack true causal\nunderstanding, relying instead on correlations. This paper envisions causal LLM\nagents that integrate multimodal data (text, images, genomics, etc.) and\nperform intervention-based reasoning to infer cause-and-effect. Addressing this\nrequires overcoming key challenges: designing safe, controllable agentic\nframeworks; developing rigorous benchmarks for causal evaluation; integrating\nheterogeneous data sources; and synergistically combining LLMs with structured\nknowledge (KGs) and formal causal inference tools. Such agents could unlock\ntransformative opportunities, including accelerating drug discovery through\nautomated hypothesis generation and simulation, enabling personalized medicine\nthrough patient-specific causal models. This research agenda aims to foster\ninterdisciplinary efforts, bridging causal concepts and foundation models to\ndevelop reliable AI partners for biomedical progress.", "authors": ["Adib Bazgir", "Amir Habibdoust Lafmajani", "Yuwen Zhang"], "published_date": "2025-05-22", "title_zh": "超越關聯性：邁向生物醫學領域的因果大型語言模型代理", "summary_zh": "這篇論文探討如何讓大型語言模型（LLMs）不只學習關聯性，而是真正理解生物醫學領域的因果關係。透過整合多模態數據（文字、圖片、基因組等），並進行干預式推理，研究人員希望打造能夠推斷因果關係的因果LLM代理。實現這個目標需要克服安全控制、嚴格評估、數據整合和知識融合等多項挑戰。這種因果LLM代理潛力巨大，例如能自動生成假設、模擬實驗，加速藥物發現，以及建立個人化的因果模型，實現精準醫療。", "applications": ["**輔助醫生診斷與治療：** LLM代理可以分析病人的病歷、影像報告和基因數據，找出疾病的根本原因，並推薦更有效的治療方案。", "**加速新藥開發：** LLM代理可以模擬藥物與人體作用，預測藥物效果和副作用，減少臨床試驗的成本和時間。", "**個人化健康管理：** LLM代理可以根據個人的生活習慣、基因數據和健康指標，提供客製化的健康建議和預防措施。"], "pitch": "想像一下，我們能打造一個像頂尖醫學專家一樣思考的人工智慧，它不只是記錄數據，更能理解疾病的真正原因，並針對每個病患提出最有效的治療方案。這就是我們正在開發的因果大型語言模型代理。這項技術的核心價值在於，它能加速新藥開發，降低臨床試驗成本，並推動個人化醫療的發展。我們將整合多模態數據、應用因果推理技術，並建立嚴謹的評估標準，確保AI的建議既安全又可靠。藥廠可以利用它來加速藥物研發流程；醫院可以用它來提升診斷準確性和治療效果；保險公司則可以利用它來提供更精準的風險評估和健康管理服務。我們相信，這項技術將徹底改變生物醫學領域，帶來巨大的經濟效益和社會價值，成為醫療領域的下一個重大突破。", "audio": "audios/2505.16982v1.mp3", "timestamp": "2025-05-24T16:15:22.280146"}
{"query": "Diffusion Model", "id": "2505.17013v1", "url": "http://arxiv.org/abs/2505.17013v1", "title": "When Are Concepts Erased From Diffusion Models?", "summary": "Concept erasure, the ability to selectively prevent a model from generating\nspecific concepts, has attracted growing interest, with various approaches\nemerging to address the challenge. However, it remains unclear how thoroughly\nthese methods erase the target concept. We begin by proposing two conceptual\nmodels for the erasure mechanism in diffusion models: (i) reducing the\nlikelihood of generating the target concept, and (ii) interfering with the\nmodel's internal guidance mechanisms. To thoroughly assess whether a concept\nhas been truly erased from the model, we introduce a suite of independent\nevaluations. Our evaluation framework includes adversarial attacks, novel\nprobing techniques, and analysis of the model's alternative generations in\nplace of the erased concept. Our results shed light on the tension between\nminimizing side effects and maintaining robustness to adversarial prompts.\nBroadly, our work underlines the importance of comprehensive evaluation for\nerasure in diffusion models.", "authors": ["Kevin Lu", "Nicky Kriplani", "Rohit Gandikota", "Minh Pham", "David Bau", "Chinmay Hegde", "Niv Cohen"], "published_date": "2025-05-22", "title_zh": "概念何時從擴散模型中被抹除？", "summary_zh": "概念抹除，即選擇性地阻止模型生成特定概念的能力，日益受到關注，也出現了各種方法來應對這個挑戰。然而，這些方法究竟能多徹底地抹除目標概念仍然不明朗。我們首先提出了兩個關於擴散模型中抹除機制的概念模型：（i）降低生成目標概念的可能性，以及（ii）干擾模型的內部引導機制。為了徹底評估一個概念是否真的從模型中被抹除，我們引入了一套獨立的評估方法。我們的評估框架包括對抗性攻擊、新型探測技術，以及對模型在抹除概念後產生的替代生成內容的分析。我們的結果揭示了最小化副作用和保持對抗性提示的穩健性之間的緊張關係。總的來說，我們的工作強調了對擴散模型中抹除操作進行全面評估的重要性。", "applications": ["**內容審查和安全:**  在AI圖像生成中，避免產生包含暴力、歧視或其他有害內容的圖像。例如，可以防止生成槍枝、炸彈或種族歧視相關圖像。", "**藝術風格保護:**  保護特定藝術家的風格不被濫用或抄襲。例如，可以防止他人使用AI生成類似梵谷或莫內的風格的圖像，用於商業用途。", "**個人隱私保護:**  在生成人臉圖像時，抹除特定人物的身份信息，避免洩漏個人隱私。例如，在AI生成人像照片的應用中，避免生成與真實人物太過相似的圖像。"], "pitch": "概念抹除技術解決了AI生成模型中內容安全和知識產權保護的關鍵問題。透過精準控制AI模型的生成能力，我們可以打造更安全、更可信賴的AI應用生態。我們的研究提供了一套嚴謹的評估框架，確保概念抹除的有效性和安全性。這為開發具有高度可控性和道德性的AI產品奠定了基礎。想像一下，一個能夠在保護用戶免受有害內容侵害的同時，尊重藝術家權益並維護個人隱私的AI平台，這將帶來巨大的商業價值，並在內容創作、數據安全和品牌保護等領域開闢新的市場機會。我們的技術有潛力成為下一代AI應用的核心安全組件，吸引對AI安全性和責任性有高度要求的企業和政府機構。", "audio": "audios/2505.17013v1.mp3", "timestamp": "2025-05-24T16:15:48.266543"}
{"query": "AI", "id": "2505.17021v1", "url": "http://arxiv.org/abs/2505.17021v1", "title": "ARB: A Comprehensive Arabic Multimodal Reasoning Benchmark", "summary": "As Large Multimodal Models (LMMs) become more capable, there is growing\ninterest in evaluating their reasoning processes alongside their final outputs.\nHowever, most benchmarks remain focused on English, overlooking languages with\nrich linguistic and cultural contexts, such as Arabic. To address this gap, we\nintroduce the Comprehensive Arabic Multimodal Reasoning Benchmark (ARB), the\nfirst benchmark designed to evaluate step-by-step reasoning in Arabic across\nboth textual and visual modalities. ARB spans 11 diverse domains, including\nvisual reasoning, document understanding, OCR, scientific analysis, and\ncultural interpretation. It comprises 1,356 multimodal samples paired with\n5,119 human-curated reasoning steps and corresponding actions. We evaluated 12\nstate-of-the-art open- and closed-source LMMs and found persistent challenges\nin coherence, faithfulness, and cultural grounding. ARB offers a structured\nframework for diagnosing multimodal reasoning in underrepresented languages and\nmarks a critical step toward inclusive, transparent, and culturally aware AI\nsystems. We release the benchmark, rubric, and evaluation suit to support\nfuture research and reproducibility. Code available at:\nhttps://github.com/mbzuai-oryx/ARB", "authors": ["Sara Ghaboura", "Ketan More", "Wafa Alghallabi", "Omkar Thawakar", "Jorma Laaksonen", "Hisham Cholakkal", "Salman Khan", "Rao Muhammad Anwer"], "published_date": "2025-05-22", "title_zh": "ARB：一個全面的阿拉伯語多模態推理基準", "summary_zh": "大型多模態模型越來越強大，但現有基準主要集中在英語，忽略了阿拉伯語等具有豐富語言和文化背景的語言。為此，我們推出了「ARB：全面的阿拉伯語多模態推理基準」，這是首個評估阿拉伯語文本和視覺模態逐步推理的基準。 ARB涵蓋11個領域，包括視覺推理、文檔理解、OCR、科學分析和文化詮釋。我們評估了12個領先的LMM模型，發現它們在連貫性、忠實度和文化基礎方面仍然存在挑戰。 ARB提供了一個結構化的框架，用於診斷代表性不足語言的多模態推理，並標誌著邁向包容、透明和具有文化意識的AI系統的關鍵一步。", "applications": ["**智能教育：** 根據阿拉伯文化背景，自動批改阿拉伯語教材中的多模態作業，例如分析學生對歷史圖片或詩歌的理解，並提供個性化的反饋和學習建議。", "**文化遺產保護：** 幫助研究人員分析古代阿拉伯文稿和文物圖片，自動識別關鍵信息、推斷歷史事件，並對文物進行分類和記錄，加速文化遺產的數位化保存。", "**金融風險評估：** 分析阿拉伯語新聞、社交媒體文章和財務報告，結合圖像信息（如建築外觀、產品圖片）來評估企業的信譽和潛在風險，從而輔助投資決策。"], "pitch": "ARB基準的推出，解決了AI領域長期以來忽視阿拉伯語及阿拉伯文化背景的問題，填補了多模態推理能力評估的空白。這不僅能幫助提升現有LMM在阿拉伯語環境下的表現，更為開發面向阿拉伯世界的AI應用奠定了基礎。我們相信，基於ARB基準訓練出的AI模型，將在教育、文化遺產保護、金融等領域產生巨大的商業價值，並有望推動阿拉伯地區的數位化轉型。未來，我們可以通過提供基於ARB基準的AI服務、開發垂直領域的解決方案，例如針對阿拉伯語教育的AI輔導工具、針對阿拉伯文化遺產的AI分析平台，實現可持續的商業模式，並且搶佔中東及北非地區的AI市場先機。", "audio": "audios/2505.17021v1.mp3", "timestamp": "2025-05-24T18:16:44.387956"}
{"query": "Foundation Model", "id": "2505.16982v1", "url": "http://arxiv.org/abs/2505.16982v1", "title": "Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine", "summary": "Large Language Models (LLMs) show promise in biomedicine but lack true causal\nunderstanding, relying instead on correlations. This paper envisions causal LLM\nagents that integrate multimodal data (text, images, genomics, etc.) and\nperform intervention-based reasoning to infer cause-and-effect. Addressing this\nrequires overcoming key challenges: designing safe, controllable agentic\nframeworks; developing rigorous benchmarks for causal evaluation; integrating\nheterogeneous data sources; and synergistically combining LLMs with structured\nknowledge (KGs) and formal causal inference tools. Such agents could unlock\ntransformative opportunities, including accelerating drug discovery through\nautomated hypothesis generation and simulation, enabling personalized medicine\nthrough patient-specific causal models. This research agenda aims to foster\ninterdisciplinary efforts, bridging causal concepts and foundation models to\ndevelop reliable AI partners for biomedical progress.", "authors": ["Adib Bazgir", "Amir Habibdoust Lafmajani", "Yuwen Zhang"], "published_date": "2025-05-22", "title_zh": "超越關聯性：邁向生物醫學領域的因果大型語言模型代理", "summary_zh": "目前的大型語言模型在生物醫學領域展現潛力，但缺乏真正的因果理解，僅依賴相關性。本研究旨在打造因果大型語言模型代理，整合多模態資料（文本、圖像、基因組等），並執行基於干預的推理來推斷因果關係。這需要克服設計安全可控的代理框架、開發嚴謹的因果評估基準、整合異構資料來源，以及將大型語言模型與結構化知識（知識圖譜）和正式因果推理工具結合等挑戰。這些代理有望通過自動化假設生成和模擬加速藥物發現，並通過患者特定的因果模型實現個性化醫療。本研究旨在促進跨學科合作，橋接因果概念和基礎模型，為生物醫學進展開發可靠的人工智慧夥伴。", "applications": ["**加速新藥開發：** 模擬不同藥物組合對特定疾病的影響，預測藥物療效，並找出潛在副作用，從而縮短研發時程，降低成本。", "**個性化精準醫療：** 根據病患的基因、生活習慣等個人資料，建立個人化的因果模型，預測疾病風險，並提供量身定制的治療方案，提高治療效果。", "**輔助臨床決策：** 分析病患的醫療數據，找出關鍵致病因素，協助醫生進行更準確的診斷，並制定最佳治療策略。"], "pitch": "想像一下，一個能夠理解因果關係的人工智慧，它不再只是分析數據，而是能夠主動找出疾病的真正原因，預測治療效果，甚至幫助科學家們發現全新的藥物。我們正在開發的因果大型語言模型代理，正是這樣一個突破性的技術。它整合了多模態的生物醫學數據，運用先進的因果推理方法，為藥物開發、個性化醫療和臨床決策帶來革命性的改變。這個技術的商業價值巨大，可以加速藥物上市，降低醫療成本，提高治療效果，最終改善人類健康。我們正在尋找有遠見的投資者，一同打造這個未來醫療的核心引擎。", "audio": "audios/2505.16982v1.mp3", "timestamp": "2025-05-24T18:17:00.816672"}
{"query": "Diffusion Model", "id": "2505.17013v1", "url": "http://arxiv.org/abs/2505.17013v1", "title": "When Are Concepts Erased From Diffusion Models?", "summary": "Concept erasure, the ability to selectively prevent a model from generating\nspecific concepts, has attracted growing interest, with various approaches\nemerging to address the challenge. However, it remains unclear how thoroughly\nthese methods erase the target concept. We begin by proposing two conceptual\nmodels for the erasure mechanism in diffusion models: (i) reducing the\nlikelihood of generating the target concept, and (ii) interfering with the\nmodel's internal guidance mechanisms. To thoroughly assess whether a concept\nhas been truly erased from the model, we introduce a suite of independent\nevaluations. Our evaluation framework includes adversarial attacks, novel\nprobing techniques, and analysis of the model's alternative generations in\nplace of the erased concept. Our results shed light on the tension between\nminimizing side effects and maintaining robustness to adversarial prompts.\nBroadly, our work underlines the importance of comprehensive evaluation for\nerasure in diffusion models.", "authors": ["Kevin Lu", "Nicky Kriplani", "Rohit Gandikota", "Minh Pham", "David Bau", "Chinmay Hegde", "Niv Cohen"], "published_date": "2025-05-22", "title_zh": "擴散模型何時會抹除概念？", "summary_zh": "概念抹除，也就是選擇性地阻止模型生成特定概念的能力，正受到越來越多的關注。雖然已經出現了多種方法來解決這個問題，但這些方法究竟能多徹底地抹除目標概念仍然不明確。本研究提出了兩個關於擴散模型中抹除機制的概念模型：（一）降低生成目標概念的可能性，以及（二）干擾模型的內部指導機制。為了徹底評估一個概念是否真的從模型中被抹除，我們引入了一套獨立的評估方法。我們的評估框架包括對抗性攻擊、新型探測技術，以及對模型替代生成（代替被抹除概念）的分析。研究結果揭示了最小化副作用和保持對抗性提示的穩健性之間的張力。總體而言，我們的工作強調了對擴散模型中抹除進行全面評估的重要性。", "applications": ["**內容過濾與審查：** 自動從圖像或文本生成中移除不適當或敏感的內容，例如仇恨言論、暴力內容或個人資訊，以符合平台的內容規範。", "**藝術風格轉移控制：** 在進行藝術風格轉移時，可以選擇性地抹除某些風格元素，例如，避免將敏感政治人物的特徵融合到藝術作品中。", "**數據增強中的偏差移除：** 在使用擴散模型生成數據以訓練其他模型時，可以抹除數據中的偏差（例如性別或種族偏見），從而提高下游模型的公平性。"], "pitch": "我們發現了現有擴散模型「概念抹除」技術的盲點，並開發了一套更全面的評估框架。這項技術的核心價值在於精確控制 AI 生成內容的能力，允許在不犧牲模型生成能力的同時，移除特定概念。這在內容審查、藝術風格轉移控制、以及數據增強等多個領域具有廣泛的應用前景。我們的商業模式可以是：\n\n*   **提供API服務：** 為需要內容過濾或數據淨化的企業提供基於我們的概念抹除技術的API服務，按使用量或訂閱收費。\n*   **授權技術：** 將我們的技術授權給大型科技公司或研究機構，用於改進他們的AI模型。\n*   **開發垂直應用：** 針對特定領域（例如醫療影像、金融數據）開發專業的概念抹除工具，提供給相關行業的客戶。\n\n我們的優勢在於我們對概念抹除的深入理解和全面的評估方法，能確保抹除的有效性和安全性，避免不必要的副作用。這將使我們的解決方案在市場上具有競爭力，並帶來可觀的商業回報。", "audio": "audios/2505.17013v1.mp3", "timestamp": "2025-05-24T18:17:18.228241"}
{"query": "AI", "id": "2505.17021v1", "url": "http://arxiv.org/abs/2505.17021v1", "title": "ARB: A Comprehensive Arabic Multimodal Reasoning Benchmark", "summary": "As Large Multimodal Models (LMMs) become more capable, there is growing\ninterest in evaluating their reasoning processes alongside their final outputs.\nHowever, most benchmarks remain focused on English, overlooking languages with\nrich linguistic and cultural contexts, such as Arabic. To address this gap, we\nintroduce the Comprehensive Arabic Multimodal Reasoning Benchmark (ARB), the\nfirst benchmark designed to evaluate step-by-step reasoning in Arabic across\nboth textual and visual modalities. ARB spans 11 diverse domains, including\nvisual reasoning, document understanding, OCR, scientific analysis, and\ncultural interpretation. It comprises 1,356 multimodal samples paired with\n5,119 human-curated reasoning steps and corresponding actions. We evaluated 12\nstate-of-the-art open- and closed-source LMMs and found persistent challenges\nin coherence, faithfulness, and cultural grounding. ARB offers a structured\nframework for diagnosing multimodal reasoning in underrepresented languages and\nmarks a critical step toward inclusive, transparent, and culturally aware AI\nsystems. We release the benchmark, rubric, and evaluation suit to support\nfuture research and reproducibility. Code available at:\nhttps://github.com/mbzuai-oryx/ARB", "authors": ["Sara Ghaboura", "Ketan More", "Wafa Alghallabi", "Omkar Thawakar", "Jorma Laaksonen", "Hisham Cholakkal", "Salman Khan", "Rao Muhammad Anwer"], "published_date": "2025-05-22", "title_zh": "ARB：一個全面的阿拉伯語多模態推理基準測試", "summary_zh": "我們推出了一個名為ARB的綜合阿拉伯語多模態推理基準測試，旨在評估大型多模態模型（LMM）在阿拉伯語環境下，結合文本和視覺信息進行逐步推理的能力。該基準涵蓋11個不同領域，包含1356個多模態樣本，並搭配5119個人工策劃的推理步驟。我們評估了12個最先進的LMM，發現它們在連貫性、真實性和文化基礎方面仍然存在挑戰。ARB提供了一個結構化的框架，用於診斷在代表性不足的語言中的多模態推理，標誌著邁向包容、透明和具有文化意識的人工智能系統的關鍵一步。我們已釋出該基準測試、評分標準和評估工具包。", "applications": ["**智慧教育：** 根據阿拉伯語教材的圖片和文本，自動生成逐步的解題過程和講解，幫助學生更好地理解知識。", "**文化遺產保護：** 識別和分析阿拉伯世界的歷史文獻、藝術品和建築，提供更準確和深入的文化背景解讀。", "**跨文化交流：** 輔助翻譯和理解阿拉伯文化相关的多模态信息，例如阿拉伯美食的食谱、节日习俗的说明等，消除跨文化交流的障碍。"], "pitch": "ARB是首個專為阿拉伯語設計的多模態推理基準測試，解决了现有LMM在非英语环境，特别是阿拉伯语文化理解上的短板。這使得LMM可以在更廣泛的全球市場中應用，尤其是在中東和北非地區。其潛在商業價值體現在以下幾個方面：\n\n* **市場差異化：** 能够理解和处理阿拉伯语多模态信息的LMM，相比竞争对手具有显著的优势，能够抢占市场先机。\n* **特定领域应用：** 在教育、文化、旅游等领域，ARB可以帮助开发更智能、更贴合本地需求的应用，例如个性化教育、文化遗产数字化、智能旅游导览等。\n* **数据增值：** ARB本身就是一个有价值的数据集，可以用于进一步训练和改进LMM，并可通过授权使用或提供API服务等方式实现商业化。\n* **解决实际痛点：** ARB有助于解决目前LMM在处理复杂阿拉伯语场景时遇到的问题，例如宗教典籍解读、古籍数字化、文化理解等，具有重要的社会价值和商业潜力。因此，投資ARB及其衍生應用具有高度的商業回報。", "audio": "audios/2505.17021v1.mp3", "timestamp": "2025-05-24T19:10:10.976557"}
{"query": "Foundation Model", "id": "2505.16982v1", "url": "http://arxiv.org/abs/2505.16982v1", "title": "Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine", "summary": "Large Language Models (LLMs) show promise in biomedicine but lack true causal\nunderstanding, relying instead on correlations. This paper envisions causal LLM\nagents that integrate multimodal data (text, images, genomics, etc.) and\nperform intervention-based reasoning to infer cause-and-effect. Addressing this\nrequires overcoming key challenges: designing safe, controllable agentic\nframeworks; developing rigorous benchmarks for causal evaluation; integrating\nheterogeneous data sources; and synergistically combining LLMs with structured\nknowledge (KGs) and formal causal inference tools. Such agents could unlock\ntransformative opportunities, including accelerating drug discovery through\nautomated hypothesis generation and simulation, enabling personalized medicine\nthrough patient-specific causal models. This research agenda aims to foster\ninterdisciplinary efforts, bridging causal concepts and foundation models to\ndevelop reliable AI partners for biomedical progress.", "authors": ["Adib Bazgir", "Amir Habibdoust Lafmajani", "Yuwen Zhang"], "published_date": "2025-05-22", "title_zh": "超越關聯性：邁向生物醫學領域的因果大型語言模型代理", "summary_zh": "這篇論文探討大型語言模型（LLM）在生物醫學的應用，但指出它們目前僅依賴關聯性而非真正的因果理解。論文提出一個願景，打造能整合多模態數據（文本、圖像、基因組等），並進行干預式推理以推斷因果關係的因果LLM代理。這需要克服許多挑戰，包括設計安全可控的代理框架，建立嚴謹的因果評估基準，整合異構數據源，以及結合LLM與結構化知識（知識圖譜）和正式的因果推論工具。這些代理有望帶來變革性機會，例如通過自動化假設生成和模擬來加速藥物發現，以及通過患者特定的因果模型來實現個性化醫療。", "applications": ["**藥物靶點發現加速：** 利用因果LLM代理分析大量生物醫學數據，識別更精準的藥物靶點，從而加速新藥研發，降低研發成本。", "**個性化醫療方案制定：** 整合患者的基因組、病史、生活習慣等多模態數據，建立個體化的因果模型，預測不同治療方案的效果，制定更有效的個性化醫療方案。", "**疾病傳播預測與控制：** 透過分析疫情相關數據（地理位置、人口密度、病毒變異等），建構疾病傳播的因果模型，預測疾病爆發的趨勢和熱點地區，為公共衛生決策提供更科學的依據。"], "pitch": "想像一下，一個AI夥伴可以像資深研究員一樣，從海量的生物醫學數據中挖掘出隱藏的因果關係，加速藥物開發，優化醫療決策。我們提出的因果LLM代理正是這樣一個變革性的工具。它不僅能分析數據，更能理解數據背後的因果邏輯，實現真正的智能決策。市場潛力巨大，從加速藥物研發，到實現個性化醫療，再到預測疾病傳播，每個領域都充滿了商業機會。我們需要投資，將這個願景變成現實，在生物醫學領域掀起一場AI驅動的革命。", "audio": "audios/2505.16982v1.mp3", "timestamp": "2025-05-24T19:10:25.728425"}
{"query": "Diffusion Model", "id": "2505.17013v1", "url": "http://arxiv.org/abs/2505.17013v1", "title": "When Are Concepts Erased From Diffusion Models?", "summary": "Concept erasure, the ability to selectively prevent a model from generating\nspecific concepts, has attracted growing interest, with various approaches\nemerging to address the challenge. However, it remains unclear how thoroughly\nthese methods erase the target concept. We begin by proposing two conceptual\nmodels for the erasure mechanism in diffusion models: (i) reducing the\nlikelihood of generating the target concept, and (ii) interfering with the\nmodel's internal guidance mechanisms. To thoroughly assess whether a concept\nhas been truly erased from the model, we introduce a suite of independent\nevaluations. Our evaluation framework includes adversarial attacks, novel\nprobing techniques, and analysis of the model's alternative generations in\nplace of the erased concept. Our results shed light on the tension between\nminimizing side effects and maintaining robustness to adversarial prompts.\nBroadly, our work underlines the importance of comprehensive evaluation for\nerasure in diffusion models.", "authors": ["Kevin Lu", "Nicky Kriplani", "Rohit Gandikota", "Minh Pham", "David Bau", "Chinmay Hegde", "Niv Cohen"], "published_date": "2025-05-22", "title_zh": "擴散模型中的概念何時被抹除？", "summary_zh": "概念抹除，即選擇性地阻止模型生成特定概念的能力，正受到越來越多的關注。雖然出現了各種方法來應對這個挑戰，但這些方法到底能多徹底地抹除目標概念仍然不明朗。本研究首先提出兩個擴散模型中抹除機制的概念模型：（i）降低生成目標概念的可能性，以及（ii）干擾模型的內部引導機制。為了徹底評估一個概念是否真的從模型中被抹除，我們引入了一套獨立的評估方法，包括對抗性攻擊、新穎的探測技術，以及對模型在抹除概念後生成替代內容的分析。我們的結果揭示了最小化副作用和保持對對抗性提示的魯棒性之間的權衡。總體而言，我們的研究強調了對擴散模型中的抹除進行全面評估的重要性。", "applications": ["**内容审查与品牌安全：** 避免生成包含敏感信息或有害内容的图像，例如仇恨言论、虚假新闻、以及侵犯品牌版权的图像。", "**个性化教育内容生成：** 针对特定学习需求，屏蔽不相关的或超出当前学习范围的概念，集中生成目标知识点的相关图像，辅助教学。", "**艺术风格控制与创意工具：** 允許用户在生成图像时排除特定艺术风格或元素，例如“不要生成毕加索风格的猫”，从而更精细地控制生成结果，提升创意自由度。"], "pitch": "各位投資人，我們正在解決生成式 AI 領域一個日益重要的問題：如何安全且可控地使用 AI 生成内容。我們的研究深入探討了擴散模型中概念抹除的技術，並建立了一套嚴謹的評估框架，可以驗證特定概念是否真正從模型中被移除。這項技術的商業潛力巨大，它可以應用於内容审查、品牌安全、教育、以及創意工具等多個領域。 想像一下，一家公司可以利用我們的技術，確保其 AI 生成的廣告素材不會包含任何違規或不當的元素，從而避免法律風險和品牌聲譽的損失。 教育平台可以客製化生成學習內容，排除不必要的資訊，提升學習效率。 此外，我們的研究成果也為開發更安全、更可靠的生成式 AI 產品奠定了基礎。 我們相信，這項技術將成為下一代 AI 平台的關鍵组成部分，具有高度的投資價值。", "audio": "audios/2505.17013v1.mp3", "timestamp": "2025-05-24T19:10:40.149070"}
{"query": "AI", "id": "2505.17021v1", "url": "http://arxiv.org/abs/2505.17021v1", "title": "ARB: A Comprehensive Arabic Multimodal Reasoning Benchmark", "summary": "As Large Multimodal Models (LMMs) become more capable, there is growing\ninterest in evaluating their reasoning processes alongside their final outputs.\nHowever, most benchmarks remain focused on English, overlooking languages with\nrich linguistic and cultural contexts, such as Arabic. To address this gap, we\nintroduce the Comprehensive Arabic Multimodal Reasoning Benchmark (ARB), the\nfirst benchmark designed to evaluate step-by-step reasoning in Arabic across\nboth textual and visual modalities. ARB spans 11 diverse domains, including\nvisual reasoning, document understanding, OCR, scientific analysis, and\ncultural interpretation. It comprises 1,356 multimodal samples paired with\n5,119 human-curated reasoning steps and corresponding actions. We evaluated 12\nstate-of-the-art open- and closed-source LMMs and found persistent challenges\nin coherence, faithfulness, and cultural grounding. ARB offers a structured\nframework for diagnosing multimodal reasoning in underrepresented languages and\nmarks a critical step toward inclusive, transparent, and culturally aware AI\nsystems. We release the benchmark, rubric, and evaluation suit to support\nfuture research and reproducibility. Code available at:\nhttps://github.com/mbzuai-oryx/ARB", "authors": ["Sara Ghaboura", "Ketan More", "Wafa Alghallabi", "Omkar Thawakar", "Jorma Laaksonen", "Hisham Cholakkal", "Salman Khan", "Rao Muhammad Anwer"], "published_date": "2025-05-22", "title_zh": "ARB：一個全面的阿拉伯語多模態推理基準測試", "summary_zh": "大型多模態模型（LMMs）越來越強大，評估它們的推理過程變得至關重要。但現有基準測試大多以英語為主，忽略了像阿拉伯語這樣具有豐富語言和文化背景的語言。我們推出了ARB，首個專為評估阿拉伯語文本和視覺多模態推理的基準測試。ARB涵蓋視覺推理、文檔理解、OCR、科學分析和文化詮釋等11個不同領域，包含1,356個多模態樣本，並配有5,119個人工整理的推理步驟和相應操作。我們評估了12個最先進的開放和閉源LMMs，發現它們在連貫性、忠實性和文化基礎方面仍然面臨挑戰。ARB提供了一個結構化的框架，用於診斷代表性不足的語言中的多模態推理，並標誌著邁向包容、透明和具有文化意識的AI系統的關鍵一步。我們發布了基準測試、評分標準和評估工具，以支持未來的研究和可重複性。", "applications": ["**文化遺產保存：** 自動分析阿拉伯古籍和文物圖像，理解其歷史背景和文化意義，並生成易於理解的解釋和描述，輔助文物修復和教育。", "**醫療診斷輔助：** 結合阿拉伯語病歷文本和醫學影像（如X光片），協助醫生進行更準確的診斷，尤其是在缺乏足夠醫療資源的地區。", "**智慧城市管理：** 通過分析阿拉伯語社交媒體文本和監控視頻，了解城市居民的需求和問題，並提供及時的公共服務和信息。"], "pitch": "ARB基準測試解決了阿拉伯語多模態AI發展的關鍵瓶頸。現有的多模態模型在阿拉伯語環境下的推理能力不足，限制了其在文化、醫療、教育等領域的應用。ARB提供了一個標準化的評估框架，能夠有效提升模型的性能，並吸引更多開發者和研究者投入阿拉伯語AI領域。這將帶來巨大的商業價值：\n\n*   **市場潛力巨大：** 中東和北非地區的阿拉伯語使用者眾多，對本地化AI解決方案需求旺盛。\n*   **技術壁壘高：** 阿拉伯語的複雜性和文化背景需要專門的解決方案，不易被通用模型替代。\n*   **商業模式多樣：** 可通過模型授權、定制開發、雲服務等方式實現商業化。我們將基於ARB構建領先的阿拉伯語多模態AI平台，搶占市場先機，並與政府、企業和學術機構建立廣泛合作，共同推動阿拉伯語AI的發展。", "audio": "audios/2505.17021v1.mp3", "timestamp": "2025-05-24T20:14:07.902784"}
{"query": "Foundation Model", "id": "2505.16982v1", "url": "http://arxiv.org/abs/2505.16982v1", "title": "Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine", "summary": "Large Language Models (LLMs) show promise in biomedicine but lack true causal\nunderstanding, relying instead on correlations. This paper envisions causal LLM\nagents that integrate multimodal data (text, images, genomics, etc.) and\nperform intervention-based reasoning to infer cause-and-effect. Addressing this\nrequires overcoming key challenges: designing safe, controllable agentic\nframeworks; developing rigorous benchmarks for causal evaluation; integrating\nheterogeneous data sources; and synergistically combining LLMs with structured\nknowledge (KGs) and formal causal inference tools. Such agents could unlock\ntransformative opportunities, including accelerating drug discovery through\nautomated hypothesis generation and simulation, enabling personalized medicine\nthrough patient-specific causal models. This research agenda aims to foster\ninterdisciplinary efforts, bridging causal concepts and foundation models to\ndevelop reliable AI partners for biomedical progress.", "authors": ["Adib Bazgir", "Amir Habibdoust Lafmajani", "Yuwen Zhang"], "published_date": "2025-05-22", "title_zh": "超越關聯性：邁向生物醫學領域的因果大型語言模型代理", "summary_zh": "大型語言模型在生物醫學領域展現潛力，但缺乏真正的因果理解，僅仰賴關聯性。本文設想一種因果大型語言模型代理，整合多模態數據（文本、圖像、基因組等），並進行基於干預的推理來推斷因果關係。實現這一目標需要克服設計安全可控的代理框架、開發嚴格的因果評估基準、整合異質數據源，以及將大型語言模型與結構化知識（知識圖譜）和正式的因果推論工具協同結合等關鍵挑戰。這種代理有望釋放變革性機會，包括通過自動假設生成和模擬加速藥物發現，以及通過患者特定的因果模型實現個性化醫療。本研究旨在促進跨學科合作，將因果概念和基礎模型聯繫起來，為生物醫學的進步開發可靠的AI夥伴。", "applications": ["醫生診斷輔助：整合病患病歷、影像和基因數據，基於因果推理預測最佳治療方案，減少誤診率。", "藥物研發加速：通過模擬不同藥物對人體的因果影響，篩選有潛力的候選藥物，降低研發成本。", "公共衛生政策制定：模擬不同政策干預對疾病傳播的因果影響，為政府提供科學的決策依據。"], "pitch": "我們正在開發基於因果推理的生物醫學大型語言模型代理，這將徹底改變醫療保健和藥物研發領域。與傳統模型僅關注數據關聯性不同，我們的技術能夠理解因果關係，從而實現更準確的預測、更有效的治療方案和更快速的藥物發現。想像一下，一個 AI 醫生能夠整合所有病患數據，不僅僅是找到相似的病例，而是真正理解導致疾病的原因，從而做出最佳的診斷和治療建議。在藥物研發方面，我們的平台能夠模擬藥物在人體內的影響，大幅縮短臨床試驗時間，降低研發成本。這項技術的商業價值巨大，可以通過向醫院、藥廠和研究機構提供訂閱服務來實現盈利。我們相信，我們的因果大型語言模型代理將成為生物醫學領域的遊戲規則改變者，為投資者帶來豐厚的回報。", "audio": "audios/2505.16982v1.mp3", "timestamp": "2025-05-24T20:14:23.586096"}
{"query": "Diffusion Model", "id": "2505.17013v1", "url": "http://arxiv.org/abs/2505.17013v1", "title": "When Are Concepts Erased From Diffusion Models?", "summary": "Concept erasure, the ability to selectively prevent a model from generating\nspecific concepts, has attracted growing interest, with various approaches\nemerging to address the challenge. However, it remains unclear how thoroughly\nthese methods erase the target concept. We begin by proposing two conceptual\nmodels for the erasure mechanism in diffusion models: (i) reducing the\nlikelihood of generating the target concept, and (ii) interfering with the\nmodel's internal guidance mechanisms. To thoroughly assess whether a concept\nhas been truly erased from the model, we introduce a suite of independent\nevaluations. Our evaluation framework includes adversarial attacks, novel\nprobing techniques, and analysis of the model's alternative generations in\nplace of the erased concept. Our results shed light on the tension between\nminimizing side effects and maintaining robustness to adversarial prompts.\nBroadly, our work underlines the importance of comprehensive evaluation for\nerasure in diffusion models.", "authors": ["Kevin Lu", "Nicky Kriplani", "Rohit Gandikota", "Minh Pham", "David Bau", "Chinmay Hegde", "Niv Cohen"], "published_date": "2025-05-22", "title_zh": "擴散模型中的概念何時被抹除？", "summary_zh": "概念抹除，也就是選擇性地阻止模型生成特定概念的能力，越來越受關注。雖然湧現了許多方法來解決這個挑戰，但這些方法究竟能多徹底地抹除目標概念仍然不明朗。我們提出了兩種擴散模型中抹除機制的概念模型：（一）降低生成目標概念的可能性；（二）干擾模型內部的引導機制。為了徹底評估一個概念是否真正從模型中被抹除，我們引入了一套獨立的評估方法，包括對抗性攻擊、新穎的探測技術，以及分析模型在抹除概念後生成的替代方案。我們的結果揭示了最小化副作用與保持對抗性提示的魯棒性之間的張力。總體而言，我們的工作強調了全面評估擴散模型中概念抹除的重要性。", "applications": ["**內容審查與合規性：** 用於自動移除不當內容（例如仇恨言論、暴力圖像）或敏感資料（例如個人資料、機密商業資訊），確保生成內容符合法規與道德規範。", "**客製化藝術創作：** 允許使用者要求模型避免生成特定元素或風格，從而創造出更符合個人喜好的獨特藝術作品，例如要求AI畫作中不要出現特定品牌標誌或政治符號。", "**智慧財產權保護：** 防止AI模型生成與現有版權內容過於相似的圖像，降低侵權風險。 例如，讓遊戲公司確保AI生成的角色造型不侵犯其他遊戲的角色版權。"], "pitch": "這項研究解決了擴散模型中一個關鍵的挑戰：概念抹除。 想像一下，一個AI可以根據你的指示，聰明地避免生成任何你不想要的東西。這不僅僅是技術，更是一種責任。目前AI生成的內容可能涉及侵權、敏感資訊洩露等問題。我們的技術提供了一個強大的工具，可以精準控制AI的生成行為，大幅降低這些風險。我們可以將這項技術授權給內容平台、遊戲公司、廣告商等，幫助他們建立更安全、更合規、更具創意的AI應用。 我們提供的不僅僅是技術，而是一個更值得信賴的AI生態系統。 這是一個潛力無限的市場，現在加入正是時候。", "audio": "audios/2505.17013v1.mp3", "timestamp": "2025-05-24T20:14:37.608123"}
{"query": "AI", "id": "2505.17021v1", "url": "http://arxiv.org/abs/2505.17021v1", "title": "ARB: A Comprehensive Arabic Multimodal Reasoning Benchmark", "summary": "As Large Multimodal Models (LMMs) become more capable, there is growing\ninterest in evaluating their reasoning processes alongside their final outputs.\nHowever, most benchmarks remain focused on English, overlooking languages with\nrich linguistic and cultural contexts, such as Arabic. To address this gap, we\nintroduce the Comprehensive Arabic Multimodal Reasoning Benchmark (ARB), the\nfirst benchmark designed to evaluate step-by-step reasoning in Arabic across\nboth textual and visual modalities. ARB spans 11 diverse domains, including\nvisual reasoning, document understanding, OCR, scientific analysis, and\ncultural interpretation. It comprises 1,356 multimodal samples paired with\n5,119 human-curated reasoning steps and corresponding actions. We evaluated 12\nstate-of-the-art open- and closed-source LMMs and found persistent challenges\nin coherence, faithfulness, and cultural grounding. ARB offers a structured\nframework for diagnosing multimodal reasoning in underrepresented languages and\nmarks a critical step toward inclusive, transparent, and culturally aware AI\nsystems. We release the benchmark, rubric, and evaluation suit to support\nfuture research and reproducibility. Code available at:\nhttps://github.com/mbzuai-oryx/ARB", "authors": ["Sara Ghaboura", "Ketan More", "Wafa Alghallabi", "Omkar Thawakar", "Jorma Laaksonen", "Hisham Cholakkal", "Salman Khan", "Rao Muhammad Anwer"], "published_date": "2025-05-22", "title_zh": "ARB：一個全面的阿拉伯語多模態推理基準", "summary_zh": "大型多模態模型（LMMs）的能力日益增強，人們越來越關注評估其推理過程。然而，大多數基準測試仍然側重於英語，忽略了像阿拉伯語這樣具有豐富語言和文化背景的語言。為了解決這個問題，我們推出了全面的阿拉伯語多模態推理基準（ARB），這是第一個旨在評估阿拉伯語文本和視覺模態中逐步推理的基準。ARB 涵蓋 11 個不同的領域，包括視覺推理、文檔理解、OCR、科學分析和文化詮釋。它包含 1,356 個多模態樣本，並配有 5,119 個人工管理的推理步驟和相應的操作。我們評估了 12 個最先進的開源和閉源 LMM，發現它們在連貫性、忠實性和文化基礎方面仍然存在挑戰。ARB 提供了一個結構化的框架，用於診斷代表性不足的語言中的多模態推理，並標誌著朝著包容、透明和具有文化意識的 AI 系統邁出的關鍵一步。我們發布基準、評分標準和評估套件，以支持未來的研究和可重複性。", "applications": ["**阿拉伯語文檔智能處理：** 自動處理阿拉伯語文檔，例如合同、發票和法律文件，提取關鍵資訊並進行風險評估，例如辨識條款中的文化敏感性，避免誤解和法律風險。", "**阿拉伯文化內容創作與審核：** 輔助生成符合阿拉伯文化規範的內容，例如廣告文案、新聞報導和教育材料，並自動審核現有內容，確保其不違反當地法律法規和社會習俗。", "**多語言教育與文化交流：** 開發基於阿拉伯語的沉浸式學習體驗，例如通過分析圖像和文本，提供阿拉伯歷史、藝術和文化的深入理解。例如，分析古蘭經經文，並結合相關歷史圖片，幫助學習者理解經文的背景和意義。"], "pitch": "ARB作為首個針對阿拉伯語多模態推理的綜合性基準，解決了LLM在阿拉伯語環境下的文化理解與應用瓶頸。其潛在商業價值巨大，體現在：\n\n1. **數據稀缺性：** 提供高質量、人工標注的阿拉伯語多模態數據集，為企業開發針對阿拉伯市場的AI產品提供寶貴的訓練資源，降低研發成本。\n2. **行業應用廣泛：** 可應用於金融、法律、教育、媒體等領域，提升阿拉伯語文檔處理、內容創作與審核、文化交流等方面的效率與準確性。\n3. **市場先發優勢：** 在阿拉伯語AI市場尚不成熟的階段，率先推出基於ARB的解決方案，能夠迅速佔領市場份額，建立競爭優勢。\n4. **技術壁壘：** ARB的開發需要深厚的阿拉伯語語言學、文化背景以及AI技術積累，形成較高的技術壁壘，不易被競爭對手模仿。\n\n總而言之，ARB不僅是學術研究的貢獻，更是unlock阿拉伯AI市場的鑰匙，具有巨大的商業潛力，值得投資。", "audio": "audios/2505.17021v1.mp3", "timestamp": "2025-05-24T21:12:11.116700"}
{"query": "Foundation Model", "id": "2505.16982v1", "url": "http://arxiv.org/abs/2505.16982v1", "title": "Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine", "summary": "Large Language Models (LLMs) show promise in biomedicine but lack true causal\nunderstanding, relying instead on correlations. This paper envisions causal LLM\nagents that integrate multimodal data (text, images, genomics, etc.) and\nperform intervention-based reasoning to infer cause-and-effect. Addressing this\nrequires overcoming key challenges: designing safe, controllable agentic\nframeworks; developing rigorous benchmarks for causal evaluation; integrating\nheterogeneous data sources; and synergistically combining LLMs with structured\nknowledge (KGs) and formal causal inference tools. Such agents could unlock\ntransformative opportunities, including accelerating drug discovery through\nautomated hypothesis generation and simulation, enabling personalized medicine\nthrough patient-specific causal models. This research agenda aims to foster\ninterdisciplinary efforts, bridging causal concepts and foundation models to\ndevelop reliable AI partners for biomedical progress.", "authors": ["Adib Bazgir", "Amir Habibdoust Lafmajani", "Yuwen Zhang"], "published_date": "2025-05-22", "title_zh": "超越關聯性：邁向生物醫學領域的因果大型語言模型代理", "summary_zh": "這篇論文探討如何開發具備因果推理能力的生物醫學大型語言模型代理。現有的大型語言模型主要依賴關聯性，缺乏真正的因果理解。作者提出整合多模態數據（文本、圖像、基因組等），並進行基於干預的推理，來推斷因果關係的設想。開發這種代理需要克服一些挑戰，包括設計安全可控的框架、建立嚴格的因果評估標準、整合異構數據源，以及將大型語言模型與結構化知識（知識圖譜）和正式的因果推理工具結合。這種代理有望在生物醫學領域帶來變革，例如加速藥物發現，實現個性化醫療等。", "applications": ["**藥物副作用預測：**根據患者的基因、生活習慣和用藥歷史，預測某種藥物可能產生的副作用，幫助醫生選擇更安全的治療方案。", "**疾病診斷輔助：**整合病人的病歷、影像報告和基因數據，協助醫生診斷病因複雜的疾病，提高診斷準確性。", "**健康生活方式建議：**分析個人的生理數據和飲食習慣，提供個性化的健康建議，預測不同生活方式選擇對健康狀況的長期影響。"], "pitch": "想像一下，一個能像頂尖醫生一樣思考的AI助手，它不僅能讀懂醫學文獻，更能理解疾病的真正成因，並預測治療效果。我們正在開發的因果大型語言模型代理，將徹底改變生物醫學領域的研究和應用。它能加速藥物開發，降低研發成本，並根據個體差異提供個性化的醫療方案，大幅提升治療效果。這項技術的潛在市場規模巨大，涵蓋藥物研發、診斷試劑、個性化醫療等領域。我們相信，這項技術將為人類健康帶來前所未有的改善，並為投資者帶來豐厚的回報。目前，我們正尋求資金支持，用於建立完善的數據平台、開發高性能的因果推理算法，以及進行臨床驗證。加入我們，一起開創生物醫學AI的未來！", "audio": "audios/2505.16982v1.mp3", "timestamp": "2025-05-24T21:12:38.594655"}
{"query": "Diffusion Model", "id": "2505.17013v1", "url": "http://arxiv.org/abs/2505.17013v1", "title": "When Are Concepts Erased From Diffusion Models?", "summary": "Concept erasure, the ability to selectively prevent a model from generating\nspecific concepts, has attracted growing interest, with various approaches\nemerging to address the challenge. However, it remains unclear how thoroughly\nthese methods erase the target concept. We begin by proposing two conceptual\nmodels for the erasure mechanism in diffusion models: (i) reducing the\nlikelihood of generating the target concept, and (ii) interfering with the\nmodel's internal guidance mechanisms. To thoroughly assess whether a concept\nhas been truly erased from the model, we introduce a suite of independent\nevaluations. Our evaluation framework includes adversarial attacks, novel\nprobing techniques, and analysis of the model's alternative generations in\nplace of the erased concept. Our results shed light on the tension between\nminimizing side effects and maintaining robustness to adversarial prompts.\nBroadly, our work underlines the importance of comprehensive evaluation for\nerasure in diffusion models.", "authors": ["Kevin Lu", "Nicky Kriplani", "Rohit Gandikota", "Minh Pham", "David Bau", "Chinmay Hegde", "Niv Cohen"], "published_date": "2025-05-22", "title_zh": "擴散模型中的概念何時被抹除？", "summary_zh": "概念抹除，即有選擇地阻止模型生成特定概念的能力，越來越受到關注，並湧現出各種方法來應對這一挑戰。然而，這些方法究竟能多麼徹底地抹除目標概念仍不明朗。我們首先提出了兩種擴散模型中抹除機制的概念模型：（i）降低生成目標概念的可能性，以及（ii）干擾模型的內部引導機制。為了徹底評估一個概念是否已真正從模型中抹除，我們引入了一套獨立的評估方法。我們的評估框架包括對抗性攻擊、新穎的探測技術以及對模型在抹除概念後產生的替代生成的分析。我們的結果揭示了最小化副作用和保持對抗性提示的穩健性之間的張力。總體而言，我們的工作強調了對擴散模型中抹除進行全面評估的重要性。", "applications": ["**內容安全與隱私保護：** 過濾掉AI生成圖像中涉及個人隱私或敏感資訊的部分，例如移除人臉上的特定特徵，防止AI生成深度偽造內容。", "**客製化與品牌安全：** 防止AI生成帶有負面或爭議性意涵的圖像，確保生成的內容符合品牌形象和價值觀，例如避免生成帶有政治立場或歧視性信息的圖像。", "**教育與創作自由：** 在教育領域，可過濾掉不適合成年人觀看的內容，讓孩子安全使用AI繪圖工具。同時，在創作領域，可以抹除不想要的元素，更精準地實現創作者的意圖。"], "pitch": "我們正在解決生成式AI一個核心但經常被忽視的問題：如何精確且可靠地控制AI的輸出，避免生成不需要甚至有害的內容。我們的研究揭示了現有概念抹除技術的局限性，並提供了一套更嚴謹的評估標準。這對於確保AI產品的安全性、合規性和道德性至關重要。想像一下，一個能完全控制AI生成內容的平台，可以安全地應用於廣告、教育、娛樂等各個領域，並且符合嚴格的監管要求。這不僅能降低企業的法律風險，更能提升品牌價值。我們的技術能為AI模型提供精確的控制力，確保其生成內容符合倫理規範和商業目標。我們正在尋找投資者，共同打造一個更安全、更可信賴的AI未來，並在這個快速增長的市場中佔據領先地位。", "audio": "audios/2505.17013v1.mp3", "timestamp": "2025-05-24T21:13:12.015788"}
{"query": "AI", "id": "2505.17021v1", "url": "http://arxiv.org/abs/2505.17021v1", "title": "ARB: A Comprehensive Arabic Multimodal Reasoning Benchmark", "summary": "As Large Multimodal Models (LMMs) become more capable, there is growing\ninterest in evaluating their reasoning processes alongside their final outputs.\nHowever, most benchmarks remain focused on English, overlooking languages with\nrich linguistic and cultural contexts, such as Arabic. To address this gap, we\nintroduce the Comprehensive Arabic Multimodal Reasoning Benchmark (ARB), the\nfirst benchmark designed to evaluate step-by-step reasoning in Arabic across\nboth textual and visual modalities. ARB spans 11 diverse domains, including\nvisual reasoning, document understanding, OCR, scientific analysis, and\ncultural interpretation. It comprises 1,356 multimodal samples paired with\n5,119 human-curated reasoning steps and corresponding actions. We evaluated 12\nstate-of-the-art open- and closed-source LMMs and found persistent challenges\nin coherence, faithfulness, and cultural grounding. ARB offers a structured\nframework for diagnosing multimodal reasoning in underrepresented languages and\nmarks a critical step toward inclusive, transparent, and culturally aware AI\nsystems. We release the benchmark, rubric, and evaluation suit to support\nfuture research and reproducibility. Code available at:\nhttps://github.com/mbzuai-oryx/ARB", "authors": ["Sara Ghaboura", "Ketan More", "Wafa Alghallabi", "Omkar Thawakar", "Jorma Laaksonen", "Hisham Cholakkal", "Salman Khan", "Rao Muhammad Anwer"], "published_date": "2025-05-22", "title_zh": "ARB：一個全面的阿拉伯語多模態推理基準", "summary_zh": "大型多模態模型 (LMMs) 日益強大，但評估其推理過程的研究主要集中在英語。為了解決這個缺口，我們推出了阿拉伯語多模態推理基準 (ARB)，這是首個評估阿拉伯語文本和視覺模態逐步推理的基準。ARB 涵蓋 11 個領域，包含 1356 個多模態樣本和 5119 個人工策劃的推理步驟。我們評估了 12 個最先進的 LMMs，發現它們在連貫性、忠實性和文化基礎方面仍然面臨挑戰。ARB 提供了一個結構化的框架，用於診斷代表性不足語言的多模態推理，是邁向包容、透明和具有文化意識的 AI 系統的關鍵一步。", "applications": ["**中東地區智慧城市服務：** 基於阿拉伯語多模態推理的AI系統，能夠理解當地文化和習俗，提供更精準的導航、購物推薦和政府服務，例如識別阿拉伯語路牌和文件，並結合圖像資料提供導覽或資訊。", "**阿拉伯語教育輔助：** 用於開發更有效的阿拉伯語學習平台，通過視覺和文本資料，輔助學生理解複雜的概念和語法規則，並能評估學生的推理過程，提供個性化的學習建議。", "**阿拉伯藝術品與文物分析：** 結合文物圖片和相關阿拉伯語文獻，分析文物的歷史背景、文化意義和真偽，為博物館和藝術市場提供更可靠的資訊。例如，通過圖像識別和文字分析，判斷古籍的年代和作者。"], "pitch": "ARB 作為首個阿拉伯語多模態推理基準，解決了 LMM 在阿拉伯語領域的空白。這項技術的潛在商業價值巨大，尤其是在中東和北非地區，這些地區的阿拉伯語使用者眾多，但相關的 AI 技術發展相對滯後。基於 ARB 訓練的 LMM，能夠在中東智慧城市建設、文化遺產保護、阿拉伯語教育等多個領域提供更精準、更個性化的服務。對於投資者而言，這是一個進入快速增長的阿拉伯語 AI 市場的絕佳機會，可以搶佔先機，建立領導地位，並產生巨大的社會影響力，同時帶來豐厚的回報。", "audio": "audios/2505.17021v1.mp3", "timestamp": "2025-05-24T22:12:56.303538"}
{"query": "Foundation Model", "id": "2505.16982v1", "url": "http://arxiv.org/abs/2505.16982v1", "title": "Beyond Correlation: Towards Causal Large Language Model Agents in Biomedicine", "summary": "Large Language Models (LLMs) show promise in biomedicine but lack true causal\nunderstanding, relying instead on correlations. This paper envisions causal LLM\nagents that integrate multimodal data (text, images, genomics, etc.) and\nperform intervention-based reasoning to infer cause-and-effect. Addressing this\nrequires overcoming key challenges: designing safe, controllable agentic\nframeworks; developing rigorous benchmarks for causal evaluation; integrating\nheterogeneous data sources; and synergistically combining LLMs with structured\nknowledge (KGs) and formal causal inference tools. Such agents could unlock\ntransformative opportunities, including accelerating drug discovery through\nautomated hypothesis generation and simulation, enabling personalized medicine\nthrough patient-specific causal models. This research agenda aims to foster\ninterdisciplinary efforts, bridging causal concepts and foundation models to\ndevelop reliable AI partners for biomedical progress.", "authors": ["Adib Bazgir", "Amir Habibdoust Lafmajani", "Yuwen Zhang"], "published_date": "2025-05-22", "title_zh": "超越相關性：邁向生物醫學領域的因果大型語言模型代理", "summary_zh": "大型語言模型在生物醫學領域展現潛力，但缺乏真正的因果理解，多仰賴相關性。本文提出因果大型語言模型代理的概念，整合多模態數據（文本、圖像、基因組等），並進行基於干預的推理，以推斷因果關係。實現此目標需克服多項挑戰：設計安全、可控的代理框架；開發嚴格的因果評估基準；整合異構數據源；以及將大型語言模型與結構化知識（知識圖譜）和正式因果推論工具協同結合。此類代理有潛力解鎖變革性機會，包括通過自動化假設生成和模擬來加速藥物發現，以及通過患者特異性因果模型實現個性化醫療。本研究旨在促進跨學科合作，將因果概念與基礎模型相結合，以開發可靠的人工智能夥伴，推動生物醫學進步。", "applications": ["**藥物研發加速器：** 模擬不同藥物對特定疾病的干預效果，大幅縮短臨床試驗週期並降低成本，例如預測某種基因編輯技術對於治療罕見疾病的效果。", "**個性化醫療助手：** 根據患者的基因、生活習慣、病史等數據，建立個體化的因果模型，預測不同治療方案的療效，從而為醫生提供更精準的治療建議，例如判斷某位高血壓患者對哪種降壓藥物的反應最佳。", "**公共衛生決策支持系統：** 分析大規模人群數據，找出環境因素、生活方式與疾病之間的因果關係，為公共衛生政策制定提供科學依據，例如評估某種飲食習慣對特定地區居民慢性病發病率的影響。"], "pitch": "我們正在開發基於因果理解的生物醫學大型語言模型，它不僅能分析數據，更能理解數據背後的因果關係，從而做出更準確的預測和決策。想像一下，一個能夠快速找到新藥靶點、為每個患者提供個性化治療方案、並能預測疫情發展趨勢的人工智能。我們的技術將顛覆生物醫學研究和臨床實踐，創造巨大的商業價值。我們尋求投資者共同打造這個劃時代的平台，將科學研究的成果更快、更有效地轉化為改善人類健康的工具，在萬億級的生物醫學市場中佔據領導地位。", "audio": "audios/2505.16982v1.mp3", "timestamp": "2025-05-24T22:13:20.791239"}
{"query": "Diffusion Model", "id": "2505.17013v1", "url": "http://arxiv.org/abs/2505.17013v1", "title": "When Are Concepts Erased From Diffusion Models?", "summary": "Concept erasure, the ability to selectively prevent a model from generating\nspecific concepts, has attracted growing interest, with various approaches\nemerging to address the challenge. However, it remains unclear how thoroughly\nthese methods erase the target concept. We begin by proposing two conceptual\nmodels for the erasure mechanism in diffusion models: (i) reducing the\nlikelihood of generating the target concept, and (ii) interfering with the\nmodel's internal guidance mechanisms. To thoroughly assess whether a concept\nhas been truly erased from the model, we introduce a suite of independent\nevaluations. Our evaluation framework includes adversarial attacks, novel\nprobing techniques, and analysis of the model's alternative generations in\nplace of the erased concept. Our results shed light on the tension between\nminimizing side effects and maintaining robustness to adversarial prompts.\nBroadly, our work underlines the importance of comprehensive evaluation for\nerasure in diffusion models.", "authors": ["Kevin Lu", "Nicky Kriplani", "Rohit Gandikota", "Minh Pham", "David Bau", "Chinmay Hegde", "Niv Cohen"], "published_date": "2025-05-22", "title_zh": "擴散模型中的概念何時被抹除？", "summary_zh": "概念抹除，即選擇性地阻止模型生成特定概念的能力，日益受到關注，並湧現出各種應對挑戰的方法。然而，這些方法抹除目標概念的徹底程度仍然不明確。我們首先提出了兩種用於擴散模型中抹除機制的概念模型：（i）降低生成目標概念的可能性，以及（ii）干擾模型的內部引導機制。為了徹底評估概念是否已真正從模型中抹除，我們引入了一套獨立的評估方法。我們的評估框架包括對抗性攻擊、新穎的探測技術，以及對模型在抹除概念後生成的替代方案的分析。我們的結果闡明了最小化副作用和保持對對抗性提示的魯棒性之間的緊張關係。總體而言，我們的工作強調了對擴散模型中抹除進行全面評估的重要性。", "applications": ["**內容審核：**自動生成內容的平台可以利用概念抹除技術，防止模型生成仇恨言論、暴力內容或錯誤信息，確保平台內容的合規性和安全性。", "**保護個人隱私：**在處理包含個人信息的圖像或數據時，可以抹除敏感概念（例如人臉、地址），以保護個人隱私，同時仍能利用其他有用的信息。", "**藝術創作：**藝術家可以利用概念抹除技術來消除創作過程中的特定元素或主題，以探索新的藝術方向，並創造出更具創意和獨特性的作品。"], "pitch": "我們開發的技術能精準控制AI生成模型，徹底抹除特定概念，避免不良內容產生、保護隱私。這項技術不僅適用於內容平台的風險控管，降低法律風險，更能在資料處理、藝術創作等領域創造全新價值。想像一下，一個能自我審查的AI內容生成引擎，或者是一個能精準移除敏感個資的數據分析工具，市場潛力巨大。我們提供更安全、更可控的AI解決方案，讓企業在享受AI紅利的同時，也能有效避免潛在風險。", "audio": "audios/2505.17013v1.mp3", "timestamp": "2025-05-24T22:13:47.216127"}
{"query": "AI", "id": "2505.16619v1", "url": "http://arxiv.org/abs/2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "published_date": "2025-05-22", "title_zh": "開放且永續的人工智慧：生命科學領域的挑戰、機會與未來之路", "summary_zh": "這篇論文探討了人工智慧在生命科學領域快速發展所帶來的挑戰，特別是可重用性、可再現性不足導致對AI研究成果信任度下降，以及對環境永續性的影響。論文強調AI生態系統的碎片化，並提出一套實際的開放且永續AI(OSAI)建議，將研究人員與相關的AI資源連接起來，以促進永續、可重用且透明的AI實施。目的是協助制定政策和結構化路徑，指導AI在生命科學領域的應用。", "applications": ["**加速新藥開發：** 利用OSAI原則，確保AI模型訓練數據的透明度和可追溯性，以及模型的可重用性，加速藥物靶點的發現和先導化合物的篩選，大幅縮短新藥研發週期。", "**精準醫療診斷：** 透過開放的AI模型和數據，可以建立更可靠的疾病診斷模型，幫助醫生更準確地診斷疾病，並制定個性化的治療方案，提高治療效果。", "**環境監測與保護：** 利用AI分析環境數據，例如空氣品質、水質等，早期發現環境污染問題，並預測未來的環境變化趨勢。基於OSAI原則，確保數據的開放性和模型的透明度，讓更多人參與到環境保護的行動中來。"], "pitch": "各位投資人，我們正處於AI技術與生命科學交匯的黃金時代。這篇論文揭示了行業痛點：AI在生命科學的應用雖潜力無限，但可重現性差、資源分散，阻礙了其發展。我們的機會在於，將論文提出的OSAI原則商業化，打造一個開放、可信賴、可持續的AI平台，專注於生命科學領域。想象一下，一個能大幅加速新藥研發、提供精準診斷方案、提升環境監測效率的平台！我們將通過提供標準化的數據集、開源模型庫、以及專業的AI諮詢服務，吸引藥企、醫院、科研機構等客戶。我們的商業模式包括訂閱服務、模型定製和諮詢費用。憑藉對行業痛點的深刻理解和獨特的OSAI解決方案，我們有信心在這個百億美元級的市場中佔據一席之地，為投資人帶來豐厚的回報。這不僅是一項商業投資，更是對未來生命科學發展的投資，讓我們共同見證AI如何改變世界！", "audio": "audios/2505.16619v1.mp3", "timestamp": "2025-05-25T00:56:40.527521"}
{"query": "Foundation Model", "id": "2505.14975v1", "url": "http://arxiv.org/abs/2505.14975v1", "title": "Flattening Hierarchies with Policy Bootstrapping", "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "authors": ["John L. Zhou", "Jonathan C. Kao"], "published_date": "2025-05-20", "title_zh": "利用策略引導扁平化層級結構", "summary_zh": "離線目標條件強化學習 (GCRL) 是一種有前景的方法，可用於在大型無獎勵軌跡數據集上預訓練通用策略，類似於用於訓練電腦視覺和自然語言處理基礎模型的自我監督目標。然而，由於稀疏獎勵和折扣的結合，使得遠距離目標下原始動作的相對優勢不明顯，因此將 GCRL 擴展到更長的時間範圍仍然具有挑戰性。分層強化學習方法在長時距目標達成任務上取得了強大的經驗成果，但它們對模組化、特定時間尺度的策略和子目標生成模型的依賴引入了顯著的額外複雜性，並阻礙了擴展到高維目標空間。在這項工作中，我們介紹了一種算法，通過對子目標條件策略進行優勢加權重要性採樣引導，來訓練扁平（非分層）目標條件策略。我們的方法消除了對目標空間（子目標）生成模型的需求，我們發現這對於在高維狀態空間中擴展到高維控制至關重要。我們進一步表明，現有的分層和基於引導的方法對應於我們推導中的特定設計選擇。在全面的基於狀態和基於像素的運動和操縱基準測試中，我們的方法與最先進的離線 GCRL 算法相匹配或超越，並且可以擴展到以前的方法失敗的複雜長時距任務。", "applications": ["**智能家居控制:** 讓機器人學習複雜的家居任務，例如整理凌亂的房間或準備包含多個步驟的食物，而無需預先定義所有子任務。", "**自動駕駛規劃:** 訓練自動駕駛系統處理複雜的駕駛場景，例如在高流量環境中進行多車道變換和安全超車，並在沒有明確中間目標的情況下安全抵達目的地。", "**機器人手術輔助:** 指導機器人完成精確的手術操作，例如在狹窄空間中定位和操作微型器械，而無需人工定義每個步驟的精確位置和角度。"], "pitch": "我們開發了一種突破性的離線強化學習技術，能夠訓練機器人或其他智能體在複雜、長時距任務中表現出色，且無需昂貴的人工標注或複雜的層級結構設計。我們的算法通過『扁平化』學習過程，極大地簡化了訓練，並能有效利用大量的無標註數據。這項技術的潛在商業價值巨大，能夠顛覆諸如智能家居、自動駕駛、機器人自動化等領域。想像一下，無需專業編程人員，機器人就能通過學習大量的數據輕鬆掌握複雜的任務。我們相信，我們的技術將成為下一代 AI 智能體的核心引擎，為各行業帶來革命性的變革。我們的團隊正在尋求種子輪融資，以加速算法的完善和商業化落地，把握這一巨大的市場機遇。", "audio": "audios/2505.14975v1.mp3", "timestamp": "2025-05-25T00:57:14.345584"}
{"query": "Diffusion Model", "id": "2505.16174v1", "url": "http://arxiv.org/abs/2505.16174v1", "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "authors": ["Ping Liu", "Chi Zhang"], "published_date": "2025-05-22", "title_zh": "擦除還是休眠？透過可逆性重新思考概念擦除", "summary_zh": "現有的概念擦除技術，例如Unified Concept Editing和Erased Stable Diffusion，聲稱可以移除生成模型中特定概念的生成能力。但這篇論文質疑這些技術是否真的完全移除了概念，還是只是做到了表面上的壓制。研究發現，這些方法雖然可以暫時阻止模型生成特定概念，但透過輕微的微調，這些概念很容易重新浮現，顯示這些技術只是壓制了潛在的生成表示，並沒有徹底消除它們。因此，論文強調，現有的概念擦除方法存在關鍵局限性，需要更深入的表示層級干預和更嚴格的評估標準，以確保真正且不可逆地從生成模型中移除概念。", "applications": ["**內容審核與安全：** 針對可能產生有害內容的特定概念（例如暴力、仇恨言論）進行更有效的擦除，確保模型不會生成相關內容，降低平台風險。", "**客製化模型與風格轉移：**  更精確地擦除原始模型的某些風格或特定對象，讓使用者能更自由地控制生成結果，創造出更個性化的內容，例如移除特定畫家的風格。", "**智慧財產權保護：**  移除模型訓練資料中的敏感或受版權保護的內容，防止模型生成侵權內容，保護創作者的權益。"], "pitch": "想像一下，你擁有一項技術，能讓AI徹底忘記某個概念。這不僅僅是表面功夫，而是真正從模型的核心中移除。這篇論文揭示了現有技術的局限性，但也指出了未來發展的方向。我們的願景是開發一種更強大的概念擦除技術，其商業價值巨大：\n\n*   **提升AI平台的安全性與合規性：** 降低生成有害或侵權內容的風險，避免法律訴訟與品牌聲譽受損。\n*   **賦予使用者更大的控制權：** 開放更多客製化選項，滿足不同使用者的需求，提高產品吸引力。\n*   **開創新的應用場景：** 在內容創作、智慧財產權保護等領域，提供獨特的解決方案，建立市場競爭優勢。\n\n我們正在尋求投資，以加速研發更有效的概念擦除技術，並將其商業化。這不僅是一項技術投資，更是一項對未來AI發展方向的投資，確保AI的發展符合倫理、安全和法律規範，並為社會帶來更大的價值。", "audio": "audios/2505.16174v1.mp3", "timestamp": "2025-05-25T00:57:43.910291"}
{"query": "AI", "id": "2505.16619v1", "url": "http://arxiv.org/abs/2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "published_date": "2025-05-22", "title_zh": "開放且永續的人工智慧：生命科學領域的挑戰、機遇與未來發展方向", "summary_zh": "人工智慧正在革新生命科學，但快速發展也帶來了可重複性、可再用性以及環境永續性等問題，導致信任度下降。本論文探討了AI生態系統的碎片化，並提出一套實用的開放且永續AI（OSAI）建議，涵蓋300多個AI生態系統組成部分，旨在促進生命科學研究的可持續、可重複和透明的人工智慧應用。", "applications": ["**加速新藥開發：** 透過可重複且透明的AI模型，藥廠能更快速地驗證藥物療效，降低研發成本和時間。", "**精準醫療診斷：** 基於開放數據和永續AI模型，醫生能更準確地診斷疾病，提供個人化的治療方案，提升醫療品質。", "**環境監測與生物多樣性保護：** 利用AI分析環境數據，監測污染狀況和生物多樣性變化，協助制定有效的保護策略，維護生態平衡。"], "pitch": "生命科學AI正處於爆發期，但缺乏標準化和永續性將阻礙其發展。我們的團隊提供一套完整的OSAI解決方案，協助生命科學公司和研究機構構建可信賴、可重複且具備環境意識的AI模型。透過提高研發效率、降低風險和實現數據共享，我們將加速生命科學領域的創新，創造巨大的商業價值。想像一下，一家藥廠利用我們的OSAI框架，在短短幾個月內成功開發出一種治療阿茲海默症的新藥，這將帶來數十億美元的收入和無數患者的福音。我們正在構建生命科學AI的未來，歡迎加入我們，共同引領這場變革！", "audio": "audios/2505.16619v1.mp3", "timestamp": "2025-05-25T01:10:59.166131"}
{"query": "Foundation Model", "id": "2505.14975v1", "url": "http://arxiv.org/abs/2505.14975v1", "title": "Flattening Hierarchies with Policy Bootstrapping", "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "authors": ["John L. Zhou", "Jonathan C. Kao"], "published_date": "2025-05-20", "title_zh": "利用策略引導展平層級結構", "summary_zh": "離線目標條件強化學習 (GCRL) 是一種很有前景的方法，可以在大量的無獎勵軌跡數據集上預訓練通用策略，類似於用於訓練計算機視覺和自然語言處理基礎模型的自監督目標。然而，由於稀疏獎勵和折扣的結合，使得原始動作相對於遠期目標的比較優勢變得模糊，將 GCRL 擴展到更長的時間範圍仍然具有挑戰性。層級強化學習方法在長時期目標達成任務上取得了強大的經驗結果，但它們對模塊化、特定時間尺度的策略和子目標生成的依賴引入了顯著的額外複雜性，並阻礙了擴展到高維目標空間。在這項工作中，我們引入了一種算法，通過對具有優勢加權重要性抽樣的子目標條件策略進行引導，來訓練一個扁平（非層級）的目標條件策略。我們的方法消除了對 (子) 目標空間的生成模型的需要，我們發現這是在大型狀態空間中擴展到高維控制的關鍵。我們進一步表明，現有的層級和基於引導的方法對應於我們推導中的特定設計選擇。在全面的基於狀態和基於像素的運動和操縱基準測試中，我們的算法與最先進的離線 GCRL 算法相匹配或超過，並可擴展到先前方法失敗的複雜的長時期任務。", "applications": ["**機器人流程自動化：** 讓機器人能夠學習複雜的組裝或維修任務，即使在訓練數據有限且沒有明確獎勵的情況下，例如自動組裝電子產品或修理汽車引擎。", "**虛擬人物控制：** 在遊戲或虛擬現實環境中，創造能夠自主完成複雜任務的虛擬人物，例如在開放世界遊戲中導航、社交互動或執行複雜的動作。", "**個性化醫療：** 根據患者的歷史數據和目標（例如，控制血糖、減肥），制定個性化的治療方案。模型可以學習在沒有明確獎勵的情況下，哪些干預措施對患者有效。"], "pitch": "這項研究成果解決了離線強化學習在長程目標達成任務中的關鍵瓶頸。通過展平層級結構，消除了對複雜子目標生成模型的依賴，顯著提高了算法的可擴展性和效率。這使得我們能夠在計算機視覺、機器人流程自動化以及個性化醫療等領域開發更強大、更通用的 AI 解決方案。其潛在的商業價值在於大幅降低開發成本、提高自動化程度、並實現前所未有的個性化服務，為相關產業帶來顛覆性變革。", "audio": "audios/2505.14975v1.mp3", "timestamp": "2025-05-25T01:11:26.182020"}
{"query": "Diffusion Model", "id": "2505.16174v1", "url": "http://arxiv.org/abs/2505.16174v1", "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "authors": ["Ping Liu", "Chi Zhang"], "published_date": "2025-05-22", "title_zh": "擦除還是休眠？透過可逆性重新思考概念擦除", "summary_zh": "這篇論文探討了概念擦除技術在扩散模型中消除生成能力的效果。現有的技術主要集中在特定文字提示下的概念抑制，但本文深入研究這些技術是否真正移除了生成目標概念的能力，還是僅僅實現了表面上的、提示特定的抑制。研究人員通過測試兩種代表性的概念擦除方法，發現這些方法雖然能抑制潛在的生成表示，但並不能完全消除它們。經過輕微的調整，被“擦除”的概念通常會重新出現，表明需要更深層次的干預和更嚴格的評估標準，才能確保從生成模型中真正且不可逆地移除概念。", "applications": ["**內容審核與過濾：** 用於過濾生成模型產生的不適當內容，例如仇恨言論、暴力圖像或色情內容。若僅為表面抑制，則模型可能在稍作調整後再度生成違規內容，突顯了不可逆擦除的重要性。", "**個性化教育：** 針對特定學習者調整生成式學習工具，避免生成可能引起反感或不適的圖像或文本。例如，避免生成涉及學生創傷經歷的內容。", "**品牌保護：** 確保品牌生成內容時，不會產生與競爭對手相關或損害品牌形象的內容。確保擦除競品概念的不可逆性，避免在不知情下生成競品相關內容。"], "pitch": "我們發現現有的AI概念擦除技術存在重大缺陷，無法真正且不可逆地移除生成模型中的有害或不適當概念。這為我們提供了一個巨大的商業機會，開發更有效的概念擦除解決方案，確保生成式AI的安全性、合規性和商業價值。我們的技術將賦能企業安全地使用生成式AI，避免潛在的法律和聲譽風險，並開創個性化內容和安全AI的新市場。這個市場規模龐大，隨著生成式AI的普及，對於可信賴的內容過濾和控制解決方案的需求將持續增長。我們的團隊擁有領先的研究能力和技術實力，能夠率先解決這個關鍵挑戰，成為行業領導者。", "audio": "audios/2505.16174v1.mp3", "timestamp": "2025-05-25T01:11:50.317313"}
{"query": "AI", "id": "2505.16619v1", "url": "http://arxiv.org/abs/2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "published_date": "2025-05-22", "title_zh": "開放且永續的人工智慧：生命科學領域的挑戰、機會與未來發展之路", "summary_zh": "人工智慧在生命科學領域迎來突破性進展，為研究人員以前所未有的能力解讀生物資訊提供了可能。然而，AI快速普及也加劇了長期存在的科研挑戰，如AI研究成果的可重複使用性與可再現性不佳，進而侵蝕信任並影響環境永續性。本文探討了AI生態系統的碎片化問題，並提出一套基於生命科學社群共識、與現有努力保持一致的「開放且永續AI (OSAI)」實用建議，旨在連接研究人員與相關AI資源，促進可持續、可重複使用和透明的AI實施，並為未來政策制定和AI實施的結構化路徑提供指導。", "applications": ["**加速新藥開發：** 透過可重複使用的AI模型，更快速篩選潛在藥物靶點，降低藥物開發成本。", "**精準醫療診斷：** 開放且透明的AI診斷模型，能夠提高診斷準確性，減少誤診，為患者提供個性化治療方案。", "**永續農業優化：** 利用AI分析土壤、氣候等數據，開發可持續的農業模型，提高作物產量，減少對環境的負擔。"], "pitch": "生命科學領域正在經歷AI驅動的革命，但重複性差、永續性不足的問題阻礙了發展。我們的OSAI解決方案提供了一個標準化的框架和資源庫，確保AI模型的可重複使用、可持續發展。這不僅能加速新藥開發、精準醫療等領域的創新，更能為生命科學研究帶來更高的投資回報率。想像一下，一個開放的AI平台，讓全球的研究人員協同合作，共同解決人類面臨的健康挑戰。這不僅是一個技術投資，更是一個對人類福祉的投資。 我們正在打造一個開放、透明、可信賴的AI生態系統，引領生命科學的未來。", "audio": "audios/2505.16619v1.mp3", "timestamp": "2025-05-25T01:24:56.495559"}
{"query": "Foundation Model", "id": "2505.14975v1", "url": "http://arxiv.org/abs/2505.14975v1", "title": "Flattening Hierarchies with Policy Bootstrapping", "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "authors": ["John L. Zhou", "Jonathan C. Kao"], "published_date": "2025-05-20", "title_zh": "利用策略引導展平層級結構", "summary_zh": "離線目標條件強化學習 (GCRL) 是一種很有潛力的技術，可以在大型無獎勵軌跡數據集上預訓練通用策略，就像用於訓練電腦視覺和自然語言處理基礎模型的自我監督目標一樣。 然而，由於稀疏獎勵和折扣的結合，使得原始動作相對於遠期目標的比較優勢變得模糊，因此將 GCRL 擴展到更長的時間範圍仍然具有挑戰性。層級強化學習方法在長時程目標達成任務上取得了強大的經驗結果，但它們對模組化、特定時間尺度的策略和子目標生成的依賴引入了顯著的額外複雜性，並阻礙了擴展到高維目標空間。 在這項工作中，我們引入了一種算法，通過使用優勢加權重要性採樣引導子目標條件策略，來訓練一個扁平的（非層級的）目標條件策略。 我們的算法消除了對（子）目標空間的生成模型的需求，我們發現這對於在高維狀態空間中擴展到高維控制至關重要。 我們進一步表明，現有的基於層級和引導的方法對應於我們推導中的特定設計選擇。 在一套全面的基於狀態和像素的運動和操作基準測試中，我們的方法與最先進的離線 GCRL 算法相匹配或超過了它們，並且可以擴展到以前的方法失敗的複雜的長時程任務。", "applications": ["**機器人家庭助理：**訓練機器人完成複雜的家務，例如從冰箱取出飲料並送到指定地點。透過學習分解任務，機器人可以更有效率地完成長序列動作。", "**自動駕駛：**在模擬環境中訓練自動駕駛汽車，使其能夠安全地完成長途駕駛任務，例如從一個城市導航到另一個城市，而無需人工標記大量獎勵數據。", "**遊戲AI：**開發更聰明的遊戲AI，讓AI角色能夠制定長期策略，例如在即時戰略遊戲中有效地管理資源和執行複雜的攻擊計畫。"], "pitch": "這項技術突破能夠讓機器學習模型，特別是機器人，以更有效率的方式學習執行複雜、長期的任務。傳統上，這需要複雜的層級結構，但我們的'展平'方法不僅簡化了流程，還提升了效能，尤其是在高維環境中。想像一下，不再需要大量的人工數據，AI就能夠自主學習執行多步驟的任務，例如組裝產品、駕駛車輛，甚至是照護病人。其商業價值在於降低了開發成本，提高了自動化的可能性，並開創了全新的應用場景。我們的技術能夠讓機器人更智能、更自主，從而改變製造業、物流業、醫療保健等產業。我們需要資金來進一步優化算法，並將其應用於不同的領域，以證明其廣泛的適用性和商業可行性。我們相信，我們的技術將成為下一代自主系統的基礎。", "audio": "audios/2505.14975v1.mp3", "timestamp": "2025-05-25T01:25:19.548329"}
{"query": "Diffusion Model", "id": "2505.16174v1", "url": "http://arxiv.org/abs/2505.16174v1", "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "authors": ["Ping Liu", "Chi Zhang"], "published_date": "2025-05-22", "title_zh": "抹除還是休眠？透過可逆性重新思考概念抹除", "summary_zh": "這篇論文探討了現有的概念抹除技術，像是用來過濾掉擴散模型中不想要的內容，究竟是真正移除了模型生成特定概念的能力，還是只是表面上的抑制。研究發現，現有的抹除方法其實只是讓這些概念暫時休眠，透過輕微的調整就能讓它們重新出現。這代表現在的技術只能抑制潛在的生成表現，而無法徹底根除這些概念。研究強調需要更深入的干預措施和更嚴格的評估標準，才能真正且不可逆地從生成模型中移除概念。", "applications": ["**內容安全過濾器強化：** 開發更可靠的內容過濾器，防止生成帶有偏見、歧視或有害內容的圖像，即使模型經過惡意調整也難以繞過。", "**IP保護與藝術風格控制：** 藝術家或品牌可以保護自己的風格，防止未經授權的模仿，並更精準地控制AI生成的圖像風格，確保不會意外出現不想要的風格元素。", "**模型客製化與微調：** 用戶可以更精確地移除預訓練模型中不必要的概念，為特定應用客製化模型，例如醫療影像分析中移除會分散注意力的雜訊，提升診斷準確性。"], "pitch": "現有的AI圖像生成模型存在潛在的風險，可能會生成不當或侵權內容。我們基於這項研究，開發了一種更強大的概念抹除技術，能真正且不可逆地從模型中移除特定概念，而非僅僅是表面抑制。這項技術將成為AI圖像生成平台的基礎安全層，降低法律風險、保護IP，並賦予用戶更精確的控制權。我們的商業模式包括技術授權、開發工具套件，以及為企業提供客製化的抹除服務。目標客戶包含但不限於：AI圖像生成平台、內容審核服務供應商、以及擁有大量IP的企業。這項技術解決了目前市場上缺乏有效內容安全方案的痛點，具備巨大的商業潛力，預計將在AI圖像生成市場中佔據領先地位。", "audio": "audios/2505.16174v1.mp3", "timestamp": "2025-05-25T01:25:33.984659"}
{"query": "AI", "id": "2505.16619v1", "url": "http://arxiv.org/abs/2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "published_date": "2025-05-22", "title_zh": "開放且永續的人工智慧：生命科學領域的挑戰、機會與前進之路", "summary_zh": "人工智慧在生命科學領域取得了突破性進展，但快速採用也加劇了研究挑戰，例如可重用性、可再現性差導致信任度下降，並對環境永續性產生影響。研究指出人工智慧生態系統的碎片化以及缺乏指導方針支持開放且永續的人工智慧(OSAI)模型開發。為此，研究提出了實用的OSAI建議，對應於人工智慧生態系統的300多個組件，旨在幫助研究人員連結相關資源，促進永續、可重用和透明的人工智慧應用。這些建議基於生命科學界的共識，並與現有努力方向一致，旨在協助制定未來政策和結構化路徑，以指導人工智慧的實施。", "applications": ["**加速新藥開發：** 透過更容易重現和使用的AI模型，縮短新藥研發週期，更快找到治療疾病的有效藥物。", "**精準醫療：** 基於透明且可解釋的AI模型，醫生能更準確地診斷疾病，並為患者提供個性化的治療方案。", "**永續農業：** 利用AI模型分析作物生長數據，優化灌溉和施肥方案，減少資源浪費，提高農業生產效率。"], "pitch": "生命科學領域的AI應用潛力巨大，但可重現性和透明度問題阻礙了其發展。我們的解決方案透過提供開放且永續的AI模型開發框架，解決這些核心問題。這將帶來以下商業價值：\n\n* **加速AI在生命科學領域的應用：** 提高AI模型的可靠性和可用性，縮短研發週期，加速產品上市。\n* **降低研發成本：** 透過資源共享和標準化流程，減少重複性工作，降低AI模型開發和維護成本。\n* **建立信任：** 提高AI模型的透明度和可解釋性，建立使用者對AI技術的信任，促進其廣泛應用。\n* **創造新的商業模式：** 基於開放的AI平台，可以開發各種增值服務，例如模型訓練、資料分析和諮詢服務。我們正在打造一個可信賴、永續發展的生命科學AI生態系統，為投資者帶來長期回報。", "audio": "audios/2505.16619v1.mp3", "timestamp": "2025-05-25T03:16:04.371463"}
{"query": "Foundation Model", "id": "2505.14975v1", "url": "http://arxiv.org/abs/2505.14975v1", "title": "Flattening Hierarchies with Policy Bootstrapping", "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "authors": ["John L. Zhou", "Jonathan C. Kao"], "published_date": "2025-05-20", "title_zh": "利用策略引導平坦化階層結構", "summary_zh": "離線目標條件強化學習(GCRL)有望在大型無獎勵軌跡數據集上預訓練通用策略，類似於電腦視覺和自然語言處理中用於訓練基礎模型的自我監督目標。然而，由於稀疏獎勵和折扣的組合，GCRL擴展到更長的時間跨度仍然具有挑戰性，這模糊了原始動作相對於遠距離目標的相對優勢。階層式強化學習方法在長時間跨度目標達成任務中取得了強大的實證結果，但其對模組化、特定時間尺度的策略和子目標生成之依賴引入了顯著的額外複雜性，並阻礙了擴展到高維目標空間。在這項工作中，我們引入了一種算法，通過使用優勢加權重要性抽樣來引導子目標條件策略，從而訓練平坦（非階層式）目標條件策略。我們的辦法消除了對(子)目標空間生成模型的需求，我們發現這對於在大狀態空間中擴展到高維控制至關重要。我們進一步表明，現有的基於階層式和引導式的方法對應於我們推導中的特定設計選擇。在全面的基於狀態和基於像素的運動和操縱基準測試套件中，我們的方法與最先進的離線GCRL算法相匹配或超過，並且可以擴展到複雜的、長時間跨度的任務，而先前的方法則失敗。", "applications": ["**自動化機器人組裝線：** 機器人無需人工編程，即可學習複雜的組裝流程，例如將不同零件組裝成完整的產品，即使零件的放置位置不完全一致。", "**個性化健身教練：** AI可以分析用戶的運動數據，並根據用戶的長期目標（例如減肥、增肌）制定個性化的訓練計劃，即使用戶的動作不標準，也能引導他們朝著正確的方向努力。", "**智能家居環境控制：** 系統可以學習用戶的偏好，例如溫度、濕度和光線，並自動調整家居環境，以最大限度地提高用戶的舒適度，即使用戶的行為模式發生變化。"], "pitch": "我們正在開發一種突破性的離線強化學習技術，它能讓AI像人類一樣學習複雜任務，無需大量實時互動和標籤數據。我們的算法擺脫了傳統階層式強化學習的複雜性，通過策略引導，使AI能夠在更廣泛的應用場景中高效學習和執行長程目標。這種技術具有巨大的商業潛力，可以應用於自動化、機器人、醫療保健、金融等領域，大幅降低開發成本，提高效率和智能化水平。我們的團隊擁有多年的AI研究經驗，並擁有堅實的技術基礎。我們正在尋求種子輪投資，以加速產品開發和市場拓展，共同打造下一代智能解決方案。", "audio": "audios/2505.14975v1.mp3", "timestamp": "2025-05-25T03:16:26.388050"}
{"query": "Diffusion Model", "id": "2505.16174v1", "url": "http://arxiv.org/abs/2505.16174v1", "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "authors": ["Ping Liu", "Chi Zhang"], "published_date": "2025-05-22", "title_zh": "抹除還是休眠？透過可逆性重新思考概念擦除", "summary_zh": "現有的概念擦除技術真的能徹底移除生成模型產生特定概念的能力嗎？這篇論文研究了兩種具代表性的概念擦除方法：Unified Concept Editing 和 Erased Stable Diffusion。研究發現，這些方法實際上並未完全移除概念，僅僅是表面上的抑制。即使經過擦除，這些概念在經過輕微的微調後，仍能重新以高度的視覺保真度生成出來。這表明目前的技術僅僅是抑制了潛在的生成表示，而沒有完全消除它們。因此，需要更深入、更底層的干預手段和更嚴格的評估標準，以確保從生成模型中真正且不可逆地移除概念。", "applications": ["**內容審查強化：** 對於生成式AI模型，例如圖像生成模型，徹底擦除涉及仇恨言論、暴力內容或不適當圖像的概念，避免模型在任何情況下（即使是惡意提示）生成相關內容，確保平台的內容安全性和合規性。", "**智慧財產權保護：** 永久性地移除模型生成侵權內容的能力，例如，防止模型生成與特定品牌形象過於相似的設計，避免版權糾紛，並促進合法合規的使用。", "**個人化體驗優化：** 允許使用者選擇性地擦除或抑制某些概念，以便根據個人偏好定制AI模型的行為。例如，移除可能觸發負面情緒的圖像概念，或者避免模型生成使用者不感興趣的內容，从而提升用户体验。"], "pitch": "我們開發的技術能真正、不可逆地從生成式AI模型中移除特定概念，而非僅僅是表面上的抑制。這解決了目前概念擦除技術的關鍵缺陷，提供了更強大的內容審查、智慧財產權保護和客製化能力。考慮到生成式AI模型的廣泛應用以及對安全、合規和個人化的日益增長的需求，我們的技術具有巨大的商業潛力。我們可以授權這項技術給大型科技公司、內容平台和安全機構，或開發一套獨立的AI安全產品，以確保生成式AI模型的安全和負責使用，創造可觀的營收和社會效益。", "audio": "audios/2505.16174v1.mp3", "timestamp": "2025-05-25T03:16:47.301977"}
{"query": "AI", "id": "2505.16619v1", "url": "http://arxiv.org/abs/2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "published_date": "2025-05-22", "title_zh": "開放且永續的人工智慧：生命科學領域的挑戰、機會與未來發展之路", "summary_zh": "人工智慧在生命科學領域取得突破性進展，但快速發展也加劇了長期存在的挑戰，例如可重複性差、重用性低，進而影響環境永續性。此外，AI生態系統碎片化，缺乏開放且永續的AI模型開發指導路徑。本研究針對AI生態系統的300多個組件，提出了一套實用的開放且永續AI（OSAI）建議，旨在連接研究人員與相關資源，促進永續、可重複使用和透明的人工智慧實施，並為未來的政策制定和AI實施路徑提供指引。", "applications": ["**加速新藥開發：** 通過開放且可重複使用的AI模型，研究人員可以更快速地識別潛在的藥物靶點，縮短新藥研發週期，降低成本。", "**個性化醫療：** 基於透明且可解釋的AI模型，醫生可以根據患者的基因組、生活方式等信息，制定個性化的治療方案，提高治療效果。", "**農業優化：** 利用開放的AI模型分析作物生長數據、氣候數據等，幫助農民更好地管理農作物，提高產量和效率，同時減少農藥和化肥的使用。"], "pitch": "在生命科學領域，AI正以前所未有的速度發展，帶來巨大的商業機會。然而，缺乏開放性、透明性和可重複性，阻礙了其潛力的充分釋放。我們的研究提供了一套實用的開放且永續AI（OSAI）框架，旨在解決這些痛點，加速AI在生命科學領域的應用。通過投資我們的OSAI平台，您將能夠：1) 參與新藥研發、個性化醫療、農業優化等高增長市場；2) 建立可信賴且合規的AI解決方案，降低風險；3) 提升企業的永續發展形象。我們相信，開放且永續的AI是未來生命科學發展的關鍵，而我們的平台將是您成功的基石。", "audio": "audios/2505.16619v1.mp3", "timestamp": "2025-05-25T04:20:25.451469"}
{"query": "Diffusion Model", "id": "2505.16174v1", "url": "http://arxiv.org/abs/2505.16174v1", "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "authors": ["Ping Liu", "Chi Zhang"], "published_date": "2025-05-22", "title_zh": "抹除還是休眠？從可逆性的角度重新思考概念抹除", "summary_zh": "這篇論文探討了現有的概念抹除技術，例如用於圖片生成的擴散模型中，是否真的能完全移除特定概念的生成能力，還是僅僅在特定提示詞下抑制了這些概念的生成。研究發現，常用的概念抹除方法，實際上只是讓這些概念進入「休眠」狀態，透過輕量级的微調，就能讓這些被「抹除」的概念重新顯現，而且視覺效果相當好。這表明現有的方法並未真正消除模型中這些概念的潛在表示，需要更深入、針對表示層級的干預，以及更嚴格的評估標準，才能確保真正、不可逆地從生成模型中移除概念。", "applications": ["**內容過濾與審查：** 針對新聞、社群媒體等平台，確保模型生成的圖像不會包含不當內容，如仇恨言論、暴力畫面等。以往認為已抹除的功能可能只是潛伏，此研究有助於開發更有效的過濾系統。", "**保護知識產權：**  確保 AI 模型不會生成侵犯版權的圖像，例如特定角色的風格或特定商標。研究可以幫助開發者徹底移除模型中與版權相關的元素，降低侵權風險。", "**客製化模型訓練：** 在訓練生成模型時，避免模型學習到特定的偏差或不良特徵。例如，避免生成帶有種族歧視或性別歧視的圖像。研究可以協助開發者更精準地控制模型的學習內容，打造更公正的模型。"], "pitch": "現今AI模型越來越強大，但隨之而來的風險，例如產生不當內容或侵犯智慧財產權，也日益增長。我們的研究揭示了現有概念抹除技術的局限性，證明其只能讓特定概念休眠，無法徹底消除。這個發現具有巨大的商業價值。我們將開發更有效的概念抹除技術，為企業提供更安全、可靠的AI模型。這項技術可以應用於內容審查、知識產權保護、客製化模型訓練等多個領域。想像一下，一個能夠完全消除特定風險概念的AI模型，將為企業帶來巨大的競爭優勢，並減少潛在的法律風險。我們的技術將成為AI安全領域的基石，打造一個更值得信賴的AI生態系統。我們尋求創投的資金支持，加速技術開發，並將這項技術推向市場，成為AI安全領域的領導者。", "audio": "audios/2505.16174v1.mp3", "timestamp": "2025-05-25T04:20:46.257354"}
{"query": "AI", "id": "2505.16619v1", "url": "http://arxiv.org/abs/2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "published_date": "2025-05-22", "title_zh": "開放且永續的AI：生命科學領域的挑戰、機會與未來發展之路", "summary_zh": "AI在生命科學領域取得了突破性進展，但隨之而來的可重複性、可重用性問題正在削弱人們對AI研究成果的信任，並影響環境永續性。 本論文旨在檢視AI生態系統的碎片化問題，並提出一套實用的開放且永續AI (OSAI) 建議，將研究人員與相關AI資源連接起來，促進AI在生命科學領域的永續、可重複和透明應用，為未來的政策制定和AI實施提供指導。", "applications": ["**加速新藥開發：** 透過可重複且透明的AI模型，更快速且可靠地分析生物數據，縮短新藥開發週期，降低開發成本。", "**精準醫療診斷：** 利用開放且永續的AI模型，整合患者的基因組數據、影像資料和臨床病歷，實現更精準的疾病診斷和個人化治療方案。", "**環境保護與生物多樣性監測：** 應用AI分析大量的環境數據，例如動植物分布、水質監測等，預測環境變化趨勢，制定更有效的保護策略。"], "pitch": "我們正面臨生命科學領域AI發展的關鍵轉捩點。雖然AI潛力巨大，但缺乏可重複性和透明度正阻礙其廣泛應用。我們的研究提供一套實用的開放且永續AI (OSAI) 框架，旨在解決這些問題。透過投資我們的OSAI解決方案，您將能: 1) 顯著加速新藥開發和精準醫療，搶佔市場先機；2) 建立一個更具透明度和可信度的AI生態系統，提升企業形象；3) 參與建構更永續的AI發展模式，符合ESG投資趨勢。這不僅是一項技術投資，更是一項對未來醫療和環境永續發展的戰略投資，將帶來巨大的商業價值和社會效益。", "audio": "audios/2505.16619v1.mp3", "timestamp": "2025-05-25T05:12:54.094966"}
{"query": "Foundation Model", "id": "2505.14975v1", "url": "http://arxiv.org/abs/2505.14975v1", "title": "Flattening Hierarchies with Policy Bootstrapping", "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "authors": ["John L. Zhou", "Jonathan C. Kao"], "published_date": "2025-05-20", "title_zh": "利用策略引導扁平化層級結構", "summary_zh": "離線目標導向強化學習 (GCRL) 有潛力利用大規模無獎勵軌跡資料集預訓練通用策略，類似於電腦視覺和自然語言處理中用於訓練基礎模型的自監督目標。然而，由於稀疏獎勵和折扣的結合，使得原始動作相對於遠程目標的相對優勢變得模糊，因此將 GCRL 擴展到更長的時間範圍仍然具有挑戰性。層級強化學習方法在長程目標到達任務中取得了強大的實證結果，但它們對模組化、特定時間尺度的策略和子目標生成的依賴引入了顯著的額外複雜性，並阻礙了擴展到高維目標空間。在這項工作中，我們提出了一種算法，通過使用優勢加權重要性採樣引導子目標導向策略來訓練扁平（非層級）目標導向策略。我們的方法消除了對（子）目標空間生成模型的需求，我們發現這對於在大狀態空間中擴展到高維控制至關重要。我們進一步表明，現有的層級和基於引導的方法對應於我們推導中的特定設計選擇。在全面的基於狀態和像素的運動和操縱基準測試套件中，我們的方法與最先進的離線 GCRL 算法相匹配或超越，並可擴展到以前方法失敗的複雜、長程任務。", "applications": ["**智慧家庭控制：** 機器人可以學習如何透過觀察人類行為來執行複雜的家庭任務，例如準備晚餐、整理房間，無需明確的獎勵信號。例如，觀察到有人打開冰箱、拿出食材、使用爐子等步驟，機器人就能學習準備一道菜。", "**自動駕駛：** 透過大量駕駛數據，訓練自動駕駛系統學習應對各種複雜路況，例如在沒有明確指令的情況下，自主變換車道、超車、避讓行人，並安全到達目的地。系統能根據周圍環境和過去的經驗調整駕駛策略。", "**工業機器人協作：** 在複雜的生產線上，機器人可以通過觀察人類工人的操作，學習執行各種複雜的裝配任務，例如組裝電子產品、汽車零件等，並且能夠與人類工人進行協作，提高生產效率。"], "pitch": "我們開發了一種創新的策略引導技術，可以有效地訓練複雜的長程目標導向機器人，克服了傳統層級強化學習的局限性。這項技術的關鍵優勢在於它不需要手動設計子目標，而是通過觀察大量數據自動學習高效策略。想像一下，一個機器人只需觀察人類行為，就能夠在複雜的環境中執行各種任務。這種能力在智慧家庭、自動駕駛和工業自動化等領域具有巨大的商業潛力。透過我們的方法，可以大幅降低開發和部署複雜機器人系統的成本，加速機器人在各行各業的應用，創造巨大的市場價值。", "audio": "audios/2505.14975v1.mp3", "timestamp": "2025-05-25T05:13:14.310192"}
{"query": "Diffusion Model", "id": "2505.16174v1", "url": "http://arxiv.org/abs/2505.16174v1", "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "authors": ["Ping Liu", "Chi Zhang"], "published_date": "2025-05-22", "title_zh": "抹除還是休眠？透過可逆性重新思考概念抹除", "summary_zh": "這篇論文探討了在擴散模型中，概念抹除在多大程度上真正消除了生成特定概念的能力。研究發現，現有的概念抹除技術，例如Unified Concept Editing和Erased Stable Diffusion，實際上並沒有完全移除模型生成目標概念的能力，而是只實現了表面的、提示詞特定的抑制。研究者透過輕量級的微調，成功地重新激活了被「抹除」的概念，證明這些概念仍然潛藏在模型中。因此，現有的方法僅僅抑制了潛在的生成表徵，而沒有完全消除它們。這項研究揭示了當前概念抹除方法的局限性，並強調需要更深入、表示層級的干預和更嚴格的評估標準，以確保從生成模型中真正、不可逆地移除概念。", "applications": ["**審查與過濾：** 針對特定敏感詞彙或圖像內容，例如仇恨言論或不適當圖像，進行概念抹除，避免AI生成包含這些內容的圖片。", "**風格轉換與客製化：** 抹除圖片中不需要的風格或元素，例如移除卡通風格，讓AI生成更逼真的圖像；或移除特定品牌Logo，創造更通用的視覺素材。", "**資料增強與模型安全性：** 在訓練AI模型之前，抹除訓練資料中的偏差或敏感資訊，例如抹除人臉識別資料中的種族信息，以提高模型的公平性和安全性。"], "pitch": "現有AI圖像生成模型在概念抹除方面存在嚴重缺陷，表面上的抹除其實只是休眠。這項研究揭示了這個漏洞，並指出了更有效的抹除策略方向。我們的商業價值在於：1. **安全強化：**開發真正有效且不可逆的概念抹除技術，讓AI圖像生成模型不再成為潛在的風險源，確保模型輸出符合法律和道德規範。2. **市場差異化：** 將更安全的AI生成技術授權給其他公司，賦予它們市場競爭力。3. **技術授權與諮詢服務：** 提供概念抹除相關的技術授權和諮詢服務，協助企業在AI應用中降低風險，提升合規性。4. **潛在併購價值：** 在AI安全領域建立領先地位，吸引大型科技公司或政府機構的併購。", "audio": "audios/2505.16174v1.mp3", "timestamp": "2025-05-25T05:13:31.021072"}
{"query": "AI", "id": "2505.16619v1", "url": "http://arxiv.org/abs/2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "published_date": "2025-05-22", "title_zh": "開放且永續的人工智慧：生命科學領域的挑戰、機遇與前進道路", "summary_zh": "這篇論文探討了人工智慧在生命科學領域的快速發展所帶來的機遇和挑戰。雖然AI極大地提升了生物信息分析能力，但同時也加劇了研究重現性和可重用性的問題，進而影響了研究成果的可信度和環境永續性。論文針對AI生態系統的碎片化問題，提出了開放且永續AI（OSAI）的實用建議，旨在幫助研究人員連結相關AI資源，實現更永續、可重用和透明的AI應用，並為未來的政策制定和AI實施提供指導。", "applications": ["**加速新藥研發：** 利用開放且可重用的AI模型，藥廠能更快速地篩選潛在藥物標靶，降低研發成本，並提高成功率。", "**個性化醫療：** 基於透明且可追溯的AI算法，醫生可以更精確地診斷疾病，制定個性化的治療方案，提升患者療效。", "**精準農業：** 通過開放的AI模型分析土壤、氣候和作物數據，農民可以更有效地管理農田，減少資源浪費，提高農作物產量和品質。"], "pitch": "我們正處於AI在生命科學領域應用爆發的時代，但數據孤島、模型不可解釋以及重現性差等問題嚴重阻礙了行業發展。我們的解決方案是提供一套開放且永續的AI框架(OSAI)，它不僅能解決這些根本問題，還能大幅降低研發成本，加速成果轉化。透過OSAI，我們將建立一個更加透明、高效和可信賴的AI生態系統，為生命科學領域的創新帶來指數級的增長。我們相信，OSAI將成為未來生命科學領域AI應用的黃金標準，具有巨大的市場潛力，並能創造長期可持續的價值。", "audio": "audios/2505.16619v1.mp3", "timestamp": "2025-05-25T06:17:47.022737"}
{"query": "Foundation Model", "id": "2505.14975v1", "url": "http://arxiv.org/abs/2505.14975v1", "title": "Flattening Hierarchies with Policy Bootstrapping", "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "authors": ["John L. Zhou", "Jonathan C. Kao"], "published_date": "2025-05-20", "title_zh": "利用策略引導展平層級結構", "summary_zh": "離線目標條件強化學習 (GCRL) 有望在大型無獎勵軌跡數據集上預訓練通用策略，類似於電腦視覺和自然語言處理中用於訓練基礎模型的自監督目標。然而，由於稀疏獎勵和折扣的結合，使得原始動作在遠期目標方面的相對優勢不明顯，因此將 GCRL 擴展到更長的視野仍然具有挑戰性。分層強化學習方法在長時程目標達成任務上取得了強大的經驗結果，但它們對模組化、特定時間尺度的策略和子目標生成的依賴引入了顯著的額外複雜性，並阻礙了擴展到高維目標空間。在這項工作中，我們引入了一種算法，通過使用優勢加權重要性抽樣對子目標條件策略進行引導，來訓練一個扁平（非分層）的目標條件策略。我們的方法消除了對（子）目標空間上的生成模型的需求，我們發現這對於在大型狀態空間中擴展到高維控制至關重要。我們進一步表明，現有的分層和基於引導的方法對應於我們推導中的特定設計選擇。在全面的基於狀態和基於像素的運動和操縱基準測試中，我們的方法與最先進的離線 GCRL 算法相匹配或超越，並可擴展到先前的算法失敗的複雜的長時程任務。", "applications": ["**機器人流程自動化：** 訓練機器人執行複雜的裝配或倉儲任務，例如在無人倉庫中自動揀選和放置物品，即使任務流程非常長且涉及多個步驟。", "**自動駕駛行為規劃：** 在模擬環境中訓練自動駕駛汽車，使其能夠安全高效地處理長距離駕駛，例如規劃整個城市的路線，並在遇到突發狀況時做出正確決策。", "**遊戲AI：** 訓練遊戲中的 AI 角色執行更複雜的任務，例如策略遊戲中的長期戰略規劃，讓AI能夠學習如何資源分配、軍事調動等，最終贏得勝利。"], "pitch": "我們正在開發一種革命性的強化學習算法，能夠讓機器人和 AI 系統在沒有明確獎勵的情況下，自主學習執行複雜的、長時程的任務。傳統方法需要大量的標註數據和人工干預，而我們的技術則能利用海量未標註數據，像訓練大型語言模型一樣訓練 AI，大幅降低開發成本，並加速 AI 應用落地。想像一下，一個能夠自主學習工廠操作流程的機器人，或者一個能夠在複雜交通環境中安全駕駛的自動駕駛系統，我們的技術將賦予 AI 更強的適應性和通用性，在機器人、自動駕駛、遊戲等領域帶來巨大的商業價值。我們正在尋找投資者，一同開創 AI 的新紀元。", "audio": "audios/2505.14975v1.mp3", "timestamp": "2025-05-25T06:18:04.385509"}
{"query": "Diffusion Model", "id": "2505.16174v1", "url": "http://arxiv.org/abs/2505.16174v1", "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "authors": ["Ping Liu", "Chi Zhang"], "published_date": "2025-05-22", "title_zh": "抹除還是休眠？透過可逆性重新思考概念抹除", "summary_zh": "這篇論文探討了現有的概念抹除技術，在擴散模型中，是否真的能徹底消除生成特定概念的能力。研究發現，現有的方法大多只是表面上的壓制，通過簡單的微調，被「抹除」的概念就能重新出現，並且具有相當高的視覺逼真度。這意味著現有技術只是抑制了潛在的生成表示，並沒有完全消除它們。論文強調，需要更深入的表示層級干預和更嚴格的評估標準，才能確保真正且不可逆地從生成模型中移除概念。", "applications": ["**內容審查工具增強：** 現有內容審查系統可能誤判已被「抹除」的概念，導致潛在的風險內容被忽略。了解抹除技術的局限性，可以幫助開發更可靠、更精準的內容審查工具。", "**個性化廣告的道德考量：** 如果廣告模型「抹除」了用戶的某些偏好（例如，為了避免冒犯），但這些偏好實際上仍然存在於模型中，那麼重新激活這些偏好的可能性就會引發隱私和操縱方面的問題。", "**藝術創作的安全保障：** 藝術家可以使用生成模型進行創作，但他們可能希望確保模型不會生成某些特定的內容（例如，侵犯版權的圖像）。如果抹除技術不夠可靠，那麼藝術家就可能面臨法律風險。"], "pitch": "各位投資人，我們發現現有的AI內容審查和圖像生成控制技術存在重大漏洞。論文揭示，聲稱已「抹除」的概念，實際上只是被休眠，只需少量微調就能重新激活。這不僅帶來法律和道德風險，也限制了AI技術在商業應用中的可靠性。我們的團隊正在開發下一代概念抹除技術，目標是實現真正、不可逆的概念移除。這將為AI內容審查、個性化廣告、以及藝術創作提供更安全、更可控的環境。想像一下，一個能夠完全消除仇恨言論或不雅圖像的AI系統，其市場潛力巨大！我們相信，通過我們的技術，可以打造更值得信賴的AI生態系統，並在快速增長的AI市場中佔據領先地位。我們正在尋求資金支持，以加速研發進程，並將我們的技術推向市場，成為AI安全領域的領導者。", "audio": "audios/2505.16174v1.mp3", "timestamp": "2025-05-25T06:18:20.693780"}
{"query": "AI", "id": "2505.16619v1", "url": "http://arxiv.org/abs/2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "published_date": "2025-05-22", "title_zh": "開放且永續的AI：生命科學領域的挑戰、機遇與未來發展方向", "summary_zh": "人工智慧在生命科學領域取得了突破性進展，但快速採用也加劇了研究中長期存在的問題，例如AI研究成果的可重複使用性和可再現性差，導致對AI研究產出的信任度下降，並對環境永續性產生影響。本文探討了AI生態系統的碎片化問題，並針對開放且永續的AI (OSAI) 模型開發，提出了一系列與AI生態系統的300多個組成部分直接相關的實用建議，旨在幫助研究人員找到相關的AI資源，促進永續、可重複使用和透明的AI實施，並為未來的AI政策制定提供指導。", "applications": ["**藥物研發加速：** 利用OSAI原則開發的模型，可以更快速、更可靠地篩選潛在的藥物候選物，縮短藥物開發週期，降低研發成本，並提高新藥上市成功率。", "**精準醫療診斷：** 基於開放且永續的AI模型，可以更準確地分析患者的基因、病歷和生活方式等數據，提供個性化的診斷和治療方案，提高治療效果。", "**環境監測與保護：** 利用OSAI AI模型分析生物數據、環境數據，例如生物多樣性、水質、空氣品質等，更有效地監測環境變化，預測潛在的環境風險，並制定相應的保護措施。"], "pitch": "我們正在構建一個開放、可信賴的AI平台，專注於生命科學領域。該平台基於開放且永續的AI原則，提供一系列可重複使用、透明且可持續的AI模型和工具，旨在加速藥物研發、促進精準醫療、支持環境保護。生命科學領域的AI市場潛力巨大，但缺乏統一的標準和可信賴的平台。我們的平台通過提供高品質、可驗證的AI解決方案，降低行業門檻，促進創新，並確保AI的應用符合倫理和環境要求。我們相信，這個平台將成為生命科學領域AI研究人員和企業的關鍵基礎設施，並為投資者帶來豐厚的回報，尤其是在藥物開發、診斷試劑、農業科技和環境監測等領域。", "audio": "audios/2505.16619v1.mp3", "timestamp": "2025-05-25T07:12:16.330180"}
{"query": "Foundation Model", "id": "2505.14975v1", "url": "http://arxiv.org/abs/2505.14975v1", "title": "Flattening Hierarchies with Policy Bootstrapping", "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "authors": ["John L. Zhou", "Jonathan C. Kao"], "published_date": "2025-05-20", "title_zh": "利用策略引導扁平化層級結構", "summary_zh": "離線目標條件強化學習 (GCRL) 是一種很有潛力的技術，能用大量無獎勵軌跡資料集預訓練通用策略，就像電腦視覺和自然語言處理中使用的自我監督目標訓練基礎模型一樣。然而，由於稀疏獎勵和折扣的結合，使得基本動作在遠程目標上的比較優勢變得模糊，將GCRL擴展到更長的時間範圍仍然具有挑戰性。層級強化學習方法在長時間範圍的目標達成任務上取得了優異的實證成果，但它們對模組化、特定時間尺度的策略和子目標生成的依賴，引入了顯著的額外複雜性，並阻礙了擴展到高維目標空間。在這項工作中，我們引入了一種算法，通過使用優勢加權重要性抽樣在子目標條件策略上進行引導，來訓練扁平（非層級）目標條件策略。 我們的研究方法消除了對（子）目標空間生成模型的需求，我們發現這是擴展到大型狀態空間中的高維控制的關鍵。 我們進一步表明，現有的基於層級和引導的方法對應於我們推導中的特定設計選擇。 在一套全面的基於狀態和基於像素的運動和操作基準測試中，我們的方法與最先進的離線GCRL算法相匹配或超越，並且可以擴展到先前方法失敗的複雜、長時間範圍的任務。", "applications": ["**機器人流程自動化 (RPA):** 訓練機器人完成複雜的工廠組裝任務，即使步驟繁多且目標難以立即實現（例如：精準組裝微型零件）。機器人能夠根據過去的數據學習最佳操作路徑，即使沒有明確的每一步獎勵。", "**自動駕駛：** 訓練自動駕駛車輛在複雜城市環境中導航，即使路線規劃漫長且存在多種可能的路徑（例如：在擁擠的城市中尋找停車位）。車輛可以學習利用先前駕駛數據來優化路線和駕駛行為。", "**個人化健康管理：** 根據用戶的健康數據和生活習慣，訓練AI模型來提供個性化的健康建議和運動計劃。即使長期效果難以立即評估，AI也能學習根據子目標（例如：每日步數、睡眠質量）來引導用戶達到整體健康目標。"], "pitch": "我們開發了一種革命性的強化學習算法，能讓AI在複雜、長時間的任務中表現出色，無需繁瑣的人工分層設計。這就像給AI裝上了一顆更聰明的大腦，能更快、更有效地學習和解決問題。 我們的技術在機器人、自動駕駛和醫療保健等領域具有巨大的應用潛力。想像一下，工廠裡的機器人可以自主完成複雜的組裝工作，無需人工編程；自動駕駛車輛可以更安全、更高效地在城市中導航；個人化的健康管理AI可以幫助人們實現長期的健康目標。我們正在尋找合作夥伴，共同將這項技術推向市場，改變未來的生活和工作方式，創造數十億美元的商業價值。", "audio": "audios/2505.14975v1.mp3", "timestamp": "2025-05-25T07:12:36.629513"}
{"query": "Diffusion Model", "id": "2505.16174v1", "url": "http://arxiv.org/abs/2505.16174v1", "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "authors": ["Ping Liu", "Chi Zhang"], "published_date": "2025-05-22", "title_zh": "抹除還是休眠？透過可逆性重新思考概念抹除", "summary_zh": "這篇論文探討了概念抹除技術在diffusion models中是否真的消除了生成特定概念的能力。現有的抹除方法通常只針對特定提示詞來評估概念的抑制效果。這篇研究通過探測兩種代表性的抹除方法（Unified Concept Editing和Erased Stable Diffusion）的魯棒性和可逆性，來檢驗它們是否真正移除了生成目標概念的能力。研究發現，這些方法只是抑制了潛在的生成表徵，並沒有完全消除它們。即使经过“抹除”，目标概念在经过轻微调整后仍然可以以相当高的视觉保真度重新出现。這顯示現有方法存在局限性，需要更深層次的干預和更嚴格的評估標準，以確保從生成模型中真正、不可逆地移除概念。", "applications": ["**審查與內容安全：** 自動識別並抑制生成式AI模型中產出仇恨言論、暴力內容或不雅圖片的能力，避免模型被濫用。", "**版權保護與IP安全：** 移除特定藝術家的風格、版權圖像或受保護的商標，防止AI模型生成侵權內容。", "**客製化與風格控制：** 讓使用者可以選擇性地移除或抑制模型中的特定風格或概念，更精確地控制AI生成的結果，例如在設計logo時移除特定字體或元素。"], "pitch": "我們發現現有的概念抹除技術並不可靠，這意味著生成式AI模型仍然存在濫用風險，例如生成有害或侵權內容。我們的研究揭示了這一關鍵漏洞，並為開發真正安全的AI模型提供了方向。我們的技術能更有效地清除模型中的有害概念，降低法律和聲譽風險。同時，它也為AI模型的商業應用打開了新的可能，例如，提供更精確的風格控制選項，滿足不同客戶的需求。投資我們，您將領先一步，在快速發展的AI安全領域佔據優勢地位，打造更安全、更可靠的AI生態系統。我们团队正在开发下一代概念抹除技术，目标是实现真正的不可逆转的抹除，这将成为AI内容生成的安全基石。", "audio": "audios/2505.16174v1.mp3", "timestamp": "2025-05-25T07:12:52.802173"}
{"query": "AI", "id": "2505.16619v1", "url": "http://arxiv.org/abs/2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "published_date": "2025-05-22", "title_zh": "開放且永續的人工智慧：生命科學領域的挑戰、機會與未來發展之路", "summary_zh": "人工智慧正迅速改變生命科學，但隨之而來的問題，例如重現性和再利用性不足，正在降低研究的可信度並影響環境永續性。本論文探討了這些問題，並提出了實用的「開放且永續的人工智慧」(OSAI) 建議，涵蓋 AI 生態系統的 300 多個組成部分，旨在幫助研究人員找到相關資源，實施永續、可重複使用且透明的人工智慧模型。我們的目標是基於社群共識，為未來 AI 政策和實施路徑的發展提供指導。", "applications": ["**個性化醫療診斷:** 利用開放的 AI 模型，結合患者的基因組、生活方式和病歷數據，提供更精準的疾病診斷和治療方案。", "**加速藥物開發:** 通過共享的 AI 模型和數據集，研究人員可以更快地識別潛在的藥物靶點，預測藥物的療效和副作用，從而縮短藥物開發週期。", "**環境監測與保護:** 使用 AI 模型分析生物多樣性數據、氣候數據和污染數據，及早發現環境問題並提出有效的解決方案，例如預測物種滅絕風險或優化汙水處理流程。"], "pitch": "生命科學領域的人工智慧應用潛力巨大，但缺乏標準和共享阻礙了發展。我們的「開放且永續的人工智慧」解決方案，旨在建立一個可信賴、高效且環保的 AI 生態系統，加速新藥開發、提升診斷精準度、並促進環境保護。透過標準化模型、開放數據和明確的實施路徑，我們能大幅降低開發成本、提升研究效率，並確保 AI 技術的長期可持續性。這將為生命科學領域帶來革命性的變革，創造巨大的商業價值，例如縮短藥物上市時間、降低臨床試驗成本，以及開闢新的個性化醫療市場。我們正在構建生命科學 AI 的未來，現在加入正是時候。", "audio": "audios/2505.16619v1.mp3", "timestamp": "2025-05-25T08:15:46.195209"}
{"query": "Foundation Model", "id": "2505.14975v1", "url": "http://arxiv.org/abs/2505.14975v1", "title": "Flattening Hierarchies with Policy Bootstrapping", "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "authors": ["John L. Zhou", "Jonathan C. Kao"], "published_date": "2025-05-20", "title_zh": "用策略引導展平層級結構", "summary_zh": "這篇論文提出了一種新的離線目標條件強化學習(GCRL)方法，旨在克服在長時程任務中訓練通用策略的挑戰。傳統GCRL受限於稀疏獎勵和折扣因子，難以分辨不同動作對達成遠期目標的貢獻。雖然階層式強化學習(Hierarchical RL)在長時程任務表現出色，但其複雜的結構和子目標生成阻礙了在高維目標空間的擴展。論文提出一種新的演算法，通過引導子目標條件策略，訓練一個扁平(非階層式)的目標條件策略。這種方法無需對（子）目標空間建立生成模型，這對於在高維狀態空間中擴展控制至關重要。實驗證明，該方法在運動和操作基準測試中，能與現有最先進的離線GCRL演算法相媲美甚至超越，並可擴展到先前方法無法處理的複雜長時程任務。", "applications": ["**家庭機器人任務規劃：** 訓練機器人執行複雜的家庭任務，例如從冰箱取食物並準備晚餐，而無需事先定義精確的子目標。機器人可以通過觀察人類完成任務的數據，學習如何達成最終目標。", "**自動駕駛路徑規劃：** 在城市交通數據中，訓練自動駕駛汽車學習複雜的駕駛策略，例如避開擁堵路段，選擇最佳路線到達目的地，並適應各種突發狀況，無需預先設定所有可能的子目標。", "**遊戲AI角色控制：** 讓遊戲AI角色學習更複雜的行為模式，例如在戰略遊戲中根據戰場局勢制定長期戰略，或在角色扮演遊戲中自主完成任務，而非僅僅執行預先編寫的指令。"], "pitch": "這項技術的核心價值在於它能夠讓AI系統在複雜、長時程的任務中，僅僅通過觀察和學習就能高效地達成目標，無需人工干預設計詳細的子目標和策略。這突破了傳統強化學習的瓶頸，打開了廣闊的商業應用空間。想像一下，一個可以自行學習並優化生產流程的工業機器人，一個能夠獨立完成複雜手術的醫療機器人，或者一個可以根據市場變化自動調整投資組合的金融AI。我們的技術將大幅降低開發成本，提升系統的適應性和智能化水平，在智能製造、醫療健康、金融服務等領域都擁有巨大的商業潛力。我們正在尋找合作夥伴，共同將這項技術推向市場，引領下一代AI革命。", "audio": "audios/2505.14975v1.mp3", "timestamp": "2025-05-25T08:16:02.075673"}
{"query": "Diffusion Model", "id": "2505.16174v1", "url": "http://arxiv.org/abs/2505.16174v1", "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "authors": ["Ping Liu", "Chi Zhang"], "published_date": "2025-05-22", "title_zh": "抹除還是休眠？透過可逆性重新思考概念抹除", "summary_zh": "這篇論文探討了現有的概念抹除技術，在擴散模型中，是否真正地消除了生成特定概念的能力，還是僅僅實現了表面的、針對特定提示詞的抑制。研究者評估了兩種具代表性的概念抹除方法（Unified Concept Editing 和 Erased Stable Diffusion）的穩健性和可逆性，發現被“抹除”的概念在經過輕微的微調後，通常能以相當高的視覺保真度重新出現。這表明現有的方法只是抑制了潛在的生成表達，而沒有完全消除它們。因此，論文強調了需要更深入、更具表達層次的干預，以及更嚴格的評估標準，以確保從生成模型中真正地、不可逆轉地移除概念。", "applications": ["**兒童內容過濾：**應用於生成兒童內容的AI模型，確保敏感或不適當的概念（如暴力、性暗示）被真正移除，而不是僅僅在特定情境下被抑制。例如，生成童話故事，即使使用模糊或隱晦的提示詞，也不會出現相關的負面意象。", "**品牌形象保護：**用於確保企業使用的AI生成內容不會無意間生成與競爭對手品牌相關的圖像或文字。即使是設計師無意間輸入了類似的提示詞，也能避免生成可能侵犯競爭對手商標權的內容。", "**政治宣傳防禦：**應用於新聞媒體或公共機構，防止AI模型生成帶有特定政治傾向或偏見的內容。即使是試圖操縱模型的用戶輸入暗示性提示詞，也能避免產生誤導性的信息。"], "pitch": "現在的AI生成模型存在“概念抹除”的假象，看似抹除了某些概念，但實際上只是休眠，隨時可能被重新激活。我們的研究揭示了這個漏洞，並指出了更有效、更安全的抹除方法的方向。想像一下，一個能夠真正移除敏感內容、避免品牌侵權、防止政治操縱的AI生成平台，這將極大地提升AI的安全性、可靠性和社會價值。我們將開發出更有效的概念抹除算法，並建立一套嚴格的評估標準，確保我們的模型能夠真正安全地應用於各種商業場景。這個市場潛力巨大，因為越來越多的企業和組織需要安全、可靠的AI生成內容，而我們將成為這個領域的領導者。", "audio": "audios/2505.16174v1.mp3", "timestamp": "2025-05-25T08:16:18.328673"}
{"query": "AI", "id": "2505.16619v1", "url": "http://arxiv.org/abs/2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "published_date": "2025-05-22", "title_zh": "開放且永續的人工智慧：生命科學領域的挑戰、機會與未來之路", "summary_zh": "生命科學領域正經歷AI的革命性突破，但隨之而來的問題是可重複性、可再用性和環境永續性的缺失，導致對AI研究成果的信任度降低。論文探討了AI生態系統的碎片化，缺乏支持開放且永續AI模型發展的指導路徑。為此，作者提出一套實用的開放且永續AI建議，並對應到AI生態系統的300多個組件，旨在連結研究人員與相關資源，促進永續、可重複使用且透明的AI應用，並為未來的政策和AI實施架構提供參考。", "applications": ["**加速藥物發現：** 利用開放且永續的AI模型，藥廠可以更快速且可靠地分析生物數據，加速藥物候選物的篩選與測試，降低開發成本並縮短上市時間。", "**精準醫療：** 開放的AI模型能讓醫生更好地利用患者的基因、生活方式和環境數據，制定更個性化的治療方案，提高治療效果並減少副作用。", "**疾病監測與預測：** 通過開放數據和AI模型，可以更有效地監測疾病傳播趨勢，預測疫情爆發，提前採取預防措施，降低公共衛生風險。"], "pitch": "我們正在構建一個生命科學領域的AI開放平台，解決AI發展中可重複性、可再用性和環境永續性的挑戰。通過整合數據、模型和計算資源，並提供清晰的指導路徑，我們將幫助藥廠、研究機構和醫療機構更高效地利用AI，加速藥物發現、推進精準醫療和改善公共衛生。我們的商業模式基於訂閱服務、數據分析服務和模型定制服務。生命科學AI市場潛力巨大，我們相信開放且永續的模式是未來趨勢，我們將成為這個領域的領導者。", "audio": "audios/2505.16619v1.mp3", "timestamp": "2025-05-25T09:12:35.533316"}
{"query": "Foundation Model", "id": "2505.14975v1", "url": "http://arxiv.org/abs/2505.14975v1", "title": "Flattening Hierarchies with Policy Bootstrapping", "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "authors": ["John L. Zhou", "Jonathan C. Kao"], "published_date": "2025-05-20", "title_zh": "利用策略引導展平層級結構", "summary_zh": "離線目標條件強化學習(GCRL)是預訓練通用型策略的潛力方法，它使用大量無獎勵的軌跡數據集，類似於電腦視覺和自然語言處理中訓練基礎模型的自監督目標。然而，由於稀疏獎勵和折扣的組合，GCRL在長時域上的擴展仍然具有挑戰性，這模糊了原始動作在達到遠程目標方面的比較優勢。分層強化學習方法在長時域目標達成任務上取得了很好的經驗效果，但它們對模組化、特定時間尺度的策略和子目標生成的依賴引入了顯著的額外複雜性，並阻礙了在高維目標空間中的擴展。在這項工作中，我們引入了一種算法，通過使用優勢加權重要性抽樣來引導子目標條件策略，從而訓練一個扁平的（非分層的）目標條件策略。我們的方法消除了對目標（子目標）空間的生成模型的需求，我們發現這是擴展到大型狀態空間中的高維控制的關鍵。我們進一步表明，現有的基於分層和引導的方法對應於我們推導中的特定設計選擇。在一套全面的基於狀態和像素的運動和操作基準測試中，我們的方法與最先進的離線GCRL算法相匹配或超過了它們，並且可以擴展到以前的方法失敗的複雜長時域任務。", "applications": ["**家用機器人導航：** 讓機器人在複雜的家居環境中，即使沒有明確的地圖，也能夠完成長距離的取物、搬運等任務，例如從臥室拿取遙控器到客廳。", "**自動駕駛車輛規劃：** 幫助自動駕駛汽車在複雜的交通環境中，規劃更長遠的路徑，例如繞過施工區域，提前預判擁堵情況，並採取相應的變道策略。", "**醫療機器人輔助手術：** 在微創手術中，機器人能夠根據醫生的指令，精準地操作器械到達手術目標位置，避免不必要的組織損傷，提高手術成功率。"], "pitch": "我們正在開發一種突破性的強化學習技術，可以解決機器人在複雜環境中執行長距離任務的難題。傳統的分層強化學習方法複雜且難以擴展，而我們的扁平化策略引導方法，能讓機器人更高效、更準確地完成任務，無需繁瑣的子目標規劃。想像一下，一個能夠自主規劃路線並完成複雜任務的家庭機器人，或是在手術室中精準無誤地輔助醫生的醫療機器人。這項技術的應用前景廣闊，涵蓋了機器人、自動駕駛、醫療等多個高增長領域。我們相信，憑藉這項技術，我們能夠在機器人智能領域建立領先地位，並為投資者帶來豐厚的回報。", "audio": "audios/2505.14975v1.mp3", "timestamp": "2025-05-25T09:12:51.175971"}
{"query": "Diffusion Model", "id": "2505.16174v1", "url": "http://arxiv.org/abs/2505.16174v1", "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "authors": ["Ping Liu", "Chi Zhang"], "published_date": "2025-05-22", "title_zh": "抹除還是休眠？通過可逆性重新思考概念抹除", "summary_zh": "現有的概念抹除技術真的能徹底移除生成模型生成特定概念的能力，還是僅僅實現了表面上的、針對特定提示詞的抑制？ 本研究通過探討兩種代表性的概念抹除方法，發現這些方法雖然可以修改模型參數以抑制不想要的語義概念，但通常只是抑制了潛在的生成表示，而沒有完全消除它們。通過少量微調，抹除的概念往往可以重新浮現，顯示現有方法存在局限性，需要更深入的表示層級干預和更嚴格的評估標準，以確保真正且不可逆地從生成模型中移除概念。", "applications": ["**內容審查與安全:** 防止AI生成包含仇恨言論、不雅內容或侵犯版權的圖像。例如，移除生成種族歧視圖像或deepfake的能力，以確保AI模型的負責任使用。", "**個性化體驗與定製:** 允許用戶在不完全移除某概念的情況下，暫時抑制其生成，以實現更精細的內容控制。例如，一個藝術家可以禁用AI生成特定風格，以便探索新的視覺方向，之後可以輕鬆恢復該風格。", "**醫學影像與診斷:** 在生成用於訓練或輔助診斷的醫學影像時，可以暫時移除某些不相關或可能造成誤導的解剖結構或病灶。例如，為了更好地研究心臟的特定部位，可以暫時“抹除”周圍組織，但保留恢復完整影像的能力。"], "pitch": "我們發現現有的AI概念抹除技術並不像想像中那樣有效，它們更像是讓概念進入休眠狀態而非徹底刪除。這帶來巨大的商業機會，一方面，對於需要嚴格內容控制的場景（如兒童教育內容、品牌形象維護），我們提供真正安全可靠的概念抹除解決方案，避免內容風險。另一方面，對於需要靈活調整AI生成內容的場景（如遊戲開發、廣告設計），我們提供可逆的概念抑制技術，讓用戶在創作過程中擁有更大的自由度和控制權。我們的技術不僅能提升AI內容的安全性，還能賦予使用者更強大的創作能力，具有廣闊的市場前景和巨大的商業價值。", "audio": "audios/2505.16174v1.mp3", "timestamp": "2025-05-25T09:13:04.014676"}
{"query": "AI", "id": "2505.16619v1", "url": "http://arxiv.org/abs/2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "published_date": "2025-05-22", "title_zh": "開放與永續AI：生命科學領域的挑戰、機遇與前進方向", "summary_zh": "這篇論文探討了人工智慧在生命科學領域的快速發展，以及由此衍生的研究挑戰，例如AI研究結果的可重複性、可重用性問題，以及對環境永續性的影響。論文重點關注AI生態系統的碎片化，並提出了一套實用的「開放與永續AI (OSAI)」建議，將研究人員與相關AI資源聯繫起來，促進永續、可重用且透明的AI實施。這項研究旨在協助制定政策和結構化的AI實施指導方針。", "applications": ["**加速新藥開發：** 利用OSAI原則，確保AI驅動的藥物發現過程透明且可重複，縮短新藥研發週期，降低成本，並提高成功率。", "**個性化醫療診斷：** 通過共享和驗證AI模型，可以更準確地診斷疾病，並根據個體差異制定更有效的治療方案，提高醫療質量。", "**改善農業生產：** 開放且可持續的AI模型可以幫助農民優化種植策略，預測病蟲害，提高作物產量，並減少對環境的影響。"], "pitch": "生命科學領域正經歷AI驅動的革命，但可重複性、永續性等問題阻礙了其發展。我們的OSAI框架旨在解決這些挑戰，打造一個開放、透明且可信賴的AI生態系統。這將加速新藥研發、個性化醫療、農業優化等領域的進展，產生巨大的經濟和社會效益。我們提供一套明確的實施指南，與現有資源對接，降低AI部署門檻，賦能各領域研究人員。我們的商業價值體現在：\n\n*   **提高研發效率：** 減少重複工作，縮短產品上市時間。\n*   **降低研發風險：** 通過驗證和共享模型，提高研究的可靠性。\n*   **開創新的商業模式：** 基於開放AI平台，發展數據分析、模型訓練、諮詢服務等多元業務。\n\n我們相信，投資於OSAI框架，就是投資於生命科學的未來，將帶來豐厚的回報。", "audio": "audios/2505.16619v1.mp3", "timestamp": "2025-05-25T10:13:12.093220"}
{"query": "Foundation Model", "id": "2505.14975v1", "url": "http://arxiv.org/abs/2505.14975v1", "title": "Flattening Hierarchies with Policy Bootstrapping", "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "authors": ["John L. Zhou", "Jonathan C. Kao"], "published_date": "2025-05-20", "title_zh": "利用策略引導扁平化層級結構", "summary_zh": "離線目標條件強化學習（GCRL）有潛力利用大量無獎勵軌跡資料集預訓練通用策略，類似於電腦視覺和自然語言處理中用於訓練基礎模型的自監督目標。然而，由於稀疏獎勵和折扣的結合，使得遠端目標下原始動作的相對優勢不明顯，因此將 GCRL 擴展到更長的時間範圍仍然具有挑戰性。層級強化學習方法在長時間範圍目標導向任務上取得了強大的實驗結果，但它們對模組化、特定時間尺度的策略和子目標生成的依賴引入了顯著的額外複雜性，並阻礙了在高維目標空間中的擴展。在這項工作中，我們引入了一種算法，通過基於優勢加權重要性抽樣的子目標條件策略進行引導，來訓練一個扁平（非層級）的目標條件策略。我們的做法消除了對（子）目標空間生成模型的需求，我們發現這是擴展到大型狀態空間中高維控制的關鍵。我們進一步表明，現有的層級和基於引導的方法對應於我們推導中的特定設計選擇。在一套全面的基於狀態和像素的運動和操縱基準測試中，我們的方法與最先進的離線 GCRL 算法相匹配或超越，並可擴展到先前方法失敗的複雜、長時間範圍任務。", "applications": ["**機器人流程自動化 (RPA):**  讓機器人能夠自主學習如何完成複雜的組裝任務，例如電子產品的裝配，無需人工編寫詳細的步驟，只要設定最終目標即可。想像一下一個可以自行學習如何組裝電腦的機器人，只要給它零件和最終組裝完成的電腦圖片，它就能自己探索和學習完成任務。", "**自動駕駛行為規劃:**  允許自動駕駛系統在複雜的交通環境中，自行規劃並執行更長期的駕駛策略，例如繞過擁堵路段，或是根據乘客的喜好選擇更舒適的路線，而無需事先定義所有可能的駕駛情境。讓汽車更像一個有經驗的司機，可以自主判斷和決策。", "**遊戲 AI 開發:**  創造更智能、更具適應性的遊戲 AI，讓 NPC 能夠更自然地與玩家互動，並根據遊戲世界的變化和玩家的行為，學習新的策略和技能。這將極大提升遊戲的沉浸感和挑戰性。"], "pitch": "想像一下，一個可以訓練機器人完成複雜任務，而無需編寫每一行程式碼的系統。我們的技術透過離線強化學習，讓機器人從現有的數據中學習，並自動生成最佳策略。這就像是給機器人一個大腦，讓它自己學習如何解決問題。這項技術的商業潛力巨大，因為它可以大幅降低機器人開發和部署的成本，並加速自動化進程。我們專注於解決長程規劃的挑戰，讓機器人能夠處理更複雜的任務，例如自主組裝、複雜環境導航等。投資我們的技術，就是投資於下一代智能自動化的未來，抓住市場先機，成為自動化革命的領導者。", "audio": "audios/2505.14975v1.mp3", "timestamp": "2025-05-25T10:13:32.942893"}
{"query": "Diffusion Model", "id": "2505.16174v1", "url": "http://arxiv.org/abs/2505.16174v1", "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "authors": ["Ping Liu", "Chi Zhang"], "published_date": "2025-05-22", "title_zh": "抹除還是休眠？透過可逆性重新思考概念抹除", "summary_zh": "這篇論文探討了目前在生成模型，特別是擴散模型中，概念抹除技術的實際效果。研究發現，現有的概念抹除方法，例如統一概念編輯和抹除穩定擴散，實際上並沒有真正消除模型生成特定概念的能力，而只是實現了表面上的、提示詞特定的抑制。研究人員通過輕量級微調，重新激活了被認為已抹除的概念，結果顯示這些概念可以以相當高的視覺保真度重新出現。這表明現有方法只是抑制了潛在的生成表示，並沒有完全消除它們。因此，論文強調需要更深入的、表示層面的干預和更嚴格的評估標準，以確保從生成模型中真正、不可逆地移除概念。", "applications": ["**內容審核與安全：** 防止生成帶有仇恨言論或敏感內容的圖片，確保AI生成內容符合倫理標準。", "**客製化創作工具：** 允許使用者精確控制生成模型，例如移除特定風格或元素，以創造出獨一無二的藝術作品或設計。", "**智慧財產權保護：** 移除模型生成侵犯智慧財產權內容的能力，例如消除生成特定角色或品牌的圖像，防止盜版和侵權。"], "pitch": "現今AI生成模型如雨後春筍般湧現，但其生成的內容往往難以控制，存在倫理、安全和法律風險。本研究揭示了現有概念抹除技術的局限性，表明它們只是表面功夫，並未真正消除生成模型產生特定概念的能力。我們正在開發更深層、更有效的概念抹除技術，以確保AI生成內容的安全可控。想像一下，一個能完全杜絕生成有害或侵權內容的AI，將會為內容創作、審核、智慧財產權保護等領域帶來革命性的改變。這項技術不僅能降低企業的法律風險，更能提升AI應用的社會價值，具有巨大的商業潛力。我們正在尋求投資，共同打造一個更安全、更負責任的AI未來。", "audio": "audios/2505.16174v1.mp3", "timestamp": "2025-05-25T10:13:48.404882"}
{"query": "AI", "id": "2505.16619v1", "url": "http://arxiv.org/abs/2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "published_date": "2025-05-22", "title_zh": "開放且永續的人工智慧：生命科學領域的挑戰、機會與未來發展之路", "summary_zh": "生命科學領域的人工智慧正經歷變革性突破。為了最大化基於人工智慧的生命科學研究的投資回報並加速發展，解決快速採用人工智慧方法所帶來的長期研究挑戰變得非常迫切。本文探討了由於可重用性和可複製性差等問題導致的對人工智慧研究成果信任度下降，及其對環境永續性的影響。此外，還討論了人工智慧生態系統的碎片化以及缺乏指導開放且永續人工智慧（OSAI）模型開發的最佳途徑。為此，本文提出了一套實用的OSAI建議，直接對應於人工智慧生態系統的300多個組件，旨在將研究人員與相關的人工智慧資源連接起來，促進永續、可重用和透明的人工智慧的實施。該研究基於生命科學界的共識並與現有工作相一致，旨在幫助未來制定政策和結構化路徑，以指導人工智慧的實施。", "applications": ["**加速新藥開發:** 開放且可重用的人工智慧模型可以幫助藥廠更快地識別潛在的藥物靶點，並預測藥物的有效性和安全性，從而縮短藥物開發週期。", "**精準醫療診斷:** 利用透明且可解釋的人工智慧算法，醫生可以更準確地診斷疾病，並為患者提供個性化的治療方案，減少誤診和漏診。", "**環境監測與保護:** 使用人工智慧分析大量的環境數據，例如空氣和水質數據，可以更有效地監測環境污染，並預測自然災害的發生，從而制定更有效的保護措施。"], "pitch": "生命科學領域的人工智慧正處於爆發期，但研究成果的不可重現性、不透明性以及缺乏統一標準阻礙了其發展。我們的研究提供了一套開放且永續的人工智慧框架，旨在解決這些問題，提高研究效率和可信度。透過這個框架，我們可以大幅縮短新藥研發週期、提升診斷準確性、並更有效地保護環境。這不僅能為生命科學帶來革命性的進展，還能創造巨大的商業價值。想像一下，一家能夠迅速推出有效且安全的藥物的公司，或者一家能夠提供精準診斷服務的醫療機構，他們的市場競爭力將會得到質的提升。因此，投資於我們的開放且永續的人工智慧框架，就是投資於生命科學的未來，並有望獲得豐厚的回報。", "audio": "audios/2505.16619v1.mp3", "timestamp": "2025-05-25T12:23:21.995803"}
{"query": "Foundation Model", "id": "2505.14975v1", "url": "http://arxiv.org/abs/2505.14975v1", "title": "Flattening Hierarchies with Policy Bootstrapping", "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "authors": ["John L. Zhou", "Jonathan C. Kao"], "published_date": "2025-05-20", "title_zh": "利用策略自舉法扁平化層級結構", "summary_zh": "本研究提出一種新的離線目標導向強化學習方法，透過策略自舉的方式，訓練一個扁平（非層級式）的目標條件策略。這種方法利用優勢權重的重要性抽樣，在子目標條件策略上進行自舉。與傳統的層級式強化學習相比，我們的方法無需生成子目標，簡化了複雜性，並能更好地擴展到高維目標空間和大型狀態空間。實驗結果顯示，我們的方法在多種運動和操作基準測試中，都能達到或超越現有技術水平，特別是在複雜、長期的任務中表現出色。", "applications": ["**機器人居家服務：** 讓機器人能夠在複雜的居家環境中，完成一系列任務，例如：從廚房拿取食材到客廳，或者協助整理玩具。由於環境複雜且任務流程長，傳統方法難以有效訓練，但利用此技術可以更容易訓練出能完成多步驟操作的機器人。", "**自動駕駛輔助系統：** 提升自動駕駛系統在複雜交通情境下的決策能力，例如：在高流量的城市道路上，安全地變換車道並抵達目的地。透過離線資料學習，系統能夠從大量的駕駛數據中學習，即使遇到未曾出現過的情境也能做出正確的判斷。", "**智慧製造流程優化：** 在製造工廠中，讓機器人能夠更靈活地完成複雜的組裝和檢測任務，例如：精確地組裝電子產品或檢測產品的缺陷。由於生產流程變化快速，利用此技術可以快速調整機器人的行為，提升生產效率。"], "pitch": "我們解決了離線強化學習在長線任務上的擴展性問題，傳統層級式強化學習方法複雜且難以推廣到高維度環境，而我們的策略自舉法，在簡化模型複雜度的同時，還能高效學習長線任務。這項技術的潛在商業價值體現在：\n\n*   **更快的部署速度：** 由於無需生成子目標，訓練速度更快，部署時間更短。\n*   **更廣泛的應用場景：** 能夠應對更複雜、更長線的任務，例如自動駕駛、機器人服務等。\n*   **更低的運營成本：** 學習到的策略更通用，所需的人工干預更少，降低了運營成本。\n\n我們相信，這項技術將革新離線強化學習領域，為各行各業帶來更高效、更智能的解決方案。尋求投資，以擴大我們的研究團隊，並將這項技術商業化，推向市場。", "audio": "audios/2505.14975v1.mp3", "timestamp": "2025-05-25T12:23:37.882058"}
{"query": "Diffusion Model", "id": "2505.16174v1", "url": "http://arxiv.org/abs/2505.16174v1", "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "authors": ["Ping Liu", "Chi Zhang"], "published_date": "2025-05-22", "title_zh": "抹除還是休眠？透過可逆性重新思考概念抹除", "summary_zh": "這篇論文探討了現有的概念抹除技術，在生成模型（如Diffusion Model）中，究竟是徹底消除了特定概念的生成能力，還是僅僅在特定提示詞下抑制了這些概念的生成？研究團隊發現，目前流行的概念抹除方法，例如Unified Concept Editing和Erased Stable Diffusion，實際上並未完全移除目標概念的生成能力，這些被「抹除」的概念，在經過輕微的微調後，依然可以重新出現。這表示現有的方法只是在模型內部潛藏了這些概念，並未真正地從根本上消除它們。因此，論文呼籲需要更深入、更具代表性的干預措施，以及更嚴格的評估標準，以確保能夠真正且不可逆地從生成模型中移除概念。", "applications": ["**內容審查與版權保護：** 確保生成模型不再產生侵犯版權或包含敏感資訊的圖像，例如特定人物肖像或受保護的品牌標誌，避免法律風險。", "**風格控制與藝術創作：** 精確地移除或抑制特定藝術風格或元素，讓藝術家能夠更精準地控制AI生成的內容，並創造出更符合其意圖的作品。", "**數據安全與隱私保護：** 從訓練數據中移除敏感信息，如個人身份信息或專有技術數據，以防止AI模型在生成內容時洩漏這些信息，保障數據安全。"], "pitch": "想像一下，過去我們以為已經刪除的敏感資料，其實只是被藏起來了，隨時可能被輕易地恢復。這正是目前AI模型概念抹除的現狀。我們的研究揭示了這個漏洞，為AI安全領域帶來了革命性的機會。我們正在開發一種新型的、真正不可逆的概念抹除技術，它能從AI模型的核心層面徹底移除不需要的概念，解決了潛在的法律、安全和商業風險。這項技術的潛在市場巨大，涵蓋內容審查、版權保護、數據安全等各個領域。投資我們的團隊，您將不僅僅是投資一個技術，更是投資一個更安全、更可控的AI未來。", "audio": "audios/2505.16174v1.mp3", "timestamp": "2025-05-25T12:23:51.409222"}
{"query": "AI", "id": "2505.16619v1", "url": "http://arxiv.org/abs/2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "published_date": "2025-05-22", "title_zh": "開放且永續的人工智慧：生命科學領域的挑戰、機遇與未來發展方向", "summary_zh": "人工智慧在生命科學領域取得了突破性進展，但隨之而來的是研究結果可重用性和可再現性不足，導致信任度下降，影響環境永續性。此外，AI生態系統零散，缺乏明確的指導路徑來支持開放且永續的AI模型開發。本文提出了一系列實用的OSAI建議，涵蓋AI生態系統的300多個組成部分，旨在連接研究人員與相關的AI資源，促進永續、可重用和透明的AI實施。這些建議基於生命科學界的共識，並與現有努力相結合，旨在幫助制定未來的政策和結構化路徑，以指導AI的實施。", "applications": ["**加速藥物發現：** 利用開放且永續的AI模型，研究人員可以更容易地分享和驗證藥物候選物的預測結果，加速新藥開發流程，降低成本。", "**個性化醫療診斷：** 基於共享的AI模型，醫生可以更精確地診斷疾病，並根據患者的基因和生活方式，提供個性化的治療方案，提高治療效果。", "**精準農業：** 利用AI分析農作物生長數據，預測病蟲害發生，優化灌溉和施肥，提高農業生產效率，同時減少對環境的影響。"], "pitch": "我們正在構建一個開放且永續的AI平台，專注於生命科學領域。我們的平台將提供一系列工具和資源，幫助研究人員、醫療機構和企業開發、共享和驗證AI模型。通過消除AI開發的壁壘，促進知識共享，並確保AI模型的透明度和可追溯性，我們相信我們的平台能夠加速生命科學領域的創新，解決重大的健康和環境挑戰。我們的商業模式包括訂閱費用、數據分析服務和模型開發諮詢。生命科學AI市場巨大且不斷增長，我們相信我們有潛力成為這個領域的領先平台，為投資者帶來可觀的回報。", "audio": "audios/2505.16619v1.mp3", "timestamp": "2025-05-25T13:21:10.021346"}
{"query": "Foundation Model", "id": "2505.14975v1", "url": "http://arxiv.org/abs/2505.14975v1", "title": "Flattening Hierarchies with Policy Bootstrapping", "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "authors": ["John L. Zhou", "Jonathan C. Kao"], "published_date": "2025-05-20", "title_zh": "利用策略引導展平層級結構", "summary_zh": "離線目標條件強化學習 (GCRL) 是一種很有前景的方法，可以在大量的無獎勵軌跡數據集上預訓練通用策略，類似於用於訓練計算機視覺和自然語言處理基礎模型的自監督目標。然而，由於稀疏獎勵和折扣的結合，GCRL 擴展到更長的時程仍然具有挑戰性，這使得原始動作相對於遠期目標的比較優勢變得模糊。層級強化學習方法在長時程目標到達任務上取得了良好的實證結果，但它們對模塊化、特定時間尺度的策略和子目標生成的依賴引入了顯著的額外複雜性，並阻礙了擴展到高維目標空間。在這項工作中，我們引入了一種算法，通過基於子目標條件策略進行引導，並採用優勢加權重要性抽樣，來訓練一個展平的（非層級的）目標條件策略。 我們的做法消除了對目標（子目標）空間生成模型的需求，我們發現這是在大型狀態空間中擴展到高維控制的關鍵。 我們進一步表明，現有的基於層級和引導的方法對應於我們推導中的特定設計選擇。 在一套全面的基於狀態和像素的運動和操縱基準測試中，我們的方法與最先進的離線 GCRL 算法相匹配或超過，並且可以擴展到先前的研究方法失敗的複雜的、長時程任務。", "applications": ["**機器人廚師：** 訓練機器人執行複雜的烹飪任務，例如從冰箱裡取出食材，按照食譜完成一道菜，而無需手動編程每個步驟。 使用該算法，機器人可以從過去的成功或失敗的烹飪嘗試中學習，逐步提升廚藝。", "**自動駕駛汽車：** 讓自動駕駛汽車在複雜的城市環境中更好地導航。 傳統的自動駕駛系統可能需要大量的手動標註和調整，而該算法可以讓汽車自主學習如何應對各種交通狀況，例如繞過障礙物、變換車道和遵守交通規則。", "**倉庫自動化：** 提升倉庫機器人的效率，使其能夠更快、更準確地完成揀貨、包裝和運輸等任務。 該算法可以讓機器人從大量的歷史數據中學習最佳的搬運路線和操作策略，從而提高整體倉庫的運營效率。"], "pitch": "我們正在開發一種革命性的 AI 算法，它能讓機器人在複雜環境中自主學習並完成長時程任務，無需昂貴的手動編程。 我們的技術基於策略引導的展平層級結構方法，能克服傳統強化學習算法在處理長時程和高維目標空間時的挑戰。 相較於現有方法，我們的算法更高效、更易於擴展，並且在多個基準測試中取得了領先的性能。 我們相信我們的技術將對自動化、機器人、自動駕駛等行業產生深遠影響，帶來巨大的商業價值。 想像一下，一個完全自動化的倉庫，一個能夠安全高效地駕駛汽車的 AI 系統，或者一個能夠勝任各種複雜任務的機器人助手。 這就是我們的技術的潛力，我們正在努力將其變為現實。 我們正在尋找合作夥伴和投資者，共同打造未來智能自動化的藍圖。", "audio": "audios/2505.14975v1.mp3", "timestamp": "2025-05-25T13:21:36.920288"}
{"query": "Diffusion Model", "id": "2505.16174v1", "url": "http://arxiv.org/abs/2505.16174v1", "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "authors": ["Ping Liu", "Chi Zhang"], "published_date": "2025-05-22", "title_zh": "被抹除還是休眠？從可逆性角度重新思考概念抹除", "summary_zh": "這篇論文探討了目前的概念抹除技術是否真的能徹底消除擴散模型中特定概念的生成能力。現有的方法主要關注在特定提示詞下概念的抑制，但本文研究了更根本的問題：這些技術只是表面上的抑制，還是真正移除了生成能力？研究者評估了兩種代表性的概念抹除方法，發現被「抹除」的概念在經過輕微調整後，往往能夠重新浮現，表明現有方法僅抑制了潛在的生成表徵，而沒有完全消除它們。因此，需要更深入、更底層的干預和更嚴格的評估標準，以確保從生成模型中真正且不可逆地移除概念。", "applications": ["**安全內容過濾:** 在AI圖像生成應用中，徹底移除仇恨言論、暴力內容或敏感資訊的概念，避免模型產生有害內容。", "**品牌保護:** 確保AI生成的内容不包含未授權的品牌標誌、商標或人物肖像，防止侵權行為。", "**藝術風格控制:**  允许艺术家更精确地控制AI生成的图像风格，例如永久性地去除某种风格的特征，创造出独特的、前所未有的艺术作品。"], "pitch": "目前AI模型的內容審核和安全控制高度依賴概念抹除技術，但這篇論文揭示了這些技術的局限性，表明現有方法並不能完全消除模型生成特定概念的能力。這為我們提供了一個巨大的機會，開發更有效、更可靠的概念抹除解決方案。我們的商業模式可以包括：\n\n*   **軟體授權:** 將我們開發的強化概念抹除技術授權給AI模型開發商和平台。\n*   **諮詢服務:** 為企業提供定制化的概念抹除解決方案，幫助他們保護品牌、遵守法規、並構建更安全可靠的AI應用。\n*   **API服務:** 提供API接口，讓第三方應用程式能夠輕鬆整合我們的概念抹除功能。\n\n這個領域擁有巨大的市場潛力，因為隨著AI技術的普及，對安全、可靠和可控的AI內容生成的需求將會持續增長。我們的技術能解決現有概念抹除方案的根本問題，為客戶提供更有效、更持久的解決方案，從而在這個快速發展的市場中佔據領先地位。我們相信，我們能夠打造一個更安全、更可信賴的AI生態系統。", "audio": "audios/2505.16174v1.mp3", "timestamp": "2025-05-25T13:21:56.402811"}
{"query": "AI", "id": "2505.16619v1", "url": "http://arxiv.org/abs/2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "published_date": "2025-05-22", "title_zh": "開放且永續的AI：生命科學領域的挑戰、機會與未來之路", "summary_zh": "這篇論文探討了人工智慧在生命科學領域的快速發展所帶來的機會與挑戰。雖然AI極大地提升了生物信息解讀能力，但也加劇了研究結果的可重複性、可重用性和環境永續性問題，導致對AI研究產出的信任度降低。論文進一步指出AI生態系統的碎片化以及缺乏指導開放且永續AI（OSAI）模型開發的路徑。為了解決這些問題，論文提出了一系列實用的OSAI建議，涵蓋AI生態系統的300多個組成部分，旨在連接研究人員與相關AI資源，促進永續、可重用且透明的AI實施，並為未來的政策制定提供指導。", "applications": ["**藥物研發加速：** 利用OSAI原則，讓藥廠更容易重用、驗證和改進現有的AI模型，加速藥物篩選和臨床試驗設計，降低研發成本。", "**精準醫療診斷：** 通過公開、透明的AI模型，醫生可以更準確地分析患者數據，進行個性化治療方案的制定，並提高診斷的準確性。", "**農業優化：** 使用OSAI模型分析土壤、氣候和作物數據，可以幫助農民做出更明智的決策，提高作物產量，並減少對環境的影響。例如，預測病蟲害爆發，優化灌溉方案。"], "pitch": "生命科學AI正迎來爆發期，但可重複性、永續性問題阻礙了其發展。這篇論文的核心價值在於提出了可行的OSAI框架，能有效解決這些痛點，加速AI在藥物研發、精準醫療、農業等領域的應用。我們投資的不是單一AI模型，而是整個OSAI生態系統的建設。這個生態系統具有強大的網絡效應：越多人參與，模型越可靠、應用越廣泛，數據集也越豐富。通過投資OSAI，我們可以打造一個開放、可信任的AI基礎設施，為生命科學的各個領域賦能，最終創造巨大的經濟價值和社會效益，並在這個千億美元級別的市場中佔據領導地位。", "audio": "audios/2505.16619v1.mp3", "timestamp": "2025-05-25T14:11:35.068745"}
{"query": "Foundation Model", "id": "2505.14975v1", "url": "http://arxiv.org/abs/2505.14975v1", "title": "Flattening Hierarchies with Policy Bootstrapping", "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "authors": ["John L. Zhou", "Jonathan C. Kao"], "published_date": "2025-05-20", "title_zh": "使用策略引導來扁平化層級結構", "summary_zh": "離線目標條件強化學習(GCRL)有望在大型無獎勵軌跡數據集上預訓練通用策略，類似於用於訓練計算機視覺和自然語言處理基礎模型的自監督目標。然而，由於稀疏獎勵和折扣的結合，使得原始動作相對於遙遠目標的比較優勢變得模糊，將GCRL擴展到更長的時程仍然具有挑戰性。分層強化學習方法在長時程目標達成任務上取得了強勁的經驗結果，但它們對模塊化、時間尺度特定的策略和子目標生成的依賴引入了顯著的額外複雜性，並阻礙了擴展到高維目標空間。在這項工作中，我們引入了一種算法，通過使用優勢加權重要性採樣在子目標條件策略上進行引導，來訓練扁平（非分層）目標條件策略。我們的方法消除了對（子）目標空間的生成模型的需求，我們發現這是擴展到大型狀態空間中的高維控制的關鍵。我們進一步表明，現有的分層和基於引導的方法對應於我們推導中的特定設計選擇。在一套全面的基於狀態和像素的運動和操作基準測試中，我們的方法與最先進的離線GCRL算法相匹配或超過了它們，並且可以擴展到先前的算法失敗的複雜、長時程任務。", "applications": ["**機器人家庭助手：** 訓練一個機器人，使其能夠完成一系列複雜的家務，例如整理房間、準備餐點，而無需人工持續編程指導，只需要提供最終目標，機器人可以自我學習如何分解任務並完成。", "**自動駕駛汽車：** 訓練自動駕駛系統，使其能夠在複雜且不可預測的交通環境中安全地導航，例如在沒有明確路線指導的情況下，到達特定目的地。", "**虛擬遊戲角色 (AI Agent)：** 創造更具智能和適應性的遊戲角色，能夠根據玩家的行為和遊戲環境，自主地制定策略和完成任務，提升遊戲體驗。"], "pitch": "我們開發了一種全新的離線強化學習技術，可以讓AI在沒有大量人工干預下，學習完成複雜的、長期的目標。這項技術克服了傳統分層強化學習的複雜性問題，大幅簡化了AI訓練的流程。其潛在市場非常廣闊，包括機器人、自動駕駛、遊戲AI等等。我們的優勢在於：1）顯著降低了訓練成本和時間；2）可以處理更複雜、更長期的任務；3）在現有測試中表現超越其他技術。我們相信這項技術將會是下一代AI發展的關鍵，擁有巨大的商業價值，例如提供授權技術給相關產業、將此技術與既有產品整合等等，投資回報潛力巨大。", "audio": "audios/2505.14975v1.mp3", "timestamp": "2025-05-25T14:11:57.848137"}
{"query": "Diffusion Model", "id": "2505.16174v1", "url": "http://arxiv.org/abs/2505.16174v1", "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "authors": ["Ping Liu", "Chi Zhang"], "published_date": "2025-05-22", "title_zh": "抹除還是休眠？透過可逆性重新思考概念抹除", "summary_zh": "現有概念抹除技術真的能完全移除生成模型中特定概念的生成能力嗎？這篇論文探討了這個問題。研究發現，目前的概念抹除技術，像是Unified Concept Editing和Erased Stable Diffusion，其實只是表面上的抑制，而非真正的移除。即使經過這些技術處理，只要稍微調整，被「抹除」的概念仍然能夠以高擬真度重新浮現。這代表現有的技術只是抑制了潛在的生成表示，並沒有完全消除。研究結果表明，現有的概念抹除方法存在重大缺陷，需要更深入的表示層面的干預以及更嚴格的評估標準，才能確保從生成模型中真正、不可逆地移除概念。", "applications": ["**內容審核與安全：** 應用於移除生成式AI模型中的有害概念，例如暴力、仇恨言論等，確保模型產出的內容符合道德規範和法律法規。例如，電玩遊戲公司可以避免AI生成不適合兒童的圖像。", "**個性化內容創建：** 在保護用戶隱私的前提下，讓用戶可以安全地控制AI生成內容中包含或排除的特定元素，例如，允許使用者排除個人照片中的某些背景元素，產生更精緻的個人化圖像。", "**藝術風格控制與變形：** 允許藝術家或設計師在生成式AI模型中精確控制風格元素，並可安全地移除或變更特定風格，從而實現更具實驗性的藝術創作。例如，可以從模型中移除特定畫家的風格，以確保生成的作品風格獨特，不涉及版權問題。"], "pitch": "現有AI模型存在一個隱患：所謂的『概念抹除』，實際上並非真正抹除，而是潛伏休眠。這項研究揭示了這個漏洞，並為我們提供了一個契機。試想一下，如果我們能開發出真正可靠的概念抹除技術，就能打造更安全、更可控的AI內容生成平台。這將直接解決內容審核、用戶隱私保護等關鍵痛點，例如避免AI生成仇恨言論或侵犯版權的內容，從而釋放AI在各個領域的巨大潛力。我們的技術將成為AI安全領域的基礎設施，擁有廣泛的應用前景和巨大的商業價值。我們需要更深入地干預表示層面，並建立更嚴格的評估標準，來確保AI模型中的概念抹除是真正且不可逆的。這不僅是一個技術問題，更是一個關乎AI倫理和社會責任的重大議題，先行者將佔據市場制高點。", "audio": "audios/2505.16174v1.mp3", "timestamp": "2025-05-25T14:12:27.967114"}
{"query": "AI", "id": "2505.16619v1", "url": "http://arxiv.org/abs/2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "published_date": "2025-05-22", "title_zh": "開放與永續AI：生命科學領域的挑戰、機遇與前進方向", "summary_zh": "這篇論文探討了AI在生命科學領域快速發展所帶來的挑戰。雖然AI極大地提升了生物資訊的解讀能力，但也加劇了研究結果的可重複性、可重用性以及對環境永續性的影響等問題。論文提出了一系列開放與永續AI (OSAI) 的實用建議，旨在連接研究人員與相關AI資源，促進可持續、可重複使用且透明的AI應用，並為未來的政策制定和AI實施提供結構化的指導。", "applications": ["**加速新藥開發：** 利用OSAI原則，研究人員可以更容易地共享和驗證藥物開發數據和模型，從而加速新藥的發現和臨床試驗。", "**精準醫療診斷：** 通過建立可信賴且可重複使用的AI模型，醫生可以更準確地診斷疾病，並根據個人基因組和生活方式制定更精準的治療方案。", "**環境保護與生物多樣性監測：** 利用OSAI原則開發的AI模型可以更有效地分析環境數據和生物多樣性數據，從而更好地監測環境變化並保護生態系統。"], "pitch": "在生命科學領域，AI的應用正呈現爆發式增長。然而，研究結果的可重複性和透明度不足，阻礙了AI的真正潛力。我們的解決方案基於開放與永續AI (OSAI) 的原則，旨在建立一個可信賴、可共享的AI生態系統。通過提供實用的建議和連接研究人員與相關資源，我們將加速新藥開發、精準醫療和環境保護等領域的創新。我們的商業模式可以基於提供OSAI諮詢服務、AI模型驗證平台以及數據共享基礎設施。隨著AI在生命科學領域的應用越來越廣泛，我們相信OSAI將成為一個巨大的市場，為投資者帶來豐厚的回報。", "audio": "audios/2505.16619v1.mp3", "timestamp": "2025-05-25T15:12:33.022864"}
{"query": "Foundation Model", "id": "2505.14975v1", "url": "http://arxiv.org/abs/2505.14975v1", "title": "Flattening Hierarchies with Policy Bootstrapping", "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "authors": ["John L. Zhou", "Jonathan C. Kao"], "published_date": "2025-05-20", "title_zh": "使用策略引導來展平層級結構", "summary_zh": "這篇論文提出了一種新的離線目標條件強化學習(GCRL)算法，透過策略引導來訓練一個扁平(非階層式)的目標條件策略。此算法利用優勢加權重要性抽樣，透過次目標條件策略來引導學習。相較於傳統的階層式強化學習方法，此方法無需生成次目標空間的模型，更易於擴展到高維度控制和大型狀態空間。實驗結果表明，此方法在複雜、長期的任務中，表現超越了現有的離線GCRL算法。", "applications": ["**機器人組裝：** 教導機器人執行複雜的組裝任務，例如組裝家具或電子產品。不再需要人工設定每個步驟，機器人可以從大量的組裝影片中學習，自主完成任務。", "**自動駕駛路線規劃：** 在沒有明確獎勵的情況下，讓自動駕駛系統學習複雜的駕駛路線。例如，讓車輛從A地開到B地，即使中間遇到不同的路況和障礙，也能夠安全到達。", "**遊戲AI：** 訓練遊戲AI在複雜的遊戲環境中達成目標，例如在多人線上戰鬥競技場(MOBA)遊戲中，透過觀察玩家的操作，學習如何有效地推進和擊敗對手。"], "pitch": "我們開發了一種突破性的離線強化學習技術，能夠讓AI系統在沒有明確獎勵的情況下，從大量的歷史數據中學習複雜的任務。這就像教會機器人通過觀察影片自學組裝一樣。傳統的階層式方法複雜且難以擴展，而我們的扁平策略引導方法則解決了這個問題，大幅提升了學習效率和可擴展性。這項技術的潛在商業價值巨大：它可以應用於機器人自動化、自動駕駛、遊戲AI等領域，幫助企業降低成本、提高效率，並創造全新的產品和服務。我們正在尋求投資，以加速技術的商業化，並將這項技術推廣到更廣闊的市場。我們的團隊擁有深厚的技術積累和商業經驗，相信能夠將這項技術打造成為一個成功的商業模式。", "audio": "audios/2505.14975v1.mp3", "timestamp": "2025-05-25T15:12:49.928198"}
{"query": "Diffusion Model", "id": "2505.16174v1", "url": "http://arxiv.org/abs/2505.16174v1", "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "authors": ["Ping Liu", "Chi Zhang"], "published_date": "2025-05-22", "title_zh": "抹除還是休眠？透過可逆性重新思考概念擦除", "summary_zh": "現有的概念擦除技術，目的是消除生成式模型（如文生圖模型）中特定概念的能力。但論文探討這些技術是真的移除了生成能力，還是只是表面上、針對特定提示詞的抑制。研究發現，經過擦除後，這些概念仍然可以透過簡單的微調重新激活，顯示目前的擦除方法只是抑制了潛在的生成表徵，並沒有徹底消除它們。這表明現有的方法有局限性，需要更深入的干預和更嚴格的評估標準，才能確保概念從生成模型中真正且不可逆地移除。", "applications": ["**兒童內容過濾：** 擦除模型中暴力、色情等不適宜兒童的概念，並確保即使給出誘導性的提示詞，這些概念也不會重新生成，保護兒童的瀏覽體驗。", "**法律合規性應用：** 在法律禁止生成特定內容（如涉及特定政治人物的虛假訊息）的情況下，確保模型在任何提示下都無法生成相關內容，避免法律風險。", "**品牌形象維護：** 企業可以擦除模型中與品牌形象不符的概念，例如競爭對手的標誌，確保使用者在使用模型生成內容時，不會出現損害品牌形象的結果。"], "pitch": "我們發現現有AI模型的『概念擦除』技術並不徹底，這帶來了潛在的風險和機遇。想像一下，如果用於兒童內容審核的模型，本以為已經移除了不適宜的內容，但卻能被輕易激活，後果不堪設想。我們的研究揭示了這個漏洞，並指出了更有效擦除技術的方向。這意味著一個巨大的商業機會：我們可以開發更安全、可靠的AI內容生成和過濾解決方案，解決兒童保護、法律合規、品牌保護等痛點。我們將開發不可逆的概念擦除技術，並提供驗證服務，確保AI模型真正安全可控。這將在AI安全領域開闢新的市場，並為企業和使用者提供更值得信賴的AI工具。", "audio": "audios/2505.16174v1.mp3", "timestamp": "2025-05-25T15:13:09.183841"}
{"query": "AI", "id": "2505.16619v1", "url": "http://arxiv.org/abs/2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "published_date": "2025-05-22", "title_zh": "開放且永續的人工智慧：生命科學領域的挑戰、機會與未來之路", "summary_zh": "人工智慧在生命科學領域取得突破性進展，但也帶來了長期研究挑戰的加劇，如可重用性和可重複性的問題，並影響環境永續性。本文探討了人工智慧生態系統的碎片化問題，並提出了開放且永續的人工智慧（OSAI）建議，旨在連接研究人員與相關人工智慧資源，促進永續、可重用和透明的人工智慧應用。這些建議基於生命科學界的共識，旨在協助未來政策制定和結構化路徑，引導人工智慧在生命科學領域的實施。", "applications": ["**加速新藥開發：** 利用可重用且透明的AI模型，更快速地篩選藥物候選化合物，並預測其療效和副作用，降低開發成本。", "**精準醫療診斷：** 建立基於OSAI原則的診斷模型，分析患者的基因、生活方式和環境因素，提供更精準的個人化治療方案，提高治療效果。", "**優化農業生產：** 運用AI分析土壤數據、氣候信息和作物生長情況，優化灌溉、施肥和病蟲害防治策略，提高農業產量並減少資源浪費。"], "pitch": "我們正處於生命科學AI革命的開端，但進展受限於模型的可重用性和透明度問題。我們的OSAI框架提供了解決方案，旨在建立一個可信任、可持續且高效的AI生態系統。想像一下：新藥開發成本降低數十億美元，精準醫療觸手可及，糧食安全得到保障。我們的框架不是理論，而是行動藍圖。我們正在尋找投資者共同打造一個更健康、更可持續的未來，利用AI的力量徹底改變生命科學。我們不只是開發AI模型，我們正在建立一個開放且永續的AI生態系統，這是一個數十億美元規模的市場，且具有巨大的社會影響力。投資我們，就是投資未來。", "audio": "audios/2505.16619v1.mp3", "timestamp": "2025-05-25T16:14:53.491990"}
{"query": "Foundation Model", "id": "2505.14975v1", "url": "http://arxiv.org/abs/2505.14975v1", "title": "Flattening Hierarchies with Policy Bootstrapping", "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "authors": ["John L. Zhou", "Jonathan C. Kao"], "published_date": "2025-05-20", "title_zh": "透過策略自舉法扁平化層級結構", "summary_zh": "離線目標條件強化學習 (GCRL) 是一種很有潛力的技術，能利用大量無獎勵軌跡數據預訓練通用的策略，類似於電腦視覺和自然語言處理中訓練基礎模型時使用的自我監督目標。 然而，由於稀疏獎勵和折扣因素的組合，GCRL 擴展到更長的時間範圍仍然具有挑戰性，這掩蓋了原始動作在遠程目標方面的相對優勢。 層級強化學習方法在長時間範圍的目標達成任務上取得了良好的實證結果，但它們對模組化、特定時間尺度的策略和子目標生成產生了依賴，這引入了顯著的額外複雜性，並阻礙了擴展到高維目標空間。 在這項工作中，我們介紹了一種演算法，透過基於具有優勢加權重要性抽樣的子目標條件策略進行自舉，來訓練扁平（非層級）目標條件策略。 我們的研究方法消除了對 (子) 目標空間生成模型的需要，我們發現這對於在高維狀態空間中擴展到高維控制至關重要。 我們進一步表明，現有的基於層級和自舉的方法對應於我們推導中的特定設計選擇。 在一套全面的基於狀態和像素的運動和操作基準測試中，我們的研究方法與最先進的離線 GCRL 演算法相匹配或超過，並且可以擴展到先前方法失敗的複雜、長時間範圍的任務。", "applications": ["**機器人流程自動化 (RPA) 的提升：** 讓機器人在沒有人類明確示範所有步驟的情況下，學習更複雜的流程，例如組裝產品或處理訂單。", "**遊戲 AI 的進化：** 開發更智能、適應性更強的遊戲 AI，能學習更長時間的策略，並在複雜的遊戲環境中做出更好的決策，提升玩家體驗。", "**智慧家庭控制的增強：** 允許智慧家庭設備學習如何自動執行更複雜的任務，例如根據用戶的習慣和環境變化來調整照明、溫度和娛樂系統。"], "pitch": "我們正在革新機器學習，透過一種創新的策略自舉法，成功地將複雜的層級結構策略轉化為更簡潔、更易於擴展的扁平化模型。 這項技術突破解決了在長時間任務中應用強化學習的關鍵瓶頸，尤其是在高維空間中。 我們的早期成果已在多個基準測試中超越了現有技術水平。 市場機會巨大，涵蓋了機器人、遊戲、自動化等領域。 我們正在尋求投資，以加速我們的研發，並將這項技術商業化，打造新一代的智能系統，徹底改變人機互動的方式。", "audio": "audios/2505.14975v1.mp3", "timestamp": "2025-05-25T16:15:12.313710"}
{"query": "Diffusion Model", "id": "2505.16174v1", "url": "http://arxiv.org/abs/2505.16174v1", "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "authors": ["Ping Liu", "Chi Zhang"], "published_date": "2025-05-22", "title_zh": "抹除還是休眠？透過可逆性重新思考概念抹除", "summary_zh": "這篇論文探討了概念抹除技術是否真的能從擴散模型中徹底消除生成特定概念的能力。研究發現，現有的概念抹除方法，像是透過修改模型參數來抑制不需要的概念，實際上只是在表面上抑制了這些概念，並沒有真正消除其生成能力。研究者透過對抹除後的模型進行輕量級微調，發現被抹除的概念很容易重新浮現，顯示現有方法只是抑制了潛在的生成表示，而沒有完全消除。這突顯了現有概念抹除方法的局限性，並呼籲需要更深入的、表示層級的干預，以及更嚴格的評估標準，以確保能真正且不可逆地從生成模型中移除概念。", "applications": ["**防止模型生成有害內容：** 模型可以被訓練成不生成涉及暴力、仇恨言論或其他有害內容的圖像。例如，可以抹除模型生成特定種族刻板印象的能力。", "**保護版權：** 可以訓練模型不生成與受版權保護的角色或藝術風格相似的圖像。例如，遊戲開發者可以使用此技術來避免生成與競爭對手的遊戲角色相似的角色。", "**客製化內容生成：** 允許用戶安全地排除特定概念，以更精準地控制生成圖像的內容。例如，設計師可以指定模型不生成帶有特定品牌標誌的圖片。", "**教育用途：** 防止學生使用AI生成帶有不適當或不安全行為的影像。"], "pitch": "這項研究揭示了現有AI模型「概念抹除」技術的根本缺陷，表面上的移除只是暫時抑制，並非真正抹除。這代表著現有AI的安全護欄並不像我們想像的可靠。我們的商業機會在於開發一套**真正不可逆的AI概念抹除技術**，並將其應用於以下領域：\n\n*   **企業合規：** 協助企業安全地部署生成式AI，確保模型不會生成違反公司政策或法律法規的內容，避免潛在的法律風險和聲譽損害。例如，金融機構可以確保AI生成的廣告不會涉及誤導性資訊。\n*   **政府監管：** 提供政府機構監管AI內容生成的工具，防止假新聞、煽動性言論和有害資訊的傳播。這可以提高社會穩定性。\n*   **數據安全：** 提供一種方法來安全地分享AI模型，同時確保敏感信息（例如個人身份信息）無法被重新激活和生成。對於醫療、金融等數據敏感產業來說，這具有極高的價值。\n\n我們的技術可以建立AI安全的新標準，並在一個日益依賴生成式AI的世界中，提供一個更安全、可控和負責任的解決方案。先行者優勢明顯，市場潛力巨大，具有極高的投資價值。", "audio": "audios/2505.16174v1.mp3", "timestamp": "2025-05-25T16:15:31.842343"}
{"query": "AI", "id": "2505.16619v1", "url": "http://arxiv.org/abs/2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "published_date": "2025-05-22", "title_zh": "開放且永續的人工智慧：生命科學領域的挑戰、機會與未來發展方向", "summary_zh": "人工智慧在生命科學領域取得突破性進展，以前所未有的能力解讀生物信息。但快速採用AI也加劇了研究挑戰，如可重複性和可重用性不足，導致對AI研究產出的信任度下降，並影響環境永續性。本論文探討了AI生態系統的碎片化問題，並提出一系列開放且永續AI (OSAI) 建議，旨在連接研究人員與相關資源，促進永續、可重用和透明AI的實施。這些建議基於生命科學社群共識，並與現有努力保持一致，旨在協助制定AI實施的政策和結構化途徑。", "applications": ["**加速新藥開發：** 利用OSAI原則建立的AI模型，可以更有效地篩選潛在藥物，降低研發成本，縮短上市時間。透過公開透明的資料和演算法，增加藥廠和監管機構的信任度。", "**精準醫療診斷：** 建立開放共享的醫療影像資料庫，結合OSAI模型，能更準確地診斷疾病，提供個人化的治療方案。例如，利用AI分析基因序列，預測疾病風險，並制定預防策略。", "**農業優化與永續發展：** 使用OSAI模型分析土壤數據、氣候資訊和作物生長狀況，優化農業生產流程，提高產量，同時減少農藥和化肥的使用，實現永續農業。"], "pitch": "我們正在打造一個開放、透明且永續的人工智慧平台，專注於生命科學領域。目前生命科學的AI發展面臨信任度低、重用性差的問題，導致資源浪費和研究效率下降。我們的平台基於OSAI原則，提供標準化的資料集、可重複驗證的演算法和透明的評估指標，解決這些痛點。潛在的商業價值巨大：降低新藥開發成本、提升醫療診斷精準度、促進農業永續發展。我們將透過訂閱制、授權模式和諮詢服務實現營收。我們相信，透過開放合作，我們可以加速生命科學的創新，為人類健康和環境永續做出貢獻。我們正在尋找策略投資者，共同打造一個改變世界的AI生態系統。", "audio": "audios/2505.16619v1.mp3", "timestamp": "2025-05-25T18:17:13.452154"}
{"query": "Foundation Model", "id": "2505.14975v1", "url": "http://arxiv.org/abs/2505.14975v1", "title": "Flattening Hierarchies with Policy Bootstrapping", "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "authors": ["John L. Zhou", "Jonathan C. Kao"], "published_date": "2025-05-20", "title_zh": "利用策略自舉法扁平化層級結構", "summary_zh": "本研究提出一種新的離線目標條件強化學習(GCRL)方法，旨在訓練通用的策略，使其能夠在大量無獎勵軌跡的數據集上進行預訓練，類似於計算機視覺和自然語言處理中使用的自監督目標來訓練基礎模型。傳統GCRL在處理長跨度的任務時面臨挑戰，因為稀疏的獎勵和折扣會掩蓋原始動作對於遠期目標的相對優勢。雖然層級強化學習(Hierarchical RL)在長跨度目標任務上表現出色，但它依賴於模組化、時間尺度特定的策略和子目標生成，這增加了複雜性，並阻礙了在高維目標空間中的擴展。本研究透過在子目標條件策略上使用優勢加權重要性抽樣進行自舉，來訓練一個扁平(非層級)的目標條件策略。這種方法消除了對子目標空間生成模型的需求，這對於在大狀態空間中擴展到高維控制至關重要。研究還表明，現有的層級和基於自舉的方法對應於本研究推導中的特定設計選擇。在全面的基於狀態和像素的運動和操縱基準測試中，本研究的方法在複雜、長跨度任務上的表現與最先進的離線GCRL算法相媲美，甚至超越了它們，而先前的算法在這些任務中往往會失敗。", "applications": ["**智慧倉儲機器人:**  讓機器人學習在複雜的倉庫環境中，從A點高效地搬運貨物到B點，即使路徑很長且中間需要經過多個步驟，也能夠克服傳統機器人需要人工設定路線的限制，實現自主導航和操作。", "**家庭服務機器人:**  訓練機器人完成複雜的家務任務，例如：'把髒衣服從臥室拿到洗衣機'，即便這個任務需要機器人執行多個子任務（移動、拿起、導航），也能夠一次性完成，無需人工干預或編程。", "**無人機巡檢:**  讓無人機能夠在大型的設施（例如：油田、電網）中自主巡檢，根據預設目標（例如：檢查某個設備的讀數）規劃路徑，並自動執行檢查操作，減少人工巡檢的成本和風險。"], "pitch": "現今的機器人學習過於碎片化，需要針對每個任務進行繁瑣的編程和訓練。我們的技術利用離線數據，透過策略自舉法，讓機器人能夠學習更通用、更長跨度的任務，就像人類一樣，能夠一次性完成複雜的任務鏈。這將會大幅降低機器人部署的成本和時間，加速機器人在各個行業的應用。想像一下，不再需要為每個倉儲搬運路線進行編程，機器人可以直接從過去的數據中學習，自主完成任務。這種通用性將釋放巨大的商業價值，我們將專注於智慧倉儲、家庭服務和工業巡檢等高潛力市場，透過提供高效、低成本的機器人解決方案，顛覆傳統產業。", "audio": "audios/2505.14975v1.mp3", "timestamp": "2025-05-25T18:17:49.887043"}
{"query": "Diffusion Model", "id": "2505.16174v1", "url": "http://arxiv.org/abs/2505.16174v1", "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "authors": ["Ping Liu", "Chi Zhang"], "published_date": "2025-05-22", "title_zh": "抹除還是休眠？透過可逆性重新思考概念抹除", "summary_zh": "這篇論文探討了目前用來在生成式模型（例如擴散模型）中移除特定概念的方法，是否真的能徹底消除生成該概念的能力。研究發現，現有的概念抹除技術，例如Unified Concept Editing和Erased Stable Diffusion，往往只是表面上抑制了特定提示詞下的生成，而無法完全消除模型內部的相關潛在表示。透過輕量級微調，這些被認為已抹除的概念可以重新激活，並生成具有高度視覺保真度的圖像。因此，目前的概念抹除方法存在重大局限性，需要更深入的、針對表示層面的干預措施，以及更嚴格的評估標準，才能確保真正且不可逆地從生成模型中移除概念。", "applications": ["**內容安全與審查：** 針對生成式AI產生的不當內容，例如仇恨言論、假新聞等，可以更有效地移除或避免生成相關的概念。以往的表面抑制可能被繞過，但這篇論文指出需要更深入的抹除方式。", "**客製化與風格控制：** 在設計領域，允許使用者永久性地移除不想要的風格或元素，確保生成結果符合個人偏好和品牌規範。例如，設計師可以永久移除某種特定的藝術風格，避免AI生成類似風格的設計。", "**智慧財產權保護：** 避免生成式AI侵犯版權。例如，永久移除特定藝術家的風格或作品元素，防止模型無意間生成侵權的圖像。這將有助於建立更安全的AI版權環境。"], "pitch": "想像一下，能夠從AI模型中徹底移除潛在的風險概念，或者確保設計作品永遠不會侵犯任何智慧財產權。這正是我們這項研究的商業價值所在。現有的AI模型抹除技術只是表面功夫，容易被繞過，但我們的研究揭示了這個問題，並指出了更有效的解決方案方向。基於這項研究，我們可以開發更強大的概念抹除工具，應用於內容安全、客製化設計和智慧財產權保護等領域。這將解決生成式AI的一個關鍵痛點，為企業提供更安全、更可靠、更可控的AI服務，具有巨大的商業潛力和先發優勢。例如，我們可以將這項技術授權給大型雲端服務提供商，或者直接打造一個AI安全公司，為企業提供概念抹除服務。市場需求巨大，回報可期。", "audio": "audios/2505.16174v1.mp3", "timestamp": "2025-05-25T18:18:15.862022"}
{"query": "AI", "id": "2505.16619v1", "url": "http://arxiv.org/abs/2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "published_date": "2025-05-22", "title_zh": "開放與永續AI：生命科學領域的挑戰、機遇與前進之路", "summary_zh": "人工智慧正在轉變生命科學，讓研究人員以前所未有的能力解讀生物資訊。然而，快速採用AI也加劇了研究中長期存在的挑戰，例如可重用性、可再現性差，導致對AI研究產出的信任度下降，並影響環境永續性。本研究探討了AI生態系統的碎片化問題，並提出了針對AI生態系統中超過300個組件的開放與永續AI（OSAI）建議，旨在促進永續、可重用和透明的AI實施，協助生命科學社群開發相關政策和指導AI實施的結構化途徑。", "applications": ["**藥物研發加速：** 利用開放與永續的AI模型，藥廠可以更快速且可信地篩選潛在藥物，縮短研發週期並降低成本，同時確保研究結果的可重複性與可靠性。", "**精準醫療診斷：** 醫院可以基於開放的AI模型，結合患者的基因、生活方式等數據，更精準地預測疾病風險並制定個人化的治療方案，提升醫療效果。", "**農業改良：** 農業研究人員可以利用開放的AI模型分析土壤、氣候和作物數據，優化種植方式，提高作物產量，並減少農藥和化肥的使用，實現更永續的農業發展。"], "pitch": "生命科學領域正經歷AI驅動的巨大變革，但可重複性、信任和永續性是關鍵挑戰。我們提供一套實用、開放且永續的AI解決方案，連接研究人員與關鍵資源，加速藥物研發、精準醫療和農業改良等領域的創新。我們的價值主張是：加速研發、降低成本、提升可信度，並確保AI應用的永續性。想像一下，更快速找到治療癌症的方法、更精準地預測流行病爆發，或更有效地利用資源應對氣候變遷。這就是開放與永續AI的力量。我們尋求創投夥伴，共同打造一個更高效、更公平、更永續的AI生態系統，引領生命科學的未來。", "audio": "audios/2505.16619v1.mp3", "timestamp": "2025-05-25T19:09:53.498998"}
{"query": "Foundation Model", "id": "2505.14975v1", "url": "http://arxiv.org/abs/2505.14975v1", "title": "Flattening Hierarchies with Policy Bootstrapping", "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "authors": ["John L. Zhou", "Jonathan C. Kao"], "published_date": "2025-05-20", "title_zh": "利用策略引導展平層級結構", "summary_zh": "離線目標條件強化學習(GCRL)是預訓練通用策略的一個很有潛力的方向，它使用大量無獎勵的軌跡數據，類似於電腦視覺和自然語言處理中訓練基礎模型的自監督目標。然而，由於稀疏獎勵和折扣的結合，將GCRL擴展到更長的時間跨度仍然具有挑戰性，這會掩蓋原始動作相對於遠端目標的比較優勢。分層強化學習方法在長程目標達成任務上取得了強大的經驗結果，但它們對模塊化、特定時間尺度的策略和子目標生成的依賴，引入了顯著的額外複雜性，並阻礙了向高維目標空間的擴展。在這項工作中，我們引入了一種算法，通過使用優勢加權重要性抽樣，在子目標條件策略上引導訓練一個扁平(非分層)的目標條件策略。我們的方法消除了對(子)目標空間的生成模型的需求，我們發現這對於在大狀態空間中擴展到高維控制至關重要。我們進一步表明，現有的基於分層和引導的方法對應於我們推導中的特定設計選擇。在全面的基於狀態和像素的運動和操縱基準測試中，我們的方法與最先進的離線GCRL算法相匹配或超越，並可擴展到先前的算法失敗的複雜的、長程任務。", "applications": ["**家庭機器人導航：** 讓掃地機器人或家庭助手機器人學會在複雜環境中更有效地完成任務，例如從客廳到廚房取特定物品，即使沒有明確的路線規劃，也能依靠過去的經驗完成。", "**自動駕駛：** 在模擬駕駛環境中，訓練自動駕駛系統學會更長時間的駕駛行為，例如在城市中完成更長的旅程，應對更複雜的交通狀況，而無需每次都進行手動標註和獎勵。", "**醫療流程自動化：** 協助醫生進行手術或診斷。例如，在模擬手術環境中，訓練機器人手臂完成複雜的手術步驟，即使步驟之間存在較長時間的延遲或不確定性。"], "pitch": "這項技術解決了離線強化學習中長期任務難以處理的痛點，通過展平層級結構，大幅降低了複雜性，並提高了訓練效率。其商業價值體現在：\n\n*   **降低開發成本：** 減少了對人工標註和複雜模型設計的依賴，降低了AI系統的開發和部署成本。\n*   **提升性能：** 在複雜任務中，性能超越現有方法，為自動化應用帶來質的提升。\n*   **加速產品上市：** 由於訓練效率的提高，可以更快地將AI應用推向市場，搶占先機。\n\n具體而言，在機器人、自動駕駛、醫療等領域擁有廣闊的應用前景，能夠顯著提高這些領域的自動化水平和效率。我們相信，這項技術將引領下一代AI應用浪潮。", "audio": "audios/2505.14975v1.mp3", "timestamp": "2025-05-25T19:10:11.965664"}
{"query": "Diffusion Model", "id": "2505.16174v1", "url": "http://arxiv.org/abs/2505.16174v1", "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "authors": ["Ping Liu", "Chi Zhang"], "published_date": "2025-05-22", "title_zh": "抹除還是休眠？透過可逆性重新思考概念抹除", "summary_zh": "這篇論文探討了概念抹除技術在擴散模型中，是否真的能消除生成特定概念的能力。過去的研究主要關注特定文本提示下的概念抑制，而這篇論文則進一步研究：目前的抹除技術是否只是表面上的抑制，還是真正移除了生成目標概念的能力。研究團隊評估了兩種代表性的概念抹除方法 (統一概念編輯和抹除穩定擴散) 的穩健性和可逆性，發現這些方法雖然嘗試修改模型內部參數來抑制概念，但抹除後的概念往往可以在經過少量調整後重新出現，表明目前的技術只是抑制了潛在的生成表示，而沒有完全消除它們。研究結果揭示了現有概念抹除方法的關鍵局限性，並強調需要更深入的表示層級干預和更嚴格的評估標準，以確保從生成模型中真正、不可逆地移除概念。", "applications": ["**安全內容過濾：** 開發更強大的內容過濾器，防止生成模型生成有害或不當內容 (例如仇恨言論、色情內容)。現有的過濾器很容易被繞過，這個研究有助於開發不易恢復的抹除技術。", "**智慧財產權保護：** 防止生成模型生成侵犯版權的內容 (例如模仿特定藝術家的風格或使用受保護的角色)。如果能徹底抹除模型學習到的特定風格或角色，就能減少侵權風險。", "**客製化和個人化：** 允許使用者真正地移除他們不喜歡或不需要的概念，從而更精準地客製化和個人化生成模型的輸出。例如，使用者可以完全移除模型生成特定政治人物或品牌的相關內容的能力。"], "pitch": "我們發現現有的AI生成模型概念抹除技術存在根本缺陷，並提出更有效的抹除方案框架。目前市場上缺乏真正可靠的內容過濾和智慧財產權保護機制，我們的技術能有效降低這些風險，提供更安全的AI服務。想像一下，一個AI圖像生成平台，可以保證不會生成涉及仇恨言論或兒童不宜的內容。透過我們的技術，我們可以為內容創作者、平台提供商和終端使用者帶來巨大的價值，建立一個更安全、更可靠的AI生態系統。這不僅能提升用戶體驗，也能避免潛在的法律風險。我們相信這是一個具有龐大商業潛力的市場，現在是投資的絕佳時機。", "audio": "audios/2505.16174v1.mp3", "timestamp": "2025-05-25T19:10:26.721927"}
{"query": "AI", "id": "2505.16619v1", "url": "http://arxiv.org/abs/2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "published_date": "2025-05-22", "title_zh": "開放且永續的人工智慧：生命科學領域的挑戰、機遇與未來之路", "summary_zh": "這篇論文探討了人工智慧在生命科學領域快速發展所帶來的機遇與挑戰。雖然AI在生命科學領域取得了突破性進展，但重用性和可重複性問題日益嚴重，導致對AI研究結果的信任度下降，並對環境永續性產生負面影響。此外，AI生態系統分散，缺乏指導方針來支持開放且永續的AI模型開發。論文提出了一套實用的開放且永續的AI建議，對應了AI生態系統的300多個組件，旨在幫助研究人員找到相關的AI資源，以實現永續、可重複使用且透明的AI。最終目標是為AI在生命科學領域的政策制定和結構化路徑提供指導。", "applications": ["**個性化醫療診斷:** 利用AI分析病患的基因數據、生活習慣和病史，以更精確地診斷疾病，並制定個性化的治療方案。例如，通過分析腫瘤的基因組信息，選擇最有效的靶向藥物。", "**加速藥物研發:** 使用AI預測藥物分子的活性、毒性和副作用，加速藥物篩選和臨床試驗進程，降低研發成本。例如，利用AI模擬藥物與靶點蛋白的相互作用，快速識別潛在的候選藥物。", "**精準農業生產:** 運用AI分析土壤、氣候和作物的數據，優化灌溉、施肥和病蟲害防治策略，提高農作物產量和品質。例如，通過AI圖像識別技術，及時發現農作物病害，精準噴灑農藥。"], "pitch": "我們正在打造一個平台，解決生命科學領域AI應用的關鍵痛點：數據孤島和模型不可靠。我們的平台基於開放且永續的AI原則，提供標準化的數據格式、可追溯的模型開發流程和可驗證的AI結果。這不僅能提升研究效率，降低重複研究成本，更能提高AI模型的商業轉化率。想像一下，一款AI驅動的藥物發現平台，由於數據透明且模型經過嚴格驗證，能夠更快地獲得監管批准，並以更高的成功率進入市場。這是一個價值數十億美元的市場機會，我們正在建立行業標準，成為這個領域的領先者。", "audio": "audios/2505.16619v1.mp3", "timestamp": "2025-05-25T20:13:53.732282"}
{"query": "Foundation Model", "id": "2505.14975v1", "url": "http://arxiv.org/abs/2505.14975v1", "title": "Flattening Hierarchies with Policy Bootstrapping", "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "authors": ["John L. Zhou", "Jonathan C. Kao"], "published_date": "2025-05-20", "title_zh": "藉由策略引導平坦化層級結構", "summary_zh": "離線目標條件式強化學習(GCRL)有潛力在大型無獎勵軌跡數據集上預訓練通用策略，類似於電腦視覺和自然語言處理中用於訓練基礎模型的自監督目標。然而，由於稀疏獎勵和折扣的組合，使原始動作相對於遠程目標的比較優勢變得模糊，因此將GCRL擴展到更長的時間跨度仍然具有挑戰性。分層強化學習方法在長程目標達成任務上取得了強大的經驗成果，但它們對模塊化、特定時間尺度的策略和子目標生成的依賴引入了顯著的額外複雜性，並阻礙了擴展到高維目標空間。在這項工作中，我們提出了一種算法，通過使用優勢加權重要性抽樣在子目標條件策略上進行引導，來訓練一個平坦的（非分層的）目標條件策略。我們的方法消除了對(子)目標空間的生成模型的需求，我們發現這是擴展到大型狀態空間中高維控制的關鍵。我們進一步表明，現有的基於分層和引導的方法對應於我們推導中的特定設計選擇。在一個全面的基於狀態和像素的運動和操作基準測試套件中，我們的方法與最先進的離線GCRL算法相匹配或超過，並擴展到先前的算法失敗的複雜的長程任務。", "applications": ["**機器人倉儲自動化：** 讓機器人可以在複雜的倉庫環境中，無需人工編程，就能夠自主規劃路徑並執行搬運任務，例如從A點到B點，甚至是在擁擠的環境中繞過障礙物。", "**自動駕駛汽車：** 提升自動駕駛在複雜交通狀況下的決策能力，例如在無地圖環境中安全抵達目的地，或者在遇到突發事件時做出更合理的應變策略。", "**虛擬角色行為設計：** 在遊戲或模擬環境中，讓虛擬角色能夠更自然地與環境互動，自主完成複雜任務，例如在一個虛擬城市中寻找特定物品并送到指定地点。"], "pitch": "我們的新演算法解決了在複雜環境中訓練人工智慧的關鍵挑戰，尤其是在需要長時間規劃和決策的任務上。透過消除對複雜分層結構的依賴，我們的技術可以更高效地訓練出能夠應對各種場景的通用型AI，並實現遠超現有方法的長程任務規劃能力。這將徹底改變機器人技術、自動駕駛和遊戲AI等領域，為創業者提供巨大的市場機會，例如，開發更高效的倉儲機器人、更安全的自動駕駛系統或更智能的遊戲角色。我們正在尋找投資者來加速我們的研發進程，並將這項技術推向市場。", "audio": "audios/2505.14975v1.mp3", "timestamp": "2025-05-25T20:14:17.445248"}
{"query": "Diffusion Model", "id": "2505.16174v1", "url": "http://arxiv.org/abs/2505.16174v1", "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "authors": ["Ping Liu", "Chi Zhang"], "published_date": "2025-05-22", "title_zh": "抹除還是休眠？透過可逆性重新思考概念抹除", "summary_zh": "這篇論文探討了概念抹除技術在消除擴散模型中的生成能力方面的效果。研究發現，現有的抹除方法並不能真正移除模型生成特定概念的能力，而只是在特定提示下進行了表面上的抑制。透過對兩種代表性的概念抹除方法（Unified Concept Editing 和 Erased Stable Diffusion）的評估，研究人員發現被抹除的概念在經過輕微的微調後，就能夠重新產生具有高視覺保真度的圖像。這表明現有的方法只是抑制了潛在的生成表示，並沒有完全消除它們。因此，論文強調需要更深入的、表示層面的干預，以及更嚴格的評估標準，以確保從生成模型中真正、不可逆地移除概念。", "applications": ["**內容審核與過濾：** 應用於自動審核圖像內容，例如移除仇恨言論或不當圖片，確保平台內容的合規性。", "**客製化內容生成：** 用戶可指定避免生成的內容類型，例如避免生成包含特定政治立場或敏感話題的圖像，從而實現更加個性化的內容體驗。", "**智慧財產權保護：** 確保模型不會生成侵犯版權的內容，例如移除生成特定品牌Logo或角色的能力，從而防止侵權行為。"], "pitch": "我們發現現有AI圖像生成模型的「概念抹除」功能並不完善，僅能表面抑制，無法徹底移除特定概念。這帶來了潛在的法律和倫理風險，例如生成不當內容或侵犯版權。我們的技術目標是開發真正不可逆的概念抹除方法，並提供更嚴格的驗證標準，建立信任。商業模式包含：1. 為內容平台提供AI審核服務，降低風險；2. 為企業提供客製化AI模型，確保品牌安全；3. 將技術授權給其他AI開發者。初期以B2B模式為主，潛在客戶包含社交媒體公司、內容供應商、以及AI模型開發商，市場規模龐大且持續增長，具備高成長潛力。", "audio": "audios/2505.16174v1.mp3", "timestamp": "2025-05-25T20:14:36.772285"}
{"query": "AI", "id": "2505.16619v1", "url": "http://arxiv.org/abs/2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "published_date": "2025-05-22", "title_zh": "開放且永續的AI：生命科學領域的挑戰、機會與未來方向", "summary_zh": "這篇論文探討了AI在生命科學領域快速發展所帶來的機遇與挑戰。AI雖然為生命科學研究帶來突破，但也加劇了長期存在的問題，例如研究成果的可重用性與可重複性低落，進而影響環境永續性。此外，AI生態系統過於分散，缺乏明確的指引來支持開放且永續的AI模型開發。因此，論文提出了一系列實用的OSAI建議，涵蓋AI生態系統的300多個組成部分，旨在幫助研究人員找到相關的AI資源，促進永續、可重用且透明的AI實施，並為未來的政策制定和AI實施路徑提供指導。", "applications": ["**加速新藥開發：** 開放且永續的AI模型可以更容易地被研究人員共享和重複使用，從而加速藥物靶點的發現、藥物篩選和臨床試驗設計。", "**個性化醫療服務：** 透過整合基因組數據、生活方式信息和臨床數據，AI可以幫助醫生為患者制定更精準的治療方案，並預測疾病風險。", "**疾病預防與監測：** 利用AI分析公共衛生數據和社交媒體信息，可以及早發現疾病爆發的跡象，並制定更有效的疾病預防措施。"], "pitch": "我們正在構建一個開放且可持續的AI平台，專注於生命科學領域。現有AI模型碎片化且缺乏透明度，導致研究效率低下，資源浪費嚴重。我們的平台將提供統一的資源入口，標準化的模型開發流程，以及可驗證的模型性能指標，從而加速生命科學研究，降低成本，並確保AI的應用符合倫理和環境保護標準。潛在市場包括製藥公司、生物技術公司、醫療機構和學術研究機構，他們迫切需要更高效、更可靠的AI解決方案。我們的商業模式將包括訂閱制平台使用費、模型開發服務費以及數據分析諮詢費。我們的競爭優勢在於對生命科學領域的深刻理解，以及對開放和可持續AI的堅定承諾。我們相信，通過投資我們的平台，能夠幫助生命科學領域實現AI轉型，並為人類健康做出重大貢獻。", "audio": "audios/2505.16619v1.mp3", "timestamp": "2025-05-25T21:11:55.161557"}
{"query": "Foundation Model", "id": "2505.14975v1", "url": "http://arxiv.org/abs/2505.14975v1", "title": "Flattening Hierarchies with Policy Bootstrapping", "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "authors": ["John L. Zhou", "Jonathan C. Kao"], "published_date": "2025-05-20", "title_zh": "利用策略引導平坦化層級結構", "summary_zh": "離線目標導向強化學習 (GCRL) 有望在大量的無獎勵軌跡數據集上預訓練通用策略，類似於計算機視覺和自然語言處理中用於訓練基礎模型的自監督目標。然而，由於稀疏獎勵和折扣的結合，使得原始動作在遙遠目標面前的相對優勢不明顯，因此將 GCRL 擴展到更長的時間範圍仍然具有挑戰性。層級強化學習方法在長時間範圍的目標到達任務上取得了強勁的實證結果，但它們對模塊化、特定時間尺度策略和子目標生成的依賴引入了顯著的額外複雜性，並阻礙了擴展到高維目標空間。在本研究中，我們提出了一種算法，通過基於優勢加權重要性抽樣引導子目標條件策略，來訓練平坦（非層級）的目標條件策略。我們的方法消除了對（子）目標空間生成模型的需求，我們發現這是擴展到大型狀態空間中高維控制的關鍵。我們進一步表明，現有的層級和基於引導的方法對應於我們推導中的特定設計選擇。在一套全面的基於狀態和像素的運動和操縱基準測試中，我們的方法與最先進的離線 GCRL 算法相匹配或超過了它們，並可擴展到先前的算法無法處理的複雜、長時間範圍任務。", "applications": ["**家庭機器人自動化：** 訓練機器人完成複雜家務，例如準備餐點，無需人工編程獎勵，僅需提供任務完成的軌跡數據即可。", "**自動駕駛技能學習：** 透過大量駕駛數據，讓自動駕駛系統學習如何在複雜路況下安全駕駛，甚至學習應對突發狀況，無需預先定義所有規則。", "**智慧製造流程優化：** 在工廠環境中，透過蒐集現有生產數據，訓練機器人或自動化系統完成精準的組裝、檢測等任務，提高生產效率和良率。"], "pitch": "我們開發了一種突破性的強化學習技術，能讓AI系統更有效地學習複雜任務，無需人工設計獎勵或明確的層級結構。想像一下，一個AI機器人可以像人類一樣，透過觀察學習如何烹飪一道複雜的菜餚，或者一個自動駕駛系統可以從海量的駕駛數據中學習如何應對各種路況。這項技術的核心優勢在於其可擴展性，它可以應用於更廣泛、更複雜的場景。我們的目標是將這項技術商業化，提供一套強大的AI引擎，讓企業能夠快速部署智慧自動化解決方案，降低開發成本，提高效率，並在諸如智慧製造、物流、家庭服務等領域創造巨大的商業價值。相較於傳統方法，我們的技術能更快地實現商業部署，具有更強的適應性和更低的維護成本，投資回報率極高。", "audio": "audios/2505.14975v1.mp3", "timestamp": "2025-05-25T21:12:35.119235"}
{"query": "Diffusion Model", "id": "2505.16174v1", "url": "http://arxiv.org/abs/2505.16174v1", "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "authors": ["Ping Liu", "Chi Zhang"], "published_date": "2025-05-22", "title_zh": "抹除還是休眠？透過可逆性重新思考概念抹除", "summary_zh": "現有的概念抹除技術，真能徹底移除生成模型中特定概念的生成能力嗎？這篇論文探討了這個問題，發現目前的方法往往只是表面上的抑制，並非真正抹除。研究人員透過輕量級的微調，成功地重新激活了原本被認為已抹除的概念，證明現有技術僅是讓相關的概念潛藏，而非徹底消除。這意味著需要更深入的干預和更嚴格的評估標準，才能確保真正且不可逆地從生成模型中移除概念。", "applications": ["**內容審查與安全：** 用於移除生成模型中仇恨言論、不雅內容或其他有害概念，例如防止生成兒童不宜的圖像。", "**版權保護：** 從生成模型中移除受版權保護的元素，例如防止生成與特定品牌或人物高度相似的圖像。", "**客製化體驗：** 允許使用者移除他們不喜歡或不需要的概念，例如移除特定風格或物件，以獲得更符合個人需求的生成結果。"], "pitch": "現有AI模型中的概念抹除技術並非表面看起來那麼有效。這項研究揭示了它們只是讓不想要的概念潛藏，而非徹底消除。這創造了一個巨大的市場機會，我們將開發更強大、更可靠的AI安全解決方案，確保模型能夠真正且不可逆地移除有害或不希望出現的概念。想像一下，一個可以完全消除仇恨言論的AI模型，或是一個不會侵犯版權的圖像生成器。我們將成為這個領域的領導者，為AI帶來真正的安全與道德規範，同時保護品牌、創作者和社會大眾的利益。我們的技術將成為AI應用程式的必要組件，市場規模將隨著AI的普及而爆炸性成長。", "audio": "audios/2505.16174v1.mp3", "timestamp": "2025-05-25T21:13:08.352279"}
{"query": "AI", "id": "2505.16619v1", "url": "http://arxiv.org/abs/2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "published_date": "2025-05-22", "title_zh": "開放與永續的人工智慧：生命科學領域的挑戰、機遇與未來之路", "summary_zh": "生命科學領域的人工智慧正在快速發展，但同時也面臨著可重複性、可重用性以及信任度降低等問題。這篇文章探討了這些挑戰，並提出了一系列實用的開放與永續人工智慧（OSAI）建議，旨在協助研究人員更好地利用AI資源，促進生命科學AI研究的可持續發展、可重複使用和透明化，並為未來的政策制定提供參考。", "applications": ["**藥物研發加速：** 透過開放的AI模型和數據，研究人員可以更快速地分析生物數據，發現潛在的藥物靶點，並預測藥物的有效性和副作用，大幅縮短藥物研發週期。", "**精準醫療診斷：** 利用共享的AI模型，醫生可以更準確地診斷疾病，並根據個人基因和生活方式制定個性化的治療方案，提高治療效果。", "**環境保護監測：** 結合開放AI模型和環境數據，可以更有效地監測環境污染、生物多樣性變化等問題，並預測潛在的環境風險，為環境保護提供數據支持。"], "pitch": "我們正在解決生命科學領域AI發展中至關重要的可重複性、可重用性和信任度問題。我們的開放與永續AI (OSAI) 框架，不僅能加速藥物研發、精準醫療和環境保護等應用，更能建立一個更具信任度和可持續性的AI生態系統。這將大幅降低研發成本、提升研究效率，並為AI在生命科學領域的應用開闢更廣闊的市場。我們的商業模式可以透過提供 OSAI 諮詢服務、AI 模型託管平台以及相關工具和資源來實現。生命科學領域的AI投資正在蓬勃發展，而我們正是那個能確保這些投資能產生最大回報的關鍵推動者。", "audio": "audios/2505.16619v1.mp3", "timestamp": "2025-05-25T22:12:45.630177"}
{"query": "Foundation Model", "id": "2505.14975v1", "url": "http://arxiv.org/abs/2505.14975v1", "title": "Flattening Hierarchies with Policy Bootstrapping", "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "authors": ["John L. Zhou", "Jonathan C. Kao"], "published_date": "2025-05-20", "title_zh": "利用策略引導展平層級結構", "summary_zh": "離線目標條件強化學習(GCRL)有望在大型無獎勵軌跡數據集上預訓練通用策略，類似於電腦視覺和自然語言處理中用於訓練基礎模型的自監督目標。然而，由於稀疏獎勵和折扣的結合，使得原始行動相對於遠程目標的比較優勢模糊，因此將GCRL擴展到更長的時程仍然具有挑戰性。層級強化學習方法在長時程目標到達任務中取得了強勁的經驗成果，但它們對模塊化、時間尺度特定策略和子目標生成的依賴引入了顯著的額外複雜性，並阻礙了擴展到高維目標空間。在這項工作中，我們介紹了一種算法，通過使用優勢加權重要性抽樣在子目標條件策略上進行引導，來訓練展平(非層級)的目標條件策略。我們的方法消除了對(子)目標空間生成模型的需要，我們發現這對於在大狀態空間中擴展到高維控制至關重要。我們進一步表明，現有的基於層級和引導的方法對應於我們推導中的特定設計選擇。在一套全面的基於狀態和像素的運動和操縱基準測試中，我們的方法匹配或超越了最先進的離線GCRL算法，並擴展到先前方法失敗的複雜、長時程任務。", "applications": ["**機器人流程自動化 (RPA)：** 機器人可以在工廠、倉庫或其他環境中學習執行複雜的、多步驟的任務，例如組裝零件、搬運物品或包裝產品，而無需事先進行精確的編程。例如，一個機器人可以通過觀察人類操作員執行這些任務的數據來學習如何操作。", "**自動駕駛汽車：** 該技術可以幫助自動駕駛汽車在複雜的城市環境中導航，例如在交通擁堵時找到最佳路線，或者在沒有明確指示的情況下安全地停車。汽車可以通過觀察其他駕駛員的行為數據來學習如何更好地駕駛。", "**虛擬助手和聊天機器人：** 開發更智能、更靈活的虛擬助手，能夠處理更複雜的用戶請求，例如預訂旅行、規劃活動或解決技術問題。助手可以通過分析大量的用戶互動數據來學習如何更好地理解用戶意圖並提供更有效的解決方案。"], "pitch": "這項技術解決了離線強化學習中長時程任務的瓶頸，為通用人工智能開闢了新的可能性。與需要複雜層級結構的方法不同，我們的創新算法通過策略引導實現了更簡潔、更易於擴展的解決方案。 想像一下，讓機器人僅通過觀察數據就能學習執行複雜任務，而無需繁瑣的編程。這意味著更快的部署、更低的成本和更廣泛的應用。我們的技術可以顛覆製造業、物流業、自動駕駛等領域，創造巨大的市場機會。我們正在尋找投資，將這項突破性的研究轉化為商業產品，並引領下一代人工智能的發展。", "audio": "audios/2505.14975v1.mp3", "timestamp": "2025-05-25T22:13:47.503421"}
{"query": "Diffusion Model", "id": "2505.16174v1", "url": "http://arxiv.org/abs/2505.16174v1", "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "authors": ["Ping Liu", "Chi Zhang"], "published_date": "2025-05-22", "title_zh": "清除還是休眠？透過可逆性重新思考概念清除", "summary_zh": "本研究探討現有的概念清除技術是否真的能從擴散模型中徹底移除生成特定概念的能力，還是僅僅實現了表面的、提示詞特定的抑制。研究通過對比「統一概念編輯」和「清除穩定擴散」兩種代表性方法，並測試它們在文字生成圖像模型中消除目標生成行為的能力，發現這些方法雖然能修改模型參數來抑制不想要的語義概念，但被清除的概念通常在經過輕微調整後就能重新出現，顯示現有方法只是抑制了潛在的生成表示，而沒有完全消除它們。這表明現有的概念清除方法存在關鍵限制，需要更深入的表示層級干預和更嚴格的評估標準，以確保真正、不可逆地從生成模型中移除概念。", "applications": ["**內容審核與過濾：** 用於徹底清除模型生成有害內容（如仇恨言論、暴力內容）的能力，而不僅僅是依賴提示詞過濾，提高內容審核的可靠性。", "**隱私保護：** 用於永久清除模型中可能洩露個人隱私的訓練數據的痕跡，例如清除特定人物的面孔或地理位置信息，確保生成內容不違反隱私法規。", "**客製化模型：** 允許用戶不可逆地移除模型中不需要的風格或主題，例如，移除模型對特定藝術家的風格偏好，以實現更精確、客製化的生成結果。"], "pitch": "我們發現現有的AI生成模型概念清除技術並不像宣稱的那麼有效，被「清除」的概念其實只是「休眠」了，很容易被喚醒。這不僅帶來潛在的法律和道德風險（例如，產生有害內容），也限制了AI模型的應用。我們的研究揭示了這一關鍵漏洞，為開發真正有效且不可逆的概念清除技術提供了方向。這項技術的商業價值巨大，尤其是在監管日趨嚴格的背景下，能有效降低AI模型產生有害內容的風險，提升其在內容審核、隱私保護和客製化模型等領域的應用價值。我們正在開發更安全、更可靠的概念清除解決方案，為AI生成內容保駕護航，創造更可信賴的AI生態系統。", "audio": "audios/2505.16174v1.mp3", "timestamp": "2025-05-25T22:14:26.915720"}
{"query": "AI", "id": "2505.16619v1", "url": "http://arxiv.org/abs/2505.16619v1", "title": "Open and Sustainable AI: challenges, opportunities and the road ahead in the life sciences", "summary": "Artificial intelligence (AI) has recently seen transformative breakthroughs\nin the life sciences, expanding possibilities for researchers to interpret\nbiological information at an unprecedented capacity, with novel applications\nand advances being made almost daily. In order to maximise return on the\ngrowing investments in AI-based life science research and accelerate this\nprogress, it has become urgent to address the exacerbation of long-standing\nresearch challenges arising from the rapid adoption of AI methods. We review\nthe increased erosion of trust in AI research outputs, driven by the issues of\npoor reusability and reproducibility, and highlight their consequent impact on\nenvironmental sustainability. Furthermore, we discuss the fragmented components\nof the AI ecosystem and lack of guiding pathways to best support Open and\nSustainable AI (OSAI) model development. In response, this perspective\nintroduces a practical set of OSAI recommendations directly mapped to over 300\ncomponents of the AI ecosystem. Our work connects researchers with relevant AI\nresources, facilitating the implementation of sustainable, reusable and\ntransparent AI. Built upon life science community consensus and aligned to\nexisting efforts, the outputs of this perspective are designed to aid the\nfuture development of policy and structured pathways for guiding AI\nimplementation.", "authors": ["Gavin Farrell", "Eleni Adamidi", "Rafael Andrade Buono", "Mihail Anton", "Omar Abdelghani Attafi", "Salvador Capella Gutierrez", "Emidio Capriotti", "Leyla Jael Castro", "Davide Cirillo", "Lisa Crossman", "Christophe Dessimoz", "Alexandros Dimopoulos", "Raul Fernandez-Diaz", "Styliani-Christina Fragkouli", "Carole Goble", "Wei Gu", "John M. Hancock", "Alireza Khanteymoori", "Tom Lenaerts", "Fabio G. Liberante", "Peter Maccallum", "Alexander Miguel Monzon", "Magnus Palmblad", "Lucy Poveda", "Ovidiu Radulescu", "Denis C. Shields", "Shoaib Sufi", "Thanasis Vergoulis", "Fotis Psomopoulos", "Silvio C. E. Tosatto"], "published_date": "2025-05-22", "title_zh": "開放且永續的人工智慧：生命科學領域的挑戰、機遇與未來發展之路", "summary_zh": "生命科學領域的人工智慧正經歷突破性發展，但快速採用也加劇了研究的可重複性、可重用性以及環境永續性等挑戰，導致對AI研究成果的信任度下降。本文探討了AI生態系統的碎片化問題，並提出一套實用的開放且永續的人工智慧（OSAI）建議，旨在連結研究人員與相關資源，促進AI在生命科學領域的永續、可重用和透明的應用，並為未來的政策制定提供參考。", "applications": ["**個性化醫療診斷：** 利用開放的AI模型，根據患者的基因、生活習慣等數據，提供更精準、更個性化的疾病診斷和治療方案。因為模型透明可驗證，醫生和患者更能信任診斷結果。", "**加速藥物研發：** 通過可重複使用的AI算法，快速篩選潛在的藥物靶點和候選藥物，降低研發成本，縮短研發周期。開放的數據和模型也能促進合作，避免重複研究。", "**環境監測與生態保護：** 利用AI分析大量的環境數據（如空氣質量、水質等），預測環境變化趨勢，並制定更有效的生態保護策略。公開透明的模型也能促進公眾參與，共同保護環境。"], "pitch": "我們正在構建一個基於開放且永續AI原則的生命科學數據平台。生命科學領域的AI應用爆發，但模型的可重複性、可重用性以及對環境的影響是巨大的挑戰。我們的平台通過一套標準化流程和開放資源，解決這些問題。我們提供：\n\n*   **可信賴的AI模型：** 確保模型的透明度、可驗證性，提高研究成果的可信度。\n*   **加速研發流程：** 降低藥物研發成本，縮短研發週期，提高研發效率。\n*   **促進合作：** 建立一個開放的生態系統，鼓勵研究人員、企業和政府機構之間的合作。\n\n我們的商業模式包括：數據訂閱、模型服務、諮詢服務以及合作研發。生命科學AI市場規模龐大，而我們在開放且永續AI方面的領先地位將幫助我們搶佔市場先機，創造巨大的商業價值。", "audio": "audios/2505.16619v1.mp3", "timestamp": "2025-05-25T23:13:24.737443"}
{"query": "Foundation Model", "id": "2505.14975v1", "url": "http://arxiv.org/abs/2505.14975v1", "title": "Flattening Hierarchies with Policy Bootstrapping", "summary": "Offline goal-conditioned reinforcement learning (GCRL) is a promising\napproach for pretraining generalist policies on large datasets of reward-free\ntrajectories, akin to the self-supervised objectives used to train foundation\nmodels for computer vision and natural language processing. However, scaling\nGCRL to longer horizons remains challenging due to the combination of sparse\nrewards and discounting, which obscures the comparative advantages of primitive\nactions with respect to distant goals. Hierarchical RL methods achieve strong\nempirical results on long-horizon goal-reaching tasks, but their reliance on\nmodular, timescale-specific policies and subgoal generation introduces\nsignificant additional complexity and hinders scaling to high-dimensional goal\nspaces. In this work, we introduce an algorithm to train a flat\n(non-hierarchical) goal-conditioned policy by bootstrapping on\nsubgoal-conditioned policies with advantage-weighted importance sampling. Our\napproach eliminates the need for a generative model over the (sub)goal space,\nwhich we find is key for scaling to high-dimensional control in large state\nspaces. We further show that existing hierarchical and bootstrapping-based\napproaches correspond to specific design choices within our derivation. Across\na comprehensive suite of state- and pixel-based locomotion and manipulation\nbenchmarks, our method matches or surpasses state-of-the-art offline GCRL\nalgorithms and scales to complex, long-horizon tasks where prior approaches\nfail.", "authors": ["John L. Zhou", "Jonathan C. Kao"], "published_date": "2025-05-20", "title_zh": "利用策略自舉法展平階層式結構", "summary_zh": "離線目標條件強化學習(GCRL)是在大量無獎勵軌跡數據集上預訓練通用策略的一種有前景的方法，類似於用於訓練電腦視覺和自然語言處理基礎模型的自我監督目標。 然而，由於稀疏獎勵和折扣的組合，GCRL擴展到更長的時間範圍仍然具有挑戰性，這掩蓋了原始動作相對於遙遠目標的比較優勢。 分層強化學習方法在長程目標達成任務上取得了強大的經驗結果，但它們對模組化、時間尺度特定的策略和子目標生成的依賴引入了顯著的額外複雜性，並阻礙了擴展到高維目標空間。 在這項工作中，我們介紹了一種算法，通過對具有優勢加權重要性抽樣的子目標條件策略進行自舉，來訓練扁平（非分層）的目標條件策略。 我們的算法消除了對 (子)目標空間生成模型的需要，我們發現這是擴展到大型狀態空間中高維控制的關鍵。 我們進一步表明，現有的分層和基於自舉的方法對應於我們推導中的特定設計選擇。 在一套全面的基於狀態和像素的運動和操作基準測試中，我們的算法與最先進的離線 GCRL 算法相匹配或超越，並且可以擴展到先前的算法失敗的複雜長程任務。", "applications": ["**智慧家庭控制:**  讓機器人能夠在雜亂的環境中，根據用戶的語音指令（例如「把遙控器從沙發上拿到茶几上」）完成複雜的、多步驟的操作，而不需要預先設定詳細的路徑和動作。", "**工業機器人自動化:**  訓練機器人處理生產線上更複雜的任務，例如組裝具有高度變化的零件，或是在擁擠的環境中進行精確的物料搬運，減少對人工干預的依賴。", "**自動駕駛輔助系統:**  提升自動駕駛系統在複雜和不確定環境中的決策能力，例如在沒有明確路線規劃的情況下，根據乘客指令（例如「開到那間紅色屋頂的房子」）導航，或是在突發狀況下做出更合理的應變措施。"], "pitch": "我們開發了一種突破性的強化學習算法，能讓AI更容易學會處理複雜的、長時間的任務，且不需要大量的標註數據。想想看，現在的機器人只能做簡單重複的事情，但我們的技術能讓它們像人一樣，根據模糊的目標，靈活地在真實世界中行動。這意味著，工廠可以更快地自動化，智慧家居更聰明，自動駕駛也更安全。 我們算法的核心優勢在於它更有效率，可以應付更複雜的目標，並且不需要花費大量時間和金錢去標註數據。 想像一下，你可以直接告訴機器人『幫我把髒衣服丟到洗衣機裡』，而不需要事先教會它每一個步驟。這是一個巨大的飛躍，將徹底改變人與機器互動的方式，為無人駕駛、智慧製造、智慧醫療等領域創造巨大的商業價值。我們正在尋找投資，以便將這項技術商業化，並引領下一代AI的發展。", "audio": "audios/2505.14975v1.mp3", "timestamp": "2025-05-25T23:14:10.807652"}
{"query": "Diffusion Model", "id": "2505.16174v1", "url": "http://arxiv.org/abs/2505.16174v1", "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "authors": ["Ping Liu", "Chi Zhang"], "published_date": "2025-05-22", "title_zh": "清除還是休眠？透過可逆性重新思考概念消除", "summary_zh": "現有的概念消除技術真的能徹底移除生成模型產生特定概念的能力嗎？這篇論文探討了這個問題。研究人員發現，現有的方法，例如 Unified Concept Editing 和 Erased Stable Diffusion，雖然能在特定提示詞下抑制概念生成，但並不能真正消除生成能力。透過輕量級的微調，被「清除」的概念往往可以重新激活並生成高品質的圖像。這表明現有的概念消除技術只是抑制了潛在的生成表示，而沒有完全消除它們。因此，需要更深層次、針對表示層面的干預措施，以及更嚴格的評估標準，才能確保從生成模型中真正、不可逆地移除概念。", "applications": ["**內容審查：** 在圖片生成模型中徹底移除仇恨言論、暴力內容等負面概念，避免模型生成相關內容，確保平台內容安全。", "**智慧財產權保護：** 從生成模型中移除受版權保護的元素（例如特定卡通人物），防止使用者利用模型侵犯智慧財產權。", "**個人化模型客製化：**  允許使用者移除模型中不相關的概念，例如特定的藝術風格或物件類型，從而更精確地控制模型生成圖像的風格和內容。"], "pitch": "生成式 AI 的快速發展帶來了巨大的商業潛力，但也伴隨著潛在的風險，例如生成不當內容、侵犯版權等。我們提出的研究揭示了現有概念消除技術的局限性，並強調了開發真正、不可逆的消除技術的重要性。這項技術的商業價值體現在以下幾個方面：\n\n*   **降低法律風險：**  一個強大的概念消除系統可以顯著降低生成式 AI 模型產生違法或不當內容的風險，從而降低企業面臨的法律訴訟和罰款的可能性。\n*   **提升品牌形象：**  通過有效控制模型的生成行為，企業可以確保生成內容符合品牌價值觀，避免因生成不當內容而損害品牌形象。\n*   **開創新的商業模式：**  基於可靠的概念消除技術，我們可以開發更安全的生成式 AI 平台，並提供客製化的服務，例如移除特定風格或對象的圖像生成，滿足不同使用者的需求。 我們相信，我們對概念消除技術的深入理解和持續研發，將為生成式 AI 領域帶來更安全、更可靠、更有價值的應用。", "audio": "audios/2505.16174v1.mp3", "timestamp": "2025-05-25T23:14:56.528685"}
{"query": "AI", "id": "2505.16809v2", "url": "http://arxiv.org/abs/2505.16809v2", "title": "Hypergraph Tversky-Aware Domain Incremental Learning for Brain Tumor Segmentation with Missing Modalities", "summary": "Existing methods for multimodal MRI segmentation with missing modalities\ntypically assume that all MRI modalities are available during training.\nHowever, in clinical practice, some modalities may be missing due to the\nsequential nature of MRI acquisition, leading to performance degradation.\nFurthermore, retraining models to accommodate newly available modalities can be\ninefficient and may cause overfitting, potentially compromising previously\nlearned knowledge. To address these challenges, we propose Replay-based\nHypergraph Domain Incremental Learning (ReHyDIL) for brain tumor segmentation\nwith missing modalities. ReHyDIL leverages Domain Incremental Learning (DIL) to\nenable the segmentation model to learn from newly acquired MRI modalities\nwithout forgetting previously learned information. To enhance segmentation\nperformance across diverse patient scenarios, we introduce the Cross-Patient\nHypergraph Segmentation Network (CHSNet), which utilizes hypergraphs to capture\nhigh-order associations between patients. Additionally, we incorporate\nTversky-Aware Contrastive (TAC) loss to effectively mitigate information\nimbalance both across and within different modalities. Extensive experiments on\nthe BraTS2019 dataset demonstrate that ReHyDIL outperforms state-of-the-art\nmethods, achieving an improvement of over 2% in the Dice Similarity Coefficient\nacross various tumor regions.", "authors": ["Junze Wang", "Lei Fan", "Weipeng Jing", "Donglin Di", "Yang Song", "Sidong Liu", "Cong Cong"], "published_date": "2025-05-22", "title_zh": "適用於缺失模態腦瘤分割的超圖Tversky-Aware領域增量學習", "summary_zh": "本研究提出一種名為ReHyDIL的腦瘤分割方法，旨在解決多模態MRI分割中，由於MRI獲取順序導致的模態缺失問題。ReHyDIL利用領域增量學習(DIL)讓模型學習新的MRI模態，同時避免遺忘先前知識。此外，通過跨患者超圖分割網絡(CHSNet)捕捉患者間的高階關聯，並結合Tversky-Aware對比損失(TAC)來平衡不同模態間的信息。在BraTS2019數據集上的實驗表明，ReHyDIL優於現有方法，在多個腫瘤區域的Dice相似性係數上提高了2%以上。", "applications": ["**臨床診斷輔助系統：** 將ReHyDIL整合到醫院的影像診斷系統中，即使某些MRI模態缺失，醫生也能更準確地進行腦瘤分割，提升診斷效率。", "**遠程醫療診斷：** 在資源有限的地區，可能無法獲得所有MRI模態。ReHyDIL可以利用現有的有限數據進行分割，為遠程醫療提供支持。", "**個性化醫療：** 結合患者的歷史MRI數據，以及新獲取的少量數據，利用ReHyDIL進行精準分割，為個性化治療方案提供更可靠的基礎。"], "pitch": "我們開發了一種創新的AI算法，專門針對腦瘤分割，解決了醫療影像分析中常見的數據缺失問題。現有方法在面對MRI模態不完整時，準確率會大幅下降。我們的ReHyDIL技術，通過超圖和增量學習，可以高效利用不完整的數據，並能隨著新數據的加入不斷學習優化，性能超越現有技術2%以上。想像一下，全球有數百萬腦瘤患者，如果我們的技術能讓醫生更快速、更準確地診斷，就能極大地改善患者的預後。這不僅是一個巨大的市場機會，更是一個具有社會影響力的項目。我們計劃將ReHyDIL整合到現有的醫療影像平台，並通過SaaS模式向醫院和診所提供服務，同時與藥廠合作，加速新藥開發。基於我們的初步測試結果和市場潛力，我們相信ReHyDIL將會成為腦瘤診斷領域的領先技術，為投資者帶來可觀的回報。", "audio": "audios/2505.16809v2.mp3", "timestamp": "2025-05-26T01:06:49.464268"}
{"query": "Foundation Model", "id": "2505.16941v2", "url": "http://arxiv.org/abs/2505.16941v2", "title": "FoMoH: A clinically meaningful foundation model evaluation for structured electronic health records", "summary": "Foundation models hold significant promise in healthcare, given their\ncapacity to extract meaningful representations independent of downstream tasks.\nThis property has enabled state-of-the-art performance across several clinical\napplications trained on structured electronic health record (EHR) data, even in\nsettings with limited labeled data, a prevalent challenge in healthcare.\nHowever, there is little consensus on these models' potential for clinical\nutility due to the lack of desiderata of comprehensive and meaningful tasks and\nsufficiently diverse evaluations to characterize the benefit over conventional\nsupervised learning. To address this gap, we propose a suite of clinically\nmeaningful tasks spanning patient outcomes, early prediction of acute and\nchronic conditions, including desiderata for robust evaluations. We evaluate\nstate-of-the-art foundation models on EHR data consisting of 5 million patients\nfrom Columbia University Irving Medical Center (CUMC), a large urban academic\nmedical center in New York City, across 14 clinically relevant tasks. We\nmeasure overall accuracy, calibration, and subpopulation performance to surface\ntradeoffs based on the choice of pre-training, tokenization, and data\nrepresentation strategies. Our study aims to advance the empirical evaluation\nof structured EHR foundation models and guide the development of future\nhealthcare foundation models.", "authors": ["Chao Pang", "Vincent Jeanselme", "Young Sang Choi", "Xinzhuo Jiang", "Zilin Jing", "Aparajita Kashyap", "Yuta Kobayashi", "Yanwei Li", "Florent Pollet", "Karthik Natarajan", "Shalmali Joshi"], "published_date": "2025-05-22", "title_zh": "FoMoH：針對結構化電子病歷的臨床意義基礎模型評估", "summary_zh": "基礎模型在醫療保健領域展現巨大潛力，因為它們能夠提取獨立於下游任務的有意義表徵。儘管如此，由於缺乏全面且有意義的任務以及充分多樣化的評估，以表徵其相對於傳統監督學習的優勢，因此對於這些模型在臨床上的潛力並未達成共識。本文提出一系列具有臨床意義的任務，涵蓋患者結果、急慢性病早期預測，並包含穩健評估的標準。我們在來自哥倫比亞大學歐文醫學中心（CUMC）的500萬患者的電子病歷數據上，評估了14個臨床相關任務上的最先進基礎模型。我們測量了總體準確性、校準和亞人群表現，以揭示基於預訓練、標記化和數據表示策略的權衡。本研究旨在推進結構化電子病歷基礎模型的實證評估，並指導未來醫療保健基礎模型的開發。", "applications": ["**早期疾病預警系統：** 利用模型分析電子病歷數據，提前預測患者罹患糖尿病、心臟病等慢性疾病的風險，以便醫生及早介入，降低發病率和醫療成本。", "**個性化治療方案推薦：** 基於患者的病歷數據，模型可以推薦最適合的治療方案，考慮到患者的個體差異，提高治療效果。", "**藥物不良反應預測：** 模型可以分析患者的病歷數據和用藥歷史，預測可能發生的藥物不良反應，提醒醫生注意，避免嚴重後果。"], "pitch": "FoMoH研究為醫療領域的基礎模型評估建立了新的標準，透過嚴謹的臨床任務和大規模的真實世界數據驗證，證明了基礎模型在結構化電子病歷分析方面的潛力。這項技術能夠大幅提升疾病預測的準確性和效率，降低醫療成本，並改善患者的治療效果。我們將以此為基礎，開發針對特定疾病或臨床需求的AI解決方案，例如早期疾病預警系統、個性化治療方案推薦引擎等，並與醫院、保險公司、製藥企業等合作，將這些解決方案應用於臨床實踐中。FoMoH的嚴謹性和臨床意義，加上我們商業化的能力，使其成為一項極具潛力的投資。", "audio": "audios/2505.16941v2.mp3", "timestamp": "2025-05-26T01:07:20.723701"}
{"query": "Diffusion Model", "id": "2505.16839v2", "url": "http://arxiv.org/abs/2505.16839v2", "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding", "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.", "authors": ["Shufan Li", "Konstantinos Kallidromitis", "Hritik Bansal", "Akash Gokul", "Yusuke Kato", "Kazuki Kozuka", "Jason Kuen", "Zhe Lin", "Kai-Wei Chang", "Aditya Grover"], "published_date": "2025-05-22", "title_zh": "LaViDa：用於多模態理解的大型擴散語言模型", "summary_zh": "LaViDa是一個基於擴散模型(Diffusion Model)建構的視覺語言模型(VLM)，旨在克服現有自迴歸(AR) VLM（如LLaVA）在推論速度和可控生成方面的限制。LaViDa透過結合視覺編碼器並進行聯合微調，實現多模態指令追蹤。它採用了互補遮罩、前綴KV快取和時間步長偏移等創新技術，提升訓練效果、推論效率和生成品質。實驗結果顯示，LaViDa在多模態基準測試（如MMMU）上表現優異，並具備擴散模型的獨特優勢，如靈活的速度-品質權衡、可控性和雙向推理。在COCO圖像描述任務中，LaViDa以1.92倍的速度提升超越Open-LLaVa-Next-8B 4.1個CIDEr點。在雙向任務中，LaViDa在受限詩歌完成任務上提升了59%。LaViDa證明了它作為AR VLM的強有力替代方案。", "applications": ["**智能客服/產品推薦：** 根據用戶上傳的圖片（例如服裝照片）和文字指令（例如「搭配這件衣服的褲子」），快速生成多個搭配建議，並允許用戶調整生成風格（例如「更休閒一點」），提供更個性化的購物體驗。", "**視覺內容創作輔助：** 協助設計師或行銷人員基於草圖或靈感描述快速生成多種風格的視覺素材，例如廣告海報、產品宣傳圖等，並能根據用戶的修改指令進行調整和優化，提高創作效率。", "**醫療影像診斷輔助：** 醫生可以上傳醫療影像（例如X光片），並結合文字描述（例如「疑似肺部結節」），利用LaViDa快速生成多種診斷結果和建議，協助醫生做出更精準的判斷，並減少誤診率。"], "pitch": "我們正在開發LaViDa，一種基於擴散模型的新一代視覺語言模型，旨在解決現有VLM在速度和可控性上的瓶頸。LaViDa的快速推論和可控生成能力使其在需要即時反饋和精準控制的應用場景中具有巨大的商業潛力。我們的技術可以廣泛應用於智能客服、視覺內容創作和醫療影像診斷等領域，創造更高效、更個性化的用戶體驗。相較於傳統自迴歸模型，LaViDa提供更靈活的速度-品質權衡，允許客戶根據自身需求調整模型性能。我們預計透過授權核心技術、提供雲端API服務和客製化解決方案等多種商業模式，迅速搶佔市場份額，並成為多模態AI領域的領導者。現正尋求種子輪融資，用於加速模型開發、擴展團隊和開拓市場。", "audio": "audios/2505.16839v2.mp3", "timestamp": "2025-05-26T01:07:54.864213"}
{"query": "AI", "id": "2505.18139v1", "url": "http://arxiv.org/abs/2505.18139v1", "title": "Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems", "summary": "This position paper argues that the theoretical inconsistency often observed\namong Responsible AI (RAI) metrics, such as differing fairness definitions or\ntradeoffs between accuracy and privacy, should be embraced as a valuable\nfeature rather than a flaw to be eliminated. We contend that navigating these\ninconsistencies, by treating metrics as divergent objectives, yields three key\nbenefits: (1) Normative Pluralism: Maintaining a full suite of potentially\ncontradictory metrics ensures that the diverse moral stances and stakeholder\nvalues inherent in RAI are adequately represented. (2) Epistemological\nCompleteness: The use of multiple, sometimes conflicting, metrics allows for a\nmore comprehensive capture of multifaceted ethical concepts, thereby preserving\ngreater informational fidelity about these concepts than any single, simplified\ndefinition. (3) Implicit Regularization: Jointly optimizing for theoretically\nconflicting objectives discourages overfitting to one specific metric, steering\nmodels towards solutions with enhanced generalization and robustness under\nreal-world complexities. In contrast, efforts to enforce theoretical\nconsistency by simplifying or pruning metrics risk narrowing this value\ndiversity, losing conceptual depth, and degrading model performance. We\ntherefore advocate for a shift in RAI theory and practice: from getting trapped\nin inconsistency to characterizing acceptable inconsistency thresholds and\nelucidating the mechanisms that permit robust, approximated consistency in\npractice.", "authors": ["Gordon Dai", "Yunze Xiao"], "published_date": "2025-05-23", "title_zh": "擁抱矛盾：理論上的不一致不會阻礙建構負責任AI系統的道路", "summary_zh": "這篇論文認為，負責任AI(RAI)指標間常見的理論不一致，例如不同的公平性定義或準確性與隱私之間的權衡，應該被視為有價值的特徵，而非需要消除的缺陷。作者主張，透過將這些指標視為不同的目標來處理，能帶來三大好處：規範多元化、知識論完整性和隱性正規化。試圖強制理論一致性反而會窄化價值觀、降低概念深度，並降低模型效能。論文因此提倡轉變RAI的理論和實踐方向：從試圖擺脫不一致性，轉為表徵可接受的不一致性閾值，並闡明在實踐中實現穩健、近似一致性的機制。", "applications": ["**醫療診斷：** 在診斷疾病時，考慮不同醫生的診斷標準（可能存在差異），以及影像判讀的精準度與患者隱私之間的平衡，建立一個更全面、更客觀的診斷系統。", "**信用評分：** 信用評分模型應考慮多種公平性指標，避免歧視特定族群，同時兼顧預測準確性。例如，即使某個族群的平均信用風險較高，也要確保個別成員有公平的機會獲得貸款。", "**自動駕駛：** 自動駕駛系統在面臨緊急情況時，需要權衡乘客安全、行人安全和交通規則等多個目標，即使這些目標之間存在衝突。例如，為了避免撞到行人，可能需要冒著乘客受傷的風險。"], "pitch": "我們正在開發一種AI框架，它能有效管理並利用負責任AI(RAI)指標之間固有的矛盾，而非試圖消除這些矛盾。這種方法能確保AI系統在倫理、公平性和透明度方面表現更佳，同時提升其在複雜真實環境中的穩健性和泛化能力。與傳統方法相比，我們的框架能夠更好地代表多元的價值觀，避免因過度簡化而導致的偏見和錯誤。這項技術的潛在商業價值巨大，適用於金融、醫療、交通等各個行業，能幫助企業構建更值得信賴、更合規、更具競爭力的AI解決方案。我們尋求投資，以加速產品開發和市場拓展，搶佔先機，引領負責任AI的新時代。", "audio": "audios/2505.18139v1.mp3", "timestamp": "2025-05-26T03:13:12.726289"}
{"query": "Foundation Model", "id": "2505.18125v1", "url": "http://arxiv.org/abs/2505.18125v1", "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations", "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.", "authors": ["Alan Arazi", "Eilam Shapira", "Roi Reichart"], "published_date": "2025-05-23", "title_zh": "TabSTAR：一個具備語義目標感知表示的基礎表格模型", "summary_zh": "深度學習在許多領域都取得了顯著的成功，但在表格學習任務上的表現一直不如梯度提升決策樹(GBDTs)。然而，最近的進展正為表格基礎模型鋪平道路，這些模型可以利用真實世界的知識並跨多樣化的數據集進行泛化，特別是當數據包含自由文本時。儘管將語言模型能力整合到表格任務中已經被探索，但大多數現有方法使用靜態的、與目標無關的文本表示，限制了它們的有效性。我們介紹TabSTAR：一個具有語義目標感知表示的基礎表格模型。TabSTAR旨在實現表格數據與文本特徵的轉移學習，其架構不含數據集特定參數。它解凍了一個預訓練的文本編碼器，並將目標token作為輸入，為模型提供學習任務特定嵌入所需的上下文。TabSTAR在具有文本特徵的分類任務的已知基準測試中，針對中型和大型數據集都取得了最先進的性能，並且其預訓練階段在數據集數量上表現出縮放法則，為進一步的性能改進提供了途徑。", "applications": ["**客戶服務聊天機器人：** 分析客戶的文字提問（例如產品問題、訂單查詢）和相關的客戶數據（例如購買歷史、會員等級），更精準地理解客戶意圖，提供更客製化的回覆和解決方案。", "**醫療診斷輔助系統：** 結合病患的病歷文本描述（例如主訴、醫囑）和結構化的健康數據（例如血壓、血糖），幫助醫生更快速準確地診斷病情，並推薦更合適的治療方案。", "**金融風險評估：** 分析借款人的申請資料中的文本部分（例如收入證明、信用描述）和結構化數據（例如年齡、職業），更全面地評估借款人的信用風險，降低壞帳率。"], "pitch": "TabSTAR 是一個革命性的表格數據分析模型，尤其擅長處理包含文本資訊的數據。它超越了傳統梯度提升樹模型，在多種真實應用場景中展現出卓越的性能。試想一下：一個能更精準判斷客戶意圖的聊天機器人，一個能輔助醫生做出更快速診斷的醫療系統，或是一個能有效降低壞帳率的金融風險評估工具。TabSTAR不僅能提升效率，更能帶來顯著的業務價值。隨著數據量的增長，TabSTAR的預訓練能力將帶來更大的優勢。我們相信 TabSTAR 有潜力成为企业在表格数据领域的核心技术，赋能企业做出更明智的决策，挖掘更多商业机会。这是一个巨大的市场，TabSTAR 拥有成为行业领导者的潜力，并带来指数级的投资回报。", "audio": "audios/2505.18125v1.mp3", "timestamp": "2025-05-26T03:13:31.500513"}
{"query": "Diffusion Model", "id": "2505.18145v1", "url": "http://arxiv.org/abs/2505.18145v1", "title": "Stochastic agent-based Monte Carlo simulations for reaction-diffusion models, population dynamics, and epidemic spreading", "summary": "We provide a succinct overview of the implementation of Monte Carlo\nalgorithms based on Markovian stochastic dynamics to study interacting and\nreacting many-particle systems away from thermal equilibrium. Such agent-based\ncomputer simulations constitute an effective tool to introduce undergraduate\nand beginning graduate students to current frontier research without requiring\nmuch prior knowledge or experience: Starting from direct visualization of\nsimulation data, students may gain immediate insight into emerging macroscopic\nfeatures of a complex model system and subsequently apply more sophisticated\ndata analysis to quantitatively characterize its often rich dynamical\nproperties, both in stationary and transient regimes. We utilize numerical\ninvestigations of paradigmatic reaction-diffusion systems, as well as\nstochastic models for population dynamics and epidemic spreading, to exemplify\nhow interdisciplinary computational research can be effectively utilized in\nbottom-up undergraduate and graduate education through learning by doing. In\naddition, we give helpful hints for the practical setup of Monte Carlo\nsimulation algorithms, provide sample codes, explain some typical data analysis\ntools, and describe various potential error sources and pitfalls, with tips for\navoiding them.", "authors": ["Mohamed Swailem", "Ulrich Dobramysl", "Ruslan Mukhamadiarov", "Uwe C. Täuber"], "published_date": "2025-05-23", "title_zh": "反應擴散模型、族群動力學與傳染病傳播之基於隨機代理人的蒙地卡羅模擬", "summary_zh": "這篇論文簡要介紹了如何利用基於馬可夫隨機動態的蒙地卡羅演算法，來研究遠離熱平衡的交互作用和反應多粒子系統。這種基於代理人的電腦模擬是一種有效的工具，能讓大學生和研究生新手接觸到前沿研究，且不需要太多先備知識或經驗。學生可以從直接視覺化模擬數據開始，立即了解複雜模型系統中湧現的宏觀特徵，然後應用更複雜的數據分析來量化其豐富的動態特性，無論是穩定還是暫態。我們利用反應擴散系統、族群動力學和傳染病傳播的數值研究，來展示跨學科的計算研究如何能透過做中學的方式，有效地應用於由下而上的大學和研究生教育。此外，我們還提供了蒙地卡羅模擬演算法的實用設置技巧、範例程式碼、一些典型的數據分析工具，並描述了各種潛在的錯誤來源和陷阱，以及避免它們的技巧。", "applications": ["**傳染病控制模擬：** 用於模擬不同防疫措施（例如戴口罩、社交距離）對疾病傳播的影響，幫助政府制定更有效的防疫政策。", "**生態系統建模：** 模擬不同物種之間的相互作用（例如捕食者與獵物），預測環境變化對生態系統的影響，支持生態保護工作。", "**材料科學：** 模擬化學反應擴散過程，幫助開發新型材料，例如更高效的電池或更耐用的塗層。"], "pitch": "本研究提供了一套易於使用的蒙地卡羅模擬工具，可應用於廣泛的領域，包括醫療保健、環境科學和材料科學。其簡便性降低了入門門檻，使其非常適合學術界和產業界。潛在的商業價值體現在：\n\n*   **傳染病模型預測平台：** 基於此模型，可以建立一個實時傳染病預測平台，為政府和醫療機構提供決策支持，並在疫情早期及時發出預警。\n*   **精準農業解決方案：** 通過模擬作物生長、病蟲害傳播和資源利用，優化農業生產，提高產量並降低成本。\n*   **新藥開發加速器：** 利用反應擴散模型模擬藥物在體內的擴散和作用機制，加速新藥篩選和開發過程。 此外，該工具開源且易於定制，具有較高的可擴展性，能夠吸引大量的用戶和開發者，形成一個活躍的生態系統，從而創造更大的商業價值。", "audio": "audios/2505.18145v1.mp3", "timestamp": "2025-05-26T03:13:49.572086"}
{"query": "AI", "id": "2505.18139v1", "url": "http://arxiv.org/abs/2505.18139v1", "title": "Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems", "summary": "This position paper argues that the theoretical inconsistency often observed\namong Responsible AI (RAI) metrics, such as differing fairness definitions or\ntradeoffs between accuracy and privacy, should be embraced as a valuable\nfeature rather than a flaw to be eliminated. We contend that navigating these\ninconsistencies, by treating metrics as divergent objectives, yields three key\nbenefits: (1) Normative Pluralism: Maintaining a full suite of potentially\ncontradictory metrics ensures that the diverse moral stances and stakeholder\nvalues inherent in RAI are adequately represented. (2) Epistemological\nCompleteness: The use of multiple, sometimes conflicting, metrics allows for a\nmore comprehensive capture of multifaceted ethical concepts, thereby preserving\ngreater informational fidelity about these concepts than any single, simplified\ndefinition. (3) Implicit Regularization: Jointly optimizing for theoretically\nconflicting objectives discourages overfitting to one specific metric, steering\nmodels towards solutions with enhanced generalization and robustness under\nreal-world complexities. In contrast, efforts to enforce theoretical\nconsistency by simplifying or pruning metrics risk narrowing this value\ndiversity, losing conceptual depth, and degrading model performance. We\ntherefore advocate for a shift in RAI theory and practice: from getting trapped\nin inconsistency to characterizing acceptable inconsistency thresholds and\nelucidating the mechanisms that permit robust, approximated consistency in\npractice.", "authors": ["Gordon Dai", "Yunze Xiao"], "published_date": "2025-05-23", "title_zh": "擁抱矛盾：理論上的不一致性不會阻礙負責任AI系統的構建之路", "summary_zh": "這篇論文指出，負責任AI(RAI)指標之間常見的理論不一致性，例如不同的公平性定義或準確性與隱私之間的權衡，應該被視為一種有價值的特性，而非需要消除的缺陷。透過將這些指標視為不同的目標，可以帶來三個關鍵好處：(1)規範多元主義：維護一套可能相互矛盾的指標，確保RAI中固有的不同道德立場和利益相關者價值得到充分體現。(2)認識論完整性：使用多個、有時相互衝突的指標，可以更全面地捕捉多方面的倫理概念，從而比任何單一、簡化的定義更能保留關於這些概念的資訊。(3)隱含正規化：聯合優化理論上相互衝突的目標，可以防止過度擬合到某個特定指標，引導模型朝向具有更強泛化能力和在真實世界複雜性下的魯棒性解決方案。總之，我們提倡轉變RAI理論與實踐的方向：從陷於不一致性，轉向描述可接受的不一致性閾值，並闡明允許在實踐中實現穩健、近似一致性的機制。", "applications": ["**醫療診斷輔助系統：** 權衡不同指標，如診斷準確性、假陽性率和不同族群的公平性，以避免針對特定族群產生偏差的診斷結果。系統可以提供多種診斷方案，並列出各自的優缺點，讓醫生根據病患的具體情況進行判斷。", "**貸款風險評估：** 在貸款審核中，考慮收入、信用評分等多個因素。不同的評估標準可能會對不同背景的申請人產生不同的影響。系統可以分析不同指標對不同人群的影響，並建議公平且風險可控的貸款方案。", "**刑事司法預測系統：** 使用多個指標來評估嫌疑人再次犯罪的風險，包括先前的犯罪記錄、年齡、居住地等。不同指標可能存在偏差，導致對特定群體的歧視。系統可以通過權衡不同指標，降低偏見，提供更公正的風險評估結果。"], "pitch": "我們正在開發一種框架，讓AI系統能夠在多個可能相互衝突的道德和性能指標之間進行優化，從而構建更負責任、更可靠的AI。傳統的AI開發往往追求單一目標，容易導致偏見和性能瓶頸。我們的框架則能夠擁抱這種矛盾，實現規範多元主義、認識論完整性和隱含正規化的優勢。想像一下，在醫療、金融、司法等高風險領域，我們的技術可以幫助企業構建更公平、更透明的AI系統，從而提高公眾信任，降低合規風險，並最終創造更大的商業價值。我們的團隊擁有在AI倫理、機器學習和軟體工程方面的深厚積累，我們正在尋求投資，以便加速產品開發，擴大市場覆蓋，並引領負責任AI的發展。", "audio": "audios/2505.18139v1.mp3", "timestamp": "2025-05-26T04:20:29.253061"}
{"query": "Foundation Model", "id": "2505.18125v1", "url": "http://arxiv.org/abs/2505.18125v1", "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations", "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.", "authors": ["Alan Arazi", "Eilam Shapira", "Roi Reichart"], "published_date": "2025-05-23", "title_zh": "TabSTAR：一個具有語義目標感知表示的基礎表格模型", "summary_zh": "傳統上，深度學習在表格資料的學習上表現不如梯度提升決策樹 (GBDTs)。但近年來，表格基礎模型開始嶄露頭角，它們能夠利用真實世界的知識並泛化到不同的資料集，特別是在資料包含自由文本時。我們提出了 TabSTAR，這是一個具有語義目標感知表示的基礎表格模型。TabSTAR 專為在具有文本特徵的表格資料上進行遷移學習而設計，其架構不包含特定於資料集的參數。它解凍了一個預訓練的文本編碼器，並將目標詞符作為輸入，為模型提供學習任務特定嵌入所需的上下文。TabSTAR 在具有文本特徵的分類任務的已知基準測試中，在中型和大型資料集上都達到了最先進的性能，並且其預訓練階段在資料集數量上表現出縮放定律，為進一步的性能改進提供了途徑。", "applications": ["**更精準的客戶服務聊天機器人：** 從客戶資料庫(表格資料)和過往對話紀錄(文本資料)學習，更了解客戶需求，提供更精準的回應，解決問題。", "**更有效的房地產估價系統：** 整合房屋基本資訊(表格資料)和房屋描述(文本資料)，更準確地預測房價，提升房地產交易效率。", "**更智能的履歷篩選：** 結合履歷中的工作經驗(表格資料)和描述(文本資料)，更快速有效地找到符合職位需求的候選人。"], "pitch": "TabSTAR 解決了深度學習在表格資料，特別是包含文本資料的表格資料上的效能瓶頸。它採用了獨特的語義目標感知表示，使得模型能夠更好地理解文本資料的語義，進而提升預測準確性。我們正處於表格資料基礎模型發展的早期階段，TabSTAR 的卓越性能和可擴展性使其在各行各業具有廣闊的應用前景。想像一下，一個能夠理解複雜文本語義的表格分析工具，它將改變金融、醫療、零售等領域的資料分析方式。我們相信 TabSTAR 將成為下一代表格資料分析的基石，並帶來巨大的商業價值。透過投資 TabSTAR，您將站在資料革命的最前沿，掌握未來資料分析的關鍵技術。", "audio": "audios/2505.18125v1.mp3", "timestamp": "2025-05-26T04:20:47.101818"}
{"query": "Diffusion Model", "id": "2505.18145v1", "url": "http://arxiv.org/abs/2505.18145v1", "title": "Stochastic agent-based Monte Carlo simulations for reaction-diffusion models, population dynamics, and epidemic spreading", "summary": "We provide a succinct overview of the implementation of Monte Carlo\nalgorithms based on Markovian stochastic dynamics to study interacting and\nreacting many-particle systems away from thermal equilibrium. Such agent-based\ncomputer simulations constitute an effective tool to introduce undergraduate\nand beginning graduate students to current frontier research without requiring\nmuch prior knowledge or experience: Starting from direct visualization of\nsimulation data, students may gain immediate insight into emerging macroscopic\nfeatures of a complex model system and subsequently apply more sophisticated\ndata analysis to quantitatively characterize its often rich dynamical\nproperties, both in stationary and transient regimes. We utilize numerical\ninvestigations of paradigmatic reaction-diffusion systems, as well as\nstochastic models for population dynamics and epidemic spreading, to exemplify\nhow interdisciplinary computational research can be effectively utilized in\nbottom-up undergraduate and graduate education through learning by doing. In\naddition, we give helpful hints for the practical setup of Monte Carlo\nsimulation algorithms, provide sample codes, explain some typical data analysis\ntools, and describe various potential error sources and pitfalls, with tips for\navoiding them.", "authors": ["Mohamed Swailem", "Ulrich Dobramysl", "Ruslan Mukhamadiarov", "Uwe C. Täuber"], "published_date": "2025-05-23", "title_zh": "用於反應擴散模型、族群動態和流行病傳播的基於隨機主體的蒙地卡羅模擬", "summary_zh": "本研究簡要概述了基於馬可夫隨機動力學的蒙地卡羅演算法，用於研究遠離熱平衡的相互作用和反應多粒子系統。這種基於主體的電腦模擬是一種有效工具，可以讓大學生和研究生入門生接觸到前沿研究，而無需太多先備知識或經驗。學生可以從直接視覺化模擬數據開始，立即了解複雜模型系統中出現的宏觀特徵，然後應用更複雜的數據分析來定量描述其豐富的動態特性，無論是在靜態還是瞬態狀態下。我們利用對範例反應擴散系統以及族群動態和流行病傳播的隨機模型的數值研究，來說明跨學科計算研究如何通過邊做邊學有效地應用於自下而上的大學和研究生教育。此外，我們還為蒙地卡羅模擬演算法的實際設置提供了有用的提示，提供了示例代碼，解釋了一些典型的數據分析工具，並描述了各種潛在的錯誤來源和陷阱，並提供了避免這些問題的技巧。", "applications": ["**模擬疫情擴散：** 模擬不同社交距離政策和疫苗接種策略對疫情的影響，協助政府制定更有效的防疫措施。", "**預測生態系統變化：** 模擬氣候變遷或外來種入侵對特定生態系統的影響，幫助保育團體制定保護計畫。", "**最佳化化學反應：** 模擬反應物濃度、溫度等參數對化學反應產率的影響，協助化學家最佳化實驗條件。"], "pitch": "這項技術提供了一種強大的模擬工具，能夠處理複雜的動態系統，例如流行病擴散、生態系統變化和化學反應。其商業價值體現在以下幾個方面：首先，可以提供更精確的預測，幫助企業或政府做出更明智的決策，例如疫情期間的資源分配或新藥開發的優先順序。其次，該技術易於上手，可降低模擬成本，並加速研發週期。最後，其跨領域的特性使其能夠應用於各行各業，從醫療保健到環境保護，甚至是金融建模，具有廣闊的市場前景。我們認為，通過將該技術與雲端運算和機器學習相結合，可以打造一個強大的模擬平台，為客戶提供定制化的解決方案，並創造巨大的商業價值。", "audio": "audios/2505.18145v1.mp3", "timestamp": "2025-05-26T04:21:01.898060"}
{"query": "AI", "id": "2505.18139v1", "url": "http://arxiv.org/abs/2505.18139v1", "title": "Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems", "summary": "This position paper argues that the theoretical inconsistency often observed\namong Responsible AI (RAI) metrics, such as differing fairness definitions or\ntradeoffs between accuracy and privacy, should be embraced as a valuable\nfeature rather than a flaw to be eliminated. We contend that navigating these\ninconsistencies, by treating metrics as divergent objectives, yields three key\nbenefits: (1) Normative Pluralism: Maintaining a full suite of potentially\ncontradictory metrics ensures that the diverse moral stances and stakeholder\nvalues inherent in RAI are adequately represented. (2) Epistemological\nCompleteness: The use of multiple, sometimes conflicting, metrics allows for a\nmore comprehensive capture of multifaceted ethical concepts, thereby preserving\ngreater informational fidelity about these concepts than any single, simplified\ndefinition. (3) Implicit Regularization: Jointly optimizing for theoretically\nconflicting objectives discourages overfitting to one specific metric, steering\nmodels towards solutions with enhanced generalization and robustness under\nreal-world complexities. In contrast, efforts to enforce theoretical\nconsistency by simplifying or pruning metrics risk narrowing this value\ndiversity, losing conceptual depth, and degrading model performance. We\ntherefore advocate for a shift in RAI theory and practice: from getting trapped\nin inconsistency to characterizing acceptable inconsistency thresholds and\nelucidating the mechanisms that permit robust, approximated consistency in\npractice.", "authors": ["Gordon Dai", "Yunze Xiao"], "published_date": "2025-05-23", "title_zh": "擁抱矛盾：理論不一致性不會阻礙負責任人工智慧系統的構建之路", "summary_zh": "這篇論文主張，在負責任人工智慧 (RAI) 指標中常見的理論不一致性，例如不同的公平性定義或準確性與隱私之間的權衡，應該被視為有價值的特性，而不是需要消除的缺陷。論文認為，透過將這些指標視為不同的目標來處理這些不一致性，將帶來三個關鍵好處：(1) 規範多元主義：維持一套完整的、可能相互矛盾的指標，確保 RAI 中固有的多樣道德立場和利益相關者價值得到充分代表。(2) 知識論完整性：使用多個、有時相互衝突的指標，可以更全面地捕捉多面向的倫理概念，從而比任何單一、簡化的定義更好地保留關於這些概念的資訊真實性。(3) 隱式正規化：聯合優化理論上相互衝突的目標，可以阻止對特定指標的過度擬合，引導模型朝向在現實世界的複雜性下具有增強泛化能力和穩健性的解決方案。因此，論文提倡 RAI 理論和實踐轉變方向：從陷入不一致性，轉向描述可接受的不一致性閾值，並闡明在實踐中允許穩健、近似一致性的機制。", "applications": ["醫療診斷：AI協助醫生診斷疾病時，不應只追求高準確度，還要考慮公平性(避免對特定族群誤判)與病人隱私。擁抱不同指標的衝突，能避免AI只針對特定族群優化，忽略其他族群的診斷需求。", "信貸評估：AI用於信貸評估時，需要兼顧準確性、公平性(避免種族或性別歧視)和透明度。擁抱不同指標的衝突，有助於避免AI演算法過度依賴某些特定因素，產生歧視性結果。", "刑事司法：AI用於預測犯罪風險時，必須考慮公平性、透明度和準確性。擁抱不同指標的衝突，能避免AI系統偏向特定族群，導致不公正的判決。"], "pitch": "這項研究揭示了負責任AI發展的重要突破口：擁抱矛盾。它不僅提供了更全面、更穩健的AI解決方案，也避免了單一指標造成的盲點和潛在歧視。我們計劃將這項理論應用到各個行業，例如醫療保健、金融和法律，開發更值得信賴、更公平的AI系統。我們的商業模式包括為企業提供定制的AI解決方案、諮詢服務以及AI倫理審查工具。市場需求巨大，因為越來越多的公司意識到負責任AI的重要性。我們的競爭優勢在於我們對理論矛盾的深刻理解和創新方法，這使得我們能夠構建更具適應性、更可靠的AI系統，最終建立一個更公平、更透明的AI生態系統，潛在市場規模巨大，具有極高的投資回報。", "audio": "audios/2505.18139v1.mp3", "timestamp": "2025-05-26T05:15:03.711928"}
{"query": "Foundation Model", "id": "2505.18125v1", "url": "http://arxiv.org/abs/2505.18125v1", "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations", "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.", "authors": ["Alan Arazi", "Eilam Shapira", "Roi Reichart"], "published_date": "2025-05-23", "title_zh": "TabSTAR：具備語義目標感知表徵的基礎表格模型", "summary_zh": "論文介紹了TabSTAR，一種新的基礎表格模型，專注於處理包含文字特徵的表格數據。它透過讓模型理解目標（也就是要預測的內容）的語義，來更有效地利用文字資訊，從而大幅提升了在表格學習任務中的表現，超越了過去深度學習在這方面的不足，並勝過傳統的梯度提升決策樹模型。TabSTAR具有可遷移學習的特性，且架構通用，不需要針對特定數據集進行調整。", "applications": ["**電商商品推薦：** 根據商品名稱、描述等文字資訊，更精準地預測使用者會購買哪些商品，改善推薦系統的準確性。", "**醫療診斷輔助：** 分析病人的病歷、症狀描述等文字資料，結合其他表格數據（如年齡、性別），協助醫生進行更準確的疾病診斷。", "**金融風險評估：** 評估客戶的信用風險，結合客戶填寫的申請表格資訊（包括文字描述），更全面地了解客戶的還款能力。"], "pitch": "TabSTAR重新定義了表格數據的處理方式，尤其是在涉及大量文本資訊的場景中。與傳統方法相比，它具備更強的泛化能力和更高的準確性。想像一下，我們可以利用TabSTAR打造更智能的客戶服務、更精準的市場預測、以及更有效的風險管理系統。其無須針對特定數據集進行調整的架構，更降低了部署和維護成本。TabSTAR的潛在商業價值巨大，將成為各行業數據驅動決策的強大助力。我們的目標是將TabSTAR打造為表格數據分析領域的領先平台，為客戶提供更智能、更高效的解決方案，並在快速發展的AI市場中佔據一席之地。", "audio": "audios/2505.18125v1.mp3", "timestamp": "2025-05-26T05:15:19.215552"}
{"query": "Diffusion Model", "id": "2505.18145v1", "url": "http://arxiv.org/abs/2505.18145v1", "title": "Stochastic agent-based Monte Carlo simulations for reaction-diffusion models, population dynamics, and epidemic spreading", "summary": "We provide a succinct overview of the implementation of Monte Carlo\nalgorithms based on Markovian stochastic dynamics to study interacting and\nreacting many-particle systems away from thermal equilibrium. Such agent-based\ncomputer simulations constitute an effective tool to introduce undergraduate\nand beginning graduate students to current frontier research without requiring\nmuch prior knowledge or experience: Starting from direct visualization of\nsimulation data, students may gain immediate insight into emerging macroscopic\nfeatures of a complex model system and subsequently apply more sophisticated\ndata analysis to quantitatively characterize its often rich dynamical\nproperties, both in stationary and transient regimes. We utilize numerical\ninvestigations of paradigmatic reaction-diffusion systems, as well as\nstochastic models for population dynamics and epidemic spreading, to exemplify\nhow interdisciplinary computational research can be effectively utilized in\nbottom-up undergraduate and graduate education through learning by doing. In\naddition, we give helpful hints for the practical setup of Monte Carlo\nsimulation algorithms, provide sample codes, explain some typical data analysis\ntools, and describe various potential error sources and pitfalls, with tips for\navoiding them.", "authors": ["Mohamed Swailem", "Ulrich Dobramysl", "Ruslan Mukhamadiarov", "Uwe C. Täuber"], "published_date": "2025-05-23", "title_zh": "基於隨機代理人的蒙地卡羅模擬，用於反應擴散模型、族群動態與流行病傳播", "summary_zh": "本論文簡要概述了基於馬可夫隨機動態的蒙地卡羅演算法，用於研究非熱平衡狀態下相互作用和反應的多粒子系統。這種基於代理人的電腦模擬，為大學本科生和研究生提供了一個有效的工具，使其能夠在不需要太多先備知識或經驗的情況下，了解當前的尖端研究。透過直接可視化模擬數據，學生可以立即了解複雜模型系統中湧現的宏觀特徵，並隨後應用更複雜的數據分析，以定量描述其豐富的動態特性，無論是在靜態還是暫態狀態下。我們利用反應擴散系統、族群動態和流行病傳播的隨機模型進行數值研究，以例證如何通過邊做邊學的方式，在自下而上的本科和研究生教育中有效地利用跨學科的計算研究。此外，我們還提供有關蒙地卡羅模擬演算法的實際設置的有用提示、範例程式碼、說明一些典型的數據分析工具，並描述各種潛在的錯誤來源和陷阱，以及避免這些錯誤的技巧。", "applications": ["**疫情預測與控制：** 開發更精準的流行病傳播模型，用於預測疫情走向，評估不同干預措施（如社交距離、疫苗接種）的效果，協助政府制定更有效的防疫策略。", "**生態系統建模：** 模擬不同物種之間的相互作用，預測族群變化，了解氣候變遷或人為干擾對生態系統的影響，為生物多樣性保護提供科學依據。", "**藥物開發與優化：** 模擬藥物在體內的擴散與反應，優化藥物結構和劑量，提高藥效並降低副作用，加速新藥研發流程。"], "pitch": "我們提供的是一個基於隨機代理人的蒙地卡羅模擬平台，能夠模擬複雜系統的動態行為，應用範圍廣泛，涵蓋疫情預測、生態建模、藥物開發等多個領域。我們的優勢在於：易用性高，降低了建模門檻，讓非專業人士也能輕鬆上手；準確性高，能夠捕捉系統的複雜性和不確定性；可擴展性強，能夠整合不同的數據來源和模型。市場潛力巨大，例如，疫情預測市場每年數十億美元，而藥物開發市場更是高達數千億美元。透過與政府、研究機構、藥廠等合作，我們能夠將平台推廣到各個行業，創造巨大的商業價值。我們的盈利模式包括平台訂閱、客製化建模服務、數據分析諮詢等。我們正在尋找對AI模擬、生物科技、醫療健康領域有興趣的投資者，共同打造一個領先的複雜系統模擬平台。", "audio": "audios/2505.18145v1.mp3", "timestamp": "2025-05-26T05:15:38.223434"}
{"query": "AI", "id": "2505.18139v1", "url": "http://arxiv.org/abs/2505.18139v1", "title": "Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems", "summary": "This position paper argues that the theoretical inconsistency often observed\namong Responsible AI (RAI) metrics, such as differing fairness definitions or\ntradeoffs between accuracy and privacy, should be embraced as a valuable\nfeature rather than a flaw to be eliminated. We contend that navigating these\ninconsistencies, by treating metrics as divergent objectives, yields three key\nbenefits: (1) Normative Pluralism: Maintaining a full suite of potentially\ncontradictory metrics ensures that the diverse moral stances and stakeholder\nvalues inherent in RAI are adequately represented. (2) Epistemological\nCompleteness: The use of multiple, sometimes conflicting, metrics allows for a\nmore comprehensive capture of multifaceted ethical concepts, thereby preserving\ngreater informational fidelity about these concepts than any single, simplified\ndefinition. (3) Implicit Regularization: Jointly optimizing for theoretically\nconflicting objectives discourages overfitting to one specific metric, steering\nmodels towards solutions with enhanced generalization and robustness under\nreal-world complexities. In contrast, efforts to enforce theoretical\nconsistency by simplifying or pruning metrics risk narrowing this value\ndiversity, losing conceptual depth, and degrading model performance. We\ntherefore advocate for a shift in RAI theory and practice: from getting trapped\nin inconsistency to characterizing acceptable inconsistency thresholds and\nelucidating the mechanisms that permit robust, approximated consistency in\npractice.", "authors": ["Gordon Dai", "Yunze Xiao"], "published_date": "2025-05-23", "title_zh": "擁抱矛盾：理論上的不一致性不會阻礙建構負責任AI系統的道路", "summary_zh": "這篇論文認為，負責任AI（RAI）指標之間常見的理論不一致性，例如不同的公平性定義或準確性與隱私之間的權衡，應該被視為一種有價值的特性，而不是需要消除的缺陷。作者主張，透過將這些指標視為不同的目標來應對這些不一致性，可以帶來三個關鍵好處：(1)規範多元化：保持一套完整的、可能相互矛盾的指標，確保RAI中固有的多樣道德立場和利害關係人價值得到充分代表。(2)知識論完整性：使用多個、有時相互衝突的指標，可以更全面地捕捉多面向的倫理概念，從而比任何單一、簡化的定義更能保留關於這些概念的資訊保真度。(3)隱性正規化：聯合優化理論上衝突的目標，可以防止過度擬合到某個特定指標，從而引導模型朝向在現實世界複雜性下具有更強泛化和穩健性的解決方案。因此，論文主張RAI的理論和實踐應該轉變：從陷入不一致性轉向表徵可接受的不一致性閾值，並闡明允許在實踐中實現穩健、近似一致性的機制。", "applications": ["**貸款審核系統：** 銀行可以利用多個公平性指標（例如，族群平等、機會平等）來評估貸款審核演算法，即使這些指標之間存在衝突。這樣可以確保更全面地考量到不同群體的利益，並避免只優化單一指標而導致對特定群體的不公平待遇。", "**醫療診斷系統：** 醫療AI系統在診斷疾病時，可能需要在準確性和隱私之間做出權衡。例如，收集更詳細的患者數據可以提高診斷準確性，但也可能侵犯患者隱私。通過同時考慮多個指標（例如，診斷準確性、隱私風險），可以找到一個平衡點，在保護患者隱私的同時，提供盡可能準確的診斷結果。", "**招聘系統：** 招聘系統如果只專注於預測應聘者的工作表現，可能會忽略多元化和公平性。通過同時考慮多個指標（例如，工作表現、性別比例、種族比例），可以構建一個更公平、更具包容性的招聘系統，避免對特定群體的歧視。"], "pitch": "我們提出了一種革命性的RAI開發方法，擺脫了追求單一完美指標的迷思，轉而擁抱指標之間固有的矛盾。這不僅能確保AI系統更符合倫理和社會價值觀，更能提高其在複雜現實環境下的穩健性和泛化能力。我們打造的平台可以幫助企業和研究機構更有效地管理和優化多個衝突指標，最終構建出真正負責任且具有競爭力的AI產品。想像一下，一個貸款審核系統在公平性、準確性與風險控制之間取得完美平衡，或是醫療診斷系統在保護患者隱私的同時，提供最精準的診斷。這就是我們的願景，也是我們正在構建的未來。我們正在尋找投資者，共同引領RAI的發展方向，在這個快速增長的市場中搶佔先機。", "audio": "audios/2505.18139v1.mp3", "timestamp": "2025-05-26T06:20:43.990880"}
{"query": "Foundation Model", "id": "2505.18125v1", "url": "http://arxiv.org/abs/2505.18125v1", "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations", "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.", "authors": ["Alan Arazi", "Eilam Shapira", "Roi Reichart"], "published_date": "2025-05-23", "title_zh": "TabSTAR：具備語義目標感知表徵的基礎表格模型", "summary_zh": "過去深度學習在表格資料上的表現不如預期，表格學習任務多由梯度提升決策樹（GBDTs）主導。然而，近期表格基礎模型的發展為表格學習帶來了新希望，它們能利用真實世界的知識並在不同的資料集上泛化，尤其是在資料包含自由文本時。我們介紹了TabSTAR：一個具備語義目標感知表徵的基礎表格模型。TabSTAR專為在具有文本特徵的表格資料上實現遷移學習而設計，其架構不含特定於資料集的參數。它解凍了預訓練的文本編碼器，並將目標詞符作為輸入，為模型提供學習特定任務嵌入所需的上下文。TabSTAR在已知文本特徵分類任務基準測試的中型和大型資料集上均取得了最先進的效能，並且其預訓練階段在資料集數量上表現出縮放法則，為進一步的效能改進提供了途徑。", "applications": ["**信貸風險評估：** 利用客戶提供的文字資料（例如：申請理由、過去交易記錄描述）更精準地評估信貸風險，減少壞帳率。", "**客戶服務自動化：** 根據客戶在客服聊天視窗輸入的問題描述，自動分類問題並導向最合適的解決方案，提升客戶滿意度。", "**醫療診斷輔助：** 分析病患的病歷描述和醫學報告，輔助醫生更快速且準確地做出診斷，減少誤診率。"], "pitch": "TabSTAR解決了表格資料中，特別是包含文本特徵的資料的分析難題。現有的表格模型在處理文本資訊時效果不佳，而TabSTAR透過語義目標感知表徵，大幅提升了模型的準確性和泛化能力。這意味著在金融、客服、醫療等行業，可以利用TabSTAR更有效地分析結構化和非結構化資料，從而做出更明智的決策，提高效率並降低成本。其基於縮放法則的特性，也意味著隨著資料量的增加，效能還能持續提升，具有巨大的商業潛力。我們可以將TabSTAR打造為一個強大的API服務，提供給各行業使用，或者針對特定行業開發垂直解決方案。相比於需要大量客製化開發的傳統模型，TabSTAR具有更強的遷移學習能力和更低的部署成本，投資回報率高。", "audio": "audios/2505.18125v1.mp3", "timestamp": "2025-05-26T06:20:59.828260"}
{"query": "Diffusion Model", "id": "2505.18145v1", "url": "http://arxiv.org/abs/2505.18145v1", "title": "Stochastic agent-based Monte Carlo simulations for reaction-diffusion models, population dynamics, and epidemic spreading", "summary": "We provide a succinct overview of the implementation of Monte Carlo\nalgorithms based on Markovian stochastic dynamics to study interacting and\nreacting many-particle systems away from thermal equilibrium. Such agent-based\ncomputer simulations constitute an effective tool to introduce undergraduate\nand beginning graduate students to current frontier research without requiring\nmuch prior knowledge or experience: Starting from direct visualization of\nsimulation data, students may gain immediate insight into emerging macroscopic\nfeatures of a complex model system and subsequently apply more sophisticated\ndata analysis to quantitatively characterize its often rich dynamical\nproperties, both in stationary and transient regimes. We utilize numerical\ninvestigations of paradigmatic reaction-diffusion systems, as well as\nstochastic models for population dynamics and epidemic spreading, to exemplify\nhow interdisciplinary computational research can be effectively utilized in\nbottom-up undergraduate and graduate education through learning by doing. In\naddition, we give helpful hints for the practical setup of Monte Carlo\nsimulation algorithms, provide sample codes, explain some typical data analysis\ntools, and describe various potential error sources and pitfalls, with tips for\navoiding them.", "authors": ["Mohamed Swailem", "Ulrich Dobramysl", "Ruslan Mukhamadiarov", "Uwe C. Täuber"], "published_date": "2025-05-23", "title_zh": "基於隨機主體的蒙地卡羅模擬，用於反應擴散模型、族群動力學與疫情擴散", "summary_zh": "本論文簡要介紹了如何使用基於馬可夫隨機動力的蒙地卡羅演算法，研究非熱平衡狀態下相互作用和反應的多粒子系統。這種基於主體的電腦模擬工具，能有效引導大學生和研究生入門前沿研究，無需太多預備知識。學生可從直接視覺化模擬數據開始，立即了解複雜模型系統中湧現的宏觀特性，並進一步應用更複雜的數據分析，量化其豐富的動態特性（無論是穩定還是暫態）。我們使用反應擴散系統、族群動力學和疫情擴散的數值研究範例，展示了如何透過實作學習，在本科生和研究生教育中有效利用跨領域計算研究。此外，我們還提供蒙地卡羅模擬演算法的實用設置技巧、範例程式碼、一些典型數據分析工具，並描述各種潛在的錯誤來源和陷阱，以及避免它們的技巧。", "applications": ["**疾病傳播預測與控制：** 使用此模型模擬不同干預措施（例如疫苗接種、隔離）對疾病傳播的影響，幫助政府和醫療機構制定更有效的防疫策略。", "**生態系統建模：** 研究不同物種間的相互作用，預測氣候變化或其他環境因素對生態系統的影響，協助制定保育政策。", "**人群行為模擬：** 模擬大規模人群在特定環境下的行為（例如疏散、購物），優化公共場所設計和管理，降低安全風險。"], "pitch": "這項技術的核心價值在於提供一個低成本、高效能的模擬平台，能用於預測複雜系統的行為，並輔助決策。我們將基於這個模型開發一個雲端平台，讓使用者能夠輕鬆地設定參數、運行模擬並分析結果。市場機會包括：\n\n*   **政府機構：** 用於疫情模擬、城市規劃、災害管理等。\n*   **醫療機構：** 用於藥物開發、疾病傳播模型建立等。\n*   **環境保護機構：** 用於生態系統建模、環境影響評估等。\n*   **大型企業：** 用於供應鏈優化、消費者行為預測等。\n\n我們的競爭優勢在於演算法的效率和靈活性，能夠模擬大規模、複雜的系統。透過 SaaS 模式，我們能快速擴展用戶規模，並收取訂閱費用。預計在未來五年內，這個市場將呈現爆發式增長，我們有信心在這個領域取得領先地位。", "audio": "audios/2505.18145v1.mp3", "timestamp": "2025-05-26T06:21:19.640584"}
{"query": "AI", "id": "2505.18139v1", "url": "http://arxiv.org/abs/2505.18139v1", "title": "Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems", "summary": "This position paper argues that the theoretical inconsistency often observed\namong Responsible AI (RAI) metrics, such as differing fairness definitions or\ntradeoffs between accuracy and privacy, should be embraced as a valuable\nfeature rather than a flaw to be eliminated. We contend that navigating these\ninconsistencies, by treating metrics as divergent objectives, yields three key\nbenefits: (1) Normative Pluralism: Maintaining a full suite of potentially\ncontradictory metrics ensures that the diverse moral stances and stakeholder\nvalues inherent in RAI are adequately represented. (2) Epistemological\nCompleteness: The use of multiple, sometimes conflicting, metrics allows for a\nmore comprehensive capture of multifaceted ethical concepts, thereby preserving\ngreater informational fidelity about these concepts than any single, simplified\ndefinition. (3) Implicit Regularization: Jointly optimizing for theoretically\nconflicting objectives discourages overfitting to one specific metric, steering\nmodels towards solutions with enhanced generalization and robustness under\nreal-world complexities. In contrast, efforts to enforce theoretical\nconsistency by simplifying or pruning metrics risk narrowing this value\ndiversity, losing conceptual depth, and degrading model performance. We\ntherefore advocate for a shift in RAI theory and practice: from getting trapped\nin inconsistency to characterizing acceptable inconsistency thresholds and\nelucidating the mechanisms that permit robust, approximated consistency in\npractice.", "authors": ["Gordon Dai", "Yunze Xiao"], "published_date": "2025-05-23", "title_zh": "擁抱矛盾：理論上的不一致性不會阻礙建構負責任AI系統的道路", "summary_zh": "這篇論文認為，負責任AI（RAI）指標之間常見的理論不一致性，例如不同的公平性定義或準確性和隱私之間的權衡，應該被視為一種有價值的特性，而非需要消除的缺陷。透過將這些指標視為不同的目標來導航這些不一致性，可以帶來三個好處：(1) 規範多元主義：維護一套完整且可能相互矛盾的指標，確保RAI中固有的多樣道德立場和利益相關者價值觀得到充分代表。(2) 知識完整性：使用多個、有時相互衝突的指標，可以更全面地捕捉多方面的倫理概念，從而比任何單一簡化的定義保留更多關於這些概念的資訊保真度。(3) 隱式正則化：共同優化理論上衝突的目標可以阻止過度擬合於某個特定指標，引導模型朝著在現實世界複雜性下具有更強泛化能力和魯棒性的解決方案發展。因此，論文提倡RAI理論和實踐轉向：從受困於不一致性，轉向表徵可接受的不一致性閾值，並闡明允許在實踐中實現穩健、近似一致性的機制。", "applications": ["**招聘系統：** 在招聘AI中，同時考慮多種公平性指標（如族群公平性、個體公平性），即使它們相互衝突。這避免了模型只關注某一群體而歧視其他群體，確保招聘過程更公正和多元。", "**醫療診斷：** 在診斷疾病時，同時使用多種評估指標（如靈敏度、特異性、陽性預測值）。即使這些指標之間存在權衡，也能提供更全面的診斷資訊，幫助醫生做出更精確的判斷，降低誤診和漏診的風險。", "**自動駕駛汽車：** 在自動駕駛系統中，同時考慮安全性、效率和舒適性。這些目標之間可能存在衝突，例如為了絕對安全而犧牲行駛效率。通過平衡這些指標，可以設計出更安全、可靠且用户友好的自動駕駛系統。"], "pitch": "想像一下，一個AI系統不再試圖將所有複雜的道德標準簡化成一個單一的指標，而是能擁抱這些標準之間的矛盾，從而做出更明智、更全面的決策。這正是我們論文的核心思想。我們的方法能有效提升AI系統的公平性、可靠性和魯棒性，在醫療、金融、交通等關鍵領域具有巨大的應用潛力。我們的創新之處在於，我們不是試圖消除這些矛盾，而是利用它們來避免過擬合，並確保AI系統能更好地應對現實世界的複雜性。這是一個全新的範式轉移，將為負責任AI的發展開闢更廣闊的道路。投資我們，就是投資未來，一個更公平、更可靠、更人性化的AI未來。我們可以幫助企業建立值得信賴的AI系統，提升品牌聲譽，並在競爭激烈的市場中脫穎而出。", "audio": "audios/2505.18139v1.mp3", "timestamp": "2025-05-26T07:16:48.897842"}
{"query": "Foundation Model", "id": "2505.18125v1", "url": "http://arxiv.org/abs/2505.18125v1", "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations", "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.", "authors": ["Alan Arazi", "Eilam Shapira", "Roi Reichart"], "published_date": "2025-05-23", "title_zh": "TabSTAR：一個具備語義目標感知表徵的基礎表格模型", "summary_zh": "深度學習在許多領域取得了顯著的成功，但在表格學習任務上的表現一直不如梯度提升決策樹 (GBDTs)。不過，最近的進展正在為表格基礎模型鋪平道路，這些模型可以利用真實世界的知識並跨多樣化的資料集進行泛化，特別是在資料包含自由文本時。雖然將語言模型功能整合到表格任務中已被探索，但大多數現有方法使用靜態、目標無關的文本表徵，限制了其有效性。我們介紹TabSTAR：一個具備語義目標感知表徵的基礎表格模型。TabSTAR 旨在實現對具有文本特徵的表格數據的遷移學習，其架構不含特定於資料集的參數。它解凍了一個預訓練的文本編碼器，並將目標 tokens 作為輸入，這些目標 tokens 為模型提供學習特定任務嵌入所需的上下文。TabSTAR 在已知具有文本特徵的分類任務基準的中型和大型資料集上都取得了最先進的性能，並且其預訓練階段在資料集數量上展現了縮放定律，為進一步的性能改進提供了途徑。", "applications": ["**金融風險評估：** 使用客戶的貸款申請表（包含自由文本描述）預測違約風險。TabSTAR 可以理解申請中的細微語氣和重要細節，提升風險評估的準確性。", "**客戶服務情緒分析：** 分析客戶的線上客服對話記錄（包含自由文本和表格資料），了解客戶的情緒和需求。TabSTAR 可以識別客戶的抱怨和訴求，幫助客服人員更有效地解決問題，提升客戶滿意度。", "**產品推薦系統：** 結合產品描述（自由文本）和使用者購買歷史（表格資料），為使用者推薦更符合需求的產品。TabSTAR 可以理解產品描述中的關鍵字和使用者偏好，提供更個性化的推薦結果。"], "pitch": "TabSTAR 是一個突破性的表格基礎模型，它能理解並利用文本資料的語義信息，在許多傳統上由 GBDT 統治的表格數據任務中超越現有模型。其核心價值在於：1) **卓越的準確性：** 在分類任務中表現出領先的性能，能顯著提升商業決策的精準度。2) **高效的遷移學習：** 無需大量特定領域的訓練資料，即可快速部署到新的數據集和應用場景。3) **可擴展性：** 預訓練階段的縮放定律表明，隨著資料量的增加，TabSTAR 的性能還將持續提升。這使得 TabSTAR 在金融、電商、醫療保健等擁有大量文本和表格混合資料的行業具有巨大的商業潛力，可以幫助企業降低成本、提高效率、並創造新的收入來源。我們相信 TabSTAR 将成為企業数据战略的重要组成部分，并将深刻地改变企业利用表格数据的方式。", "audio": "audios/2505.18125v1.mp3", "timestamp": "2025-05-26T07:17:06.101471"}
{"query": "Diffusion Model", "id": "2505.18145v1", "url": "http://arxiv.org/abs/2505.18145v1", "title": "Stochastic agent-based Monte Carlo simulations for reaction-diffusion models, population dynamics, and epidemic spreading", "summary": "We provide a succinct overview of the implementation of Monte Carlo\nalgorithms based on Markovian stochastic dynamics to study interacting and\nreacting many-particle systems away from thermal equilibrium. Such agent-based\ncomputer simulations constitute an effective tool to introduce undergraduate\nand beginning graduate students to current frontier research without requiring\nmuch prior knowledge or experience: Starting from direct visualization of\nsimulation data, students may gain immediate insight into emerging macroscopic\nfeatures of a complex model system and subsequently apply more sophisticated\ndata analysis to quantitatively characterize its often rich dynamical\nproperties, both in stationary and transient regimes. We utilize numerical\ninvestigations of paradigmatic reaction-diffusion systems, as well as\nstochastic models for population dynamics and epidemic spreading, to exemplify\nhow interdisciplinary computational research can be effectively utilized in\nbottom-up undergraduate and graduate education through learning by doing. In\naddition, we give helpful hints for the practical setup of Monte Carlo\nsimulation algorithms, provide sample codes, explain some typical data analysis\ntools, and describe various potential error sources and pitfalls, with tips for\navoiding them.", "authors": ["Mohamed Swailem", "Ulrich Dobramysl", "Ruslan Mukhamadiarov", "Uwe C. Täuber"], "published_date": "2025-05-23", "title_zh": "基於隨機代理人的蒙地卡羅模擬：應用於反應擴散模型、族群動態與流行病傳播", "summary_zh": "本研究簡要介紹了基於馬可夫隨機動態的蒙地卡羅演算法，用於研究遠離熱平衡的多粒子交互與反應系統。這種基於代理人的電腦模擬是一個有效的工具，能讓大學生和研究生在無需太多先備知識或經驗的情況下，接觸到當前的前沿研究。學生可以從直接視覺化模擬數據開始，立即了解複雜模型系統中湧現的宏觀特徵，並進一步應用更複雜的數據分析來定量描述其豐富的動態特性，包括靜態和瞬態。我們利用對典型反應擴散系統、族群動態和流行病傳播的隨機模型進行數值研究，來展示如何通過實作學習的方式，在自下而上的本科和研究生教育中有效利用跨學科的計算研究。此外，我們還提供了蒙地卡羅模擬演算法的實際設置技巧、示例程式碼、一些典型的數據分析工具，並描述了各種潛在的錯誤來源和陷阱，以及避免它們的技巧。", "applications": ["**疫情預測與控制策略評估：** 利用該模型，政府機構可以模擬不同防疫措施（如社交距離、疫苗接種）對疫情傳播的影響，從而制定更有效的控制策略。", "**生態環境保護：** 研究者可以使用該模型模擬不同物種間的相互作用，以及環境變化（如氣候變遷、棲地破壞）對族群動態的影響，為生態保護提供科學依據。", "**市場行為模擬：** 公司可以使用該模型模擬消費者行為，評估不同行銷策略的效果，或者預測新產品的市場接受度。"], "pitch": "想像一下，在醫療、環境和商業領域擁有精準預測未來的能力。這項研究成果的核心是一種強大的蒙地卡羅模擬技術，能夠建模並預測複雜系統的行為，例如疾病傳播、生態系統演變和市場動態。它的商業價值體現在三個關鍵方面：首先，它能幫助政府和企業做出更明智的決策，減少風險並提高效率。其次，這種技術可以整合到現有的分析平台中，提供更深入的洞察。第三，其模組化設計使其具有高度的適應性，可以應用於各種不同的領域。我們相信，這項技術將成為未來決策制定不可或缺的一部分，並為早期採用者帶來顯著的競爭優勢。我們正在尋找投資者，共同將這項技術商業化，打造一個基於模擬的未來。", "audio": "audios/2505.18145v1.mp3", "timestamp": "2025-05-26T07:17:22.680662"}
{"query": "AI", "id": "2505.18139v1", "url": "http://arxiv.org/abs/2505.18139v1", "title": "Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems", "summary": "This position paper argues that the theoretical inconsistency often observed\namong Responsible AI (RAI) metrics, such as differing fairness definitions or\ntradeoffs between accuracy and privacy, should be embraced as a valuable\nfeature rather than a flaw to be eliminated. We contend that navigating these\ninconsistencies, by treating metrics as divergent objectives, yields three key\nbenefits: (1) Normative Pluralism: Maintaining a full suite of potentially\ncontradictory metrics ensures that the diverse moral stances and stakeholder\nvalues inherent in RAI are adequately represented. (2) Epistemological\nCompleteness: The use of multiple, sometimes conflicting, metrics allows for a\nmore comprehensive capture of multifaceted ethical concepts, thereby preserving\ngreater informational fidelity about these concepts than any single, simplified\ndefinition. (3) Implicit Regularization: Jointly optimizing for theoretically\nconflicting objectives discourages overfitting to one specific metric, steering\nmodels towards solutions with enhanced generalization and robustness under\nreal-world complexities. In contrast, efforts to enforce theoretical\nconsistency by simplifying or pruning metrics risk narrowing this value\ndiversity, losing conceptual depth, and degrading model performance. We\ntherefore advocate for a shift in RAI theory and practice: from getting trapped\nin inconsistency to characterizing acceptable inconsistency thresholds and\nelucidating the mechanisms that permit robust, approximated consistency in\npractice.", "authors": ["Gordon Dai", "Yunze Xiao"], "published_date": "2025-05-23", "title_zh": "擁抱矛盾：理論上的不一致不會阻礙構建負責任的人工智慧系統之路", "summary_zh": "這篇論文認為，負責任AI (RAI) 指標之間常見的理論不一致，例如不同的公平性定義或準確性與隱私之間的權衡，應該被視為一種有價值的特徵，而非需要消除的缺陷。作者主張，透過將這些指標視為不同的目標來應對這些不一致性，可以帶來三個關鍵好處：規範多元主義、認識論完整性以及隱性正則化。簡而言之，別試圖簡化或消除 RAI 指標的矛盾，擁抱它們，反而能讓模型更完善、更強大，更好地應對真實世界的複雜挑戰。", "applications": ["**醫療診斷:** 一個診斷系統可能需要同時考慮準確性、不同族群的公平性以及病患的隱私。擁抱矛盾允許系統在這些目標之間取得平衡，避免過度優化某個單一指標而犧牲其他重要考量，例如避免算法對特定人種產生誤診偏差。", "**貸款審批:** 貸款審批系統需要權衡貸款違約的風險、不同種族或性別群體的公平性以及申請人的隱私。擁抱矛盾能讓系統更全面地評估申請，避免歧視性結果，同時保護申請人的敏感信息。", "**招聘系統:** 招聘系統需要考慮技能匹配度、多元化目標以及求職者的隱私。系統可以利用多個衝突指標，例如同時優化員工背景的多元性，並減少對學歷的過度依賴，提升人才庫的豐富性。"], "pitch": "我們正在構建一個全新的負責任AI開發平台，核心理念是『擁抱矛盾』。傳統的AI開發往往追求指標的統一和簡化，但這忽略了真實世界的多樣性和複雜性。我們的平台允許開發者同時導入多個可能互相矛盾的 RAI 指標，並利用先進的優化算法，找到一個最佳平衡點，打造更公平、更穩健的AI模型。這不僅能提升模型的性能，還能降低法律風險和聲譽風險。 我們相信，在監管日趨嚴格的時代，我們的平台將成為企業構建可信賴AI的關鍵工具，搶佔市場先機。 投資回報不僅體現在更好的模型和更低的風險上，更體現在品牌價值和社會責任的提升上。我們正在尋找有遠見的投資者，共同打造負責任AI的未來。", "audio": "audios/2505.18139v1.mp3", "timestamp": "2025-05-26T09:41:03.983035"}
{"query": "Foundation Model", "id": "2505.18125v1", "url": "http://arxiv.org/abs/2505.18125v1", "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations", "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.", "authors": ["Alan Arazi", "Eilam Shapira", "Roi Reichart"], "published_date": "2025-05-23", "title_zh": "TabSTAR：具備語義目標感知表示的基礎表格模型", "summary_zh": "TabSTAR是一個新型的表格基礎模型，專為處理包含文字特徵的表格數據而設計。傳統上，深度學習在表格數據上表現不佳，但TabSTAR透過結合預訓練語言模型和目標感知的嵌入表示，實現了在包含文字特徵的表格分類任務上的最先進性能。它不需要針對特定數據集進行參數調整，並且可以透過增加數據集來進一步提升性能。", "applications": ["**金融風險評估：** 利用新聞報導、公司簡介等文字資訊來評估企業或個人的信用風險，比傳統僅依賴數字的評估更全面。", "**醫療診斷輔助：** 結合病歷中的文字描述（如症狀、主訴）和檢驗數據，輔助醫生進行更精準的診斷。", "**電商產品推薦：** 分析產品描述、用戶評論等文字資訊，更精準地推薦用戶感興趣的商品。"], "pitch": "TabSTAR是深度學習在表格數據領域的重大突破，尤其擅長處理包含文字資訊的複雜表格。其核心優勢在於預訓練模型的泛化能力和目標感知的嵌入表示，使其能超越傳統基於決策樹的方法，在風險評估、醫療診斷、電商推薦等領域提供卓越的性能提升。相較於需要大量客製化調整的傳統模型，TabSTAR的架構使其更易於部署和擴展，降低了開發成本。想像一下，銀行能更精準地評估貸款風險，醫院能更快速地做出診斷，電商平台能更有效地提升銷售額。TabSTAR具有顯著的性能優勢和廣泛的應用前景，是下一代表格數據分析的關鍵技術，具有巨大的商業價值。", "audio": "audios/2505.18125v1.mp3", "timestamp": "2025-05-26T09:41:14.633952"}
{"query": "Diffusion Model", "id": "2505.18145v1", "url": "http://arxiv.org/abs/2505.18145v1", "title": "Stochastic agent-based Monte Carlo simulations for reaction-diffusion models, population dynamics, and epidemic spreading", "summary": "We provide a succinct overview of the implementation of Monte Carlo\nalgorithms based on Markovian stochastic dynamics to study interacting and\nreacting many-particle systems away from thermal equilibrium. Such agent-based\ncomputer simulations constitute an effective tool to introduce undergraduate\nand beginning graduate students to current frontier research without requiring\nmuch prior knowledge or experience: Starting from direct visualization of\nsimulation data, students may gain immediate insight into emerging macroscopic\nfeatures of a complex model system and subsequently apply more sophisticated\ndata analysis to quantitatively characterize its often rich dynamical\nproperties, both in stationary and transient regimes. We utilize numerical\ninvestigations of paradigmatic reaction-diffusion systems, as well as\nstochastic models for population dynamics and epidemic spreading, to exemplify\nhow interdisciplinary computational research can be effectively utilized in\nbottom-up undergraduate and graduate education through learning by doing. In\naddition, we give helpful hints for the practical setup of Monte Carlo\nsimulation algorithms, provide sample codes, explain some typical data analysis\ntools, and describe various potential error sources and pitfalls, with tips for\navoiding them.", "authors": ["Mohamed Swailem", "Ulrich Dobramysl", "Ruslan Mukhamadiarov", "Uwe C. Täuber"], "published_date": "2025-05-23", "title_zh": "反應-擴散模型、族群動力學與流行病傳播之基於隨機代理人的蒙地卡羅模擬", "summary_zh": "本研究提供馬可夫隨機動力學為基礎的蒙地卡羅演算法之簡潔概述，用於研究遠離熱平衡的交互作用和反應的多粒子系統。這種基於代理人的電腦模擬，讓大學部與研究生能快速入門前沿研究，無需太多先備知識。學生可透過直接可視化的模擬數據，立即了解複雜模型系統中出現的宏觀特徵，並進一步應用更複雜的數據分析，量化其豐富的動態特性，不論是在穩定還是暫態狀態。我們利用反應-擴散系統、族群動力學和流行病傳播的數值研究，闡明跨領域計算研究如何有效地透過「做中學」的方式，應用於大學部和研究所的教育。此外，我們也提供蒙地卡羅模擬演算法的實用設置技巧、範例程式碼、典型數據分析工具說明，並描述各種潛在的錯誤來源和陷阱，以及避免它們的技巧。", "applications": ["**疾病爆發模擬與控制策略評估：** 用於預測不同干預措施（如疫苗接種、社交距離）對疫情擴散的影響，幫助制定更有效的防疫策略。", "**生態系統建模與管理：** 模擬不同物種間的互動，預測環境變化（如氣候變遷、棲息地喪失）對生態系統的影響，支持生態保育決策。", "**城市規劃與交通流量優化：** 模擬城市人口移動和交通模式，優化交通流量，減少擁堵，並改善城市規劃，提高居民生活品質。"], "pitch": "這項技術能加速複雜系統的建模和分析，具有巨大的商業價值。想像一下，一家保險公司可以使用我們的模擬工具來預測自然災害的影響，制定更精準的風險評估模型。或者，一家藥廠可以利用它來加速藥物開發，通過模擬藥物在人體內的擴散和反應，找到更有效的藥物配方。從疫情預測、生態保育到城市規劃，這項技術的應用潛力無限。我們尋求的投資將用於開發更易於使用的模擬平台，擴大應用場景，並建立一個專業的諮詢團隊，協助各行業客戶解決實際問題，快速將研究成果轉化為商業價值，實現高回報。", "audio": "audios/2505.18145v1.mp3", "timestamp": "2025-05-26T09:41:29.553770"}
{"query": "AI", "id": "2505.18139v1", "url": "http://arxiv.org/abs/2505.18139v1", "title": "Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems", "summary": "This position paper argues that the theoretical inconsistency often observed\namong Responsible AI (RAI) metrics, such as differing fairness definitions or\ntradeoffs between accuracy and privacy, should be embraced as a valuable\nfeature rather than a flaw to be eliminated. We contend that navigating these\ninconsistencies, by treating metrics as divergent objectives, yields three key\nbenefits: (1) Normative Pluralism: Maintaining a full suite of potentially\ncontradictory metrics ensures that the diverse moral stances and stakeholder\nvalues inherent in RAI are adequately represented. (2) Epistemological\nCompleteness: The use of multiple, sometimes conflicting, metrics allows for a\nmore comprehensive capture of multifaceted ethical concepts, thereby preserving\ngreater informational fidelity about these concepts than any single, simplified\ndefinition. (3) Implicit Regularization: Jointly optimizing for theoretically\nconflicting objectives discourages overfitting to one specific metric, steering\nmodels towards solutions with enhanced generalization and robustness under\nreal-world complexities. In contrast, efforts to enforce theoretical\nconsistency by simplifying or pruning metrics risk narrowing this value\ndiversity, losing conceptual depth, and degrading model performance. We\ntherefore advocate for a shift in RAI theory and practice: from getting trapped\nin inconsistency to characterizing acceptable inconsistency thresholds and\nelucidating the mechanisms that permit robust, approximated consistency in\npractice.", "authors": ["Gordon Dai", "Yunze Xiao"], "published_date": "2025-05-23", "title_zh": "擁抱矛盾：理論上的不一致性不會阻礙構建負責任的 AI 系統之路", "summary_zh": "這篇論文認為，在負責任的 AI (RAI) 指標中常見的理論不一致性，例如不同的公平性定義或準確性與隱私之間的權衡，應該被視為一種有價值的特性，而不是需要消除的缺陷。透過將這些指標視為不同的目標，可以帶來規範多元、知識完整以及隱性正規化的好處。試圖強行統一這些指標反而會喪失價值多樣性、概念深度，並降低模型性能。因此，論文提倡改變 RAI 的理論與實踐方向：從被困在不一致性中，轉向描述可接受的不一致性閾值，並闡明在實踐中實現穩健、近似一致性的機制。", "applications": ["**貸款審批:** 銀行在評估貸款申請時，可以同時考慮多種公平性指標（例如，性別、種族、年齡），即使這些指標之間存在衝突，也能更全面地反映申請人的情況，避免歧視，並提供更客觀的審批結果。", "**招聘系統:** 人力資源部門使用 AI 篩選履歷時，可以同時優化準確性、公平性和多元化。雖然這些目標可能互相衝突，但透過容忍一定程度的不一致性，可以建立一個更公平、更具包容性的招聘流程，找到更優秀、更多元的人才。", "**醫療診斷:** AI 輔助診斷系統在判斷疾病時，可以同時考慮多種指標，例如準確率、召回率、避免假陽性率等。即使這些指標之間存在衝突，也能更全面地評估診斷結果，減少誤診和漏診的風險，提高醫療品質。"], "pitch": "想像一下，一個能同時優化準確性、公平性、隱私性和多樣性的AI引擎。我們不是消除衝突的指標，而是擁抱它們。我們的技術能讓企業在複雜的環境中做出更明智、更負責任的決策，解決醫療、金融、招聘等各行各業的痛點。透過設定可接受的不一致性閾值，我們實現了性能和道德責任之間的微妙平衡。這不僅提升了模型的泛化能力和穩健性，也確保了 AI 系統符合多樣化的道德標準和利益相關者的價值觀。我們為企業提供的不僅僅是一個算法，更是一個負責任的AI解決方案，在嚴格的監管環境中，這將成為他們獲得競爭優勢的關鍵。投資我們的技術，就是投資AI的未來，一個更公平、更透明、更可信賴的未來。", "audio": "audios/2505.18139v1.mp3", "timestamp": "2025-05-26T10:18:23.998544"}
{"query": "Foundation Model", "id": "2505.18125v1", "url": "http://arxiv.org/abs/2505.18125v1", "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations", "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.", "authors": ["Alan Arazi", "Eilam Shapira", "Roi Reichart"], "published_date": "2025-05-23", "title_zh": "TabSTAR：具備語義目標感知表徵的基礎表格模型", "summary_zh": "過往深度學習在表格數據上的表現不如傳統的梯度提升決策樹。TabSTAR 是一種新型的基礎表格模型，它能利用語義目標感知表徵，讓模型更有效地處理帶有自由文本的表格數據。 TabSTAR 透過解鎖預訓練的文本編碼器，並將目標標記作為輸入，提供模型學習任務特定嵌入所需的上下文。在帶有文本特徵的分類任務中，TabSTAR 在中、大型數據集上均展現了最先進的性能，並顯示出隨著數據集數量增加，效能也會隨之提升的趨勢。", "applications": ["**金融詐欺檢測：** 利用客戶的交易記錄（表格數據）和相關的文字描述（例如：交易備註、聊天記錄等）來預測詐欺行為。TabSTAR 可以分析這些數據，找出與詐欺行為相關的語義模式，從而提高檢測的準確性。", "**醫療診斷輔助：** 整合病人的病歷（表格數據，例如：年齡、性別、檢查結果）和醫生的診斷記錄（文字描述）來預測疾病。TabSTAR 可以利用診斷記錄中的關鍵詞，更精準地分析病歷數據，提供醫生更全面的診斷建議。", "**產品評論情感分析與推薦：** 分析產品的規格參數（表格數據）和用戶評論（文字描述），不僅評估情感傾向，還能理解評論背後的具體理由，進而提供更精準的產品推薦。 例如：手機電池續航力（表格）搭配評論「電池續航一天很輕鬆」（文字），模型可學習理解兩者之間的關係。"], "pitch": "TabSTAR 解決了深度學習在表格數據，特別是包含自由文本的表格數據上的瓶頸。傳統方法無法有效利用文本信息，導致效能受限。TabSTAR 通過語義目標感知表徵，顯著提升了模型在分類任務上的表現，並展現出良好的擴展性。想像一下，金融機構可以利用它更準確地檢測詐欺，醫療機構可以利用它輔助醫生進行更精準的診斷。 我們相信 TabSTAR 有潜力成为表格數據分析领域的新一代基石模型，带来数据驱动决策的革命。 我們尋求投資，以加速 TabSTAR 的商業化，建立完善的應用生態系統，並在各個行業中推廣其應用， 抢占市场先机。", "audio": "audios/2505.18125v1.mp3", "timestamp": "2025-05-26T10:18:41.207731"}
{"query": "Diffusion Model", "id": "2505.18145v1", "url": "http://arxiv.org/abs/2505.18145v1", "title": "Stochastic agent-based Monte Carlo simulations for reaction-diffusion models, population dynamics, and epidemic spreading", "summary": "We provide a succinct overview of the implementation of Monte Carlo\nalgorithms based on Markovian stochastic dynamics to study interacting and\nreacting many-particle systems away from thermal equilibrium. Such agent-based\ncomputer simulations constitute an effective tool to introduce undergraduate\nand beginning graduate students to current frontier research without requiring\nmuch prior knowledge or experience: Starting from direct visualization of\nsimulation data, students may gain immediate insight into emerging macroscopic\nfeatures of a complex model system and subsequently apply more sophisticated\ndata analysis to quantitatively characterize its often rich dynamical\nproperties, both in stationary and transient regimes. We utilize numerical\ninvestigations of paradigmatic reaction-diffusion systems, as well as\nstochastic models for population dynamics and epidemic spreading, to exemplify\nhow interdisciplinary computational research can be effectively utilized in\nbottom-up undergraduate and graduate education through learning by doing. In\naddition, we give helpful hints for the practical setup of Monte Carlo\nsimulation algorithms, provide sample codes, explain some typical data analysis\ntools, and describe various potential error sources and pitfalls, with tips for\navoiding them.", "authors": ["Mohamed Swailem", "Ulrich Dobramysl", "Ruslan Mukhamadiarov", "Uwe C. Täuber"], "published_date": "2025-05-23", "title_zh": "反應擴散模型、族群動態與流行病傳播之隨機基於個體蒙地卡羅模擬", "summary_zh": "本研究提供基於馬可夫隨機動態的蒙地卡羅演算法的簡潔概述，用於研究遠離熱平衡的相互作用和反應的多粒子系統。這種基於個體的電腦模擬是一種有效的工具，可以讓大學生和剛開始的研究生接觸到當前的尖端研究，而不需要太多的先備知識或經驗。學生可以從直接視覺化模擬數據開始，立即深入了解複雜模型系統的新興巨觀特徵，然後應用更複雜的數據分析來定量描述其通常豐富的動態特性，無論是在靜態還是瞬態狀態下。我們利用範例反應擴散系統，以及族群動態和流行病傳播的隨機模型進行數值研究，以舉例說明如何通過邊做邊學的方式，在自下而上的大學和研究生教育中有效利用跨學科的計算研究。此外，我們還提供了蒙地卡羅模擬演算法的實用設置提示、示例程式碼、解釋了一些典型的數據分析工具，並描述了各種潛在的錯誤來源和陷阱，並提供了避免它們的技巧。", "applications": ["**疫情控制模擬:** 模擬不同干預措施（例如疫苗接種、封鎖）對疫情傳播的影響，幫助制定更有效的公共衛生政策。", "**生態系統建模:** 模擬不同物種之間的相互作用和環境變化對族群數量的影響，用於保護瀕危物種和維護生態平衡。", "**城市規劃模擬:** 模擬人口增長、交通流量和資源分配對城市發展的影響，幫助規劃更宜居的城市環境。"], "pitch": "我們開發的蒙地卡羅模擬引擎，能以更低的成本和更高的靈活性，模擬複雜系統的動態演變，例如疫情傳播、族群動態、資源分配等。相比傳統的基於方程式的模型，我們的方法更能捕捉個體行為的隨機性和異質性，提供更精確的預測和更深入的洞察。目標客戶包括：政府機構（用於制定公共政策）、科研院所（用於學術研究）、企業（用於風險管理和市場預測）。我們將提供客製化模擬服務和雲端平台，讓客戶可以輕鬆地進行模擬，並根據模擬結果做出更明智的決策。 我們的商業價值在於：\n1. **降低決策風險:** 為政策制定者、科研人員和企業提供決策參考。\n2. **加速研究進程:** 通過模擬驗證假設，加速新發現的產生。\n3. **開拓新的商業模式:** 將模擬能力整合到其他產品或服務中，創造新的價值。", "audio": "audios/2505.18145v1.mp3", "timestamp": "2025-05-26T10:18:59.987014"}
{"query": "AI", "id": "2505.18139v1", "url": "http://arxiv.org/abs/2505.18139v1", "title": "Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems", "summary": "This position paper argues that the theoretical inconsistency often observed\namong Responsible AI (RAI) metrics, such as differing fairness definitions or\ntradeoffs between accuracy and privacy, should be embraced as a valuable\nfeature rather than a flaw to be eliminated. We contend that navigating these\ninconsistencies, by treating metrics as divergent objectives, yields three key\nbenefits: (1) Normative Pluralism: Maintaining a full suite of potentially\ncontradictory metrics ensures that the diverse moral stances and stakeholder\nvalues inherent in RAI are adequately represented. (2) Epistemological\nCompleteness: The use of multiple, sometimes conflicting, metrics allows for a\nmore comprehensive capture of multifaceted ethical concepts, thereby preserving\ngreater informational fidelity about these concepts than any single, simplified\ndefinition. (3) Implicit Regularization: Jointly optimizing for theoretically\nconflicting objectives discourages overfitting to one specific metric, steering\nmodels towards solutions with enhanced generalization and robustness under\nreal-world complexities. In contrast, efforts to enforce theoretical\nconsistency by simplifying or pruning metrics risk narrowing this value\ndiversity, losing conceptual depth, and degrading model performance. We\ntherefore advocate for a shift in RAI theory and practice: from getting trapped\nin inconsistency to characterizing acceptable inconsistency thresholds and\nelucidating the mechanisms that permit robust, approximated consistency in\npractice.", "authors": ["Gordon Dai", "Yunze Xiao"], "published_date": "2025-05-23", "title_zh": "擁抱矛盾：理論上的不一致性不會阻礙負責任的 AI 系統構建之路", "summary_zh": "這篇論文主張，負責任 AI（RAI）指標之間常見的理論不一致，例如不同的公平性定義或準確性與隱私之間的權衡，應被視為寶貴的特性而非需要消除的缺陷。透過將這些指標視為不同的目標來處理，可以帶來三項好處：(1) 規範多元主義：維持一整套可能互相矛盾的指標，確保 RAI 中固有的多樣道德立場和利害關係人價值觀得到充分代表。(2) 知識完整性：使用多個，有時相互衝突的指標，可以更全面地捕捉多方面的倫理概念，從而比任何單一、簡化的定義保持更大的信息保真度。(3) 隱式正則化：聯合優化理論上相互衝突的目標，可以避免過度擬合到特定指標，從而引導模型朝著在真實世界複雜性下具有增強的泛化和魯棒性的解決方案。因此，我們提倡 RAI 理論和實踐的轉變：從陷入不一致性到描述可接受的不一致性閾值，並闡明允許在實踐中實現穩健、近似一致性的機制。", "applications": ["醫療診斷：在診斷癌症時，模型同時考慮準確性、假陽性率和假陰性率，避免過度強調單一指標而導致誤診或延遲治療。", "貸款批准：在自動批准貸款時，模型考慮公平性（不同族群的批准率）、準確性（預測違約能力）和解釋性（為何批准或拒絕），避免演算法歧視並提供透明的決策過程。", "自動駕駛：在自動駕駛系統中，模型同時優化安全性（避免事故）、效率（行駛時間）和舒適性（平穩駕駛），避免為了追求絕對安全而犧牲乘客體驗或造成交通堵塞。"], "pitch": "負責任 AI 的核心挑戰不在於消除矛盾，而在於管理矛盾。我們提出的方法論，透過擁抱指標間的固有不一致，能打造更公平、更穩健、更具泛化能力的 AI 系統。想像一下，一個不會因過度擬合單一指標而造成偏見的 AI，這代表更高的用戶信任度、更低的法律風險以及更廣闊的應用市場。我們的技術讓企業能夠在追求商業利益的同時，確保 AI 的道德性和社會責任，這是一個巨大的市場機會，因為越來越多的企業和政府機構正在尋找負責任 AI 的解決方案。投資我們，您將投資於下一代 AI 技術，它不僅聰明，而且負責任。", "audio": "audios/2505.18139v1.mp3", "timestamp": "2025-05-26T11:11:13.059997"}
{"query": "Foundation Model", "id": "2505.18125v1", "url": "http://arxiv.org/abs/2505.18125v1", "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations", "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.", "authors": ["Alan Arazi", "Eilam Shapira", "Roi Reichart"], "published_date": "2025-05-23", "title_zh": "TabSTAR：具備語義目標感知表徵的基礎表格模型", "summary_zh": "儘管深度學習在許多領域取得了顯著的成功，但在表格學習任務上的表現一直不如傳統的梯度提升決策樹（GBDTs）。然而，最近的進展正在為表格基礎模型鋪平道路，這些模型可以利用真實世界的知識並跨多樣化的數據集進行泛化，尤其是在數據包含自由文本時。儘管將語言模型功能整合到表格任務中已經被探索過，但大多數現有方法都使用靜態的、目標無關的文本表徵，限制了它們的有效性。我們介紹 TabSTAR：一種具備語義目標感知表徵的基礎表格模型。TabSTAR 旨在實現具有文本特徵的表格數據的遷移學習，其架構不包含數據集特定的參數。它解凍了一個預訓練的文本編碼器，並將目標標記作為輸入，這些標記為模型提供了學習特定於任務的嵌入所需的上下文。TabSTAR 在具有文本特徵的分類任務的已知基準測試中，針對中型和大型數據集都實現了最先進的性能，並且其預訓練階段在數據集數量上表現出縮放定律，為進一步的性能改進提供了途徑。", "applications": ["**貸款風險評估：**銀行可以利用TabSTAR分析包含客戶文字描述（例如，貸款用途）的表格數據，更準確地預測貸款違約風險，優於傳統的GBDT方法。", "**產品評價分析：**電商平台可以利用TabSTAR分析產品表格數據，其中包含產品描述和用戶評價，更有效地識別劣質產品或詐欺行為，提升平台品質和用戶信任。", "**醫療診斷輔助：**醫療機構可以利用TabSTAR分析包含患者病歷數據（例如，症狀描述）的表格數據，輔助醫生進行疾病診斷，提高診斷效率和準確性。"], "pitch": "TabSTAR是一個突破性的表格數據處理模型，尤其擅長處理含有文本特徵的表格數據。傳統的表格數據處理方式往往忽略了文本的語義信息，而TabSTAR通過引入語義目標感知表徵，顯著提升了模型的預測能力。這意味著我們可以利用TabSTAR在金融、電商、醫療等各個行業中，對現有的表格數據進行更深入的分析，挖掘出更有價值的洞見。例如，可以更精準地進行信用評估、風險預測、客戶分群、產品推薦等等。TabSTAR的潛在商業價值巨大，我們相信它可以成為企業提升決策效率、優化業務流程的重要工具，具有快速成長為行業領導者的潛力。", "audio": "audios/2505.18125v1.mp3", "timestamp": "2025-05-26T11:11:29.171814"}
{"query": "Diffusion Model", "id": "2505.18145v1", "url": "http://arxiv.org/abs/2505.18145v1", "title": "Stochastic agent-based Monte Carlo simulations for reaction-diffusion models, population dynamics, and epidemic spreading", "summary": "We provide a succinct overview of the implementation of Monte Carlo\nalgorithms based on Markovian stochastic dynamics to study interacting and\nreacting many-particle systems away from thermal equilibrium. Such agent-based\ncomputer simulations constitute an effective tool to introduce undergraduate\nand beginning graduate students to current frontier research without requiring\nmuch prior knowledge or experience: Starting from direct visualization of\nsimulation data, students may gain immediate insight into emerging macroscopic\nfeatures of a complex model system and subsequently apply more sophisticated\ndata analysis to quantitatively characterize its often rich dynamical\nproperties, both in stationary and transient regimes. We utilize numerical\ninvestigations of paradigmatic reaction-diffusion systems, as well as\nstochastic models for population dynamics and epidemic spreading, to exemplify\nhow interdisciplinary computational research can be effectively utilized in\nbottom-up undergraduate and graduate education through learning by doing. In\naddition, we give helpful hints for the practical setup of Monte Carlo\nsimulation algorithms, provide sample codes, explain some typical data analysis\ntools, and describe various potential error sources and pitfalls, with tips for\navoiding them.", "authors": ["Mohamed Swailem", "Ulrich Dobramysl", "Ruslan Mukhamadiarov", "Uwe C. Täuber"], "published_date": "2025-05-23", "title_zh": "基於隨機代理人的蒙地卡羅模擬，用於反應擴散模型、族群動力學與傳染病擴散", "summary_zh": "本研究簡介了使用基於馬可夫隨機動力的蒙地卡羅演算法，來研究遠離熱平衡的交互作用與反應多粒子系統。這種基於代理人的電腦模擬，提供了一個有效的工具，讓大學部和剛開始的研究生，能在不需要太多先備知識或經驗的情況下，接觸到當前的前沿研究。透過直接視覺化模擬數據，學生可以立即了解複雜模型系統中湧現的巨觀特徵，並進一步應用更複雜的數據分析，來量化描述其豐富的動態特性，無論是在穩定狀態還是暫態狀態。 我們利用範例反應擴散系統、以及族群動力學和傳染病傳播的隨機模型之數值研究，來展示如何透過邊做邊學的方式，在由下而上的大學部和研究所教育中，有效地利用跨領域的計算研究。 此外，我們還提供了蒙地卡羅模擬演算法的實用設定技巧，提供範例程式碼，解釋了一些典型的數據分析工具，並描述了各種潛在的錯誤來源和陷阱，以及避免它們的提示。", "applications": ["**傳染病模型預測：** 預測新型病毒或疾病的傳播速度、範圍和影響，協助政府和醫療機構制定更有效的防疫措施，例如隔離策略、疫苗接種優先順序等。", "**生態系統建模與保護：** 模擬不同物種的相互作用和環境變化對生態系統的影響，幫助制定保護瀕危物種和維持生態平衡的策略。", "**城市規劃與交通管理：** 模擬城市人口流動、交通流量等，優化交通網絡設計、公共運輸系統，並預測突發事件（例如：大型活動）對城市交通的影響。"], "pitch": "各位投資人，我們正在開發一套基於代理人的蒙地卡羅模擬平台，針對反應擴散模型、族群動力學和傳染病擴散等複雜系統提供強大的模擬能力。 這個平台不僅能幫助學術研究，更具備廣泛的商業應用潛力。想像一下，我們可以協助政府預測疫情走向、幫助企業優化物流網路、甚至協助生態學家保護瀕危物種。我們的競爭優勢在於演算法的效率和靈活性，以及易於使用的界面，讓非專業人士也能快速上手。透過SaaS模式，我們可以向政府機構、研究機構、企業和教育機構提供訂閱服務，創造穩定的營收。我們相信這個平台能夠成為解決複雜問題的強大工具，並在多個領域創造巨大的價值，請各位給予我們支持！", "audio": "audios/2505.18145v1.mp3", "timestamp": "2025-05-26T11:11:47.036589"}
{"query": "AI", "id": "2505.18139v1", "url": "http://arxiv.org/abs/2505.18139v1", "title": "Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems", "summary": "This position paper argues that the theoretical inconsistency often observed\namong Responsible AI (RAI) metrics, such as differing fairness definitions or\ntradeoffs between accuracy and privacy, should be embraced as a valuable\nfeature rather than a flaw to be eliminated. We contend that navigating these\ninconsistencies, by treating metrics as divergent objectives, yields three key\nbenefits: (1) Normative Pluralism: Maintaining a full suite of potentially\ncontradictory metrics ensures that the diverse moral stances and stakeholder\nvalues inherent in RAI are adequately represented. (2) Epistemological\nCompleteness: The use of multiple, sometimes conflicting, metrics allows for a\nmore comprehensive capture of multifaceted ethical concepts, thereby preserving\ngreater informational fidelity about these concepts than any single, simplified\ndefinition. (3) Implicit Regularization: Jointly optimizing for theoretically\nconflicting objectives discourages overfitting to one specific metric, steering\nmodels towards solutions with enhanced generalization and robustness under\nreal-world complexities. In contrast, efforts to enforce theoretical\nconsistency by simplifying or pruning metrics risk narrowing this value\ndiversity, losing conceptual depth, and degrading model performance. We\ntherefore advocate for a shift in RAI theory and practice: from getting trapped\nin inconsistency to characterizing acceptable inconsistency thresholds and\nelucidating the mechanisms that permit robust, approximated consistency in\npractice.", "authors": ["Gordon Dai", "Yunze Xiao"], "published_date": "2025-05-23", "title_zh": "擁抱矛盾：理論上的不一致性不會阻礙構建負責任AI系統的道路", "summary_zh": "這篇論文認為，在負責任AI（RAI）指標中常見的理論不一致性，例如不同的公平性定義或準確性與隱私之間的權衡，應該被視為一種有價值的特性，而不是需要消除的缺陷。我們認為，通過將這些指標視為不同的目標來應對這些不一致性，可以帶來三個關鍵好處：(1)規範多元主義：維持一套完整的、可能相互矛盾的指標，確保RAI中固有的不同道德立場和利害關係人價值得到充分代表。(2)認識論的完整性：使用多個、有時相互衝突的指標，可以更全面地捕捉多方面的倫理概念，從而比任何單一、簡化的定義保留更多的資訊保真度。(3)隱式正規化：共同優化理論上衝突的目標，可以防止過度擬合到某個特定指標，從而引導模型朝著在現實世界的複雜性下具有更強泛化和穩健性的解決方案發展。相反，通過簡化或修剪指標來強制執行理論一致性的努力，可能會縮小這種價值多樣性，喪失概念深度，並降低模型性能。因此，我們提倡RAI理論和實踐的轉變：從陷入不一致性到描述可接受的不一致性閾值，並闡明允許在實踐中實現穩健、近似一致性的機制。", "applications": ["醫療診斷AI：在診斷疾病時，可能需要同時考慮模型的準確性、公平性（例如，對不同種族群體的診斷偏差）和患者隱私。擁抱矛盾，允許模型在某些情況下犧牲一點準確性以提高公平性或隱私，而不是強求一個在所有方面都完美的單一指標。", "信用評分模型：在評估個人信用時，需要考慮模型的預測能力、避免歧視（例如，性別或種族歧視）和保護個人財務數據。擁抱矛盾，允許模型在不同群體間略微調整評分標準，以確保整體公平性，即便這會略微影響整體預測準確性。", "人臉辨識系統：在公共安全監控中使用人臉辨識時，需要在辨識準確性、個人隱私和避免錯誤辨識（尤其對弱勢群體）之間取得平衡。擁抱矛盾，可以允許系統在一些情境下為了保護隱私而降低辨識精度，或者對特定群體增加辨識確認步驟，以降低誤判風險。"], "pitch": "負責任AI（RAI）是未來AI發展的關鍵。我們的技術跳脫傳統框架，擁抱RAI指標間的內在矛盾，將其轉化為優勢。不同於競爭對手追求單一完美指標，我們的方法確保了價值多元、認識論完整和模型泛化能力。這意味著，我們能打造更公平、更穩健、更能適應真實世界的AI系統，降低潛在的法律和聲譽風險。醫療、金融、安防，各領域都需要我們的解決方案。投資我們，您將投資於AI倫理的未來，並在蓬勃發展的RAI市場中佔據領導地位。我們不僅僅是開發技術，我們是在構建值得信賴的AI生態系統。", "audio": "audios/2505.18139v1.mp3", "timestamp": "2025-05-26T13:25:06.043640"}
{"query": "Foundation Model", "id": "2505.18125v1", "url": "http://arxiv.org/abs/2505.18125v1", "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations", "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.", "authors": ["Alan Arazi", "Eilam Shapira", "Roi Reichart"], "published_date": "2025-05-23", "title_zh": "TabSTAR：具備語義目標感知表徵的基礎表格模型", "summary_zh": "傳統上，深度學習在表格數據學習方面表現不佳，梯度提升決策樹（GBDTs）仍然佔主導地位。然而，近年來，表格基礎模型開始嶄露頭角，它能夠利用真實世界的知識並泛化到不同的數據集，特別是當數據包含自由文本時。我們提出TabSTAR，一種具備語義目標感知表徵的基礎表格模型。TabSTAR旨在實現對帶有文本特徵的表格數據的轉移學習，其架構不依賴於數據集特定的參數。它解凍了預訓練的文本編碼器，並將目標 tokens 作為輸入，為模型提供學習任務特定嵌入所需的上下文。TabSTAR 在具有文本特徵的分類任務的已知基準測試中，在中型和大型數據集上都實現了最先進的性能，並且其預訓練階段在數據集數量上表現出縮放定律，為進一步的性能改進提供了途徑。", "applications": ["**貸款風險評估：** 結合客戶的信用數據（表格數據）和申請理由（文本數據）來更精準地評估貸款風險，降低壞帳率。", "**產品推薦系統：** 結合用戶的購買歷史（表格數據）和評論（文本數據）來更精準地推薦商品，提升銷售額。", "**醫療診斷輔助：** 結合病患的病歷數據（表格數據）和病症描述（文本數據）來輔助醫生進行診斷，提高診斷效率和準確性。"], "pitch": "TabSTAR 是一個突破性的基礎表格模型，它透過整合文本資訊，在傳統上深度學習難以攻克的表格數據分析領域取得了顯著進展。它能夠利用現有的文本數據，例如客戶評論、醫療紀錄等，結合傳統的表格數據，大幅提升預測準確性。這對於需要處理大量表格數據並希望從文本中提取價值的企業來說，是一個 game-changing 的技術。試想，保險公司可以更精準地評估風險，電商平台可以提供更個性化的推薦，醫院可以獲得更有效的診斷支持。TabSTAR 的潛在商業價值巨大，我們相信它將引領下一代表格數據分析技術的發展，並且在金融、零售、醫療等行業產生深遠影響。 我們正在尋找投資，以加速 TabSTAR 的研發和應用，將這項革命性的技術推向市場，共同開創表格數據分析的新紀元。", "audio": "audios/2505.18125v1.mp3", "timestamp": "2025-05-26T13:25:26.074607"}
{"query": "Diffusion Model", "id": "2505.18145v1", "url": "http://arxiv.org/abs/2505.18145v1", "title": "Stochastic agent-based Monte Carlo simulations for reaction-diffusion models, population dynamics, and epidemic spreading", "summary": "We provide a succinct overview of the implementation of Monte Carlo\nalgorithms based on Markovian stochastic dynamics to study interacting and\nreacting many-particle systems away from thermal equilibrium. Such agent-based\ncomputer simulations constitute an effective tool to introduce undergraduate\nand beginning graduate students to current frontier research without requiring\nmuch prior knowledge or experience: Starting from direct visualization of\nsimulation data, students may gain immediate insight into emerging macroscopic\nfeatures of a complex model system and subsequently apply more sophisticated\ndata analysis to quantitatively characterize its often rich dynamical\nproperties, both in stationary and transient regimes. We utilize numerical\ninvestigations of paradigmatic reaction-diffusion systems, as well as\nstochastic models for population dynamics and epidemic spreading, to exemplify\nhow interdisciplinary computational research can be effectively utilized in\nbottom-up undergraduate and graduate education through learning by doing. In\naddition, we give helpful hints for the practical setup of Monte Carlo\nsimulation algorithms, provide sample codes, explain some typical data analysis\ntools, and describe various potential error sources and pitfalls, with tips for\navoiding them.", "authors": ["Mohamed Swailem", "Ulrich Dobramysl", "Ruslan Mukhamadiarov", "Uwe C. Täuber"], "published_date": "2025-05-23", "title_zh": "用於反應擴散模型、族群動力學和傳染病傳播的隨機基於個體的蒙地卡羅模擬", "summary_zh": "本研究簡要介紹了基於馬可夫隨機動態的蒙地卡羅演算法，用於研究遠離熱平衡的相互作用多粒子系統。這種基於個體的電腦模擬提供了一個有效的工具，讓大學部和研究生入門學生能夠接觸到當前的前沿研究，而不需要太多的先備知識。學生可以透過直接視覺化模擬數據，立即洞察複雜模型系統中湧現的巨觀特徵，並進一步應用更複雜的數據分析來量化其豐富的動態特性，無論是在穩態還是瞬態狀態下。我們利用反應擴散系統、族群動力學和傳染病傳播的數值研究，來闡述如何透過邊做邊學，有效地將跨學科的計算研究應用於由下而上的大學部和研究生教育中。此外，我們還提供有關蒙地卡羅模擬演算法的實用設置、範例程式碼、一些典型的數據分析工具，以及各種潛在的錯誤來源和陷阱，並提供避免它們的技巧。", "applications": ["模擬餐廳尖峰時刻人流，優化座位安排和服務人員配置，減少顧客等待時間，提升翻桌率。", "模擬野生動物族群的遷移和繁殖模式，協助制定更有效的保育策略，例如設立保護區的位置。", "模擬社交網路上的謠言傳播，了解哪些因素會加速或減緩謠言擴散，協助資訊平台制定應對策略。"], "pitch": "我們提供一套基於個體的蒙地卡羅模擬工具，能夠準確預測和分析複雜系統的行為，例如人群流動、生物族群動態和疾病傳播。透過易於使用的介面和可視化功能，我們的產品能夠幫助企業、政府和研究機構在資源分配、風險評估和策略制定方面做出更明智的決策。例如，餐廳可以使用我們的模擬工具優化運營流程，政府部門可以利用它制定更有效的防疫政策，科研機構則可以加速研究進程。我們的商業模式是提供訂閱服務，根據模擬的複雜度和數據量收取費用。相較於傳統的分析方法，我們的優勢在於能夠處理高度複雜和不確定的情況，提供更全面和精確的預測，具有巨大的市場潛力。我們尋求種子輪融資，用於擴大研發團隊，完善產品功能，並開拓市場。", "audio": "audios/2505.18145v1.mp3", "timestamp": "2025-05-26T13:25:49.092784"}
{"query": "AI", "id": "2505.18139v1", "url": "http://arxiv.org/abs/2505.18139v1", "title": "Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems", "summary": "This position paper argues that the theoretical inconsistency often observed\namong Responsible AI (RAI) metrics, such as differing fairness definitions or\ntradeoffs between accuracy and privacy, should be embraced as a valuable\nfeature rather than a flaw to be eliminated. We contend that navigating these\ninconsistencies, by treating metrics as divergent objectives, yields three key\nbenefits: (1) Normative Pluralism: Maintaining a full suite of potentially\ncontradictory metrics ensures that the diverse moral stances and stakeholder\nvalues inherent in RAI are adequately represented. (2) Epistemological\nCompleteness: The use of multiple, sometimes conflicting, metrics allows for a\nmore comprehensive capture of multifaceted ethical concepts, thereby preserving\ngreater informational fidelity about these concepts than any single, simplified\ndefinition. (3) Implicit Regularization: Jointly optimizing for theoretically\nconflicting objectives discourages overfitting to one specific metric, steering\nmodels towards solutions with enhanced generalization and robustness under\nreal-world complexities. In contrast, efforts to enforce theoretical\nconsistency by simplifying or pruning metrics risk narrowing this value\ndiversity, losing conceptual depth, and degrading model performance. We\ntherefore advocate for a shift in RAI theory and practice: from getting trapped\nin inconsistency to characterizing acceptable inconsistency thresholds and\nelucidating the mechanisms that permit robust, approximated consistency in\npractice.", "authors": ["Gordon Dai", "Yunze Xiao"], "published_date": "2025-05-23", "title_zh": "擁抱矛盾：理論上的不一致性不會阻礙建構負責任的AI系統之路", "summary_zh": "這篇論文主張，負責任AI(RAI)指標間常見的理論不一致性，例如不同的公平性定義或準確性與隱私的權衡，應該被視為寶貴的特性，而非需要消除的缺陷。透過將這些指標視為不同的目標，我們可以獲得三個好處：確保多元的道德立場和利益相關者價值得到充分代表、更全面地捕捉多面向的倫理概念、以及透過聯合優化衝突目標來增強模型的泛化能力和穩健性。因此，我們提倡轉變RAI的理論與實踐方式，從糾結於不一致性轉向描述可接受的不一致性閾值，並闡明在實踐中實現穩健且近似一致性的機制。", "applications": ["**貸款審核系統：** 在貸款審核中，同時考慮多個公平性指標(例如，種族、性別、年齡)可能會產生衝突。擁抱這種衝突，而不是簡化成單一的公平性定義，可以確保不同群體的利益都得到考量，避免產生系統性的歧視。", "**醫療診斷AI：** 醫療診斷AI在追求高準確性的同時，也需要考慮隱私保護。權衡準確性與隱私保護，並允許一定程度的誤差，可以避免過度收集個人資料，同時仍提供可靠的診斷建議。", "**自動駕駛汽車：** 自動駕駛汽車在不同交通狀況下需要考慮安全、效率和乘客舒適度。這些目標可能相互衝突，例如，為了安全可能需要減速，但減速會降低效率。透過同時優化這些衝突目標，可以設計出更穩健且更貼近真實交通情況的自動駕駛系統。"], "pitch": "這項技術的核心價值在於重新定義了我們如何看待AI的負責任性。不再追求完美的理論一致性，而是擁抱矛盾，能讓我們開發出更穩健、更具彈性，且更貼近真實世界的AI系統。想像一下，一個能夠在複雜的倫理困境中做出更好決策的AI。這不僅僅是技術上的進步，更是倫理上的飛躍。透過專注於理解和管理指標間的衝突，我們能打造出更可信賴、更能回應社會需求的AI。潛在商業價值巨大，涵蓋了需要高度倫理考量的各行各業，例如金融、醫療、交通運輸等等。這項技術將大幅降低法律風險，提升品牌形象，並為企業在AI時代建立競爭優勢。", "audio": "audios/2505.18139v1.mp3", "timestamp": "2025-05-26T14:14:14.288259"}
{"query": "Foundation Model", "id": "2505.18125v1", "url": "http://arxiv.org/abs/2505.18125v1", "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations", "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.", "authors": ["Alan Arazi", "Eilam Shapira", "Roi Reichart"], "published_date": "2025-05-23", "title_zh": "TabSTAR：具備語義目標感知表徵的基礎表格模型", "summary_zh": "過去，深度學習在表格資料的學習效果不如梯度提升決策樹，但現在情況正在改變。我們提出 TabSTAR，一個基礎表格模型，它利用語義目標感知的表徵，在帶有文字特徵的表格資料上實現遷移學習。TabSTAR 架構不包含特定於資料集的參數，並能解鎖預訓練的文字編碼器，輸入目標標記，讓模型學習特定任務的嵌入。在帶有文字特徵的分類任務中，TabSTAR 在中型和大型資料集上都達到了最先進的性能，並且其預訓練階段顯示出隨著資料集數量的增加而呈現的比例法則，為進一步的性能提升提供了途徑。", "applications": ["**金融風險評估：**基於客戶的表格資料（如收入、資產）和文字資料（如貸款申請描述），TabSTAR 可以更精準地預測貸款違約風險。", "**醫學診斷輔助：**結合病人的表格數據（如年齡、病史）和病歷中的文本描述，幫助醫生更快速、準確地診斷疾病。", "**產品推薦系統：**結合用戶的表格數據（如年齡、性別）和產品評論、描述等文本信息，打造更個性化和有效的產品推薦。"], "pitch": "TabSTAR 重新定義了表格數據分析，尤其是在包含大量文本信息的場景下。傳統上，表格數據分析被 GBDT 等模型主導，但 TabSTAR 通過引入語義目標感知的表徵，大幅提升了模型性能，超越了現有方法。想像一下，一個可以理解複雜文本背後含義的表格數據模型，它能夠更精準地預測客戶流失、評估信貸風險、甚至協助診斷疾病。這意味著企業可以做出更明智的決策，降低風險，提高效率。 TabSTAR 的預訓練可擴展性意味著隨著數據量的增長，性能還能持續提升。 我們相信 TabSTAR 具有巨大的商業潛力，它不僅可以取代現有的表格數據分析工具，還可以開創全新的應用場景，例如基於文本語義理解的智能決策支持系統。 我們正在尋找投資者，共同將 TabSTAR 打造成表格數據分析領域的領先者。", "audio": "audios/2505.18125v1.mp3", "timestamp": "2025-05-26T14:14:29.901440"}
{"query": "Diffusion Model", "id": "2505.18145v1", "url": "http://arxiv.org/abs/2505.18145v1", "title": "Stochastic agent-based Monte Carlo simulations for reaction-diffusion models, population dynamics, and epidemic spreading", "summary": "We provide a succinct overview of the implementation of Monte Carlo\nalgorithms based on Markovian stochastic dynamics to study interacting and\nreacting many-particle systems away from thermal equilibrium. Such agent-based\ncomputer simulations constitute an effective tool to introduce undergraduate\nand beginning graduate students to current frontier research without requiring\nmuch prior knowledge or experience: Starting from direct visualization of\nsimulation data, students may gain immediate insight into emerging macroscopic\nfeatures of a complex model system and subsequently apply more sophisticated\ndata analysis to quantitatively characterize its often rich dynamical\nproperties, both in stationary and transient regimes. We utilize numerical\ninvestigations of paradigmatic reaction-diffusion systems, as well as\nstochastic models for population dynamics and epidemic spreading, to exemplify\nhow interdisciplinary computational research can be effectively utilized in\nbottom-up undergraduate and graduate education through learning by doing. In\naddition, we give helpful hints for the practical setup of Monte Carlo\nsimulation algorithms, provide sample codes, explain some typical data analysis\ntools, and describe various potential error sources and pitfalls, with tips for\navoiding them.", "authors": ["Mohamed Swailem", "Ulrich Dobramysl", "Ruslan Mukhamadiarov", "Uwe C. Täuber"], "published_date": "2025-05-23", "title_zh": "基於隨機代理人的蒙地卡羅模擬，用於反應擴散模型、族群動力學和流行病傳播", "summary_zh": "本研究簡要介紹了基於馬可夫隨機動力學的蒙地卡羅演算法，用於研究遠離熱平衡的相互作用和反應的多粒子系統。這種基於代理人的電腦模擬，是讓大學生和研究生入門當前前沿研究的有效工具，無需太多先備知識。學生可以從直接視覺化模擬數據開始，立即深入了解複雜模型系統的新興宏觀特徵，並隨後應用更複雜的數據分析來定量地描述其豐富的動態特性，無論是在靜態還是瞬態狀態下。我們利用典型反應擴散系統、族群動力學和流行病傳播的隨機模型進行數值研究，以例證跨學科計算研究如何通過邊做邊學的方式，有效地應用於自下而上的本科生和研究生教育。此外，我們還提供了蒙地卡羅模擬演算法實踐設置的有用提示，提供了範例程式碼，解釋了一些典型的數據分析工具，並描述了各種潛在的錯誤來源和陷阱，並提供了避免這些問題的技巧。", "applications": ["**餐廳座位安排最佳化：** 使用基於代理人的蒙地卡羅模擬，根據不同用餐人數、用餐時間、顧客偏好（如靠窗座位）等因素，優化餐廳的座位安排，最大化座位利用率和顧客滿意度。", "**模擬交通擁堵以改善交通流量：** 利用蒙地卡羅模擬，將每輛車視為一個代理人，模擬不同交通流量和道路條件下的交通擁堵情況，從而評估不同交通管理策略（如紅綠燈調整、匝道控制）的效果，並提出改善交通流量的方案。", "**疫情擴散預測與控制策略評估：** 建立基於代理人的模型，模擬人群中疫情的傳播，考慮個體行為（如戴口罩、保持社交距離）和疫苗接種等因素，預測疫情發展趨勢，並評估不同控制策略（如封城、隔離）的效果，協助政府制定更有效的防疫政策。"], "pitch": "我們開發了一套基於代理人的蒙地卡羅模擬引擎，專門針對複雜系統的建模與分析。相較於傳統建模方法，我們的引擎更能精準捕捉個體行為對整體系統的影響。首階段，我們鎖定流行病預測、城市交通規劃、和零售業供應鏈優化等高潛力市場。透過與政府部門、交通運輸公司、以及大型零售企業合作，我們提供客製化的模擬分析服務，協助客戶做出更明智的決策，降低風險並提升效率。我們的核心競爭力在於演算法的效率和模型的靈活性，可以快速適應不同領域的需求。預計未來將透過SaaS模式，將模擬引擎推廣至更廣泛的用戶，創造巨大的商業價值。", "audio": "audios/2505.18145v1.mp3", "timestamp": "2025-05-26T14:14:50.476403"}
{"query": "AI", "id": "2505.18139v1", "url": "http://arxiv.org/abs/2505.18139v1", "title": "Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems", "summary": "This position paper argues that the theoretical inconsistency often observed\namong Responsible AI (RAI) metrics, such as differing fairness definitions or\ntradeoffs between accuracy and privacy, should be embraced as a valuable\nfeature rather than a flaw to be eliminated. We contend that navigating these\ninconsistencies, by treating metrics as divergent objectives, yields three key\nbenefits: (1) Normative Pluralism: Maintaining a full suite of potentially\ncontradictory metrics ensures that the diverse moral stances and stakeholder\nvalues inherent in RAI are adequately represented. (2) Epistemological\nCompleteness: The use of multiple, sometimes conflicting, metrics allows for a\nmore comprehensive capture of multifaceted ethical concepts, thereby preserving\ngreater informational fidelity about these concepts than any single, simplified\ndefinition. (3) Implicit Regularization: Jointly optimizing for theoretically\nconflicting objectives discourages overfitting to one specific metric, steering\nmodels towards solutions with enhanced generalization and robustness under\nreal-world complexities. In contrast, efforts to enforce theoretical\nconsistency by simplifying or pruning metrics risk narrowing this value\ndiversity, losing conceptual depth, and degrading model performance. We\ntherefore advocate for a shift in RAI theory and practice: from getting trapped\nin inconsistency to characterizing acceptable inconsistency thresholds and\nelucidating the mechanisms that permit robust, approximated consistency in\npractice.", "authors": ["Gordon Dai", "Yunze Xiao"], "published_date": "2025-05-23", "title_zh": "擁抱矛盾：理論上的不一致性不會阻礙負責任AI系統的構建之路", "summary_zh": "這篇論文認為，負責任AI（RAI）指標間常見的理論不一致性，例如不同的公平性定義或準確度與隱私的權衡，應該被視為一種寶貴的特性而非需要消除的缺陷。 將這些指標視為不同的目標來應對這種不一致性，可以帶來以下好處：規範的多元性、認識論的完整性、以及隱式的正規化，能提升模型的泛化能力和魯棒性。 因此，我們主張將RAI的理論和實踐轉變為：從陷於不一致性，轉為描述可接受的不一致性閾值，並闡明在實踐中允許穩健的近似一致性的機制。", "applications": ["**貸款審核系統：** 銀行可以使用多種公平性指標（例如，基於種族、性別等不同群體的批准率差距），即使這些指標相互矛盾，也能更全面地評估貸款審核系統的公平性，避免單一指標造成的偏見。 最終，系統會針對不同的群體，提供更公平且更穩健的貸款決策。", "**招聘系統：** 人力資源部門可以同時考慮多個關於候選人的指標，包括技能匹配度、經驗、教育背景，甚至是性格測試結果。 即使這些指標之間存在衝突（例如，經驗豐富但技能不完全匹配），也能更全面地評估候選人，避免過於強調單一指標而錯失潛在的優秀人才。 此外，多元化的指標能提升招聘的公平性，減少無意識的偏見。", "**醫療診斷系統：** 醫生可以使用多個指標，包括實驗室數據、影像學檢查結果、以及患者的病史和主訴，來評估患者的病情。 即使這些指標之間存在不一致（例如，影像學顯示沒有明顯異常，但患者主訴強烈不適），也能更全面地評估患者的病情，避免過於依賴單一指標而做出錯誤的診斷。"], "pitch": "負責任AI（RAI）市場正處於爆發前夕，企業迫切需要確保其AI系統的公平性、透明度和可靠性。 我們的技術獨特之處在於，我們擁抱RAI指標間的固有矛盾，而非試圖消除它們。 這不僅能更全面地反映現實世界的複雜性，還能提升模型的泛化能力和魯棒性。 我們的解決方案能幫助企業有效地管理和評估AI系統的倫理風險，滿足監管要求，並建立消費者信任。 我們預計我們的技術將在金融、醫療、招聘等各個領域得到廣泛應用，帶來巨大的商業價值。 我們正在尋求戰略投資，以加速產品開發和市場擴張，成為RAI領域的領導者。", "audio": "audios/2505.18139v1.mp3", "timestamp": "2025-05-26T15:14:25.229154"}
{"query": "Foundation Model", "id": "2505.18125v1", "url": "http://arxiv.org/abs/2505.18125v1", "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations", "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.", "authors": ["Alan Arazi", "Eilam Shapira", "Roi Reichart"], "published_date": "2025-05-23", "title_zh": "TabSTAR：具備語義目標感知表徵的基礎表格模型", "summary_zh": "過去在表格數據學習上，深度學習表現不如梯度提升決策樹 (GBDTs)。但最近，表格基礎模型開始嶄露頭角，能利用真實世界知識並在不同數據集上進行泛化，尤其是在包含自由文本的數據中。TabSTAR 是一種具備語義目標感知表徵的基礎表格模型，專為在帶有文本特徵的表格數據上實現遷移學習而設計，其架構不含特定於數據集的參數。它會解凍預訓練的文本編碼器，並將目標 tokens 作為輸入，為模型提供學習任務特定嵌入所需的上下文。TabSTAR 在具有文本特徵的分類任務的已知基準測試中，在中型和大型數據集上都取得了最先進的性能，並且其預訓練階段在數據集數量上表現出縮放定律，為進一步的性能改進提供了途徑。", "applications": ["**智能客服與客戶關係管理：** 分析客戶表格數據（如交易記錄、人口統計資料）以及客戶的文本訊息（如郵件、聊天記錄），更精準地預測客戶需求和問題，提供個性化的客戶服務。", "**金融風險評估：** 結合貸款申請表格數據（如收入、信用評分）和申請人的文字描述（如還款計劃、財務狀況說明），更全面地評估貸款風險，降低壞帳率。", "**醫療診斷輔助：** 結合患者的病歷表格數據（如檢驗結果、用藥記錄）和醫生的病歷記錄文本（如問診記錄、檢查報告），輔助醫生進行更準確的診斷，提高診斷效率和準確性。"], "pitch": "TabSTAR 打破了深度學習在表格數據領域的瓶頸，尤其是在處理包含文本特徵的數據時，性能遠超傳統方法。它不僅在中大型數據集上表現出色，更具備良好的可擴展性。想像一下，無需針對每個數據集進行大量客製化訓練，TabSTAR 就能快速適應各種表格數據，並從中提取有價值的洞見。這對於需要處理大量複雜表格數據的行業來說，如金融、醫療、零售等，具有巨大的商業價值。TabSTAR 能夠顯著提升效率、降低成本，並帶來更精準的預測和決策，進而創造巨大的商業機會。", "audio": "audios/2505.18125v1.mp3", "timestamp": "2025-05-26T15:14:49.341343"}
{"query": "Diffusion Model", "id": "2505.18145v1", "url": "http://arxiv.org/abs/2505.18145v1", "title": "Stochastic agent-based Monte Carlo simulations for reaction-diffusion models, population dynamics, and epidemic spreading", "summary": "We provide a succinct overview of the implementation of Monte Carlo\nalgorithms based on Markovian stochastic dynamics to study interacting and\nreacting many-particle systems away from thermal equilibrium. Such agent-based\ncomputer simulations constitute an effective tool to introduce undergraduate\nand beginning graduate students to current frontier research without requiring\nmuch prior knowledge or experience: Starting from direct visualization of\nsimulation data, students may gain immediate insight into emerging macroscopic\nfeatures of a complex model system and subsequently apply more sophisticated\ndata analysis to quantitatively characterize its often rich dynamical\nproperties, both in stationary and transient regimes. We utilize numerical\ninvestigations of paradigmatic reaction-diffusion systems, as well as\nstochastic models for population dynamics and epidemic spreading, to exemplify\nhow interdisciplinary computational research can be effectively utilized in\nbottom-up undergraduate and graduate education through learning by doing. In\naddition, we give helpful hints for the practical setup of Monte Carlo\nsimulation algorithms, provide sample codes, explain some typical data analysis\ntools, and describe various potential error sources and pitfalls, with tips for\navoiding them.", "authors": ["Mohamed Swailem", "Ulrich Dobramysl", "Ruslan Mukhamadiarov", "Uwe C. Täuber"], "published_date": "2025-05-23", "title_zh": "用於反應擴散模型、族群動力學與傳染病傳播的基於隨機代理人的蒙地卡羅模擬", "summary_zh": "本研究簡要概述了基於馬可夫隨機動力學的蒙地卡羅演算法，用於研究非熱平衡下的交互作用和反應的多粒子系統。這種基於代理人的電腦模擬提供了一個有效的工具，讓大學部和剛開始的研究生能夠接觸到前沿研究，而無需太多的先備知識或經驗。透過直接視覺化模擬數據，學生可以立即深入了解複雜模型系統中湧現的宏觀特徵，然後應用更複雜的數據分析來定量地描述其豐富的動態特性，無論是在靜態還是瞬態狀態下。我們利用範例反應擴散系統以及族群動力學和傳染病傳播的隨機模型，展示如何透過做中學，將跨學科的計算研究有效地應用於自下而上的大學部和研究生教育。此外，我們還提供了蒙地卡羅模擬演算法的實用設置技巧、示例程式碼、解釋一些典型的數據分析工具，並描述了各種潛在的錯誤來源和陷阱，並提供避免這些錯誤的提示。", "applications": ["**精準農業病蟲害防治：** 模擬病蟲害在農田中的傳播模式，根據模擬結果，精準投放農藥或生物防治劑，降低成本並減少環境污染。", "**城市交通流量優化：** 將車輛視為代理人，模擬不同交通政策（例如單行道、紅綠燈時間調整）對交通流量的影響，找出最佳的交通管理方案，減少壅塞。", "**市場營銷活動效果預測：** 將消費者視為代理人，模擬不同營銷策略（例如折扣、廣告投放）對產品銷量的影響，評估營銷活動的效益，提升投資報酬率。"], "pitch": "我們開發了一套基於隨機代理人的蒙地卡羅模擬平台，能以低成本、高效率地模擬各種複雜系統的動態行為。這項技術能廣泛應用於農業、交通、醫療、金融等多個領域，提供決策者數據驅動的 insights。我們的競爭優勢在於其易於使用的界面和高度可客製化的模型參數，能讓非專業人士也能輕鬆上手。透過授權軟體、提供客製化模擬服務和建立雲端模擬平台，我們預計能在短期內產生可觀的收入。更重要的是，我們的技術能幫助企業和政府做出更明智的決策，優化資源配置，提升效率，創造巨大的社會價值。", "audio": "audios/2505.18145v1.mp3", "timestamp": "2025-05-26T15:15:15.004709"}
{"query": "AI", "id": "2505.18139v1", "url": "http://arxiv.org/abs/2505.18139v1", "title": "Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems", "summary": "This position paper argues that the theoretical inconsistency often observed\namong Responsible AI (RAI) metrics, such as differing fairness definitions or\ntradeoffs between accuracy and privacy, should be embraced as a valuable\nfeature rather than a flaw to be eliminated. We contend that navigating these\ninconsistencies, by treating metrics as divergent objectives, yields three key\nbenefits: (1) Normative Pluralism: Maintaining a full suite of potentially\ncontradictory metrics ensures that the diverse moral stances and stakeholder\nvalues inherent in RAI are adequately represented. (2) Epistemological\nCompleteness: The use of multiple, sometimes conflicting, metrics allows for a\nmore comprehensive capture of multifaceted ethical concepts, thereby preserving\ngreater informational fidelity about these concepts than any single, simplified\ndefinition. (3) Implicit Regularization: Jointly optimizing for theoretically\nconflicting objectives discourages overfitting to one specific metric, steering\nmodels towards solutions with enhanced generalization and robustness under\nreal-world complexities. In contrast, efforts to enforce theoretical\nconsistency by simplifying or pruning metrics risk narrowing this value\ndiversity, losing conceptual depth, and degrading model performance. We\ntherefore advocate for a shift in RAI theory and practice: from getting trapped\nin inconsistency to characterizing acceptable inconsistency thresholds and\nelucidating the mechanisms that permit robust, approximated consistency in\npractice.", "authors": ["Gordon Dai", "Yunze Xiao"], "published_date": "2025-05-23", "title_zh": "擁抱矛盾：理論上的不一致性不會阻礙建構負責任人工智慧系統的道路", "summary_zh": "這篇論文主張，負責任人工智慧（RAI）指標間常見的理論不一致性，例如不同公平性定義或準確度與隱私權之間的權衡，應該被視為有價值的特徵，而非需要消除的缺陷。作者認為，將這些指標視為不同的目標來處理，能帶來三個好處：(1) 規範多元性：維持一整套可能互相矛盾的指標，確保RAI中固有的不同道德立場和利害關係人價值觀得到充分代表。(2) 知識完整性：使用多個、有時相互衝突的指標，可以更全面地捕捉多方面的倫理概念，從而比任何單一、簡化的定義保留更多關於這些概念的資訊。(3) 隱式正則化：聯合優化理論上相互衝突的目標，可以防止模型過度擬合到某個特定指標，從而引導模型朝著在真實世界複雜性下具有更高泛化能力和穩健性的解決方案發展。相反，通過簡化或刪減指標來強制執行理論一致性的努力，可能會縮小這種價值多樣性，喪失概念深度，並降低模型性能。因此，作者提倡 RAI 理論和實踐上的轉變：從陷於不一致性，轉向表徵可接受的不一致性閾值，並闡明允許在實踐中實現穩健、近似一致性的機制。", "applications": ["金融貸款審核：利用多種相互衝突的公平性指標（例如，人口統計學均等與機會均等）來確保貸款決策不僅準確，而且不會對任何特定人群產生不公平的歧視。", "刑事司法風險評估：在評估被告再犯風險時，同時考慮準確性和多個公平性指標，以避免系統性偏見，並確保決策既能保護公共安全，又能盡可能公平地對待所有被告。", "醫療診斷輔助：平衡疾病診斷的準確性和對患者隱私的保護。使用不同的隱私保護技術，並監測其對診斷準確性的影響，確保在保障患者隱私的前提下，提供尽可能准确的诊断建议。"], "pitch": "想像一下，現在的人工智慧就像一個試圖完美達成所有目標的學生，但每個目標之間都存在衝突，像是要同時讀書又要玩樂，傳統方法是試圖讓這些目標達成平衡，但我們的研究顯示，這種方法反而會讓人工智慧失去彈性和適應性。我們的解決方案是擁抱這種矛盾，讓人工智慧同時考量多個衝突的目標，這樣不僅可以更全面地反映真實世界的複雜性，還能讓模型更具泛化能力，在不同的情境下都能表現出色。這項技術的商業價值在於打造更公平、更可靠、更穩健的人工智慧系統，應用範圍廣泛，從金融、醫療到法律，都能有效提升決策品質，降低潛在風險。我們正在尋找投資者，共同將這項突破性的研究轉化為實際的產品和服務，開創一個更負責任、更值得信賴的人工智慧新時代。", "audio": "audios/2505.18139v1.mp3", "timestamp": "2025-05-26T16:16:43.034653"}
{"query": "Foundation Model", "id": "2505.18125v1", "url": "http://arxiv.org/abs/2505.18125v1", "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations", "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.", "authors": ["Alan Arazi", "Eilam Shapira", "Roi Reichart"], "published_date": "2025-05-23", "title_zh": "TabSTAR：具語義目標感知表徵的基礎表格模型", "summary_zh": "儘管深度學習在許多領域取得了顯著的成功，但在表格學習任務上表現一直不如傳統的梯度提升決策樹（GBDTs）。然而，最近的進展正在為表格基礎模型鋪平道路，這種模型可以利用真實世界的知識並跨多種數據集進行泛化，尤其是在數據包含自由文本時。雖然將語言模型能力納入表格任務已經被探索過，但大多數現有方法都使用靜態的、目標無關的文本表徵，限制了它們的有效性。我們推出了TabSTAR：一種具有語義目標感知表徵的基礎表格模型。TabSTAR旨在實現對具有文本特徵的表格數據的遷移學習，其架構不包含特定於數據集的參數。它解凍了一個預訓練的文本編碼器，並將目標標記作為輸入，為模型提供學習特定任務嵌入所需的上下文。TabSTAR在具有文本特徵的分類任務的已知基準測試中，在中型和大型數據集上都實現了最先進的性能，並且其預訓練階段在數據集數量上表現出縮放律，為進一步的性能改進提供了途徑。", "applications": ["**客戶服務聊天機器人：** 使用TabSTAR分析客戶的文字描述和表格數據（例如客戶資料、訂單歷史記錄），更準確地理解客戶需求並提供個性化的解決方案。", "**貸款風險評估：** 結合申請人的文字描述（例如信用記錄解釋）和表格數據（例如收入、資產），更精確地評估貸款風險，降低壞帳率。", "**醫療診斷輔助：** 分析患者的文字描述（例如症狀、病史）和表格數據（例如實驗室檢測結果），幫助醫生做出更準確的診斷，提高醫療效率。"], "pitch": "TabSTAR 代表了表格數據分析的重大突破，它將大型語言模型的強大功能與表格數據的結構化優勢相結合。 傳統上，表格數據的處理一直受限於過時的 GBDT 方法。 TabSTAR 解決了這個問題，其語義目標感知方法可以從文本中提取更豐富的信息，並將其無縫集成到表格分析中。 這為金融、醫療保健和客戶服務等各個行業帶來了巨大的商業價值。 想像一下，更精確的風險評估、更個性化的客戶體驗和更高效的診斷。 TabSTAR 的可擴展性和遷移學習能力使其成為在表格數據領域部署 AI 驅動解決方案的理想選擇，具有巨大的市場潛力。", "audio": "audios/2505.18125v1.mp3", "timestamp": "2025-05-26T16:17:10.448242"}
{"query": "Diffusion Model", "id": "2505.18145v1", "url": "http://arxiv.org/abs/2505.18145v1", "title": "Stochastic agent-based Monte Carlo simulations for reaction-diffusion models, population dynamics, and epidemic spreading", "summary": "We provide a succinct overview of the implementation of Monte Carlo\nalgorithms based on Markovian stochastic dynamics to study interacting and\nreacting many-particle systems away from thermal equilibrium. Such agent-based\ncomputer simulations constitute an effective tool to introduce undergraduate\nand beginning graduate students to current frontier research without requiring\nmuch prior knowledge or experience: Starting from direct visualization of\nsimulation data, students may gain immediate insight into emerging macroscopic\nfeatures of a complex model system and subsequently apply more sophisticated\ndata analysis to quantitatively characterize its often rich dynamical\nproperties, both in stationary and transient regimes. We utilize numerical\ninvestigations of paradigmatic reaction-diffusion systems, as well as\nstochastic models for population dynamics and epidemic spreading, to exemplify\nhow interdisciplinary computational research can be effectively utilized in\nbottom-up undergraduate and graduate education through learning by doing. In\naddition, we give helpful hints for the practical setup of Monte Carlo\nsimulation algorithms, provide sample codes, explain some typical data analysis\ntools, and describe various potential error sources and pitfalls, with tips for\navoiding them.", "authors": ["Mohamed Swailem", "Ulrich Dobramysl", "Ruslan Mukhamadiarov", "Uwe C. Täuber"], "published_date": "2025-05-23", "title_zh": "反應-擴散模型、族群動力學與疫情傳播的隨機基於主體的蒙地卡羅模擬", "summary_zh": "本文簡述了基於馬可夫隨機動力學的蒙地卡羅演算法，用於研究遠離熱平衡的交互作用和反應多粒子系統。這種基於主體的電腦模擬是一個有效的工具，能讓大學生和剛入學的研究生在不需要太多先備知識或經驗的情況下，接觸到當前的尖端研究。學生可以從直接視覺化模擬數據開始，立即了解複雜模型系統的新興巨觀特徵，然後應用更複雜的數據分析來量化描述其豐富的動態特性，包括穩態和瞬態。我們利用反應-擴散系統、族群動力學和疫情傳播的數值研究，展示如何透過做中學，將跨學科計算研究有效地應用於自下而上的大學和研究所教育。此外，我們還提供了蒙地卡羅模擬演算法的實用設置技巧、範例程式碼、常見數據分析工具、潛在錯誤來源和陷阱，以及避免它們的提示。", "applications": ["**疫情預測與控制:** 利用基於主體的蒙地卡羅模擬，針對不同防疫政策（如社交距離、疫苗接種）對疫情擴散的影響進行模擬，協助政府制定更有效的應對措施。", "**生態系統模擬與保護:** 模擬不同物種間的交互作用和環境變化對族群動態的影響，預測瀕危物種的生存機率，為生態保護提供科學依據。", "**社群行為分析與行銷策略:** 模擬個體行為對產品傳播的影響，分析不同行銷策略的效果，協助企業制定更精準的行銷方案。"], "pitch": "我們正在開發一個基於雲端的模擬平台，利用基於主體的蒙地卡羅演算法，為各行各業提供快速、易用的複雜系統模擬解決方案。無論是疫情預測、生態保護還是市場行銷，我們的平台都能夠幫助使用者在不需要專業知識的情況下，理解複雜系統的動態，做出更明智的決策。我們獨特的教育模式，讓學生和研究人員能夠快速上手，並進行前沿研究。這個平台具有高度的可擴展性和廣泛的應用前景，預計在未來幾年內將成為市場上的領導者，創造巨大的商業價值。", "audio": "audios/2505.18145v1.mp3", "timestamp": "2025-05-26T16:17:34.335761"}
{"query": "AI", "id": "2505.18139v1", "url": "http://arxiv.org/abs/2505.18139v1", "title": "Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems", "summary": "This position paper argues that the theoretical inconsistency often observed\namong Responsible AI (RAI) metrics, such as differing fairness definitions or\ntradeoffs between accuracy and privacy, should be embraced as a valuable\nfeature rather than a flaw to be eliminated. We contend that navigating these\ninconsistencies, by treating metrics as divergent objectives, yields three key\nbenefits: (1) Normative Pluralism: Maintaining a full suite of potentially\ncontradictory metrics ensures that the diverse moral stances and stakeholder\nvalues inherent in RAI are adequately represented. (2) Epistemological\nCompleteness: The use of multiple, sometimes conflicting, metrics allows for a\nmore comprehensive capture of multifaceted ethical concepts, thereby preserving\ngreater informational fidelity about these concepts than any single, simplified\ndefinition. (3) Implicit Regularization: Jointly optimizing for theoretically\nconflicting objectives discourages overfitting to one specific metric, steering\nmodels towards solutions with enhanced generalization and robustness under\nreal-world complexities. In contrast, efforts to enforce theoretical\nconsistency by simplifying or pruning metrics risk narrowing this value\ndiversity, losing conceptual depth, and degrading model performance. We\ntherefore advocate for a shift in RAI theory and practice: from getting trapped\nin inconsistency to characterizing acceptable inconsistency thresholds and\nelucidating the mechanisms that permit robust, approximated consistency in\npractice.", "authors": ["Gordon Dai", "Yunze Xiao"], "published_date": "2025-05-23", "title_zh": "擁抱矛盾：理論上的不一致性不會阻礙構建負責任AI系統的道路", "summary_zh": "這篇論文認為，負責任AI (RAI) 指標之間常見的理論不一致性，例如不同的公平性定義或準確性與隱私之間的權衡，應該被視為一種有價值的特徵，而不是需要消除的缺陷。論文指出，通過將這些指標視為不同的目標來應對這些不一致性，可以帶來三個關鍵的好處：(1) 規範多元化：維持一套完整的、可能相互矛盾的指標，確保 RAI 中固有的多樣化道德立場和利害關係人價值得到充分代表。(2) 認識論完整性：使用多個、有時相互衝突的指標，可以更全面地捕捉多方面的倫理概念，從而比任何單一、簡化的定義保留更多的關於這些概念的信息保真度。(3) 隱式正則化：聯合優化理論上衝突的目標可以防止過度擬合到某個特定指標，引導模型朝著在現實世界複雜性下具有增強的泛化和魯棒性的解決方案發展。因此，我們提倡 RAI 理論和實踐轉型：從陷入不一致性轉變為表徵可接受的不一致性閾值，並闡明允許在實踐中實現穩健的、近似一致性的機制。", "applications": ["**貸款審核：** 不同族群在還款能力上有差異，單一公平性指標可能導致誤判。同時考量多個公平性指標（例如統計均等、機會均等）可以更全面地評估貸款風險，避免對特定族群的歧視，並提升整體貸款組合的表現。", "**刑事風險評估：** 預測罪犯再犯的可能性，單一準確率指標可能導致對少數族裔的過度懲罰。結合準確性、公平性、以及對不同犯罪類型的考量，可以更公正地評估風險，降低對特定群體的偏見。", "**自動駕駛：** 在緊急情況下，自動駕駛系統需要在乘客安全、行人安全和其他車輛之間做出權衡。同時考慮多個安全指標（例如最小化碰撞風險、減輕傷害程度）可以幫助系統在複雜情況下做出更明智的決策，提升整體安全性。"], "pitch": "我們正在開發一種革命性的AI框架，它能擁抱並利用負責任AI指標之間的內在矛盾，而不是試圖消除它們。 傳統的AI開發試圖簡化指標，導致價值觀單一化、資訊流失和性能下降。 我們的框架通過同時優化多個衝突指標，確保多元價值觀得到體現，模型具備更強的魯棒性和泛化能力。 我們的目標市場涵蓋金融、醫療、法律等對公平性、透明度和安全性有嚴格要求的行業。 我們的競爭優勢在於：更準確、更公平、更具解釋性的AI系統。 我們正在尋求投資，以加速我們的研發工作，並將這一突破性的技術推向市場，引領負責任AI的新時代，創造更值得信賴和公正的AI解決方案，最終成為企業構建負責任AI的標準方案提供商。", "audio": "audios/2505.18139v1.mp3", "timestamp": "2025-05-26T17:12:31.051799"}
{"query": "Foundation Model", "id": "2505.18125v1", "url": "http://arxiv.org/abs/2505.18125v1", "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations", "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.", "authors": ["Alan Arazi", "Eilam Shapira", "Roi Reichart"], "published_date": "2025-05-23", "title_zh": "TabSTAR：基於語義目標感知表徵的表格型基礎模型", "summary_zh": "儘管深度學習在許多領域取得了顯著的成功，但在表格學習任務上，其表現一直不如梯度提升決策樹（GBDTs）。然而，最近的進展為表格型基礎模型鋪平了道路，這些模型可以利用真實世界的知識並跨多樣化的數據集進行泛化，尤其是在數據包含自由文本時。雖然將語言模型能力融入表格任務已被探索，但大多數現有方法使用靜態的、與目標無關的文本表徵，限制了其有效性。我們介紹TabSTAR：一個具有語義目標感知表徵的表格型基礎模型。TabSTAR旨在實現具有文本特徵的表格數據的遷移學習，其架構不包含特定於數據集的參數。它解凍了一個預訓練的文本編碼器，並將目標token作為輸入，為模型提供學習任務特定嵌入所需的上下文。TabSTAR在具有文本特徵的分類任務的已知基準上，針對中型和大型數據集都實現了最先進的性能，並且其預訓練階段在數據集數量上表現出擴展規律，為進一步的性能改進提供了途徑。", "applications": ["**金融風控：** 結合客戶填寫的申請資料（表格）和文字描述（例如：申請理由、備註）來更精準地評估信用風險，降低壞帳率。", "**電商產品推薦：** 分析商品表格數據（價格、規格）和用戶的文字評價，提供更符合用戶需求的個性化推薦，提高轉化率。", "**醫療診斷輔助：** 整合病人的表格數據（年齡、病史）和醫生撰寫的病歷報告，幫助醫生更快更準確地做出診斷，提升醫療效率。"], "pitch": "TabSTAR 解決了深度學習在表格數據處理上的瓶頸，尤其在結合文本資訊時更具優勢。現有表格數據分析方法難以有效利用文本信息，而 TabSTAR 利用創新的語義目標感知表徵技術，顯著提升了模型準確性。這意味著在金融、電商、醫療等領域，企業可以更精準地分析數據，做出更明智的決策。其商業價值體現在：降低風險（金融風控）、提升收益（電商推薦）、以及改善效率（醫療診斷）。 TabSTAR 的基礎模型架構使其易於部署和擴展，具備廣闊的市場前景。投資 TabSTAR 類似於投資下一代的表格數據分析引擎，具有巨大的增長潛力。", "audio": "audios/2505.18125v1.mp3", "timestamp": "2025-05-26T17:12:46.089348"}
{"query": "Diffusion Model", "id": "2505.18145v1", "url": "http://arxiv.org/abs/2505.18145v1", "title": "Stochastic agent-based Monte Carlo simulations for reaction-diffusion models, population dynamics, and epidemic spreading", "summary": "We provide a succinct overview of the implementation of Monte Carlo\nalgorithms based on Markovian stochastic dynamics to study interacting and\nreacting many-particle systems away from thermal equilibrium. Such agent-based\ncomputer simulations constitute an effective tool to introduce undergraduate\nand beginning graduate students to current frontier research without requiring\nmuch prior knowledge or experience: Starting from direct visualization of\nsimulation data, students may gain immediate insight into emerging macroscopic\nfeatures of a complex model system and subsequently apply more sophisticated\ndata analysis to quantitatively characterize its often rich dynamical\nproperties, both in stationary and transient regimes. We utilize numerical\ninvestigations of paradigmatic reaction-diffusion systems, as well as\nstochastic models for population dynamics and epidemic spreading, to exemplify\nhow interdisciplinary computational research can be effectively utilized in\nbottom-up undergraduate and graduate education through learning by doing. In\naddition, we give helpful hints for the practical setup of Monte Carlo\nsimulation algorithms, provide sample codes, explain some typical data analysis\ntools, and describe various potential error sources and pitfalls, with tips for\navoiding them.", "authors": ["Mohamed Swailem", "Ulrich Dobramysl", "Ruslan Mukhamadiarov", "Uwe C. Täuber"], "published_date": "2025-05-23", "title_zh": "基於隨機代理人的蒙地卡羅模擬，用於反應擴散模型、族群動力學和流行病傳播", "summary_zh": "本文介紹了一種基於馬可夫隨機動力學的蒙地卡羅演算法，用於研究非熱平衡狀態下交互作用和反應的多粒子系統。這種基於代理人的電腦模擬工具，能有效引導大學生和研究生入門前沿研究，且無需太多先備知識。學生能透過直接視覺化模擬數據，快速理解複雜模型的宏觀特徵，並進一步進行數據分析，量化評估其豐富的動態特性。我們使用反應擴散系統、族群動力學和流行病傳播的數值研究為例，說明如何透過實作學習，將跨領域的計算研究有效應用於大學部和研究所的基礎教育。此外，我們也提供了蒙地卡羅模擬演算法的實用設置技巧、範例程式碼、典型數據分析工具、以及潛在的誤差來源與陷阱，並提供避免方法。", "applications": ["**疫情預測與控制：** 模擬特定區域內不同人群行為模式下的疾病傳播，協助政府制定更精準的防疫政策，例如封鎖策略、疫苗接種優先順序等。", "**生態系統建模：** 模擬不同物種之間的交互作用，預測氣候變遷、外來物種入侵等因素對生態平衡的影響，協助保育工作者制定更有效的保護策略。", "**社群網路傳播分析：** 模擬謠言、病毒式行銷等在社群網路上的傳播途徑和速度，協助企業制定更有效的品牌推廣策略，或協助政府部門監控和抑制不實資訊的散播。"], "pitch": "我們開發了一套高效且易於使用的蒙地卡羅模擬引擎，專注於處理複雜的反應擴散、族群動力學和流行病傳播問題。 我們的核心價值在於提供可客製化的解決方案，讓使用者 (從研究機構到企業) 能更快速、準確地預測和分析相關現象。 這將帶來多方面的商業價值：\n\n* **精準決策支持：** 為醫療機構、政府部門提供更精準的疫情預測，降低疫情爆發風險，並優化資源分配。\n* **環境風險評估：** 協助企業評估其活動對生態系統的潛在影響，制定更可持續的發展策略。\n* **市場策略優化：** 為行銷公司提供消費者行為模擬，優化行銷活動，提高投資回報率。\n\n透過我們的模擬引擎，客戶可以有效降低風險、提高效率、並做出更明智的決策。 我們正在尋找投資者，共同開拓這個潛力無限的市場。", "audio": "audios/2505.18145v1.mp3", "timestamp": "2025-05-26T17:13:03.047063"}
{"query": "AI", "id": "2505.18139v1", "url": "http://arxiv.org/abs/2505.18139v1", "title": "Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems", "summary": "This position paper argues that the theoretical inconsistency often observed\namong Responsible AI (RAI) metrics, such as differing fairness definitions or\ntradeoffs between accuracy and privacy, should be embraced as a valuable\nfeature rather than a flaw to be eliminated. We contend that navigating these\ninconsistencies, by treating metrics as divergent objectives, yields three key\nbenefits: (1) Normative Pluralism: Maintaining a full suite of potentially\ncontradictory metrics ensures that the diverse moral stances and stakeholder\nvalues inherent in RAI are adequately represented. (2) Epistemological\nCompleteness: The use of multiple, sometimes conflicting, metrics allows for a\nmore comprehensive capture of multifaceted ethical concepts, thereby preserving\ngreater informational fidelity about these concepts than any single, simplified\ndefinition. (3) Implicit Regularization: Jointly optimizing for theoretically\nconflicting objectives discourages overfitting to one specific metric, steering\nmodels towards solutions with enhanced generalization and robustness under\nreal-world complexities. In contrast, efforts to enforce theoretical\nconsistency by simplifying or pruning metrics risk narrowing this value\ndiversity, losing conceptual depth, and degrading model performance. We\ntherefore advocate for a shift in RAI theory and practice: from getting trapped\nin inconsistency to characterizing acceptable inconsistency thresholds and\nelucidating the mechanisms that permit robust, approximated consistency in\npractice.", "authors": ["Gordon Dai", "Yunze Xiao"], "published_date": "2025-05-23", "title_zh": "擁抱矛盾：理論不一致性不會阻礙負責任人工智慧系統的建構之路", "summary_zh": "這篇論文主張，負責任人工智慧（RAI）指標間常見的理論不一致性，例如不同的公平性定義或準確度和隱私之間的權衡，應該被視為寶貴的特性而非需要消除的缺陷。透過將這些指標視為不同的目標來處理這些不一致性，可以帶來三個關鍵好處：規範多元主義、知識論完整性以及隱性正規化。試圖強制理論一致性反而會縮減價值多樣性、失去概念深度並降低模型性能。因此，我們提倡 RAI 理論和實踐的轉變：從受困於不一致性，轉向描述可接受的不一致性閾值，並闡明在實踐中允許穩健、近似一致性的機制。", "applications": ["**貸款審批：** 銀行可以使用多個公平性指標來評估貸款審批模型的偏見，例如針對不同種族或性別群體的平等機會。即使這些指標存在衝突，也能確保兼顧不同群體的利益，避免單一指標導致的不公平結果。", "**招聘系統：** 人力資源部門可以同時考慮技能匹配、經驗和多元化等指標來篩選候選人。即使提高多元化可能會稍微降低平均技能水平，也能建立一個更具包容性和代表性的團隊。", "**醫療診斷：** 醫生可以使用多個準確性和敏感性指標來評估診斷模型的性能。即使提高敏感性（減少漏診）可能會稍微降低準確性（增加誤診），也能優先考慮患者的安全，避免延誤治療。"], "pitch": "我們發現了一個關鍵的突破口，將徹底改變負責任人工智慧的發展方向。目前的 RAI 方法過於強調理論一致性，導致模型缺乏彈性和真實世界的適用性。我們的研究表明，擁抱指標間的矛盾實際上可以帶來更公平、更穩健的模型。想像一下，一個可以自動評估貸款申請的 AI，它不僅僅關注傳統的信用評分，還同時考量種族、性別等多個公平性指標，即使這些指標之間存在衝突。這不僅可以消除潛在的偏見，還能建立一個更加信任和可持續的金融系統。我們的技術可以應用於任何需要公平性和透明度的 AI 應用場景，例如招聘、醫療、法律等。我們正在尋找投資者，共同打造下一代負責任的人工智慧解決方案，抓住這個價值數十億美元的市場機會，引領 AI 技術走向更公平、更安全和更可持續的未來。", "audio": "audios/2505.18139v1.mp3", "timestamp": "2025-05-26T18:18:05.679932"}
{"query": "Foundation Model", "id": "2505.18125v1", "url": "http://arxiv.org/abs/2505.18125v1", "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations", "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.", "authors": ["Alan Arazi", "Eilam Shapira", "Roi Reichart"], "published_date": "2025-05-23", "title_zh": "TabSTAR：具語義目標感知表徵的基礎表格模型", "summary_zh": "儘管深度學習在許多領域取得了顯著成功，但在表格學習任務中，其表現一直不如梯度提升決策樹（GBDTs）。然而，最近的進展正為表格基礎模型鋪平道路，這些模型可以利用真實世界的知識並跨多樣的數據集進行泛化，尤其是在數據包含自由文本時。雖然將語言模型能力整合到表格任務中已被探索，但大多數現有方法使用靜態的、目標無關的文本表徵，限制了它們的有效性。我們推出了 TabSTAR：一個具有語義目標感知表徵的基礎表格模型。TabSTAR旨在實現對具有文本特徵的表格數據的遷移學習，其架構不含數據集特定參數。它解凍了預訓練的文本編碼器，並將目標token作為輸入，這些token為模型提供了學習特定任務嵌入所需的上下文。TabSTAR在具有文本特徵的分類任務的已知基準測試中，針對中型和大型數據集都取得了最先進的性能，並且其預訓練階段在數據集數量上表現出縮放律，為進一步提高性能提供了一條途徑。", "applications": ["**線上貸款風險評估：** 結合申請人的個人資料、財務數據以及填寫的描述性文字（例如：工作經驗、貸款用途），更精準判斷還款能力和風險。", "**客戶服務智能分流：** 分析客戶提交的文字問題描述（加上基本的客戶資訊），自動將問題分發給最合適的客服人員或提供自助解決方案。", "**招聘履歷篩選：** 自動分析履歷表中的學歷、工作經驗等結構化數據，以及自我介紹、項目經驗等非結構化文字，快速篩選出符合職位要求的候選人。"], "pitch": "TabSTAR解決了表格數據處理中長期存在的痛點：如何有效利用表格中的文本信息。傳統表格模型難以理解文字的語義信息，導致預測準確度受限。TabSTAR通過引入語義目標感知表徵，大幅提升了模型的性能，使其能夠在各個行業中得到廣泛應用。想像一下，我們能夠創建一個通用的表格智能引擎，僅需少量調整就能應用於金融風控、客戶服務、人力資源等領域，為企業節省大量的人力成本和時間成本。此外，TabSTAR的預訓練模型具有良好的可擴展性，隨著數據量的增加，模型性能還將持續提升。因此，TabSTAR不僅是一個技術突破，更是一個巨大的商業機會，有潜力引領新一代的表格數據分析革命。", "audio": "audios/2505.18125v1.mp3", "timestamp": "2025-05-26T18:18:26.676775"}
{"query": "Diffusion Model", "id": "2505.18145v1", "url": "http://arxiv.org/abs/2505.18145v1", "title": "Stochastic agent-based Monte Carlo simulations for reaction-diffusion models, population dynamics, and epidemic spreading", "summary": "We provide a succinct overview of the implementation of Monte Carlo\nalgorithms based on Markovian stochastic dynamics to study interacting and\nreacting many-particle systems away from thermal equilibrium. Such agent-based\ncomputer simulations constitute an effective tool to introduce undergraduate\nand beginning graduate students to current frontier research without requiring\nmuch prior knowledge or experience: Starting from direct visualization of\nsimulation data, students may gain immediate insight into emerging macroscopic\nfeatures of a complex model system and subsequently apply more sophisticated\ndata analysis to quantitatively characterize its often rich dynamical\nproperties, both in stationary and transient regimes. We utilize numerical\ninvestigations of paradigmatic reaction-diffusion systems, as well as\nstochastic models for population dynamics and epidemic spreading, to exemplify\nhow interdisciplinary computational research can be effectively utilized in\nbottom-up undergraduate and graduate education through learning by doing. In\naddition, we give helpful hints for the practical setup of Monte Carlo\nsimulation algorithms, provide sample codes, explain some typical data analysis\ntools, and describe various potential error sources and pitfalls, with tips for\navoiding them.", "authors": ["Mohamed Swailem", "Ulrich Dobramysl", "Ruslan Mukhamadiarov", "Uwe C. Täuber"], "published_date": "2025-05-23", "title_zh": "用於反應擴散模型、族群動力學和流行病傳播的基於隨機代理的蒙地卡羅模擬", "summary_zh": "本研究提供一種簡潔的方式，使用基於馬可夫隨機動力學的蒙地卡羅演算法，來研究遠離熱平衡的交互作用多粒子系統。這種基於代理的電腦模擬是一個有效的工具，能讓大學生和剛開始的研究生，在不需要太多先備知識或經驗的情況下，接觸到當前的前沿研究。學生可以直接視覺化模擬數據，快速理解複雜模型的宏觀特徵，並應用更複雜的數據分析來定量描述其豐富的動態特性。我們利用對反應擴散系統、族群動力學和流行病傳播的數值研究，展示了如何透過實作學習，在本科和研究生教育中有效地利用跨學科的計算研究。此外，我們還提供了蒙地卡羅模擬演算法的實用設定提示、範例程式碼、常見數據分析工具，並說明了各種潛在的錯誤來源和陷阱，以及避免這些問題的技巧。", "applications": ["**餐廳排隊人數預測：** 模擬顧客到達和服務時間的隨機性，預測不同時段的排隊長度，幫助餐廳調整人手和座位安排，提升顧客滿意度。", "**交通號誌優化：** 模擬車輛在路口的行為模式，調整紅綠燈時間，減少交通堵塞，提升交通效率。", "**病毒傳播模型：** 模擬病毒在社區中的傳播，評估不同防疫措施（如戴口罩、保持社交距離）的效果，幫助政府制定更有效的防疫政策。"], "pitch": "這項技術的核心價值在於其模擬複雜系統的強大能力和易用性。基於代理的蒙地卡羅模擬可以快速建立模型，預測各種場景的發展趨勢。想像一下，一個企業可以利用它來預測市場趨勢，優化供應鏈，或評估新產品的風險。政府機構可以用於應對突發事件，例如流行病爆發或自然災害。由於其易學易用的特性，可以快速擴展應用範圍，在教育、醫療、物流、金融等多個領域都有巨大的商業潛力。團隊可以打造一個模擬平台，提供使用者友善的界面和豐富的案例庫，讓他們能夠輕鬆地建立和運行模擬模型，從而快速獲取決策所需的洞察力。訂閱制的商業模式，以及針對特定行業的定制化服務，將帶來穩定的收入來源和巨大的市場空間。這是一個具有顛覆性潛力的技術，值得投資。", "audio": "audios/2505.18145v1.mp3", "timestamp": "2025-05-26T18:18:45.583406"}
{"query": "AI", "id": "2505.18139v1", "url": "http://arxiv.org/abs/2505.18139v1", "title": "Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems", "summary": "This position paper argues that the theoretical inconsistency often observed\namong Responsible AI (RAI) metrics, such as differing fairness definitions or\ntradeoffs between accuracy and privacy, should be embraced as a valuable\nfeature rather than a flaw to be eliminated. We contend that navigating these\ninconsistencies, by treating metrics as divergent objectives, yields three key\nbenefits: (1) Normative Pluralism: Maintaining a full suite of potentially\ncontradictory metrics ensures that the diverse moral stances and stakeholder\nvalues inherent in RAI are adequately represented. (2) Epistemological\nCompleteness: The use of multiple, sometimes conflicting, metrics allows for a\nmore comprehensive capture of multifaceted ethical concepts, thereby preserving\ngreater informational fidelity about these concepts than any single, simplified\ndefinition. (3) Implicit Regularization: Jointly optimizing for theoretically\nconflicting objectives discourages overfitting to one specific metric, steering\nmodels towards solutions with enhanced generalization and robustness under\nreal-world complexities. In contrast, efforts to enforce theoretical\nconsistency by simplifying or pruning metrics risk narrowing this value\ndiversity, losing conceptual depth, and degrading model performance. We\ntherefore advocate for a shift in RAI theory and practice: from getting trapped\nin inconsistency to characterizing acceptable inconsistency thresholds and\nelucidating the mechanisms that permit robust, approximated consistency in\npractice.", "authors": ["Gordon Dai", "Yunze Xiao"], "published_date": "2025-05-23", "title_zh": "擁抱矛盾：理論上的不一致性不會阻礙構建負責任AI系統的道路", "summary_zh": "這篇論文主張，負責人AI(RAI)的指標之間經常出現理論上的不一致性，像是公平定義不同或準確度與隱私權之間的權衡，應該被視為一個有價值的特點，而不是需要消除的缺陷。論文認為，透過將這些指標視為不同的目標來應對這些不一致性，可以帶來三個主要好處：(1)規範多元化：維持一套完整且可能相互矛盾的指標，確保RAI中固有的各種道德立場和利害關係人價值得到充分體現。(2)知識完整性：使用多個、有時相互衝突的指標，可以更全面地捕捉多面向的倫理概念，從而比任何單一、簡化的定義保留更多關於這些概念的資訊保真度。(3)隱式正規化：共同優化理論上相互衝突的目標，可以防止模型過度擬合特定指標，引導模型朝向在真實世界複雜情況下具有更強泛化性和穩健性的解決方案。相反，試圖透過簡化或修剪指標來強制理論上的一致性，可能會縮小這種價值多樣性，喪失概念深度，並降低模型效能。因此，我們提倡RAI理論和實踐的轉變：從陷入不一致性，轉向描述可接受的不一致性閾值，並闡明允許在實踐中實現穩健、近似一致性的機制。", "applications": ["**貸款申請審核：** 不同背景的申請人對'公平'的定義可能不同。透過擁抱這些不同的公平性指標，AI系統可以更全面地評估貸款風險，避免歧視並提供更客製化的貸款方案。", "**醫療診斷：** 考慮病患的隱私、診斷的準確度以及治療的可負擔性。AI系統可以在這些互相衝突的目標之間取得平衡，為醫生提供更周全的診斷建議，同時尊重病患的自主權和隱私。", "**自動駕駛汽車：** 在發生意外時，AI需要權衡乘客安全、行人安全以及其他車輛的安全。透過擁抱不同且有時衝突的道德原則，AI系統可以在複雜的交通情境中做出更明智的決策。"], "pitch": "我們正在重新定義負責任AI的發展方向。傳統方法試圖消除指標之間的不一致性，但我們認為這種做法會犧牲模型的多樣性和真實世界的適應性。我們的技術允許AI系統同時考慮多個、甚至互相衝突的指標，例如公平性、隱私性和準確性。這不僅能產生更穩健、更公正的AI模型，還能開啟全新的應用場景，例如更客製化的金融服務、更周全的醫療診斷和更安全的自動駕駛系統。想像一下，一個AI系統可以理解不同利害關係人的價值觀，並在複雜的環境中做出更明智的決策。這就是我們所提供的價值。我們正在尋求投資者，共同開創一個更負責任、更可信賴的AI未來，並在這個快速增長的市場中搶佔先機。", "audio": "audios/2505.18139v1.mp3", "timestamp": "2025-05-26T19:12:04.046901"}
{"query": "Foundation Model", "id": "2505.18125v1", "url": "http://arxiv.org/abs/2505.18125v1", "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations", "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.", "authors": ["Alan Arazi", "Eilam Shapira", "Roi Reichart"], "published_date": "2025-05-23", "title_zh": "TabSTAR：具備語義目標感知表徵的基礎表格模型", "summary_zh": "過去深度學習在表格資料的學習表現不佳，通常由梯度提升決策樹(GBDTs)主導。但近期出現了表格基礎模型的進展，可以利用真實世界的知識並推廣到不同的資料集，尤其是在資料包含自由文本時。我們推出了TabSTAR，它是一個具有語義目標感知表徵的基礎表格模型，能對表格資料與文本特徵進行遷移學習，架構中沒有特定於資料集的參數。 TabSTAR將預訓練的文本編碼器解凍，並將目標tokens作為輸入，為模型提供學習任務特定嵌入所需的上下文。TabSTAR在具有文本特徵的分類任務基準測試中，在中型和大型資料集上都取得了最先進的性能，並且其預訓練階段展現了資料集數量上的縮放定律，為進一步的性能提升提供了途徑。", "applications": ["**貸款風險評估：** 利用客戶填寫的貸款申請表格（包含描述性文字）預測貸款違約風險，優於傳統的數值模型，更精準評估風險，減少損失。", "**產品評論分析：** 分析產品表格化的銷售數據，結合使用者在評論區的自由文本，更精準預測產品的銷售量，或找出影響使用者購買決策的關鍵因素。", "**醫療診斷輔助：** 將病患的病歷表格數據（包含症狀描述等文本）與醫學知識庫結合，輔助醫生進行更準確的診斷，提升診斷效率。"], "pitch": "TabSTAR是針對表格資料的革命性AI模型，尤其擅長處理包含文本資訊的複雜資料集。它超越了傳統的梯度提升決策樹，能夠更準確地預測和分類。 其商業價值體現在：\n\n*   **顯著的效能提升：** TabSTAR在多個基準測試中都取得了領先地位，能為企業帶來更精準的預測結果，提升決策品質。\n*   **跨領域應用潛力：** 從金融風控、電子商務到醫療診斷，TabSTAR都能夠應用，滿足不同產業的需求。\n*   **可擴展性：** 其預訓練架構具備擴展性，隨著更多資料的訓練，效能將持續提升。\n*   **降低對資料工程的依賴：** 簡化的模型架構降低了對複雜資料工程的需求，降低了部署成本。\n\nTabSTAR不僅是一個技術突破，更是企業提升競爭力、降低風險和創造價值的關鍵。 我們正在尋找合作夥伴，共同將TabSTAR推向市場，引領表格資料AI的發展。", "audio": "audios/2505.18125v1.mp3", "timestamp": "2025-05-26T19:12:28.918340"}
{"query": "Diffusion Model", "id": "2505.18145v1", "url": "http://arxiv.org/abs/2505.18145v1", "title": "Stochastic agent-based Monte Carlo simulations for reaction-diffusion models, population dynamics, and epidemic spreading", "summary": "We provide a succinct overview of the implementation of Monte Carlo\nalgorithms based on Markovian stochastic dynamics to study interacting and\nreacting many-particle systems away from thermal equilibrium. Such agent-based\ncomputer simulations constitute an effective tool to introduce undergraduate\nand beginning graduate students to current frontier research without requiring\nmuch prior knowledge or experience: Starting from direct visualization of\nsimulation data, students may gain immediate insight into emerging macroscopic\nfeatures of a complex model system and subsequently apply more sophisticated\ndata analysis to quantitatively characterize its often rich dynamical\nproperties, both in stationary and transient regimes. We utilize numerical\ninvestigations of paradigmatic reaction-diffusion systems, as well as\nstochastic models for population dynamics and epidemic spreading, to exemplify\nhow interdisciplinary computational research can be effectively utilized in\nbottom-up undergraduate and graduate education through learning by doing. In\naddition, we give helpful hints for the practical setup of Monte Carlo\nsimulation algorithms, provide sample codes, explain some typical data analysis\ntools, and describe various potential error sources and pitfalls, with tips for\navoiding them.", "authors": ["Mohamed Swailem", "Ulrich Dobramysl", "Ruslan Mukhamadiarov", "Uwe C. Täuber"], "published_date": "2025-05-23", "title_zh": "反應擴散模型、族群動力學和流行病傳播的隨機主體蒙地卡羅模擬", "summary_zh": "這篇論文簡要介紹了基於馬可夫隨機動力學的蒙地卡羅演算法，用於研究遠離熱平衡的交互反應多粒子系統。這種基於主體的電腦模擬，是引導大學生和研究生進入前沿研究的有效工具，且不需要太多先備知識。學生可以透過直接視覺化模擬數據，立即了解複雜模型系統中產生的巨觀特徵，並進一步應用更複雜的數據分析，以定量地描述其豐富的動態特性，無論是在穩定還是瞬態狀態下。論文利用對反應擴散系統、族群動力學和流行病傳播等範例的數值研究，展示了如何透過實作學習，在本科和研究生教育中有效利用跨學科的計算研究。此外，還提供了蒙地卡羅模擬演算法的實用設定技巧、範例程式碼、常見數據分析工具，並描述了各種潛在的錯誤來源和陷阱，以及避免它們的技巧。", "applications": ["**城市規劃與疏散模擬：** 模擬城市人口在災害（如火災、地震）發生時的疏散行為，優化疏散路線和避難場所設置，提高整體應急響應能力。", "**產品行銷策略模擬：** 模擬消費者在不同行銷策略下的購買行為，預測產品銷售量，優化行銷預算分配和廣告投放策略。", "**傳染病控制策略評估：** 模擬不同傳染病控制措施（如疫苗接種、社交距離）對疫情傳播的影響，評估策略有效性，為公共衛生決策提供數據支持。"], "pitch": "我們開發了一套基於隨機主體的蒙地卡羅模擬平台，能夠高精度地模擬複雜系統的動態行為，尤其擅長處理反應擴散、族群動力學和流行病傳播等問題。相較於傳統模型，我們的平台更貼近真實世界，能夠捕捉到系統中個體行為的細微差異和隨機性。這項技術具有廣泛的應用前景，包括智慧城市建設（交通優化、災害應對）、醫療健康（新藥研發、疫情控制）、市場行銷（消費者行為分析）等。我們相信，通過將我們的平台整合到各行業的決策流程中，能夠顯著提升效率、降低風險，並創造巨大的商業價值。我們的商業模式將採取軟體授權和客製化解決方案相結合的方式，為客戶提供量身定制的服務。", "audio": "audios/2505.18145v1.mp3", "timestamp": "2025-05-26T19:12:47.804809"}
{"query": "AI", "id": "2505.18139v1", "url": "http://arxiv.org/abs/2505.18139v1", "title": "Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems", "summary": "This position paper argues that the theoretical inconsistency often observed\namong Responsible AI (RAI) metrics, such as differing fairness definitions or\ntradeoffs between accuracy and privacy, should be embraced as a valuable\nfeature rather than a flaw to be eliminated. We contend that navigating these\ninconsistencies, by treating metrics as divergent objectives, yields three key\nbenefits: (1) Normative Pluralism: Maintaining a full suite of potentially\ncontradictory metrics ensures that the diverse moral stances and stakeholder\nvalues inherent in RAI are adequately represented. (2) Epistemological\nCompleteness: The use of multiple, sometimes conflicting, metrics allows for a\nmore comprehensive capture of multifaceted ethical concepts, thereby preserving\ngreater informational fidelity about these concepts than any single, simplified\ndefinition. (3) Implicit Regularization: Jointly optimizing for theoretically\nconflicting objectives discourages overfitting to one specific metric, steering\nmodels towards solutions with enhanced generalization and robustness under\nreal-world complexities. In contrast, efforts to enforce theoretical\nconsistency by simplifying or pruning metrics risk narrowing this value\ndiversity, losing conceptual depth, and degrading model performance. We\ntherefore advocate for a shift in RAI theory and practice: from getting trapped\nin inconsistency to characterizing acceptable inconsistency thresholds and\nelucidating the mechanisms that permit robust, approximated consistency in\npractice.", "authors": ["Gordon Dai", "Yunze Xiao"], "published_date": "2025-05-23", "title_zh": "擁抱矛盾：理論上的不一致不會阻礙建構負責任的AI系統之路", "summary_zh": "這篇論文認為，負責任AI(RAI)指標中常見的理論不一致，例如不同的公平性定義或準確性與隱私之間的權衡，應該被視為有價值的特徵，而非缺陷。透過將這些指標視為不同的目標來導航，可以帶來(1)規範多元主義，確保多元道德立場和利益相關者的價值得到充分代表；(2)知識論完整性，更全面地捕捉多面向的倫理概念；(3)隱式正則化，阻止模型過度擬合特定指標，從而提高在真實世界複雜性下的泛化能力和穩健性。因此，我們提倡RAI理論和實踐的轉變：從受困於不一致，轉向刻畫可接受的不一致閾值，並闡明在實踐中實現穩健、近似一致性的機制。", "applications": ["醫療診斷：AI系統在診斷疾病時，同時考量準確性、公平性(例如，避免族群偏見)和隱私保護，即使這些目標之間存在權衡。醫生可以基於多重指標的評估，做出更全面的判斷。", "信用評估：AI系統在評估貸款申請人信用時，不僅追求預測的準確性，還考慮不同族群之間的公平性，確保不會因為單一指標優化而造成歧視。", "自動駕駛：AI系統在決策時，必須在安全性(避免事故)、效率(減少擁堵)和乘客舒適度之間權衡。接受這些目標的潛在衝突，可以開發出更人性化和穩健的自動駕駛系統。"], "pitch": "我們的研究揭示了構建真正負責任AI系統的關鍵：擁抱矛盾而非消除不一致。傳統方法試圖簡化指標，可能導致潛在的偏見和模型的脆弱性。我們的方法允許模型同時優化多個相互衝突的目標，例如公平性、隱私和準確性，从而构建更稳健、更公正的AI系统。想像一下，一個醫療診斷AI，它不僅準確預測疾病，而且消除了種族偏見，並保護了患者的隱私。或者，一個信用評估AI，它公平地評估每個人的信譽，无论其背景如何。這就是我們技術的潛力。我們正在尋求投資，以開發一套工具和框架，帮助企業和組織構建符合倫理標準、可信賴且能真正改善人們生活的AI系統。我們相信，這不僅是一個巨大的市場機會，也是一個我們有義務去實現的願景。", "audio": "audios/2505.18139v1.mp3", "timestamp": "2025-05-26T20:15:07.501325"}
{"query": "Foundation Model", "id": "2505.18125v1", "url": "http://arxiv.org/abs/2505.18125v1", "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations", "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.", "authors": ["Alan Arazi", "Eilam Shapira", "Roi Reichart"], "published_date": "2025-05-23", "title_zh": "TabSTAR：具備語義目標感知表徵的基礎表格模型", "summary_zh": "儘管深度學習在許多領域都取得了顯著的成功，但它在表格學習任務中的表現一直不如梯度提升決策樹（GBDT）。然而，最近的進展正在為表格基礎模型鋪平道路，這些模型可以利用真實世界的知識並在不同的數據集中進行泛化，尤其是在數據包含自由文本時。儘管已經探索過將語言模型能力融入表格任務，但大多數現有方法使用靜態的、目標無關的文本表徵，限制了它們的有效性。我們介紹 TabSTAR：一個具有語義目標感知表徵的基礎表格模型。TabSTAR 旨在實現具有文本特徵的表格數據的遷移學習，其架構不包含數據集特定的參數。它解凍了一個預訓練的文本編碼器，並將目標標記作為輸入，這些標記為模型提供了學習特定任務嵌入所需的上下文。TabSTAR 在具有文本特徵的分類任務的已知基準測試中，針對中型和大型數據集都實現了最先進的性能，並且其預訓練階段在數據集數量上表現出縮放規律，為進一步的性能改進提供了途徑。", "applications": ["**房地產估價：** 輸入房屋描述、位置等文字資訊，結合房屋類型、房間數量等表格數據，更準確地預測房屋價格，優於單純基於數字的傳統模型。", "**客戶服務自動化：** 根據客戶的歷史購買記錄（表格數據）以及客戶問題的描述（文本數據），快速且精準地判斷客戶遇到的問題類型，並推薦最佳解決方案，提升客戶滿意度並降低客服成本。", "**醫療診斷輔助：** 結合病患的病史（表格數據）和醫生記錄的病徵描述（文本數據），輔助醫生診斷，提高診斷效率和準確性，減少誤診的可能性。"], "pitch": "TabSTAR 是一個突破性的表格基礎模型，它將文本處理能力與表格數據分析巧妙結合，解決了傳統表格模型無法有效利用文本信息的痛點。這意味著我們能在各行各業解鎖大量原本難以利用的混合數據，從而提升決策效率和預測準確性。考慮一下金融風險評估、市場趨勢預測、智慧醫療診斷等領域，TabSTAR 都能提供更精準、更具洞察力的分析結果。我們的模型架構具有高度的泛化能力和可擴展性，能夠快速適應不同的數據集和任務需求。隨著數據量的增長和模型的不斷優化，TabSTAR 的潛在商業價值將呈指數級增長，成為企業數據驅動決策的核心引擎。這不僅僅是一個模型，而是一個重新定義數據分析未來的平台。", "audio": "audios/2505.18125v1.mp3", "timestamp": "2025-05-26T20:15:23.028743"}
{"query": "Diffusion Model", "id": "2505.18145v1", "url": "http://arxiv.org/abs/2505.18145v1", "title": "Stochastic agent-based Monte Carlo simulations for reaction-diffusion models, population dynamics, and epidemic spreading", "summary": "We provide a succinct overview of the implementation of Monte Carlo\nalgorithms based on Markovian stochastic dynamics to study interacting and\nreacting many-particle systems away from thermal equilibrium. Such agent-based\ncomputer simulations constitute an effective tool to introduce undergraduate\nand beginning graduate students to current frontier research without requiring\nmuch prior knowledge or experience: Starting from direct visualization of\nsimulation data, students may gain immediate insight into emerging macroscopic\nfeatures of a complex model system and subsequently apply more sophisticated\ndata analysis to quantitatively characterize its often rich dynamical\nproperties, both in stationary and transient regimes. We utilize numerical\ninvestigations of paradigmatic reaction-diffusion systems, as well as\nstochastic models for population dynamics and epidemic spreading, to exemplify\nhow interdisciplinary computational research can be effectively utilized in\nbottom-up undergraduate and graduate education through learning by doing. In\naddition, we give helpful hints for the practical setup of Monte Carlo\nsimulation algorithms, provide sample codes, explain some typical data analysis\ntools, and describe various potential error sources and pitfalls, with tips for\navoiding them.", "authors": ["Mohamed Swailem", "Ulrich Dobramysl", "Ruslan Mukhamadiarov", "Uwe C. Täuber"], "published_date": "2025-05-23", "title_zh": "反應擴散模型、族群動力學及流行病傳播之隨機代理人蒙地卡羅模擬", "summary_zh": "本文簡要介紹了基於馬可夫隨機動態的蒙地卡羅演算法，用於研究遠離熱平衡的相互作用和反應的多粒子系統。這種基於代理人的電腦模擬是一種有效的工具，能讓大學部和剛入門的研究生在無需太多先備知識或經驗的情況下，接觸到前沿研究。學生可以透過直接視覺化模擬數據，立即深入了解複雜模型系統中湧現的宏觀特徵，並進一步應用更複雜的數據分析來定量描述其豐富的動態特性，包括穩態和瞬態。我們利用對範例反應擴散系統以及族群動力學和流行病傳播的隨機模型進行的數值研究，來例證跨學科的計算研究如何能透過做中學，有效地應用於自下而上的大學部和研究所教育。此外，我們還提供了蒙地卡羅模擬演算法的實用設置技巧、示例代碼、一些典型的數據分析工具，並描述了各種潛在的錯誤來源和陷阱，並提供避免這些錯誤的提示。", "applications": ["**城市規劃模擬：** 使用代理人模型模擬城市居民的移動和互動，研究交通流量、資源分配和公共服務的可及性，幫助優化城市規劃，提高生活品質。", "**生態系統建模：** 模擬不同物種之間的互動和環境因素的影響，預測生態系統的變化趨勢，為環境保護和資源管理提供決策支持。", "**線上遊戲行為分析：** 模擬玩家在遊戲中的行為模式，分析遊戲的平衡性、挑戰性和玩家體驗，幫助遊戲開發者改進遊戲設計，提高玩家黏著度。"], "pitch": "這項技術的核心價值在於其能夠以低成本、高效率的方式模擬複雜系統，並提供可視化的結果。對於政府、企業和研究機構而言，它可以作為預測和決策的強有力工具。例如，在公共衛生領域，可以模擬傳染病爆發的情景，評估不同干預措施的效果，為制定防疫政策提供科學依據。在商業領域，可以模擬市場競爭環境，預測消費者行為，幫助企業制定更有效的營銷策略。同時，其開源的特性降低了應用門檻，更容易被學術界和工業界廣泛採用。我們認為，這項技術具有廣闊的市場前景，潛在商業價值巨大。", "audio": "audios/2505.18145v1.mp3", "timestamp": "2025-05-26T20:15:37.764799"}
{"query": "AI", "id": "2505.18139v1", "url": "http://arxiv.org/abs/2505.18139v1", "title": "Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems", "summary": "This position paper argues that the theoretical inconsistency often observed\namong Responsible AI (RAI) metrics, such as differing fairness definitions or\ntradeoffs between accuracy and privacy, should be embraced as a valuable\nfeature rather than a flaw to be eliminated. We contend that navigating these\ninconsistencies, by treating metrics as divergent objectives, yields three key\nbenefits: (1) Normative Pluralism: Maintaining a full suite of potentially\ncontradictory metrics ensures that the diverse moral stances and stakeholder\nvalues inherent in RAI are adequately represented. (2) Epistemological\nCompleteness: The use of multiple, sometimes conflicting, metrics allows for a\nmore comprehensive capture of multifaceted ethical concepts, thereby preserving\ngreater informational fidelity about these concepts than any single, simplified\ndefinition. (3) Implicit Regularization: Jointly optimizing for theoretically\nconflicting objectives discourages overfitting to one specific metric, steering\nmodels towards solutions with enhanced generalization and robustness under\nreal-world complexities. In contrast, efforts to enforce theoretical\nconsistency by simplifying or pruning metrics risk narrowing this value\ndiversity, losing conceptual depth, and degrading model performance. We\ntherefore advocate for a shift in RAI theory and practice: from getting trapped\nin inconsistency to characterizing acceptable inconsistency thresholds and\nelucidating the mechanisms that permit robust, approximated consistency in\npractice.", "authors": ["Gordon Dai", "Yunze Xiao"], "published_date": "2025-05-23", "title_zh": "擁抱矛盾：理論上的不一致不會阻礙負責任的人工智慧系統的建構之路", "summary_zh": "這篇論文主張，負責任的人工智慧(RAI)指標間常見的理論不一致，例如不同的公平性定義或準確性與隱私之間的權衡，應被視為有價值的特性，而非亟待消除的缺陷。我們認為，將這些不一致視為不同的目標來處理，可以帶來三個好處：(1)規範多元化：維護一整套可能相互矛盾的指標，確保RAI中固有的多元道德立場和利害關係人的價值觀得到充分體現。(2)知識完整性：使用多個，有時相互衝突的指標，可以更全面地捕捉多方面的倫理概念，從而比任何單一、簡化的定義保留更多關於這些概念的資訊忠誠度。(3)隱性正則化：聯合優化理論上相互衝突的目標可以防止過度擬合到一個特定的指標，引導模型朝向在現實世界的複雜性下具有增強泛化和魯棒性的解決方案。因此，我們提倡RAI理論和實踐的轉變：從陷入不一致，轉向表徵可接受的不一致閾值，並闡明允許在實踐中實現穩健、近似一致性的機制。", "applications": ["**醫療診斷：** 在使用AI診斷疾病時，同時考慮準確性、公平性(避免對特定族群誤診)和隱私(保護患者數據)。即使這些目標之間存在衝突，例如為了更高的準確性可能需要更多個人數據，也要尋找平衡，確保AI系統在多個維度上都是負責任的。", "**信貸評估：** 在AI信貸評估系統中，同時考慮預測準確性、避免歧視(例如，基於種族或性別拒絕貸款)和解釋性(讓申請人了解被拒絕的原因)。即使這些目標之間存在衝突，例如為了更高的準確性模型可能變得更複雜難以解釋，也要找到一個可接受的不一致閾值，保障申請人的權益。", "**自動駕駛：** 自動駕駛汽車在行駛過程中，需要同時考慮安全性(避免事故)、效率(盡快到達目的地)和公平性(例如，在面對行人時，如何優先考慮不同人群的安全)。這些目標之間常常存在矛盾，例如，為了更高的安全性可能需要更慢的行駛速度，但擁抱這些矛盾並尋找平衡，才能開發出更可靠和負責任的自動駕駛系統。"], "pitch": "想像一個世界，AI不再是單一目標的優化機器，而是能夠同時考量多方利益、應對複雜倫理挑戰的智能體。我們正在開發的技術能夠幫助企業和研究人員擁抱RAI指標間的內在矛盾，將其轉化為提升模型魯棒性、價值觀多元化和知識完整性的動力。這不僅關乎合規，更關乎建立消費者信任、提升品牌聲譽和創造長期價值。試想，一個能夠公平、安全、可靠地提供醫療診斷、信貸評估和自動駕駛服務的AI系統，其市場潛力是巨大的。我們正在尋找的，不僅僅是資金，更是共同塑造負責任AI未來的合作夥伴。", "audio": "audios/2505.18139v1.mp3", "timestamp": "2025-05-26T21:12:49.180044"}
{"query": "Foundation Model", "id": "2505.18125v1", "url": "http://arxiv.org/abs/2505.18125v1", "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations", "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.", "authors": ["Alan Arazi", "Eilam Shapira", "Roi Reichart"], "published_date": "2025-05-23", "title_zh": "TabSTAR：具有語義目標感知表徵的基礎表格模型", "summary_zh": "過去深度學習在表格數據上的表現不如梯度提升決策樹(GBDT)。但現在，基礎表格模型有機會改變這個局面，它們能利用現實世界的知識，並在不同數據集上泛化，尤其是在包含自由文本的數據中。TabSTAR 是一種具有語義目標感知表徵的基礎表格模型，專為在具有文本特徵的表格數據上進行遷移學習而設計。它使用目標標記(target tokens)作為輸入，讓模型學習特定任務的嵌入，從而在包含文本特徵的分類任務中，於中型和大型數據集上取得最先進的性能。", "applications": ["**智慧客服：** 將客戶問題和相關產品資訊表格化，TabSTAR 可協助客服快速理解問題上下文，提供更精準的解答，提升客戶滿意度。", "**金融風險評估：** 結合用戶的財務數據（表格）與新聞評論、社群媒體發文（文本），TabSTAR 能更準確地評估用戶的信用風險或潛在的詐欺行為。", "**醫藥研發：** 將藥物特性（表格）與研究論文、臨床報告（文本）結合，TabSTAR 可加速藥物發現過程，更有效地預測藥物療效和副作用。"], "pitch": "TabSTAR 是一個突破性的基礎表格模型，它超越了傳統的梯度提升決策樹，尤其在處理包含文本特徵的表格數據時，性能顯著提升。它的核心優勢在於「語義目標感知」，能有效提取文本中與目標相關的訊息，提升預測準確性。考量到企業對精準決策、效率提升的需求日益增加，TabSTAR 具備極高的商業價值。我們可以將其應用於金融、醫療、零售等多個行業，解決複雜的數據分析問題，協助企業降低風險、提高收益。更重要的是，TabSTAR 的預訓練方法展現出良好的擴展性，意味著隨著數據量的增加，其性能還能進一步提升。這使 TabSTAR 具備了成為新一代表格數據分析平台的潛力，形成強大的競爭優勢。我們相信 TabSTAR 能成為數據分析領域的遊戲規則改變者，帶來巨大的投資回報。", "audio": "audios/2505.18125v1.mp3", "timestamp": "2025-05-26T21:13:10.776720"}
{"query": "Diffusion Model", "id": "2505.18145v1", "url": "http://arxiv.org/abs/2505.18145v1", "title": "Stochastic agent-based Monte Carlo simulations for reaction-diffusion models, population dynamics, and epidemic spreading", "summary": "We provide a succinct overview of the implementation of Monte Carlo\nalgorithms based on Markovian stochastic dynamics to study interacting and\nreacting many-particle systems away from thermal equilibrium. Such agent-based\ncomputer simulations constitute an effective tool to introduce undergraduate\nand beginning graduate students to current frontier research without requiring\nmuch prior knowledge or experience: Starting from direct visualization of\nsimulation data, students may gain immediate insight into emerging macroscopic\nfeatures of a complex model system and subsequently apply more sophisticated\ndata analysis to quantitatively characterize its often rich dynamical\nproperties, both in stationary and transient regimes. We utilize numerical\ninvestigations of paradigmatic reaction-diffusion systems, as well as\nstochastic models for population dynamics and epidemic spreading, to exemplify\nhow interdisciplinary computational research can be effectively utilized in\nbottom-up undergraduate and graduate education through learning by doing. In\naddition, we give helpful hints for the practical setup of Monte Carlo\nsimulation algorithms, provide sample codes, explain some typical data analysis\ntools, and describe various potential error sources and pitfalls, with tips for\navoiding them.", "authors": ["Mohamed Swailem", "Ulrich Dobramysl", "Ruslan Mukhamadiarov", "Uwe C. Täuber"], "published_date": "2025-05-23", "title_zh": "基於隨機代理人的蒙地卡羅模擬：用於反應擴散模型、族群動態和流行病傳播", "summary_zh": "本研究提供了一種基於馬可夫隨機動態的蒙地卡羅演算法的簡明概述，用於研究非熱平衡狀態下的相互作用和反應的多粒子系統。這種基於代理人的電腦模擬提供了一個有效的工具，讓大學部和剛開始的研究生能夠接觸到當前的尖端研究，而不需要太多的先備知識或經驗。透過直接視覺化模擬數據，學生可以立即了解複雜模型系統中湧現的宏觀特徵，並隨後應用更複雜的數據分析來定量描述其豐富的動態特性，無論是在穩態還是暫態。我們利用範例性的反應擴散系統，以及用於族群動態和流行病傳播的隨機模型進行數值研究，以說明跨學科的計算研究如何透過實作學習有效地應用於自下而上的大學部和研究所教育。此外，我們還提供了蒙地卡羅模擬演算法實際設置的實用技巧，提供範例程式碼，解釋了一些典型的數據分析工具，並描述了各種潛在的錯誤來源和陷阱，並提供了避免它們的技巧。", "applications": ["**精準行銷活動模擬：** 模擬消費者行為，預測不同行銷策略的影響，例如廣告投放、折扣活動等，以優化行銷預算和提高轉換率。", "**城市規劃與交通流量優化：** 模擬人群移動模式，預測交通瓶頸，優化交通信號燈設置和公共交通路線規劃，減少交通擁堵和通勤時間。", "**野生動物保護：** 模擬動物族群的遷移、繁殖和死亡率，預測氣候變化和棲息地喪失對動物族群的影響，制定更有效的保護策略。"], "pitch": "想像一下，您能提前預知一場流行病的傳播路徑，並精準地調整防疫策略，降低感染率，保護人民的健康。我們的技術正是基於此願景。我們開發了一種基於隨機代理人的蒙地卡羅模擬引擎，能夠精確模擬複雜系統，例如流行病傳播、市場動態和社會行為。這不僅能幫助政府機構制定更有效的公共政策，還能為企業提供決策支持，例如預測產品需求、優化供應鏈管理等。我們將以SaaS模式提供這項服務，目標客戶包括政府機構、醫療機構、金融機構和零售企業。我們相信，憑藉其高度可擴展性和客製化能力，我們的技術具有巨大的商業潛力，將在各個領域產生深遠的影響。我們的競爭優勢在於我們的演算法不僅高效，而且易於理解和使用，即使是沒有深厚程式背景的用戶也能快速上手。我們的商業模式將基於訂閱制，並根據不同的使用需求提供客製化服務。我們預計在未來三年內達到收支平衡，並在五年內實現高速增長。我們正在尋找具有遠見卓識的投資者，與我們一起將這項技術推向世界，共同創造一個更美好的未來。", "audio": "audios/2505.18145v1.mp3", "timestamp": "2025-05-26T21:13:41.085307"}
{"query": "AI", "id": "2505.18139v1", "url": "http://arxiv.org/abs/2505.18139v1", "title": "Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems", "summary": "This position paper argues that the theoretical inconsistency often observed\namong Responsible AI (RAI) metrics, such as differing fairness definitions or\ntradeoffs between accuracy and privacy, should be embraced as a valuable\nfeature rather than a flaw to be eliminated. We contend that navigating these\ninconsistencies, by treating metrics as divergent objectives, yields three key\nbenefits: (1) Normative Pluralism: Maintaining a full suite of potentially\ncontradictory metrics ensures that the diverse moral stances and stakeholder\nvalues inherent in RAI are adequately represented. (2) Epistemological\nCompleteness: The use of multiple, sometimes conflicting, metrics allows for a\nmore comprehensive capture of multifaceted ethical concepts, thereby preserving\ngreater informational fidelity about these concepts than any single, simplified\ndefinition. (3) Implicit Regularization: Jointly optimizing for theoretically\nconflicting objectives discourages overfitting to one specific metric, steering\nmodels towards solutions with enhanced generalization and robustness under\nreal-world complexities. In contrast, efforts to enforce theoretical\nconsistency by simplifying or pruning metrics risk narrowing this value\ndiversity, losing conceptual depth, and degrading model performance. We\ntherefore advocate for a shift in RAI theory and practice: from getting trapped\nin inconsistency to characterizing acceptable inconsistency thresholds and\nelucidating the mechanisms that permit robust, approximated consistency in\npractice.", "authors": ["Gordon Dai", "Yunze Xiao"], "published_date": "2025-05-23", "title_zh": "擁抱矛盾：理論不一致不會阻礙構建負責任AI系統的道路", "summary_zh": "這篇論文認為，負責任AI(RAI)指標之間常見的理論不一致，例如不同的公平性定義或準確性與隱私之間的權衡，應被視為一種寶貴的特性，而非需要消除的缺陷。透過將這些指標視為不同的目標，我們能獲得三個主要優勢：(1) 規範多元主義：維持一套完整且可能相互矛盾的指標，確保RAI中固有的多樣道德立場和利害關係人的價值觀得到充分體現。(2) 知識完整性：使用多個、有時相互衝突的指標，能夠更全面地捕捉多面向的倫理概念，從而比任何單一、簡化的定義保留更多關於這些概念的資訊忠誠度。(3) 隱式正則化：聯合優化理論上相互衝突的目標可以避免模型過度擬合特定指標，引導模型朝向在真實世界複雜情況下具有更強泛化性和魯棒性的解決方案。因此，我們提倡RAI理論和實踐的轉變：從陷入不一致，轉向描述可接受的不一致性閾值，並闡明在實踐中允許穩健、近似一致性的機制。", "applications": ["醫療診斷：AI系統在診斷疾病時，可能需要在準確性、避免誤診和保護患者隱私之間取得平衡。擁抱這些矛盾，可以設計出更安全、更符合倫理考量的診斷工具。", "貸款批准：AI在審核貸款時，會考慮公平性（避免歧視特定群體）和準確性（評估還款能力）。了解並管理這些目標之間的權衡，可以構建更公正的金融系統。", "自動駕駛汽車：自動駕駛汽車需要在安全、效率和乘客舒適度之間做出決策。接受這些目標的固有衝突，可以設計出更穩健、更能應對各種現實情況的自動駕駛系統。"], "pitch": "我們正在重新定義負責任AI。傳統方法試圖消除指標之間的矛盾，但我們認為這限制了AI的潛力。我們的研究表明，擁抱矛盾反而能帶來更強的規範多元主義、知識完整性和模型魯棒性。想像一下，一個醫療診斷AI系統，它不僅準確診斷疾病，還能同時保護患者隱私並避免潛在的歧視。或者，一個自動駕駛汽車，它在保證安全的前提下，也能提供舒適的駕駛體驗，同時考慮到道路交通效率。我們的技術能夠讓AI系統在這些看似矛盾的目標之間取得平衡，創造出更公平、更可信賴、更高效的解決方案。這將會是下一代負責任AI的基礎，我們將成為這領域的領導者。我們正在尋找投資者，共同開創負責任AI的新時代，並在醫療、金融、交通運輸等產業帶來顛覆性的改變。", "audio": "audios/2505.18139v1.mp3", "timestamp": "2025-05-26T22:12:41.315789"}
{"query": "Foundation Model", "id": "2505.18125v1", "url": "http://arxiv.org/abs/2505.18125v1", "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations", "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.", "authors": ["Alan Arazi", "Eilam Shapira", "Roi Reichart"], "published_date": "2025-05-23", "title_zh": "TabSTAR：具語義目標感知表示的基礎表格模型", "summary_zh": "過去深度學習在表格資料的學習表現不如梯度提升決策樹(GBDTs)，但近年出現了表格基礎模型的新趨勢，有望利用真實世界知識並泛化到多樣的資料集。TabSTAR 是一個具有語義目標感知表示的基礎表格模型，特別針對包含自由文本的表格資料。它能讓表格資料透過文本特徵進行遷移學習，且架構不含特定資料集的參數。TabSTAR 利用預訓練的文本編碼器，並接收目標詞符(target tokens)作為輸入，讓模型學習到特定任務的嵌入表示。實驗證明，TabSTAR 在包含文本特徵的分類任務中，於中型和大型資料集上均達到最先進的性能，且其預訓練階段展現出隨著資料集數量增加的擴展規律，代表未來有進一步提升性能的潛力。", "applications": ["**金融風險評估：** 利用客戶的交易紀錄、個人資料（包含文本描述）來預測貸款違約風險，不再只是依賴傳統的數值資料，更能精準掌握風險因子。", "**醫療診斷輔助：** 結合病人的病歷、檢查數據（包含醫生筆記等文本資料）來輔助醫生診斷疾病，提高診斷效率和準確性。", "**客戶服務聊天機器人：** 分析客戶的歷史對話紀錄、個人資料，更精準地理解客戶的需求，提供更個人化且有效的解決方案，提升客戶滿意度。"], "pitch": "TabSTAR 是一個突破性的表格基礎模型，它超越了傳統梯度提升方法，專注於利用表格中蘊含的文本資訊，為企業解鎖全新的數據價值。想像一下，過去只能被當作雜訊的文本欄位，現在成為預測未來的強大工具。TabSTAR 能顯著提升金融風險評估、醫療診斷、客戶服務等領域的效率和準確性，降低成本並創造更多收益。其可擴展的架構和遷移學習能力，也意味著更快的部署速度和更低的客製化成本。我們相信 TabSTAR 將成為各行各業數據驅動決策的關鍵引擎，提供巨大的商業潛力和投資回報。", "audio": "audios/2505.18125v1.mp3", "timestamp": "2025-05-26T22:13:00.308463"}
{"query": "Diffusion Model", "id": "2505.18145v1", "url": "http://arxiv.org/abs/2505.18145v1", "title": "Stochastic agent-based Monte Carlo simulations for reaction-diffusion models, population dynamics, and epidemic spreading", "summary": "We provide a succinct overview of the implementation of Monte Carlo\nalgorithms based on Markovian stochastic dynamics to study interacting and\nreacting many-particle systems away from thermal equilibrium. Such agent-based\ncomputer simulations constitute an effective tool to introduce undergraduate\nand beginning graduate students to current frontier research without requiring\nmuch prior knowledge or experience: Starting from direct visualization of\nsimulation data, students may gain immediate insight into emerging macroscopic\nfeatures of a complex model system and subsequently apply more sophisticated\ndata analysis to quantitatively characterize its often rich dynamical\nproperties, both in stationary and transient regimes. We utilize numerical\ninvestigations of paradigmatic reaction-diffusion systems, as well as\nstochastic models for population dynamics and epidemic spreading, to exemplify\nhow interdisciplinary computational research can be effectively utilized in\nbottom-up undergraduate and graduate education through learning by doing. In\naddition, we give helpful hints for the practical setup of Monte Carlo\nsimulation algorithms, provide sample codes, explain some typical data analysis\ntools, and describe various potential error sources and pitfalls, with tips for\navoiding them.", "authors": ["Mohamed Swailem", "Ulrich Dobramysl", "Ruslan Mukhamadiarov", "Uwe C. Täuber"], "published_date": "2025-05-23", "title_zh": "反應-擴散模型、族群動力學和流行病傳播的基於隨機代理人的蒙地卡羅模擬", "summary_zh": "本論文簡要概述如何利用基於馬可夫隨機動力學的蒙地卡羅算法，研究遠離熱平衡的交互和反應多粒子系統。這種基於代理人的電腦模擬，是引導大學生和研究生入門前沿研究的有效工具，且不需要太多先備知識或經驗。透過直接視覺化模擬數據，學生可以立即了解複雜模型系統中湧現的宏觀特徵，並進一步應用更複雜的數據分析，來定量描述其豐富的動態特性，無論是在穩態還是瞬態模式下。我們使用反應-擴散系統、族群動力學和流行病傳播的數值研究為例，說明如何在自下而上的本科和研究生教育中，有效地利用跨學科的計算研究，透過實作學習。此外，我們還提供蒙地卡羅模擬算法的實用設置提示、範例程式碼、解釋一些典型的數據分析工具，並描述各種潛在的錯誤來源和陷阱，以及避免它們的技巧。", "applications": ["**精準農業：** 模擬農作物病蟲害的傳播和擴散，幫助農民提前預防和控制，減少損失。例如，模擬特定氣候條件下，某種真菌在農田中傳播的速度和影響範圍，讓農民可以更精準地噴灑農藥。", "**城市規劃：** 模擬交通流量和人群移動，優化城市道路設計和公共資源配置，減少擁堵和提高效率。例如，模擬在高峰時段，特定路段的車流量變化，以及新建地鐵站對周邊人流的影響，從而調整紅綠燈設置或公交線路。", "**疫情管控：** 在疫情爆發初期，模擬病毒傳播路徑和感染人數，協助政府制定更有效的防疫策略，例如封鎖區域的範圍和隔離政策。例如，根據病毒傳播速度、潛伏期、接觸頻率等參數，預測不同封鎖策略對疫情控制的影響。", "**金融市場風險評估：** 模擬投資組合在不同市場條件下的表現，評估潛在風險和收益。 例如，使用歷史數據和隨機模擬來預測股價波動，幫助投資者制定更明智的投資策略。", "**社群媒體傳播分析：** 模擬資訊在社群媒體上的傳播速度和影響範圍，幫助企業了解品牌聲譽和制定更有效的行銷策略。例如，模擬特定產品的廣告在不同社交平台上的擴散程度和用户反饋，從而調整廣告投放策略。"], "pitch": "想像一下，我們能預測疫情的爆發，優化交通流量，甚至精準預防農作物病蟲害，這一切都源於對複雜系統的深刻理解。我們基於代理人的蒙地卡羅模擬技術，正是開啟這扇門的鑰匙。這項技術不僅適用於學術研究，更具備巨大的商業潛力。我們可以將其應用於精準農業，幫助農民減少損失；應用於城市規劃，提高城市效率；應用於疫情管控，挽救生命；應用於金融市場風險評估，提供更精準的投資決策。我們的商業模式將包括：提供定制化的模擬分析服務、授權模擬軟體平台，以及與相關產業合作開發解決方案。這是一個能夠創造巨大價值、影響深遠的投資機會，讓我們一起打造更智慧、更安全、更高效的未來！", "audio": "audios/2505.18145v1.mp3", "timestamp": "2025-05-26T22:13:24.374770"}
{"query": "AI", "id": "2505.18139v1", "url": "http://arxiv.org/abs/2505.18139v1", "title": "Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems", "summary": "This position paper argues that the theoretical inconsistency often observed\namong Responsible AI (RAI) metrics, such as differing fairness definitions or\ntradeoffs between accuracy and privacy, should be embraced as a valuable\nfeature rather than a flaw to be eliminated. We contend that navigating these\ninconsistencies, by treating metrics as divergent objectives, yields three key\nbenefits: (1) Normative Pluralism: Maintaining a full suite of potentially\ncontradictory metrics ensures that the diverse moral stances and stakeholder\nvalues inherent in RAI are adequately represented. (2) Epistemological\nCompleteness: The use of multiple, sometimes conflicting, metrics allows for a\nmore comprehensive capture of multifaceted ethical concepts, thereby preserving\ngreater informational fidelity about these concepts than any single, simplified\ndefinition. (3) Implicit Regularization: Jointly optimizing for theoretically\nconflicting objectives discourages overfitting to one specific metric, steering\nmodels towards solutions with enhanced generalization and robustness under\nreal-world complexities. In contrast, efforts to enforce theoretical\nconsistency by simplifying or pruning metrics risk narrowing this value\ndiversity, losing conceptual depth, and degrading model performance. We\ntherefore advocate for a shift in RAI theory and practice: from getting trapped\nin inconsistency to characterizing acceptable inconsistency thresholds and\nelucidating the mechanisms that permit robust, approximated consistency in\npractice.", "authors": ["Gordon Dai", "Yunze Xiao"], "published_date": "2025-05-23", "title_zh": "擁抱矛盾：理論上的不一致性不會阻礙構建負責任AI系統的道路", "summary_zh": "這篇論文指出，負責任AI(RAI)的各種指標之間常常存在理論上的不一致，例如不同的公平性定義或準確性與隱私之間的權衡。我們認為，這種不一致性應該被視為一種寶貴的特徵，而非需要消除的缺陷。透過將這些指標視為不同的目標，我們可以獲得三個好處：(1)規範多元主義：確保充分代表RAI中固有的各種道德立場和利益相關者價值。(2)知識完整性：更全面地捕捉多方面的道德概念，保留更多關於這些概念的信息。(3)隱式正則化：共同優化理論上衝突的目標，可以防止過度擬合到某個特定指標，並引導模型找到在真實世界複雜性下具有更強泛化能力和魯棒性的解決方案。簡而言之，擁抱指標間的矛盾，反而能讓AI更負責任、更穩健。", "applications": ["**貸款審核：** 使用多個公平性指標（例如，考慮不同種族、性別的通過率）來確保貸款決策既準確又公平，避免過度依賴單一指標造成的歧視。", "**醫療診斷：** 考慮準確性、假陽性率和假陰性率等多個指標來評估診斷模型的性能，以確保患者得到準確的診斷和適當的治療，避免單純追求高準確率而忽略了漏診的風險。", "**內容推薦：** 在推薦算法中結合用戶點擊率、互動時間和多樣性指標，以提供更平衡、更個性化的內容推薦，避免算法只關注用戶已感興趣的內容，而忽略了其他潛在的興趣。"], "pitch": "我們正在開發一種框架，能夠幫助企業在設計和部署AI系統時，有效地管理和利用負責任AI指標之間潛在的矛盾。目前的RAI解決方案往往過於簡化，試圖消除這些矛盾，導致模型表現不佳或出現倫理問題。我們的框架允許企業擁抱指標間的衝突，從而構建更公平、更穩健、更值得信賴的AI系統。這不僅能提升企業的社會形象，降低潛在的法律風險，還能顯著改善AI模型的整體性能。我們的目標客戶包括金融機構、醫療保健公司以及任何需要使用AI進行決策的企業。我們的競爭優勢在於我們能夠提供一個全面且靈活的解決方案，該方案不僅能滿足監管要求，還能幫助企業實現業務目標。我們相信，隨著RAI的重要性日益提升，我們的框架將成為企業構建負責任AI系統的必備工具。因此，我們尋求種子輪投資，用於擴大團隊、完善產品，並加速市場推廣，力爭成為RAI領域的領導者。", "audio": "audios/2505.18139v1.mp3", "timestamp": "2025-05-27T01:04:27.994172"}
{"query": "Foundation Model", "id": "2505.18125v1", "url": "http://arxiv.org/abs/2505.18125v1", "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations", "summary": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.", "authors": ["Alan Arazi", "Eilam Shapira", "Roi Reichart"], "published_date": "2025-05-23", "title_zh": "TabSTAR：具有語義目標感知表示的基礎表格模型", "summary_zh": "儘管深度學習在許多領域取得了顯著的成功，但它在表格學習任務上的表現一直不如梯度提升決策樹（GBDT）。然而，最近的進展正為表格基礎模型鋪平道路，這些模型可以利用真實世界的知識並在不同的數據集中進行泛化，尤其是在數據包含自由文本時。雖然將語言模型能力整合到表格任務中已經進行了探索，但大多數現有方法都利用靜態的、目標無關的文本表示，限制了它們的有效性。我們介紹了 TabSTAR：具有語義目標感知表示的基礎表格模型。TabSTAR 旨在實現具有文本特徵的表格數據上的遷移學習，其架構不包含特定於數據集的參數。它解凍了預訓練的文本編碼器，並將目標標記作為輸入，為模型提供學習特定任務嵌入所需的上下文。TabSTAR 在具有文本特徵的分類任務的已知基準測試中，在中型和大型數據集上都實現了最先進的性能，並且其預訓練階段在數據集數量上表現出縮放律，為進一步提高性能提供了途徑。", "applications": ["**金融風險評估：** 利用客戶填寫的申請表（包含文本信息，如職業、信用描述等）和歷史交易數據，預測貸款違約風險，優化貸款決策。", "**客戶服務自動化：** 整合客戶的產品使用記錄、線上聊天記錄（文本信息）以及客戶屬性數據，更準確地理解客戶需求，並提供更有效的自動回覆或問題路由。", "**醫學診斷輔助：** 結合患者的病歷記錄（包含文本描述，如主訴、體檢描述等）和實驗室數據，協助醫生進行疾病診斷，提高診斷準確性和效率。"], "pitch": "TabSTAR 是一個突破性的表格數據處理模型，它能理解表格數據中的文字信息，大幅提升預測準確性。想像一下，你能更精準地預測客戶流失、疾病風險，或優化供應鏈效率，這將為企業帶來巨大的成本節約和營收增長。TabSTAR 的潛在商業價值巨大，可以應用於金融、醫療、電商等各個領域。我們正在打造一個可規模化的解決方案，透過 API 服務或客製化模型，讓各行各業都能輕鬆導入 TabSTAR，挖掘數據的真正價值，並在競爭激烈的市場中取得領先地位。此外，其預訓練模型具有規模化的潜力，未來在更多數據集上訓練後，性能將會进一步提升，形成更强大的竞争力。", "audio": "audios/2505.18125v1.mp3", "timestamp": "2025-05-27T01:04:48.640442"}
{"query": "Diffusion Model", "id": "2505.18145v1", "url": "http://arxiv.org/abs/2505.18145v1", "title": "Stochastic agent-based Monte Carlo simulations for reaction-diffusion models, population dynamics, and epidemic spreading", "summary": "We provide a succinct overview of the implementation of Monte Carlo\nalgorithms based on Markovian stochastic dynamics to study interacting and\nreacting many-particle systems away from thermal equilibrium. Such agent-based\ncomputer simulations constitute an effective tool to introduce undergraduate\nand beginning graduate students to current frontier research without requiring\nmuch prior knowledge or experience: Starting from direct visualization of\nsimulation data, students may gain immediate insight into emerging macroscopic\nfeatures of a complex model system and subsequently apply more sophisticated\ndata analysis to quantitatively characterize its often rich dynamical\nproperties, both in stationary and transient regimes. We utilize numerical\ninvestigations of paradigmatic reaction-diffusion systems, as well as\nstochastic models for population dynamics and epidemic spreading, to exemplify\nhow interdisciplinary computational research can be effectively utilized in\nbottom-up undergraduate and graduate education through learning by doing. In\naddition, we give helpful hints for the practical setup of Monte Carlo\nsimulation algorithms, provide sample codes, explain some typical data analysis\ntools, and describe various potential error sources and pitfalls, with tips for\navoiding them.", "authors": ["Mohamed Swailem", "Ulrich Dobramysl", "Ruslan Mukhamadiarov", "Uwe C. Täuber"], "published_date": "2025-05-23", "title_zh": "用於反應擴散模型、族群動力學和流行病傳播的基於隨機代理的蒙地卡羅模擬", "summary_zh": "本論文介紹了一種基於馬可夫鏈隨機動力的蒙地卡羅演算法，用於研究遠離熱平衡的多粒子系統。這種基於代理的電腦模擬，對於讓大學生和研究生入門當前的前沿研究，是一個有效的工具，且不需要過多的先備知識。學生可以從直接可視化的模擬數據中，立即了解複雜模型系統中湧現的巨觀特性，並進一步應用更複雜的數據分析方法，來量化其豐富的動態特性，無論是穩態還是瞬態。我們利用反應擴散系統、族群動力學和流行病傳播的數值研究，來示範如何透過做中學，在大學和研究所教育中有效地利用跨學科的計算研究。此外，我們提供蒙地卡羅模擬演算法的實用設置提示、示例程式碼、解釋一些典型的數據分析工具，並描述各種潛在的錯誤來源和陷阱，以及避免它們的技巧。", "applications": ["**城市交通規劃：** 利用基於代理的蒙地卡羅模擬，預測不同交通流量模式下，道路壅塞發生的概率和範圍，從而優化紅綠燈時序或設計更有效的公共交通路線。", "**精準農業：** 模擬作物生長、病蟲害傳播以及肥料利用效率，協助農民針對特定環境和作物類型，制定最佳化的灌溉、施肥和防治策略，提高產量並降低成本。", "**市場行銷策略：** 模擬消費者行為，預測不同行銷活動（例如促銷、廣告）對產品銷售的影響，從而優化行銷預算分配和訊息傳遞方式。"], "pitch": "我們開發了一套基於隨機代理的蒙地卡羅模擬平台，能精準預測複雜系統的演化趨勢，例如流行病傳播、族群變化、市場行為等。與傳統的建模方法相比，我們的平台更具彈性、更能捕捉個體差異對整體結果的影響。初期，我們可以將平台應用於醫療保健領域，幫助政府和醫療機構更有效地應對疫情，預測醫療資源需求，並制定更精準的防疫策略。未來，我們將擴展應用範圍至金融、零售、交通等領域，為企業提供更精準的決策支持，協助他們在複雜的市場環境中取得競爭優勢。我們需要的資金將用於完善平台功能、擴大團隊規模、以及拓展市場。我們的商業模式包括軟體授權、定制化諮詢服務以及數據分析報告。相信憑藉我們的技術優勢和市場潛力，我們將能獲得可觀的回報。", "audio": "audios/2505.18145v1.mp3", "timestamp": "2025-05-27T01:05:08.955776"}
{"query": "AI", "id": "2505.20246v1", "url": "http://arxiv.org/abs/2505.20246v1", "title": "On Path to Multimodal Historical Reasoning: HistBench and HistAgent", "summary": "Recent advances in large language models (LLMs) have led to remarkable\nprogress across domains, yet their capabilities in the humanities, particularly\nhistory, remain underexplored. Historical reasoning poses unique challenges for\nAI, involving multimodal source interpretation, temporal inference, and\ncross-linguistic analysis. While general-purpose agents perform well on many\nexisting benchmarks, they lack the domain-specific expertise required to engage\nwith historical materials and questions. To address this gap, we introduce\nHistBench, a new benchmark of 414 high-quality questions designed to evaluate\nAI's capacity for historical reasoning and authored by more than 40 expert\ncontributors. The tasks span a wide range of historical problems-from factual\nretrieval based on primary sources to interpretive analysis of manuscripts and\nimages, to interdisciplinary challenges involving archaeology, linguistics, or\ncultural history. Furthermore, the benchmark dataset spans 29 ancient and\nmodern languages and covers a wide range of historical periods and world\nregions. Finding the poor performance of LLMs and other agents on HistBench, we\nfurther present HistAgent, a history-specific agent equipped with carefully\ndesigned tools for OCR, translation, archival search, and image understanding\nin History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of\n27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online\nsearch and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%)\nand Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These\nresults highlight the limitations of existing LLMs and generalist agents and\ndemonstrate the advantages of HistAgent for historical reasoning.", "authors": ["Jiahao Qiu", "Fulian Xiao", "Yimin Wang", "Yuchen Mao", "Yijia Chen", "Xinzhe Juan", "Siran Wang", "Xuan Qi", "Tongcheng Zhang", "Zixin Yao", "Jiacheng Guo", "Yifu Lu", "Charles Argon", "Jundi Cui", "Daixin Chen", "Junran Zhou", "Shuyao Zhou", "Zhanpeng Zhou", "Ling Yang", "Shilong Liu", "Hongru Wang", "Kaixuan Huang", "Xun Jiang", "Yuming Cao", "Yue Chen", "Yunfei Chen", "Zhengyi Chen", "Ruowei Dai", "Mengqiu Deng", "Jiye Fu", "Yunting Gu", "Zijie Guan", "Zirui Huang", "Xiaoyan Ji", "Yumeng Jiang", "Delong Kong", "Haolong Li", "Jiaqi Li", "Ruipeng Li", "Tianze Li", "Zhuoran Li", "Haixia Lian", "Mengyue Lin", "Xudong Liu", "Jiayi Lu", "Jinghan Lu", "Wanyu Luo", "Ziyue Luo", "Zihao Pu", "Zhi Qiao", "Ruihuan Ren", "Liang Wan", "Ruixiang Wang", "Tianhui Wang", "Yang Wang", "Zeyu Wang", "Zihua Wang", "Yujia Wu", "Zhaoyi Wu", "Hao Xin", "Weiao Xing", "Ruojun Xiong", "Weijie Xu", "Yao Shu", "Xiao Yao", "Xiaorui Yang", "Yuchen Yang", "Nan Yi", "Jiadong Yu", "Yangyuxuan Yu", "Huiting Zeng", "Danni Zhang", "Yunjie Zhang", "Zhaoyu Zhang", "Zhiheng Zhang", "Xiaofeng Zheng", "Peirong Zhou", "Linyan Zhong", "Xiaoyin Zong", "Ying Zhao", "Zhenxin Chen", "Lin Ding", "Xiaoyu Gao", "Bingbing Gong", "Yichao Li", "Yang Liao", "Guang Ma", "Tianyuan Ma", "Xinrui Sun", "Tianyi Wang", "Han Xia", "Ruobing Xian", "Gen Ye", "Tengfei Yu", "Wentao Zhang", "Yuxi Wang", "Xi Gao", "Mengdi Wang"], "published_date": "2025-05-26", "title_zh": "通往多模態歷史推理之路：HistBench與HistAgent", "summary_zh": "本研究探討大型語言模型(LLMs)在歷史領域的應用。歷史推理涉及多模態資料解讀、時間推理以及跨語言分析，對AI構成獨特挑戰。為此，研究團隊創建了HistBench，包含414道高品質問題的基準測試，評估AI的歷史推理能力，題目涵蓋史料檢索、手稿圖像分析、考古學、語言學、文化史等跨領域問題，且橫跨多種語言和歷史時期。研究發現LLMs在HistBench上表現不佳，因此開發了HistAgent，一個專為歷史設計的AI代理，具備OCR、翻譯、檔案搜尋和圖像理解等工具。HistAgent在HistBench上顯著優於其他LLMs和通用型AI代理，證明了其在歷史推理方面的優勢。", "applications": ["**歷史研究助手:** 協助學者快速搜尋、翻譯和分析歷史文獻、圖像等資料，加速研究進度，並提供新的研究角度。", "**互動式歷史學習平台:** 讓學生透過提問與AI互動，深入了解歷史事件，並分析原始資料，提升學習體驗與效果。", "**歷史文物數位典藏與展示:** 協助博物館和檔案館將歷史文物數位化，並透過AI提供多語言解說和導覽，增加文物價值和吸引力。"], "pitch": "HistAgent瞄準歷史領域AI應用的巨大市場，解決了通用型AI在處理複雜歷史資料時的瓶頸。HistBench的創建確立了歷史推理AI的評估標準，而HistAgent的出色表現證明了專用型AI的商業潛力。我們將以此為基礎，打造更強大的歷史研究助手和互動式學習平台，提供個性化的歷史知識服務，目標成為歷史領域AI應用的領導者。HistBench的數據集本身也具有商業價值，可以授權給其他研究機構和公司用於AI模型的訓練和評估。該團隊已經證明了在歷史領域創建專用AI的價值，這使其在競爭激烈的市場中具有顯著的先發優勢。我們尋求投資以擴大HistAgent的功能、豐富HistBench的數據，並開拓更多應用場景，例如在歷史文物鑑定、族譜研究和文化遺產保護等領域。", "audio": "audios/2505.20246v1.mp3", "timestamp": "2025-05-27T03:07:12.539782"}
{"query": "Foundation Model", "id": "2505.20202v1", "url": "http://arxiv.org/abs/2505.20202v1", "title": "PathBench: A comprehensive comparison benchmark for pathology foundation models towards precision oncology", "summary": "The emergence of pathology foundation models has revolutionized computational\nhistopathology, enabling highly accurate, generalized whole-slide image\nanalysis for improved cancer diagnosis, and prognosis assessment. While these\nmodels show remarkable potential across cancer diagnostics and prognostics,\ntheir clinical translation faces critical challenges including variability in\noptimal model across cancer types, potential data leakage in evaluation, and\nlack of standardized benchmarks. Without rigorous, unbiased evaluation, even\nthe most advanced PFMs risk remaining confined to research settings, delaying\ntheir life-saving applications. Existing benchmarking efforts remain limited by\nnarrow cancer-type focus, potential pretraining data overlaps, or incomplete\ntask coverage. We present PathBench, the first comprehensive benchmark\naddressing these gaps through: multi-center in-hourse datasets spanning common\ncancers with rigorous leakage prevention, evaluation across the full clinical\nspectrum from diagnosis to prognosis, and an automated leaderboard system for\ncontinuous model assessment. Our framework incorporates large-scale data,\nenabling objective comparison of PFMs while reflecting real-world clinical\ncomplexity. All evaluation data comes from private medical providers, with\nstrict exclusion of any pretraining usage to avoid data leakage risks. We have\ncollected 15,888 WSIs from 8,549 patients across 10 hospitals, encompassing\nover 64 diagnosis and prognosis tasks. Currently, our evaluation of 19 PFMs\nshows that Virchow2 and H-Optimus-1 are the most effective models overall. This\nwork provides researchers with a robust platform for model development and\noffers clinicians actionable insights into PFM performance across diverse\nclinical scenarios, ultimately accelerating the translation of these\ntransformative technologies into routine pathology practice.", "authors": ["Jiabo Ma", "Yingxue Xu", "Fengtao Zhou", "Yihui Wang", "Cheng Jin", "Zhengrui Guo", "Jianfeng Wu", "On Ki Tang", "Huajun Zhou", "Xi Wang", "Luyang Luo", "Zhengyu Zhang", "Du Cai", "Zizhao Gao", "Wei Wang", "Yueping Liu", "Jiankun He", "Jing Cui", "Zhenhui Li", "Jing Zhang", "Feng Gao", "Xiuming Zhang", "Li Liang", "Ronald Cheong Kin Chan", "Zhe Wang", "Hao Chen"], "published_date": "2025-05-26", "title_zh": "PathBench：針對精準腫瘤學的病理學基礎模型綜合比較基準", "summary_zh": "病理學基礎模型正在改變數位病理學領域，提升癌症診斷和預後評估的準確性和通用性。然而，這些模型在臨床應用方面面臨挑戰，例如針對不同癌症類型的最佳模型差異、評估中潛在的資料洩漏以及缺乏標準化基準。PathBench 透過多中心內部資料集、涵蓋診斷到預後的完整臨床評估，以及自動排行榜系統，解決了這些問題。它提供大規模資料，客觀比較模型，並反映真實世界的臨床複雜性。已收集來自10家醫院的超過15,888張全玻片掃描圖像，涵蓋64項診斷和預後任務。目前對19個PFM的評估表明Virchow2和H-Optimus-1是整體上最有效的模型。PathBench為研究人員提供了一個強大的模型開發平台，並為臨床醫生提供了有關PFM在各種臨床場景中表現的可行見解，最終加速了這些變革性技術向常規病理學實踐的轉化。", "applications": ["**遠程病理學診斷諮詢：** 醫生可以將病理切片上傳到PathBench平台，比較不同病理學基礎模型的診斷結果，獲得更客觀的第二意見，特別是在資源匱乏的地區。", "**個性化癌症治療方案設計：** 根據PathBench提供的模型評估，針對特定癌症類型和患者的病理特徵，選擇最準確的基礎模型進行分析，預測治療效果，制定更精準的治療方案。", "**病理學醫師培訓與學習：** 初級病理學醫師可以利用PathBench平台，比較自己的診斷結果與不同模型提供的結果，學習和提升診斷能力，加速專業成長。"], "pitch": "PathBench是加速病理學基礎模型商業化的關鍵。我們提供一個獨立、公正、全面的基準平台，解決了目前模型評估中的數據洩露、癌症類型局限等問題。透過PathBench，醫院和藥廠可以更有效地選擇最適合自身需求的模型，開發更精準的癌症診斷和治療產品。我們的商業模式包括提供付費的benchmark訪問權限、定制化的模型評估服務，以及將PathBench平台整合到現有的病理學信息系統中。這將為我們帶來可觀的營收，並在精準腫瘤學領域建立領導地位，最終改善癌症患者的生存率和生活品質。", "audio": "audios/2505.20202v1.mp3", "timestamp": "2025-05-27T03:07:29.687868"}
{"query": "Diffusion Model", "id": "2505.20171v1", "url": "http://arxiv.org/abs/2505.20171v1", "title": "Long-Context State-Space Video World Models", "summary": "Video diffusion models have recently shown promise for world modeling through\nautoregressive frame prediction conditioned on actions. However, they struggle\nto maintain long-term memory due to the high computational cost associated with\nprocessing extended sequences in attention layers. To overcome this limitation,\nwe propose a novel architecture leveraging state-space models (SSMs) to extend\ntemporal memory without compromising computational efficiency. Unlike previous\napproaches that retrofit SSMs for non-causal vision tasks, our method fully\nexploits the inherent advantages of SSMs in causal sequence modeling. Central\nto our design is a block-wise SSM scanning scheme, which strategically trades\noff spatial consistency for extended temporal memory, combined with dense local\nattention to ensure coherence between consecutive frames. We evaluate the\nlong-term memory capabilities of our model through spatial retrieval and\nreasoning tasks over extended horizons. Experiments on Memory Maze and\nMinecraft datasets demonstrate that our approach surpasses baselines in\npreserving long-range memory, while maintaining practical inference speeds\nsuitable for interactive applications.", "authors": ["Ryan Po", "Yotam Nitzan", "Richard Zhang", "Berlin Chen", "Tri Dao", "Eli Shechtman", "Gordon Wetzstein", "Xun Huang"], "published_date": "2025-05-26", "title_zh": "長上下文狀態空間影片世界模型", "summary_zh": "影片擴散模型在透過自迴歸框架預測並以動作為條件的世界建模中展現了潛力。然而，由於注意力層處理長序列的計算成本高昂，它們難以維持長期記憶。為了克服這個限制，我們提出了一種利用狀態空間模型（SSM）的新穎架構，以在不犧牲計算效率的情況下擴展時間記憶。與先前為非因果視覺任務改造SSM的方法不同，我們的方法充分利用了SSM在因果序列建模中的固有優勢。我們設計的核心是一個分塊式SSM掃描方案，該方案策略性地將空間一致性換成擴展的時間記憶，並結合密集的局部注意力，以確保連續幀之間的一致性。我們通過在擴展範圍內的空間檢索和推理任務來評估我們模型的長期記憶能力。在記憶迷宮和Minecraft數據集上的實驗表明，我們的方法在保持遠程記憶方面超越了基線，同時保持了適合交互式應用程序的實際推理速度。", "applications": ["**遊戲AI：** 讓遊戲中的NPC能夠記住玩家過去的動作，並根據這些記憶做出更真實和複雜的反應，提升遊戲體驗。", "**自動駕駛：** 預測長時間序列的交通狀況和行人行為，幫助自動駕駛系統做出更安全和更高效的決策。", "**機器人導航：** 讓機器人在複雜環境中導航時，能夠記住過去的路線和地標，避免重複犯錯，更快找到目標。"], "pitch": "我們開發的「長上下文狀態空間影片世界模型」解決了現有影片模型在處理長時間序列時的記憶瓶頸。透過創新的狀態空間模型架構，我們能夠以更低的計算成本，賦予AI更強大的長期記憶能力。這項技術在遊戲、自動駕駛和機器人等領域具有廣闊的應用前景。想像一下，一個能夠真正理解玩家意圖的遊戲AI，或者一個能夠預見未來交通狀況的自動駕駛系統。我們的技術將助力這些願景的實現。初期可以聚焦遊戲AI市場，透過授權或SDK方式獲利。未來，隨著自動駕駛和機器人技術的發展，市場潛力將更加巨大。投資我們的技術，就是投資AI的未來。", "audio": "audios/2505.20171v1.mp3", "timestamp": "2025-05-27T03:07:50.898113"}
{"query": "AI", "id": "2505.20148v1", "url": "http://arxiv.org/abs/2505.20148v1", "title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents", "summary": "Spatial Planning is a crucial part in the field of spatial intelligence,\nwhich requires the understanding and planning about object arrangements in\nspace perspective. AI agents with the spatial planning ability can better adapt\nto various real-world applications, including robotic manipulation, automatic\nassembly, urban planning etc. Recent works have attempted to construct\nbenchmarks for evaluating the spatial intelligence of Multimodal Large Language\nModels (MLLMs). Nevertheless, these benchmarks primarily focus on spatial\nreasoning based on typical Visual Question-Answering (VQA) forms, which suffers\nfrom the gap between abstract spatial understanding and concrete task\nexecution. In this work, we take a step further to build a comprehensive\nbenchmark called MineAnyBuild, aiming to evaluate the spatial planning ability\nof open-world AI agents in the Minecraft game. Specifically, MineAnyBuild\nrequires an agent to generate executable architecture building plans based on\nthe given multi-modal human instructions. It involves 4,000 curated spatial\nplanning tasks and also provides a paradigm for infinitely expandable data\ncollection by utilizing rich player-generated content. MineAnyBuild evaluates\nspatial planning through four core supporting dimensions: spatial\nunderstanding, spatial reasoning, creativity, and spatial commonsense. Based on\nMineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based\nagents, revealing the severe limitations but enormous potential in their\nspatial planning abilities. We believe our MineAnyBuild will open new avenues\nfor the evaluation of spatial intelligence and help promote further development\nfor open-world AI agents capable of spatial planning.", "authors": ["Ziming Wei", "Bingqian Lin", "Zijian Jiao", "Yunshuang Nie", "Liang Ma", "Yuecheng Liu", "Yuzheng Zhuang", "Xiaodan Liang"], "published_date": "2025-05-26", "title_zh": "MineAnyBuild：開放世界AI代理的空間規劃基準測試", "summary_zh": "這篇論文提出了一個名為MineAnyBuild的基準測試，用來評估AI代理在Minecraft遊戲中進行空間規劃的能力。現有的空間智能基準測試主要集中在視覺問答形式的空間推理，與實際任務執行之間存在差距。MineAnyBuild透過讓AI代理根據人類指令生成可執行的建築計畫，來更全面地評估其空間理解、推理、創造力和空間常識。研究人員使用MineAnyBuild評估了現有的多模態大型語言模型（MLLM），發現它們在空間規劃能力方面存在局限性，但同時也顯示了巨大的潛力。MineAnyBuild有望促進開放世界AI代理在空間規劃方面的發展。", "applications": ["**智慧居家設計助手：** 根據使用者需求和空間限制，AI能生成多個客製化的室內設計方案，並提供虛擬實境預覽，讓使用者在實際裝修前就能看到效果。", "**自動化倉儲管理：** AI可以優化貨物擺放位置，提高倉庫空間利用率，並規劃最佳的揀貨路徑，減少人力成本和時間。", "**建築設計輔助工具：** 建築師可以利用AI快速生成建築結構草圖，並根據規範和環境因素進行調整，提高設計效率，並減少潛在的設計缺陷。"], "pitch": "MineAnyBuild作為一個空間規劃的基準測試，能有效提升AI在空間智能方面的能力，就像是讓AI擁有了一雙能看懂、理解、甚至創造空間的眼睛。想像一下，我們能將AI應用於自動化倉儲、智慧城市規劃、甚至是機器人組裝等領域，這將顛覆傳統產業模式，大幅提升效率並降低成本。更重要的是，透過MineAnyBuild，我們可以持續優化AI的空間推理能力，開發出真正能理解並操作真實世界的智能體。投資MineAnyBuild的相關技術，就等於投資了未來空間智能的發展，這將是一個具有巨大商業潛力和社會價值的領域。", "audio": "audios/2505.20148v1.mp3", "timestamp": "2025-05-27T04:19:58.056265"}
{"query": "Foundation Model", "id": "2505.20003v1", "url": "http://arxiv.org/abs/2505.20003v1", "title": "TabPFN: One Model to Rule Them All?", "summary": "Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a\ntransformer-based deep learning model for regression and classification on\ntabular data, which they claim \"outperforms all previous methods on datasets\nwith up to 10,000 samples by a wide margin, using substantially less training\ntime.\" Furthermore, they have called TabPFN a \"foundation model\" for tabular\ndata, as it can support \"data generation, density estimation, learning reusable\nembeddings and fine-tuning\". If these statements are well-supported, TabPFN may\nhave the potential to supersede existing modeling approaches on a wide range of\nstatistical tasks, mirroring a similar revolution in other areas of artificial\nintelligence that began with the advent of large language models. In this\npaper, we provide a tailored explanation of how TabPFN works for a statistics\naudience, by emphasizing its interpretation as approximate Bayesian inference.\nWe also provide more evidence of TabPFN's \"foundation model\" capabilities: We\nshow that an out-of-the-box application of TabPFN vastly outperforms\nspecialized state-of-the-art methods for semi-supervised parameter estimation,\nprediction under covariate shift, and heterogeneous treatment effect\nestimation. We further show that TabPFN can outperform LASSO at sparse\nregression and can break a robustness-efficiency trade-off in classification.\nAll experiments can be reproduced using the code provided at\nhttps://github.com/qinglong-tian/tabpfn_study\n(https://github.com/qinglong-tian/tabpfn_study).", "authors": ["Qiong Zhang", "Yan Shuo Tan", "Qinglong Tian", "Pengfei Li"], "published_date": "2025-05-26", "title_zh": "TabPFN：一個模型統治所有？", "summary_zh": "一篇論文介紹了TabPFN，一種基於Transformer的深度學習模型，用於表格數據的迴歸和分類。作者聲稱它在樣本數少於10,000的數據集上，大幅超越了過去所有方法，且訓練時間更短。他們甚至將TabPFN視為表格數據的「基礎模型」，能支持數據生成、密度估計、學習可重用嵌入和微調。本文通過強調其作為近似貝葉斯推斷的解釋，為統計學受眾提供了TabPFN的工作原理的定制解釋，並展示了TabPFN在半監督參數估計、協變量偏移下的預測和異質處理效應估計等任務中，超越專門方法的優勢。此外，它還能在稀疏迴歸中勝過LASSO，並打破分類中的魯棒性-效率權衡。", "applications": ["**個人化醫療診斷：** 基於有限的患者病歷數據，快速準確地預測患病風險和最佳治療方案，無需大量歷史數據訓練特定模型。", "**中小企業信用評估：** 根據有限的財務數據和其他非結構化數據（如企業新聞），快速評估企業的信用風險，輔助銀行和投資機構做出信貸決策。", "**客戶流失預測：** 利用少量客戶行為數據（如購買記錄、網站瀏覽等），預測客戶流失的可能性，以便企業及早採取挽留措施。"], "pitch": "TabPFN 是一項突破性的技術，它能夠以更少的数据和更快的速度，在表格数据任务上超越现有方法。它具备成为表格数据领域「基础模型」的潜力，类似于大型语言模型在自然语言处理领域的地位。我们的初步实验表明，TabPFN在各种实际应用中都能展现出强大的竞争力，例如个人化医疗、金融风险评估和客户流失预测。想象一下，我们可以将它应用于任何含有表格数据的场景，无需为每个场景训练特定的模型。这代表着巨大的效率提升和成本节约。因此，我们相信 TabPFN 具有极高的商业价值，它将彻底改变表格数据分析的方式，为企业和研究机构带来革命性的改变。", "audio": "audios/2505.20003v1.mp3", "timestamp": "2025-05-27T04:20:13.312207"}
{"query": "Diffusion Model", "id": "2505.20131v1", "url": "http://arxiv.org/abs/2505.20131v1", "title": "MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning", "summary": "Molecular editing aims to modify a given molecule to optimize desired\nchemical properties while preserving structural similarity. However, current\napproaches typically rely on string-based or continuous representations, which\nfail to adequately capture the discrete, graph-structured nature of molecules,\nresulting in limited structural fidelity and poor controllability. In this\npaper, we propose MolEditRL, a molecular editing framework that explicitly\nintegrates structural constraints with precise property optimization.\nSpecifically, MolEditRL consists of two stages: (1) a discrete graph diffusion\nmodel pretrained to reconstruct target molecules conditioned on source\nstructures and natural language instructions; (2) an editing-aware\nreinforcement learning fine-tuning stage that further enhances property\nalignment and structural preservation by explicitly optimizing editing\ndecisions under graph constraints. For comprehensive evaluation, we construct\nMolEdit-Instruct, the largest and most property-rich molecular editing dataset,\ncomprising 3 million diverse examples spanning single- and multi-property tasks\nacross 10 chemical attributes. Experimental results demonstrate that MolEditRL\nsignificantly outperforms state-of-the-art methods in both property\noptimization accuracy and structural fidelity, achieving a 74\\% improvement in\nediting success rate while using 98\\% fewer parameters.", "authors": ["Yuanxin Zhuang", "Dazhong Shen", "Ying Sun"], "published_date": "2025-05-26", "title_zh": "MolEditRL：透過離散擴散與強化學習實現結構保持的分子編輯", "summary_zh": "MolEditRL是一種創新的分子編輯框架，它結合了離散圖擴散模型和強化學習，旨在修改現有分子，優化其化學性質，同時保持分子結構的相似性。它透過預訓練的圖擴散模型，根據原始結構和自然語言指令重建目標分子，然後使用強化學習微調編輯決策，顯著提升了化學性質的優化精度和結構保真度，並且參數效率更高。研究者建立了一個包含300萬個範例的大型分子編輯數據集MolEdit-Instruct，實驗結果顯示MolEditRL在編輯成功率上取得了顯著提升。", "applications": ["**藥物發現加速：** 基於已知藥物結構，快速迭代修改以提升藥效、降低副作用，或克服抗藥性，大幅縮短新藥開發週期。", "**材料設計優化：** 針對特定材料，例如高分子或奈米材料，修改分子結構以提升機械強度、導電性、耐熱性等關鍵性能。", "**農藥開發改良：** 修改現有農藥分子，使其更具選擇性、毒性更低，或更容易降解，從而減少對環境的影響。"], "pitch": "MolEditRL解決了傳統分子編輯方法在結構保真度和可控性方面的瓶頸，它利用創新的離散圖擴散和強化學習技術，能夠更精確地優化分子性質，同時最大限度地保留原始結構。這意味著我們能以更高的效率和更低的成本，開發出性能更優越的藥物、材料和農藥。其所使用的MolEdit-Instruct數據集也具有極高的價值，可以作為未來研究的基礎。 想像一下，以前需要花費數年時間和大量資源才能找到的理想分子結構，現在可以在更短的時間內，以更少的成本實現。 這種技術不僅能加速產品開發，還可以降低研發風險，為化學、製藥和材料科學領域帶來革命性的變革。 我們相信MolEditRL具有巨大的商業潛力，能夠為投資者帶來豐厚的回報。", "audio": "audios/2505.20131v1.mp3", "timestamp": "2025-05-27T04:20:31.491759"}
{"query": "AI", "id": "2505.20246v1", "url": "http://arxiv.org/abs/2505.20246v1", "title": "On Path to Multimodal Historical Reasoning: HistBench and HistAgent", "summary": "Recent advances in large language models (LLMs) have led to remarkable\nprogress across domains, yet their capabilities in the humanities, particularly\nhistory, remain underexplored. Historical reasoning poses unique challenges for\nAI, involving multimodal source interpretation, temporal inference, and\ncross-linguistic analysis. While general-purpose agents perform well on many\nexisting benchmarks, they lack the domain-specific expertise required to engage\nwith historical materials and questions. To address this gap, we introduce\nHistBench, a new benchmark of 414 high-quality questions designed to evaluate\nAI's capacity for historical reasoning and authored by more than 40 expert\ncontributors. The tasks span a wide range of historical problems-from factual\nretrieval based on primary sources to interpretive analysis of manuscripts and\nimages, to interdisciplinary challenges involving archaeology, linguistics, or\ncultural history. Furthermore, the benchmark dataset spans 29 ancient and\nmodern languages and covers a wide range of historical periods and world\nregions. Finding the poor performance of LLMs and other agents on HistBench, we\nfurther present HistAgent, a history-specific agent equipped with carefully\ndesigned tools for OCR, translation, archival search, and image understanding\nin History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of\n27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online\nsearch and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%)\nand Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These\nresults highlight the limitations of existing LLMs and generalist agents and\ndemonstrate the advantages of HistAgent for historical reasoning.", "authors": ["Jiahao Qiu", "Fulian Xiao", "Yimin Wang", "Yuchen Mao", "Yijia Chen", "Xinzhe Juan", "Siran Wang", "Xuan Qi", "Tongcheng Zhang", "Zixin Yao", "Jiacheng Guo", "Yifu Lu", "Charles Argon", "Jundi Cui", "Daixin Chen", "Junran Zhou", "Shuyao Zhou", "Zhanpeng Zhou", "Ling Yang", "Shilong Liu", "Hongru Wang", "Kaixuan Huang", "Xun Jiang", "Yuming Cao", "Yue Chen", "Yunfei Chen", "Zhengyi Chen", "Ruowei Dai", "Mengqiu Deng", "Jiye Fu", "Yunting Gu", "Zijie Guan", "Zirui Huang", "Xiaoyan Ji", "Yumeng Jiang", "Delong Kong", "Haolong Li", "Jiaqi Li", "Ruipeng Li", "Tianze Li", "Zhuoran Li", "Haixia Lian", "Mengyue Lin", "Xudong Liu", "Jiayi Lu", "Jinghan Lu", "Wanyu Luo", "Ziyue Luo", "Zihao Pu", "Zhi Qiao", "Ruihuan Ren", "Liang Wan", "Ruixiang Wang", "Tianhui Wang", "Yang Wang", "Zeyu Wang", "Zihua Wang", "Yujia Wu", "Zhaoyi Wu", "Hao Xin", "Weiao Xing", "Ruojun Xiong", "Weijie Xu", "Yao Shu", "Xiao Yao", "Xiaorui Yang", "Yuchen Yang", "Nan Yi", "Jiadong Yu", "Yangyuxuan Yu", "Huiting Zeng", "Danni Zhang", "Yunjie Zhang", "Zhaoyu Zhang", "Zhiheng Zhang", "Xiaofeng Zheng", "Peirong Zhou", "Linyan Zhong", "Xiaoyin Zong", "Ying Zhao", "Zhenxin Chen", "Lin Ding", "Xiaoyu Gao", "Bingbing Gong", "Yichao Li", "Yang Liao", "Guang Ma", "Tianyuan Ma", "Xinrui Sun", "Tianyi Wang", "Han Xia", "Ruobing Xian", "Gen Ye", "Tengfei Yu", "Wentao Zhang", "Yuxi Wang", "Xi Gao", "Mengdi Wang"], "published_date": "2025-05-26", "title_zh": "邁向多模態歷史推理之路：HistBench 與 HistAgent", "summary_zh": "大型語言模型(LLMs)在各領域取得顯著進展，但在人文領域，特別是歷史方面，其能力仍未被充分探索。歷史推理對AI提出了獨特的挑戰，包括多模態來源解釋、時間推理和跨語言分析。本文介紹了HistBench，一個包含414個高品質問題的新基準，旨在評估AI的歷史推理能力。這些問題涵蓋了廣泛的歷史問題，從基於主要來源的事實檢索到手稿和圖像的解釋性分析，再到涉及考古學、語言學或文化史的跨學科挑戰。實驗結果表明，現有的LLMs和通用型智能代理在HistBench上表現不佳。因此，作者提出了HistAgent，一個針對歷史領域的專用智能代理，配備了OCR、翻譯、檔案搜索和圖像理解等工具。基於GPT-4o的HistAgent在HistBench上取得了顯著優於其他模型和通用型代理的性能，證明了其在歷史推理方面的優勢。", "applications": ["博物館導覽與互動體驗：遊客可以使用手機或平板掃描文物，HistAgent 可以提供文物的背景故事、歷史意義，並支持多語言翻譯，提升參觀體驗。", "歷史研究輔助工具：研究人員可以使用HistAgent快速查找和分析歷史文獻、圖片和地圖等資料，加速研究進程，並發現新的研究方向。", "歷史教育遊戲與互動教材：學生可以使用HistAgent在遊戲中學習歷史知識，完成歷史推理任務，通過互動的方式提高學習興趣和效果。"], "pitch": "我們正在開發HistAgent，一個專為歷史研究和教育設計的AI引擎，它像一位不知疲倦、博學多聞的歷史學家助手。HistBench基準測試已證明其優越的性能。我們的商業模式將圍繞B2B和B2C兩個方向展開：面向博物館、圖書館和研究機構，提供定制化的AI歷史研究和資料分析服務；面向學生和歷史愛好者，提供基於HistAgent的互動學習應用和歷史遊戲。歷史數據的數位化和AI賦能具有巨大的市場潛力，我們相信HistAgent將成為歷史研究和教育領域的Game Changer。", "audio": "audios/2505.20246v1.mp3", "timestamp": "2025-05-27T06:20:02.194579"}
{"query": "Foundation Model", "id": "2505.20202v1", "url": "http://arxiv.org/abs/2505.20202v1", "title": "PathBench: A comprehensive comparison benchmark for pathology foundation models towards precision oncology", "summary": "The emergence of pathology foundation models has revolutionized computational\nhistopathology, enabling highly accurate, generalized whole-slide image\nanalysis for improved cancer diagnosis, and prognosis assessment. While these\nmodels show remarkable potential across cancer diagnostics and prognostics,\ntheir clinical translation faces critical challenges including variability in\noptimal model across cancer types, potential data leakage in evaluation, and\nlack of standardized benchmarks. Without rigorous, unbiased evaluation, even\nthe most advanced PFMs risk remaining confined to research settings, delaying\ntheir life-saving applications. Existing benchmarking efforts remain limited by\nnarrow cancer-type focus, potential pretraining data overlaps, or incomplete\ntask coverage. We present PathBench, the first comprehensive benchmark\naddressing these gaps through: multi-center in-hourse datasets spanning common\ncancers with rigorous leakage prevention, evaluation across the full clinical\nspectrum from diagnosis to prognosis, and an automated leaderboard system for\ncontinuous model assessment. Our framework incorporates large-scale data,\nenabling objective comparison of PFMs while reflecting real-world clinical\ncomplexity. All evaluation data comes from private medical providers, with\nstrict exclusion of any pretraining usage to avoid data leakage risks. We have\ncollected 15,888 WSIs from 8,549 patients across 10 hospitals, encompassing\nover 64 diagnosis and prognosis tasks. Currently, our evaluation of 19 PFMs\nshows that Virchow2 and H-Optimus-1 are the most effective models overall. This\nwork provides researchers with a robust platform for model development and\noffers clinicians actionable insights into PFM performance across diverse\nclinical scenarios, ultimately accelerating the translation of these\ntransformative technologies into routine pathology practice.", "authors": ["Jiabo Ma", "Yingxue Xu", "Fengtao Zhou", "Yihui Wang", "Cheng Jin", "Zhengrui Guo", "Jianfeng Wu", "On Ki Tang", "Huajun Zhou", "Xi Wang", "Luyang Luo", "Zhengyu Zhang", "Du Cai", "Zizhao Gao", "Wei Wang", "Yueping Liu", "Jiankun He", "Jing Cui", "Zhenhui Li", "Jing Zhang", "Feng Gao", "Xiuming Zhang", "Li Liang", "Ronald Cheong Kin Chan", "Zhe Wang", "Hao Chen"], "published_date": "2025-05-26", "title_zh": "PathBench：針對精準腫瘤學的病理學基礎模型之全面比較基準", "summary_zh": "病理學基礎模型(Pathology Foundation Models, PFMs)正改變計算病理學領域，帶來更準確、更通用的全玻片影像分析，有助於改善癌症診斷和預後評估。然而，這些模型在臨床應用上仍面臨挑戰，例如不同癌症的最佳模型差異、評估中潛在的數據洩漏，以及缺乏標準化基準。PathBench為首個全面性的基準，旨在解決這些問題，透過多中心內部數據集（涵蓋常見癌症、嚴格的洩漏預防）、從診斷到預後的完整臨床評估，以及自動化的排行榜系統，提供持續的模型評估。PathBench使用大規模數據進行客觀比較，反映真實的臨床複雜性，並嚴格排除預訓練數據的使用，以避免數據洩漏風險。目前已評估19個PFMs，結果顯示Virchow2和H-Optimus-1整體表現最佳。PathBench為研究人員提供了強大的模型開發平台，並為臨床醫生提供了PFM在不同臨床場景中的表現洞察，最終加速這些變革性技術應用於常規病理學實踐。", "applications": ["**更精準的癌症診斷：** 醫生可以利用PathBench評估過的PFM，在組織切片影像上更準確地辨識癌細胞，降低誤診率，並更快地確定癌症分期。", "**個性化治療方案：** 透過分析病理切片影像，PFM可以預測患者對特定治療方案的反應，幫助醫生制定更有效的個性化治療計畫，提高治療成功率。", "**遠程病理診斷：** 在醫療資源匱乏地區，可以利用PFM進行遠程病理影像分析，讓專家能夠遠程提供診斷建議，提高醫療可及性。"], "pitch": "PathBench解決了病理學基礎模型(PFMs)商業化過程中，缺乏標準、客觀評估的痛點。我們提供一個全面的基準平台，通過嚴格的數據控管和多樣化的臨床任務評估，協助醫院、診斷公司及藥廠客觀評估並選擇最適合其需求的PFM模型。這不僅加速了PFMs的臨床應用，更能精準地提升癌症診斷和預後評估的效率與準確性，從而降低醫療成本，提升患者生存率。我們提供的數據和分析結果，能幫助藥廠更有效地進行藥物開發和臨床試驗，加速新一代癌症治療藥物的上市。PathBench將成為精準腫瘤學領域不可或缺的基礎設施，具備巨大的市場潛力和回報。", "audio": "audios/2505.20202v1.mp3", "timestamp": "2025-05-27T06:20:17.877322"}
{"query": "Diffusion Model", "id": "2505.20171v1", "url": "http://arxiv.org/abs/2505.20171v1", "title": "Long-Context State-Space Video World Models", "summary": "Video diffusion models have recently shown promise for world modeling through\nautoregressive frame prediction conditioned on actions. However, they struggle\nto maintain long-term memory due to the high computational cost associated with\nprocessing extended sequences in attention layers. To overcome this limitation,\nwe propose a novel architecture leveraging state-space models (SSMs) to extend\ntemporal memory without compromising computational efficiency. Unlike previous\napproaches that retrofit SSMs for non-causal vision tasks, our method fully\nexploits the inherent advantages of SSMs in causal sequence modeling. Central\nto our design is a block-wise SSM scanning scheme, which strategically trades\noff spatial consistency for extended temporal memory, combined with dense local\nattention to ensure coherence between consecutive frames. We evaluate the\nlong-term memory capabilities of our model through spatial retrieval and\nreasoning tasks over extended horizons. Experiments on Memory Maze and\nMinecraft datasets demonstrate that our approach surpasses baselines in\npreserving long-range memory, while maintaining practical inference speeds\nsuitable for interactive applications.", "authors": ["Ryan Po", "Yotam Nitzan", "Richard Zhang", "Berlin Chen", "Tri Dao", "Eli Shechtman", "Gordon Wetzstein", "Xun Huang"], "published_date": "2025-05-26", "title_zh": "長脈絡狀態空間影片世界模型", "summary_zh": "影片擴散模型在透過條件化動作自迴歸幀預測的世界建模方面展現了前景。然而，由於注意力層中處理長序列所帶來的高計算成本，它們難以維持長期記憶。為了克服這個限制，我們提出了一種新穎的架構，利用狀態空間模型 (SSM) 來擴展時間記憶，同時不影響計算效率。與先前為非因果視覺任務改造 SSM 的方法不同，我們的方法充分利用了 SSM 在因果序列建模中的固有優勢。我們設計的核心是塊狀 SSM 掃描方案，它策略性地權衡空間一致性以換取擴展的時間記憶，並結合密集的局部注意力以確保連續幀之間的連貫性。我們透過在擴展的時間範圍內進行空間檢索和推理任務來評估我們模型的長期記憶能力。在 Memory Maze 和 Minecraft 數據集上的實驗表明，我們的方法在保持遠距離記憶方面優於基線，同時保持適用於互動式應用程式的實際推理速度。", "applications": ["**更真實的遊戲AI：** 讓遊戲中的NPC (非玩家角色) 能記得更久遠的事情，並做出更合理的判斷，例如記得玩家之前的行為並給予不同的反應。", "**更智能的自動駕駛：** 讓自動駕駛系統能分析更長時間的行車記錄，預測更遠距離的潛在危險，從而提高安全性。", "**更可靠的遠程控制機器人：** 讓操作員可以透過網路控制遠程機器人，即便網路延遲較高，也能讓機器人更好地理解操作員的意圖，避免誤操作。"], "pitch": "我們的長脈絡狀態空間影片世界模型，解決了傳統影片擴散模型在處理長時間序列和保持長期記憶上的瓶頸。這項技術的突破，大幅降低了計算成本，並提升了模型的推理速度，使其更適用於互動式應用。想像一下，我們的技術可以讓遊戲AI更加逼真，自動駕駛系統更加安全，遠程機器人操作更加可靠。這將革新遊戲、自動駕駛和機器人等行業，帶來巨大的商業價值。我們相信，通過進一步的商業化和產品化，我們的技術將在未來世界中扮演關鍵角色，創造數十億美元的市場機會。現在投資我們，就是投資未來。", "audio": "audios/2505.20171v1.mp3", "timestamp": "2025-05-27T06:20:33.292996"}
{"query": "AI", "id": "2505.20246v1", "url": "http://arxiv.org/abs/2505.20246v1", "title": "On Path to Multimodal Historical Reasoning: HistBench and HistAgent", "summary": "Recent advances in large language models (LLMs) have led to remarkable\nprogress across domains, yet their capabilities in the humanities, particularly\nhistory, remain underexplored. Historical reasoning poses unique challenges for\nAI, involving multimodal source interpretation, temporal inference, and\ncross-linguistic analysis. While general-purpose agents perform well on many\nexisting benchmarks, they lack the domain-specific expertise required to engage\nwith historical materials and questions. To address this gap, we introduce\nHistBench, a new benchmark of 414 high-quality questions designed to evaluate\nAI's capacity for historical reasoning and authored by more than 40 expert\ncontributors. The tasks span a wide range of historical problems-from factual\nretrieval based on primary sources to interpretive analysis of manuscripts and\nimages, to interdisciplinary challenges involving archaeology, linguistics, or\ncultural history. Furthermore, the benchmark dataset spans 29 ancient and\nmodern languages and covers a wide range of historical periods and world\nregions. Finding the poor performance of LLMs and other agents on HistBench, we\nfurther present HistAgent, a history-specific agent equipped with carefully\ndesigned tools for OCR, translation, archival search, and image understanding\nin History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of\n27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online\nsearch and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%)\nand Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These\nresults highlight the limitations of existing LLMs and generalist agents and\ndemonstrate the advantages of HistAgent for historical reasoning.", "authors": ["Jiahao Qiu", "Fulian Xiao", "Yimin Wang", "Yuchen Mao", "Yijia Chen", "Xinzhe Juan", "Siran Wang", "Xuan Qi", "Tongcheng Zhang", "Zixin Yao", "Jiacheng Guo", "Yifu Lu", "Charles Argon", "Jundi Cui", "Daixin Chen", "Junran Zhou", "Shuyao Zhou", "Zhanpeng Zhou", "Ling Yang", "Shilong Liu", "Hongru Wang", "Kaixuan Huang", "Xun Jiang", "Yuming Cao", "Yue Chen", "Yunfei Chen", "Zhengyi Chen", "Ruowei Dai", "Mengqiu Deng", "Jiye Fu", "Yunting Gu", "Zijie Guan", "Zirui Huang", "Xiaoyan Ji", "Yumeng Jiang", "Delong Kong", "Haolong Li", "Jiaqi Li", "Ruipeng Li", "Tianze Li", "Zhuoran Li", "Haixia Lian", "Mengyue Lin", "Xudong Liu", "Jiayi Lu", "Jinghan Lu", "Wanyu Luo", "Ziyue Luo", "Zihao Pu", "Zhi Qiao", "Ruihuan Ren", "Liang Wan", "Ruixiang Wang", "Tianhui Wang", "Yang Wang", "Zeyu Wang", "Zihua Wang", "Yujia Wu", "Zhaoyi Wu", "Hao Xin", "Weiao Xing", "Ruojun Xiong", "Weijie Xu", "Yao Shu", "Xiao Yao", "Xiaorui Yang", "Yuchen Yang", "Nan Yi", "Jiadong Yu", "Yangyuxuan Yu", "Huiting Zeng", "Danni Zhang", "Yunjie Zhang", "Zhaoyu Zhang", "Zhiheng Zhang", "Xiaofeng Zheng", "Peirong Zhou", "Linyan Zhong", "Xiaoyin Zong", "Ying Zhao", "Zhenxin Chen", "Lin Ding", "Xiaoyu Gao", "Bingbing Gong", "Yichao Li", "Yang Liao", "Guang Ma", "Tianyuan Ma", "Xinrui Sun", "Tianyi Wang", "Han Xia", "Ruobing Xian", "Gen Ye", "Tengfei Yu", "Wentao Zhang", "Yuxi Wang", "Xi Gao", "Mengdi Wang"], "published_date": "2025-05-26", "title_zh": "邁向多模態歷史推理之路：HistBench與HistAgent", "summary_zh": "大型語言模型(LLMs)在各領域取得顯著進展，但在人文學科，尤其是歷史領域的應用仍未充分探索。歷史推理對AI來說是獨特的挑戰，涉及多模態資料來源的解釋、時間推論和跨語言分析。本研究推出HistBench，一個包含414個高品質問題的基準測試，旨在評估AI的歷史推理能力，這些問題由40多位專家貢獻，涵蓋多種歷史問題，包含基於一手資料的事實檢索、手稿和圖像的詮釋分析，以及涉及考古學、語言學或文化史的跨學科挑戰。數據集橫跨29種古今語言，涵蓋廣泛的歷史時期和世界區域。研究發現LLMs和其他Agent在HistBench上的表現不佳，因此進一步提出了HistAgent，一個專門針對歷史領域的Agent，配備了精心設計的OCR、翻譯、檔案搜索和圖像理解工具。在HistBench上，基於GPT-4o的HistAgent實現了27.54% pass@1 和 36.47% pass@2 的準確度，顯著優於配備在線搜索的LLMs和通用Agent，包括GPT-4o（18.60%）、DeepSeek-R1（14.49%）和 Open Deep Research-smolagents（20.29% pass@1 和 25.12% pass@2）。這些結果突顯了現有LLMs和通用Agent的局限性，並證明了HistAgent在歷史推理方面的優勢。", "applications": ["**歷史文獻數位化與分析：** 將HistAgent應用於博物館、圖書館的歷史文獻數位化項目，協助研究人員快速檢索、翻譯、理解古籍文獻，提升研究效率。", "**歷史教育與學習：** 開發互動式歷史學習App或平台，學生可以透過HistAgent提問、分析歷史資料，獲得更深入、更全面的歷史知識。", "**文化遺產保護與復原：** 利用HistAgent分析古建築圖像、文獻資料，輔助考古學家、建築師進行文化遺產的復原和保護工作，並生成3D模型供公眾參觀。"], "pitch": "HistAgent解決了LLM在歷史推理方面能力的不足，透過專門設計的工具和領域知識，顯著提升了AI在歷史文獻分析、跨語言理解和多模態資料處理的效能。這將帶來巨大的商業價值，包括：1. 為博物館、圖書館等機構提供更高效的歷史文獻數位化和研究工具，提升其服務品質和研究能力；2. 開發創新性歷史教育產品，吸引更多學生和歷史愛好者，打造龐大的教育市場；3. 為文化遺產保護機構提供先進的技術支援，促進文化遺產的永續發展。 HistAgent擁有巨大的市場潛力，我們相信它將引領AI在人文學科的應用，並為歷史研究和教育帶來革命性的變革。現在投資HistAgent，就是投資歷史的未來。", "audio": "audios/2505.20246v1.mp3", "timestamp": "2025-05-27T07:14:18.060751"}
{"query": "Foundation Model", "id": "2505.20202v1", "url": "http://arxiv.org/abs/2505.20202v1", "title": "PathBench: A comprehensive comparison benchmark for pathology foundation models towards precision oncology", "summary": "The emergence of pathology foundation models has revolutionized computational\nhistopathology, enabling highly accurate, generalized whole-slide image\nanalysis for improved cancer diagnosis, and prognosis assessment. While these\nmodels show remarkable potential across cancer diagnostics and prognostics,\ntheir clinical translation faces critical challenges including variability in\noptimal model across cancer types, potential data leakage in evaluation, and\nlack of standardized benchmarks. Without rigorous, unbiased evaluation, even\nthe most advanced PFMs risk remaining confined to research settings, delaying\ntheir life-saving applications. Existing benchmarking efforts remain limited by\nnarrow cancer-type focus, potential pretraining data overlaps, or incomplete\ntask coverage. We present PathBench, the first comprehensive benchmark\naddressing these gaps through: multi-center in-hourse datasets spanning common\ncancers with rigorous leakage prevention, evaluation across the full clinical\nspectrum from diagnosis to prognosis, and an automated leaderboard system for\ncontinuous model assessment. Our framework incorporates large-scale data,\nenabling objective comparison of PFMs while reflecting real-world clinical\ncomplexity. All evaluation data comes from private medical providers, with\nstrict exclusion of any pretraining usage to avoid data leakage risks. We have\ncollected 15,888 WSIs from 8,549 patients across 10 hospitals, encompassing\nover 64 diagnosis and prognosis tasks. Currently, our evaluation of 19 PFMs\nshows that Virchow2 and H-Optimus-1 are the most effective models overall. This\nwork provides researchers with a robust platform for model development and\noffers clinicians actionable insights into PFM performance across diverse\nclinical scenarios, ultimately accelerating the translation of these\ntransformative technologies into routine pathology practice.", "authors": ["Jiabo Ma", "Yingxue Xu", "Fengtao Zhou", "Yihui Wang", "Cheng Jin", "Zhengrui Guo", "Jianfeng Wu", "On Ki Tang", "Huajun Zhou", "Xi Wang", "Luyang Luo", "Zhengyu Zhang", "Du Cai", "Zizhao Gao", "Wei Wang", "Yueping Liu", "Jiankun He", "Jing Cui", "Zhenhui Li", "Jing Zhang", "Feng Gao", "Xiuming Zhang", "Li Liang", "Ronald Cheong Kin Chan", "Zhe Wang", "Hao Chen"], "published_date": "2025-05-26", "title_zh": "PathBench：病理學基礎模型於精準腫瘤學之綜合比較基準", "summary_zh": "PathBench是一個全新的、更全面的病理學基礎模型（PFM）評估基準。它旨在解決現有基準的局限性，例如癌症類型覆蓋範圍窄、預訓練數據重疊風險以及任務覆蓋不完整。PathBench利用來自多個醫療中心的內部數據集，涵蓋多種常見癌症，並嚴格防止數據洩漏。它涵蓋了從診斷到預後的完整臨床範圍，並提供一個自動化的排行榜系統，以持續評估模型。目前對19個PFM的評估顯示，Virchow2和H-Optimus-1是總體上最有效的模型。PathBench為研究人員提供了一個強大的平台來開發模型，並為臨床醫生提供了關於PFM在不同臨床場景中表現的可行見解，最終加速了這些變革性技術轉化為常規病理學實踐。", "applications": ["**遠程病理診斷平台:** PathBench可以幫助醫療機構選擇最適合的PFM，以便在資源有限的地區提供更準確、快速的遠程病理診斷服務，減少對專業病理醫生的依賴。", "**個性化癌症治療方案推薦:** 醫生可以使用PathBench評估過的PFM來分析患者的病理切片，預測疾病進展，並基於此為患者推薦更有效的個性化治療方案，提高治療成功率。", "**藥物研發加速:** 藥物研發公司可以利用PathBench來評估不同PFM在識別藥物靶點和預測藥物反應方面的能力，加速新藥的開發和臨床試驗，降低研發成本。"], "pitch": "PathBench是一個遊戲規則改變者，它解決了病理學基礎模型商業化的關鍵痛點：缺乏標準化的、可信賴的評估基準。想像一下，一個能夠客觀地比較不同AI模型在癌症診斷和預後方面表現的平台，消除數據洩漏的風險，並涵蓋了多種癌症類型和臨床任務。PathBench正是這樣一個平台。它不僅為研究人員提供了開發更強大模型的工具，更重要的是，它賦予了臨床醫生選擇和信任AI輔助診斷工具的能力。這意味著更快速、更準確的診斷，更個性化的治療方案，以及更好的患者預後。我們相信PathBench將成為病理學AI領域的行業標準，加速其臨床應用，並最終改變癌症治療的未來。投資PathBench，就是投資精準醫療的未來，一個具有巨大商業潛力的藍海市場。", "audio": "audios/2505.20202v1.mp3", "timestamp": "2025-05-27T07:14:35.321154"}
{"query": "Diffusion Model", "id": "2505.20171v1", "url": "http://arxiv.org/abs/2505.20171v1", "title": "Long-Context State-Space Video World Models", "summary": "Video diffusion models have recently shown promise for world modeling through\nautoregressive frame prediction conditioned on actions. However, they struggle\nto maintain long-term memory due to the high computational cost associated with\nprocessing extended sequences in attention layers. To overcome this limitation,\nwe propose a novel architecture leveraging state-space models (SSMs) to extend\ntemporal memory without compromising computational efficiency. Unlike previous\napproaches that retrofit SSMs for non-causal vision tasks, our method fully\nexploits the inherent advantages of SSMs in causal sequence modeling. Central\nto our design is a block-wise SSM scanning scheme, which strategically trades\noff spatial consistency for extended temporal memory, combined with dense local\nattention to ensure coherence between consecutive frames. We evaluate the\nlong-term memory capabilities of our model through spatial retrieval and\nreasoning tasks over extended horizons. Experiments on Memory Maze and\nMinecraft datasets demonstrate that our approach surpasses baselines in\npreserving long-range memory, while maintaining practical inference speeds\nsuitable for interactive applications.", "authors": ["Ryan Po", "Yotam Nitzan", "Richard Zhang", "Berlin Chen", "Tri Dao", "Eli Shechtman", "Gordon Wetzstein", "Xun Huang"], "published_date": "2025-05-26", "title_zh": "長文脈狀態空間影片世界模型", "summary_zh": "影片擴散模型在世界建模方面展現了潛力，透過自迴歸幀預測和動作條件化。然而，由於在注意力層處理長序列需要高昂的計算成本，它們難以維持長期記憶。為了克服這個限制，我們提出了一種新穎的架構，利用狀態空間模型(SSM)來擴展時間記憶，同時不犧牲計算效率。與之前將SSM改造用於非因果視覺任務的方法不同，我們的方法充分利用了SSM在因果序列建模中的固有優勢。我們設計的核心是一種塊狀SSM掃描方案，它策略性地犧牲空間一致性來換取更長的時間記憶，並結合密集的局部注意力來確保連續幀之間的一致性。我們通過在延伸的時間範圍內進行空間檢索和推理任務來評估我們模型的長期記憶能力。在Memory Maze和Minecraft數據集上的實驗表明，我們的方法在保持長程記憶方面超越了基準線，同時保持了適用於交互式應用程序的實際推理速度。", "applications": ["**AI遊戲輔助：** 玩家在玩Minecraft時，AI能夠預測玩家的行為並提供建議，例如自動建造房屋或提供最佳的挖礦路線，因為AI能記住玩家之前的決策和遊戲世界狀態。", "**長時間影片剪輯：** 編輯軟體可以利用此模型分析長時間的影片素材，自動識別關鍵場景並生成精華片段，節省人工剪輯的時間和精力。", "**自動駕駛模擬：** 在複雜的交通環境中，自動駕駛系統可以利用此模型預測其他車輛和行人的行為，提高駕駛安全性，並在模擬環境中進行長時間的可靠測試。"], "pitch": "傳統影片生成模型在處理長時間影片時面臨記憶力不足的挑戰，限制了其在許多應用場景中的潛力。我們的長文脈狀態空間影片世界模型，突破性地解決了這個問題，能夠高效處理長序列，並保持卓越的生成質量。想像一下，一個可以長時間分析監控錄像，自動識別異常行為的智慧安防系統；或者一個能夠根據患者的病史，精準預測病情發展的醫療AI。這些都是我們技術可以賦能的領域。我們擁有強大的技術壁壘，且在性能上明顯優於現有方案，具備巨大的商業潛力，特別是在遊戲、影音編輯、自動駕駛和安防等市場。我們正在尋找能夠幫助我們將這項技術推向市場的投資者，共同開創影片生成的新時代。", "audio": "audios/2505.20171v1.mp3", "timestamp": "2025-05-27T07:14:52.984287"}
{"query": "AI", "id": "2505.20148v1", "url": "http://arxiv.org/abs/2505.20148v1", "title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents", "summary": "Spatial Planning is a crucial part in the field of spatial intelligence,\nwhich requires the understanding and planning about object arrangements in\nspace perspective. AI agents with the spatial planning ability can better adapt\nto various real-world applications, including robotic manipulation, automatic\nassembly, urban planning etc. Recent works have attempted to construct\nbenchmarks for evaluating the spatial intelligence of Multimodal Large Language\nModels (MLLMs). Nevertheless, these benchmarks primarily focus on spatial\nreasoning based on typical Visual Question-Answering (VQA) forms, which suffers\nfrom the gap between abstract spatial understanding and concrete task\nexecution. In this work, we take a step further to build a comprehensive\nbenchmark called MineAnyBuild, aiming to evaluate the spatial planning ability\nof open-world AI agents in the Minecraft game. Specifically, MineAnyBuild\nrequires an agent to generate executable architecture building plans based on\nthe given multi-modal human instructions. It involves 4,000 curated spatial\nplanning tasks and also provides a paradigm for infinitely expandable data\ncollection by utilizing rich player-generated content. MineAnyBuild evaluates\nspatial planning through four core supporting dimensions: spatial\nunderstanding, spatial reasoning, creativity, and spatial commonsense. Based on\nMineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based\nagents, revealing the severe limitations but enormous potential in their\nspatial planning abilities. We believe our MineAnyBuild will open new avenues\nfor the evaluation of spatial intelligence and help promote further development\nfor open-world AI agents capable of spatial planning.", "authors": ["Ziming Wei", "Bingqian Lin", "Zijian Jiao", "Yunshuang Nie", "Liang Ma", "Yuecheng Liu", "Yuzheng Zhuang", "Xiaodan Liang"], "published_date": "2025-05-26", "title_zh": "MineAnyBuild：開放世界 AI 代理空間規劃能力評估基準", "summary_zh": "這篇論文介紹了 MineAnyBuild，一個新的基準測試，用於評估 AI 代理在 Minecraft 遊戲中執行空間規劃任務的能力。與現有的基準測試不同，MineAnyBuild 更側重於實際任務執行，而非僅僅是抽象的空間推理。它包含 4000 個精心設計的空間規劃任務，並提供了一種可無限擴展的數據收集方法，利用玩家生成的大量內容。MineAnyBuild 通過空間理解、空間推理、創造力和空間常識四個維度來評估 AI 代理的空間規劃能力。研究團隊使用 MineAnyBuild 對現有的多模態大型語言模型（MLLM）代理進行了全面評估，揭示了它們在空間規劃能力方面的局限性以及巨大的潛力。此研究旨在為空間智能的評估開闢新途徑，並促進具有空間規劃能力的開放世界 AI 代理的進一步發展。", "applications": ["**智慧家庭設計助手：** 根據使用者的語音或文字指令，自動生成房屋格局設計圖，並考慮到家具擺放和空間利用率，提供個性化建議。", "**建築工程模擬與優化：** 在虛擬環境中模擬建築施工過程，自動規劃建築材料堆放位置、施工路線和人員調配，降低施工成本和風險。", "**機器人倉庫管理：** 指揮倉庫機器人規劃最佳貨物儲存位置，並優化揀貨路徑，提高倉庫效率和準確性。"], "pitch": "MineAnyBuild 解決了當前 AI 在空間規劃能力評估上的不足，提供了一個更貼近實際應用的基準。 想像一下，一個能夠理解人類指令並在虛擬世界中建造複雜結構的 AI。 這種技術的商業價值巨大，可以應用於建築設計、智慧城市規劃、機器人技術等領域。 我們正在開發一個更智能、更具適應性的 AI，它能夠理解並操縱物理世界。 MineAnyBuild 的數據集和評估方法將加速這一進程，並為相關企業提供一個可靠的測試平台，以評估和改進其 AI 代理的空間規劃能力。 我們相信，透過 MineAnyBuild，我們能夠打造出更強大、更實用的 AI，為社會帶來革命性的變革。 這不僅僅是一個基準，更是一個通往無限可能性的鑰匙。", "audio": "audios/2505.20148v1.mp3", "timestamp": "2025-05-27T08:18:50.786211"}
{"query": "Foundation Model", "id": "2505.20003v1", "url": "http://arxiv.org/abs/2505.20003v1", "title": "TabPFN: One Model to Rule Them All?", "summary": "Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a\ntransformer-based deep learning model for regression and classification on\ntabular data, which they claim \"outperforms all previous methods on datasets\nwith up to 10,000 samples by a wide margin, using substantially less training\ntime.\" Furthermore, they have called TabPFN a \"foundation model\" for tabular\ndata, as it can support \"data generation, density estimation, learning reusable\nembeddings and fine-tuning\". If these statements are well-supported, TabPFN may\nhave the potential to supersede existing modeling approaches on a wide range of\nstatistical tasks, mirroring a similar revolution in other areas of artificial\nintelligence that began with the advent of large language models. In this\npaper, we provide a tailored explanation of how TabPFN works for a statistics\naudience, by emphasizing its interpretation as approximate Bayesian inference.\nWe also provide more evidence of TabPFN's \"foundation model\" capabilities: We\nshow that an out-of-the-box application of TabPFN vastly outperforms\nspecialized state-of-the-art methods for semi-supervised parameter estimation,\nprediction under covariate shift, and heterogeneous treatment effect\nestimation. We further show that TabPFN can outperform LASSO at sparse\nregression and can break a robustness-efficiency trade-off in classification.\nAll experiments can be reproduced using the code provided at\nhttps://github.com/qinglong-tian/tabpfn_study\n(https://github.com/qinglong-tian/tabpfn_study).", "authors": ["Qiong Zhang", "Yan Shuo Tan", "Qinglong Tian", "Pengfei Li"], "published_date": "2025-05-26", "title_zh": "TabPFN：一統江湖的模型？", "summary_zh": "TabPFN 是一個基於 Transformer 的深度學習模型，專為表格數據的迴歸與分類設計。據研究者聲稱，它在數據量小於 10,000 的數據集上，大幅超越了所有既有方法，且訓練時間更短。他們更將其視為表格數據的「基礎模型」，能夠支援數據生成、密度估計、學習可重用嵌入和微調等功能。本文從統計學的角度解釋 TabPFN 的工作原理，著重於它作為近似貝葉斯推斷的詮釋。同時，也提供了更多證據來支持 TabPFN 作為「基礎模型」的能力，例如它在半監督參數估計、協變量偏移下的預測以及異質處理效應估計等方面，顯著優於現有的方法。此外，TabPFN 在稀疏迴歸上也能超越 LASSO，並且打破了分類中魯棒性與效率之間的權衡。", "applications": ["**小商家貸款風險評估：** 銀行使用 TabPFN 分析少量歷史數據（例如：過去幾年的銷售額、員工數量），快速準確地評估新創企業或小型商家的貸款風險，無需仰賴大量、耗時的信用評級數據。", "**醫療診斷輔助：** 醫院使用 TabPFN 分析病患的有限病歷數據（例如：幾次檢測結果、基本生理指標），幫助醫生快速篩選出潛在高風險人群，並提供初步的診斷建議，加速診斷流程。", "**個性化產品推薦：** 電商平台利用 TabPFN 分析少量用戶行為數據（例如：過去的幾次瀏覽、點擊、購買記錄），為新用戶或冷啟動用戶提供更精準的產品推薦，提高轉化率，避免用戶流失。"], "pitch": "TabPFN 代表著表格數據分析領域的革命性突破。它不僅大幅提升了小數據集上的模型效能，更作為一個「基礎模型」，拓展了數據分析的可能性。想像一下，一個模型就能應對各種統計任務，無需針對不同場景開發專用模型，大幅降低開發成本和時間。TabPFN 在金融、醫療、零售等各行各業都擁有巨大的應用潛力，尤其是在數據稀缺或獲取成本高的場景下。我們相信 TabPFN 將成為企業決策的重要工具，為投資者帶來豐厚的回報。投資 TabPFN，就是投資數據分析的未來！", "audio": "audios/2505.20003v1.mp3", "timestamp": "2025-05-27T08:19:05.777089"}
{"query": "Diffusion Model", "id": "2505.20131v1", "url": "http://arxiv.org/abs/2505.20131v1", "title": "MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning", "summary": "Molecular editing aims to modify a given molecule to optimize desired\nchemical properties while preserving structural similarity. However, current\napproaches typically rely on string-based or continuous representations, which\nfail to adequately capture the discrete, graph-structured nature of molecules,\nresulting in limited structural fidelity and poor controllability. In this\npaper, we propose MolEditRL, a molecular editing framework that explicitly\nintegrates structural constraints with precise property optimization.\nSpecifically, MolEditRL consists of two stages: (1) a discrete graph diffusion\nmodel pretrained to reconstruct target molecules conditioned on source\nstructures and natural language instructions; (2) an editing-aware\nreinforcement learning fine-tuning stage that further enhances property\nalignment and structural preservation by explicitly optimizing editing\ndecisions under graph constraints. For comprehensive evaluation, we construct\nMolEdit-Instruct, the largest and most property-rich molecular editing dataset,\ncomprising 3 million diverse examples spanning single- and multi-property tasks\nacross 10 chemical attributes. Experimental results demonstrate that MolEditRL\nsignificantly outperforms state-of-the-art methods in both property\noptimization accuracy and structural fidelity, achieving a 74\\% improvement in\nediting success rate while using 98\\% fewer parameters.", "authors": ["Yuanxin Zhuang", "Dazhong Shen", "Ying Sun"], "published_date": "2025-05-26", "title_zh": "MolEditRL：透過離散擴散與強化學習實現結構保留的分子編輯", "summary_zh": "MolEditRL是一個分子編輯框架，旨在修改現有分子以優化化學性質，同時保持結構相似性。它利用離散圖擴散模型重建目標分子，並結合強化學習微調，在圖形約束下優化編輯決策，有效提升性質優化精準度和結構保真度。研究團隊還創建了大型分子編輯數據集MolEdit-Instruct，實驗證明MolEditRL在性質優化和結構保真度方面超越現有技術。", "applications": ["**藥物設計加速：** 幫助藥物化學家在已知藥物結構的基礎上，快速迭代優化藥效、降低副作用，大幅縮短藥物開發週期。", "**新材料發現：** 可以根據特定材料需求，例如提升導電性、增加強度等，編輯現有材料的分子結構，快速篩選出潛在的新材料。", "**個性化化學品定制：** 為特定應用場景定制化學品，例如根據客戶需求調整香味分子，創造獨特的香水或清潔劑。"], "pitch": "MolEditRL解決了分子編輯領域長期存在的結構保真度和性質優化之間的trade-off問題。其核心優勢在於採用離散圖擴散和強化學習的創新結合，大幅提升了編輯成功率和結構保真度，同時顯著降低了模型參數需求。這意味著更高的效率、更低的成本和更廣泛的應用前景。MolEditRL有望顛覆傳統的藥物和材料開發流程，加速新產品的上市時間，並為個性化化學品定制開闢新的市場。考慮到藥物研發和材料科學的巨大市場規模，以及MolEditRL在效率和成本上的顯著優勢，我們相信它具有巨大的商業價值，並能為投資者帶來可觀的回報。", "audio": "audios/2505.20131v1.mp3", "timestamp": "2025-05-27T08:19:16.755428"}
{"query": "AI", "id": "2505.20148v1", "url": "http://arxiv.org/abs/2505.20148v1", "title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents", "summary": "Spatial Planning is a crucial part in the field of spatial intelligence,\nwhich requires the understanding and planning about object arrangements in\nspace perspective. AI agents with the spatial planning ability can better adapt\nto various real-world applications, including robotic manipulation, automatic\nassembly, urban planning etc. Recent works have attempted to construct\nbenchmarks for evaluating the spatial intelligence of Multimodal Large Language\nModels (MLLMs). Nevertheless, these benchmarks primarily focus on spatial\nreasoning based on typical Visual Question-Answering (VQA) forms, which suffers\nfrom the gap between abstract spatial understanding and concrete task\nexecution. In this work, we take a step further to build a comprehensive\nbenchmark called MineAnyBuild, aiming to evaluate the spatial planning ability\nof open-world AI agents in the Minecraft game. Specifically, MineAnyBuild\nrequires an agent to generate executable architecture building plans based on\nthe given multi-modal human instructions. It involves 4,000 curated spatial\nplanning tasks and also provides a paradigm for infinitely expandable data\ncollection by utilizing rich player-generated content. MineAnyBuild evaluates\nspatial planning through four core supporting dimensions: spatial\nunderstanding, spatial reasoning, creativity, and spatial commonsense. Based on\nMineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based\nagents, revealing the severe limitations but enormous potential in their\nspatial planning abilities. We believe our MineAnyBuild will open new avenues\nfor the evaluation of spatial intelligence and help promote further development\nfor open-world AI agents capable of spatial planning.", "authors": ["Ziming Wei", "Bingqian Lin", "Zijian Jiao", "Yunshuang Nie", "Liang Ma", "Yuecheng Liu", "Yuzheng Zhuang", "Xiaodan Liang"], "published_date": "2025-05-26", "title_zh": "MineAnyBuild：開放世界AI代理空間規劃能力基準測試", "summary_zh": "這篇論文提出了一個名為MineAnyBuild的基準測試，用於評估開放世界AI代理在Minecraft遊戲中的空間規劃能力。不同於以往著重視覺問答的空間推理基準，MineAnyBuild更側重於AI根據多模態指令生成可執行的建築規劃。這個基準測試包含4000個精選的空間規劃任務，並提供無限擴展數據收集的範例，通過空間理解、推理、創造力和常識四個維度評估AI的能力。研究發現，現有的多模態大型語言模型（MLLM）在空間規劃方面仍有很大的進步空間，但同時也展現了巨大的潛力。", "applications": ["**智能家居設計助手：** 根據使用者的偏好和空間限制，自動生成個性化的家居佈局方案，並提供3D預覽和材料建議，幫助使用者更高效地規劃和裝飾自己的家。", "**倉庫和工廠自動化優化：** 透過分析倉庫或工廠的空間配置，自動規劃最佳的物料儲存和運輸路徑，提高效率，降低成本，並減少人員操作風險。", "**城市規劃模擬：** 在城市規劃初期，利用AI模擬不同建築和基礎設施的佈局方案對交通、環境和居民生活的影響，協助規劃者做出更明智的決策。"], "pitch": "想像一下，AI能像Minecraft玩家一樣，根據簡單的指令創造複雜的建築結構。MineAnyBuild benchmark 正是為了測試並提升AI的這種空間規劃能力。這項技術的商業價值巨大：從智能家居設計到自動化倉儲，再到城市規劃，都能應用。我們投資MineAnyBuild 的研發，意味著投資於未來具備高度空間智能的AI，這將帶來顛覆性的效率提升和創新，並在各個產業創造巨大的商業機會。試想一下，未來的建築師和工程師將不再需要耗費大量時間繪製設計圖，而是可以透過 AI 快速生成多種方案，並進行最佳化選擇，這將極大地提升效率和降低成本。", "audio": "audios/2505.20148v1.mp3", "timestamp": "2025-05-27T09:14:23.579558"}
{"query": "Foundation Model", "id": "2505.20003v1", "url": "http://arxiv.org/abs/2505.20003v1", "title": "TabPFN: One Model to Rule Them All?", "summary": "Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a\ntransformer-based deep learning model for regression and classification on\ntabular data, which they claim \"outperforms all previous methods on datasets\nwith up to 10,000 samples by a wide margin, using substantially less training\ntime.\" Furthermore, they have called TabPFN a \"foundation model\" for tabular\ndata, as it can support \"data generation, density estimation, learning reusable\nembeddings and fine-tuning\". If these statements are well-supported, TabPFN may\nhave the potential to supersede existing modeling approaches on a wide range of\nstatistical tasks, mirroring a similar revolution in other areas of artificial\nintelligence that began with the advent of large language models. In this\npaper, we provide a tailored explanation of how TabPFN works for a statistics\naudience, by emphasizing its interpretation as approximate Bayesian inference.\nWe also provide more evidence of TabPFN's \"foundation model\" capabilities: We\nshow that an out-of-the-box application of TabPFN vastly outperforms\nspecialized state-of-the-art methods for semi-supervised parameter estimation,\nprediction under covariate shift, and heterogeneous treatment effect\nestimation. We further show that TabPFN can outperform LASSO at sparse\nregression and can break a robustness-efficiency trade-off in classification.\nAll experiments can be reproduced using the code provided at\nhttps://github.com/qinglong-tian/tabpfn_study\n(https://github.com/qinglong-tian/tabpfn_study).", "authors": ["Qiong Zhang", "Yan Shuo Tan", "Qinglong Tian", "Pengfei Li"], "published_date": "2025-05-26", "title_zh": "TabPFN：一統天下的模型？", "summary_zh": "TabPFN 是一個基於 Transformer 的深度學習模型，專門處理表格數據的迴歸和分類問題。據稱，它在樣本數少於 10,000 的數據集上，性能遠超所有先前的方法，且訓練時間大幅縮短。研究者認為 TabPFN 有潛力成為表格數據的「基礎模型」，支援數據生成、密度估計、可重複使用的嵌入和微調等功能。本論文針對統計學受眾，深入解釋 TabPFN 的工作原理，將其理解為近似貝葉斯推斷，並提供更多證據證明其「基礎模型」的能力，例如在半監督參數估計、協變量偏移下的預測和異質處理效應估計等任務中，大幅超越專業的先進方法。此外，TabPFN 在稀疏迴歸中優於 LASSO，並能打破分類中的穩健性-效率權衡。", "applications": ["**醫療診斷輔助：** 基於患者的有限臨床數據（例如病史、檢查結果），快速準確地預測患者患某種疾病的風險，輔助醫生進行早期診斷和個性化治療方案制定。", "**金融風險評估：** 根據企業的財務報表和市場數據，評估其信用風險，幫助銀行和投資機構做出更明智的貸款和投資決策，降低壞帳率。", "**市場營銷推薦：** 在用戶信息有限的情況下（例如新用戶），基於歷史數據和少量用戶屬性，快速準確地預測用戶的購買偏好，提供個性化的產品推薦，提高轉化率。"], "pitch": "TabPFN 正在重塑表格數據的建模方式，如同大型語言模型改變自然語言處理領域。它具備卓越的性能和效率，特別是在小數據集上，這是許多傳統機器學習方法的痛點。其「基礎模型」的潛力，意味著它能廣泛應用於各行各業，解決各種預測和推斷問題。想像一下，一家公司可以使用 TabPFN 快速構建高效的模型，無需大量的數據和專業知識。這將極大地降低開發成本，加速創新週期。我們的投資重點將放在 TabPFN 的應用生態系統建設上，例如開發針對特定行業的預訓練模型和 API，並提供易於使用的工具和服務，賦能各行業的數據分析師和業務人員，釋放表格數據的巨大價值。這將帶來一個龐大且持續增長的市場機會。", "audio": "audios/2505.20003v1.mp3", "timestamp": "2025-05-27T09:14:39.732089"}
{"query": "Diffusion Model", "id": "2505.20131v1", "url": "http://arxiv.org/abs/2505.20131v1", "title": "MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning", "summary": "Molecular editing aims to modify a given molecule to optimize desired\nchemical properties while preserving structural similarity. However, current\napproaches typically rely on string-based or continuous representations, which\nfail to adequately capture the discrete, graph-structured nature of molecules,\nresulting in limited structural fidelity and poor controllability. In this\npaper, we propose MolEditRL, a molecular editing framework that explicitly\nintegrates structural constraints with precise property optimization.\nSpecifically, MolEditRL consists of two stages: (1) a discrete graph diffusion\nmodel pretrained to reconstruct target molecules conditioned on source\nstructures and natural language instructions; (2) an editing-aware\nreinforcement learning fine-tuning stage that further enhances property\nalignment and structural preservation by explicitly optimizing editing\ndecisions under graph constraints. For comprehensive evaluation, we construct\nMolEdit-Instruct, the largest and most property-rich molecular editing dataset,\ncomprising 3 million diverse examples spanning single- and multi-property tasks\nacross 10 chemical attributes. Experimental results demonstrate that MolEditRL\nsignificantly outperforms state-of-the-art methods in both property\noptimization accuracy and structural fidelity, achieving a 74\\% improvement in\nediting success rate while using 98\\% fewer parameters.", "authors": ["Yuanxin Zhuang", "Dazhong Shen", "Ying Sun"], "published_date": "2025-05-26", "title_zh": "MolEditRL：透過離散擴散與強化學習實現結構保留的分子編輯", "summary_zh": "MolEditRL 是一個新的分子編輯框架，它利用離散圖擴散模型和強化學習，在修改分子以優化化學特性的同時，盡可能保留原始分子的結構。與現有方法不同，MolEditRL 能夠更好地處理分子的圖結構，從而提高編輯的準確性和可控性。研究團隊創建了一個大型分子編輯數據集 MolEdit-Instruct，並證明 MolEditRL 在屬性優化和結構保真度方面顯著優於現有技術，編輯成功率提高了 74%，同時參數數量減少了 98%。", "applications": ["**藥物發現：** 基於已知藥物結構，快速優化藥效、降低副作用，加速新藥開發流程。", "**材料科學：** 修改現有材料的分子結構，以提升特定性能，例如導電性、強度或耐熱性，用於開發新一代電池或高性能複合材料。", "**農業化學：** 編輯農藥或肥料的分子結構，使其更有效、更環保，減少對環境的影響。"], "pitch": "MolEditRL 解決了分子編輯領域長期存在的結構保真度問題，這對於藥物發現和材料科學至關重要。該技術大幅提升了編輯的成功率和效率，同時顯著降低了計算資源的需求。其潛在商業價值巨大：加速新藥開發，降低研發成本；催生更具功能性的新材料，搶佔市場先機；開發更安全、更環保的農業化學產品，符合永續發展趨勢。MolEditRL 擁有極高的擴展性，可應用於廣泛的化學領域，是一個具有顛覆性潛力的投資項目，有望在相關產業產生重大影響。", "audio": "audios/2505.20131v1.mp3", "timestamp": "2025-05-27T09:14:52.231773"}
{"query": "AI", "id": "2505.20148v1", "url": "http://arxiv.org/abs/2505.20148v1", "title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents", "summary": "Spatial Planning is a crucial part in the field of spatial intelligence,\nwhich requires the understanding and planning about object arrangements in\nspace perspective. AI agents with the spatial planning ability can better adapt\nto various real-world applications, including robotic manipulation, automatic\nassembly, urban planning etc. Recent works have attempted to construct\nbenchmarks for evaluating the spatial intelligence of Multimodal Large Language\nModels (MLLMs). Nevertheless, these benchmarks primarily focus on spatial\nreasoning based on typical Visual Question-Answering (VQA) forms, which suffers\nfrom the gap between abstract spatial understanding and concrete task\nexecution. In this work, we take a step further to build a comprehensive\nbenchmark called MineAnyBuild, aiming to evaluate the spatial planning ability\nof open-world AI agents in the Minecraft game. Specifically, MineAnyBuild\nrequires an agent to generate executable architecture building plans based on\nthe given multi-modal human instructions. It involves 4,000 curated spatial\nplanning tasks and also provides a paradigm for infinitely expandable data\ncollection by utilizing rich player-generated content. MineAnyBuild evaluates\nspatial planning through four core supporting dimensions: spatial\nunderstanding, spatial reasoning, creativity, and spatial commonsense. Based on\nMineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based\nagents, revealing the severe limitations but enormous potential in their\nspatial planning abilities. We believe our MineAnyBuild will open new avenues\nfor the evaluation of spatial intelligence and help promote further development\nfor open-world AI agents capable of spatial planning.", "authors": ["Ziming Wei", "Bingqian Lin", "Zijian Jiao", "Yunshuang Nie", "Liang Ma", "Yuecheng Liu", "Yuzheng Zhuang", "Xiaodan Liang"], "published_date": "2025-05-26", "title_zh": "MineAnyBuild：開放世界AI代理空間規劃能力基準測試", "summary_zh": "這篇論文介紹了一個名為MineAnyBuild的新基準測試，旨在評估AI代理在開放世界遊戲Minecraft中，根據人類指令生成可執行建築計劃的空間規劃能力。這個基準測試包含4000個精心設計的空間規劃任務，並提供無限擴展的數據收集範例。MineAnyBuild從空間理解、空間推理、創造力和空間常識四個維度評估AI代理的空間規劃能力。研究結果顯示，現有基於多模態大型語言模型（MLLM）的代理在空間規劃方面存在嚴重局限性，但也展現了巨大的潛力。", "applications": ["**智能家居設計助手：** 根據用戶的需求（例如：房間大小、風格偏好、預算），自動生成3D家居佈局方案，提供視覺化的展示和可修改的建議。", "**自動化工廠佈局優化：** 分析生產流程和設備需求，自動規劃工廠的空間佈局，以最大化生產效率、降低物流成本和保障安全。", "**城市規劃模擬：** 在虛擬城市環境中模擬不同建築的佈局方案，評估對交通、環境和居民生活品質的影響，幫助城市規劃者做出更明智的決策。"], "pitch": "MineAnyBuild為開放世界AI代理的空間規劃能力提供了一個標準化的評估平台。這種能力不僅僅局限於遊戲，更具有廣泛的商業應用前景。想像一下，能夠根據客戶需求自動設計個性化家居佈局，或者優化工廠生產流程，降低成本、提高效率。我們相信，透過MineAnyBuild的持續發展，將能加速AI在空間規劃領域的創新，創造出巨大的商業價值。初期可聚焦於B端市場，提供定制化的空間規劃解決方案，例如為建築設計公司、房地產開發商和製造企業提供AI輔助設計服務。隨著技術成熟，未來有望拓展到C端市場，為廣大消費者提供更智能、更便捷的空間規劃工具。", "audio": "audios/2505.20148v1.mp3", "timestamp": "2025-05-27T10:15:50.667137"}
{"query": "Foundation Model", "id": "2505.20003v1", "url": "http://arxiv.org/abs/2505.20003v1", "title": "TabPFN: One Model to Rule Them All?", "summary": "Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a\ntransformer-based deep learning model for regression and classification on\ntabular data, which they claim \"outperforms all previous methods on datasets\nwith up to 10,000 samples by a wide margin, using substantially less training\ntime.\" Furthermore, they have called TabPFN a \"foundation model\" for tabular\ndata, as it can support \"data generation, density estimation, learning reusable\nembeddings and fine-tuning\". If these statements are well-supported, TabPFN may\nhave the potential to supersede existing modeling approaches on a wide range of\nstatistical tasks, mirroring a similar revolution in other areas of artificial\nintelligence that began with the advent of large language models. In this\npaper, we provide a tailored explanation of how TabPFN works for a statistics\naudience, by emphasizing its interpretation as approximate Bayesian inference.\nWe also provide more evidence of TabPFN's \"foundation model\" capabilities: We\nshow that an out-of-the-box application of TabPFN vastly outperforms\nspecialized state-of-the-art methods for semi-supervised parameter estimation,\nprediction under covariate shift, and heterogeneous treatment effect\nestimation. We further show that TabPFN can outperform LASSO at sparse\nregression and can break a robustness-efficiency trade-off in classification.\nAll experiments can be reproduced using the code provided at\nhttps://github.com/qinglong-tian/tabpfn_study\n(https://github.com/qinglong-tian/tabpfn_study).", "authors": ["Qiong Zhang", "Yan Shuo Tan", "Qinglong Tian", "Pengfei Li"], "published_date": "2025-05-26", "title_zh": "TabPFN：一個模型統治所有模型？", "summary_zh": "TabPFN是一個基於Transformer的深度學習模型，專為表格數據的迴歸和分類而設計。據稱，它在10,000個樣本以下的數據集上，性能遠超過去所有方法，且訓練時間更短。作者甚至稱其為表格數據的「基礎模型」，具有數據生成、密度估計、可重用嵌入學習和微調等能力。本文重點在統計學角度解釋TabPFN的工作原理，並將其視為近似貝葉斯推斷。實驗證明，TabPFN在半監督參數估計、協變量偏移下的預測以及異質處理效應估計等任務中，超越了專門的state-of-the-art方法。此外，它在稀疏迴歸中優於LASSO，並打破了分類中的魯棒性-效率權衡。", "applications": ["**醫療診斷輔助：** 基於病人的歷史數據（如年齡、性別、病史、檢驗結果等）快速預測疾病風險或最佳治療方案，無需針對每種疾病訓練專用模型。", "**金融信用評估：** 根據個人財務數據（如收入、信用記錄、負債等）快速評估貸款風險，提高審批效率並降低壞賬率。", "**電商個性化推薦：** 分析用戶的購物習慣、瀏覽歷史等表格數據，實時提供更精準的商品推薦，提升用戶體驗和銷售額。"], "pitch": "TabPFN代表了表格數據建模的革命性突破，將深刻改變數據科學領域。其卓越的性能、廣泛的適用性和簡化的部署方式，使其成為各行業的理想選擇。相較於傳統方法，TabPFN顯著降低了模型開發和維護的成本，並加速了數據洞察的產生。我們相信，TabPFN將成為企業利用表格數據創造價值的重要工具，並有望成為表格數據領域的基礎設施。投資TabPFN，就是投資數據驅動的未來，回報將是巨大的。", "audio": "audios/2505.20003v1.mp3", "timestamp": "2025-05-27T10:16:04.050836"}
{"query": "Diffusion Model", "id": "2505.20131v1", "url": "http://arxiv.org/abs/2505.20131v1", "title": "MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning", "summary": "Molecular editing aims to modify a given molecule to optimize desired\nchemical properties while preserving structural similarity. However, current\napproaches typically rely on string-based or continuous representations, which\nfail to adequately capture the discrete, graph-structured nature of molecules,\nresulting in limited structural fidelity and poor controllability. In this\npaper, we propose MolEditRL, a molecular editing framework that explicitly\nintegrates structural constraints with precise property optimization.\nSpecifically, MolEditRL consists of two stages: (1) a discrete graph diffusion\nmodel pretrained to reconstruct target molecules conditioned on source\nstructures and natural language instructions; (2) an editing-aware\nreinforcement learning fine-tuning stage that further enhances property\nalignment and structural preservation by explicitly optimizing editing\ndecisions under graph constraints. For comprehensive evaluation, we construct\nMolEdit-Instruct, the largest and most property-rich molecular editing dataset,\ncomprising 3 million diverse examples spanning single- and multi-property tasks\nacross 10 chemical attributes. Experimental results demonstrate that MolEditRL\nsignificantly outperforms state-of-the-art methods in both property\noptimization accuracy and structural fidelity, achieving a 74\\% improvement in\nediting success rate while using 98\\% fewer parameters.", "authors": ["Yuanxin Zhuang", "Dazhong Shen", "Ying Sun"], "published_date": "2025-05-26", "title_zh": "MolEditRL：基於離散擴散與強化學習的結構保持分子編輯", "summary_zh": "MolEditRL是一種新的分子編輯框架，旨在修改現有分子以優化其化學特性，同時保持結構相似性。它利用離散圖擴散模型，並結合強化學習微調，在滿足結構約束的同時，精確地優化目標屬性。該研究建立了包含300萬個樣本的大型分子編輯數據集MolEdit-Instruct。實驗結果表明，MolEditRL在屬性優化準確性和結構保真度方面顯著優於現有方法，编辑成功率提升了74%，且参数量减少了98%。", "applications": ["**新藥開發：** 針對已知藥物結構，在不改變其基本骨架下，調整特定基團以提高藥效、降低副作用或改善生物利用度。", "**材料科學：** 修改現有材料的分子結構，以獲得更優越的物理或化學特性，例如更高的強度、更好的導電性或更強的耐腐蝕性。", "**農藥設計：** 微調農藥分子結構，以提高其針對特定害蟲的毒性，同時降低對環境的影響和對非目標生物的毒性。"], "pitch": "MolEditRL解決了分子編輯領域長期存在的結構保真度和屬性優化之間的權衡問題。 其基於離散擴散和強化學習的方法，不僅能有效提升目標屬性，還能最大限度地保持分子結構的完整性，這對於藥物研發、材料科學等領域至關重要。 我們打造的MolEdit-Instruct數據集，更為分子編輯研究提供了豐富的資源。 MolEditRL算法體積小，性能提升顯著，有望大幅縮短新藥和新材料的研發週期，降低研發成本，具有巨大的商業潛力。 我們相信，MolEditRL將成為化學、生物和材料科學領域的重要工具，為下一代創新產品的開發奠定基礎。", "audio": "audios/2505.20131v1.mp3", "timestamp": "2025-05-27T10:16:17.094896"}
{"query": "AI", "id": "2505.20148v1", "url": "http://arxiv.org/abs/2505.20148v1", "title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents", "summary": "Spatial Planning is a crucial part in the field of spatial intelligence,\nwhich requires the understanding and planning about object arrangements in\nspace perspective. AI agents with the spatial planning ability can better adapt\nto various real-world applications, including robotic manipulation, automatic\nassembly, urban planning etc. Recent works have attempted to construct\nbenchmarks for evaluating the spatial intelligence of Multimodal Large Language\nModels (MLLMs). Nevertheless, these benchmarks primarily focus on spatial\nreasoning based on typical Visual Question-Answering (VQA) forms, which suffers\nfrom the gap between abstract spatial understanding and concrete task\nexecution. In this work, we take a step further to build a comprehensive\nbenchmark called MineAnyBuild, aiming to evaluate the spatial planning ability\nof open-world AI agents in the Minecraft game. Specifically, MineAnyBuild\nrequires an agent to generate executable architecture building plans based on\nthe given multi-modal human instructions. It involves 4,000 curated spatial\nplanning tasks and also provides a paradigm for infinitely expandable data\ncollection by utilizing rich player-generated content. MineAnyBuild evaluates\nspatial planning through four core supporting dimensions: spatial\nunderstanding, spatial reasoning, creativity, and spatial commonsense. Based on\nMineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based\nagents, revealing the severe limitations but enormous potential in their\nspatial planning abilities. We believe our MineAnyBuild will open new avenues\nfor the evaluation of spatial intelligence and help promote further development\nfor open-world AI agents capable of spatial planning.", "authors": ["Ziming Wei", "Bingqian Lin", "Zijian Jiao", "Yunshuang Nie", "Liang Ma", "Yuecheng Liu", "Yuzheng Zhuang", "Xiaodan Liang"], "published_date": "2025-05-26", "title_zh": "MineAnyBuild：開放世界AI代理的空間規劃能力評估基準", "summary_zh": "這篇論文介紹了一個名為MineAnyBuild的全新基準測試，旨在評估AI代理在開放世界環境（例如Minecraft）中的空間規劃能力。現有的基準測試大多只側重於基於視覺問答的抽象空間推理，而MineAnyBuild則更進一步，要求AI代理根據人類指令生成可執行的建築計劃。該基準測試包含四千個精心設計的空間規劃任務，並提供無限擴展數據收集的範例，可以有效評估AI代理的空間理解、推理、創造力和空間常識。研究結果顯示，現有的基於多模態大型語言模型(MLLM)的代理在空間規劃方面存在嚴重限制，但也顯示了巨大的潛力。MineAnyBuild有望促進空間智能的評估，並加速開放世界AI代理在空間規劃方面的發展。", "applications": ["**智慧家居設計:** 根據屋主需求和空間限制，自動生成家具擺放方案，最佳化空間利用，並提供3D視覺化預覽。", "**倉儲物流優化:** 在倉庫中根據貨物尺寸、重量和存取頻率，自動規劃最佳的貨物堆放方案，提高倉儲效率，降低損壞風險。", "**遊戲地圖生成:** 為遊戲設計師提供靈感，自動生成符合特定主題和難度的遊戲地圖，節省開發時間，增加遊戲內容的多樣性。"], "pitch": "MineAnyBuild提供了一個寶貴的工具，能有效評估和提升AI在空間規劃方面的能力。想像一下，未來我們能利用AI快速設計出最佳化的工廠佈局、智慧城市規劃，甚至是在複雜的環境中操控機器人完成任務。這種技術的商業價值巨大，可以廣泛應用於建築、物流、遊戲開發、機器人控制等多個領域。初期我們可以提供 MineAnyBuild 的 API 服務，讓企業評估其 AI 模型能力，並提供客製化的訓練數據，協助其 AI 模型強化空間規劃能力。長期來看，隨著 AI 模型不斷優化，我們有機會打造一個智能化的空間規劃平台，為各行業提供全方位的解決方案，搶佔市場先機，成為空間智能領域的領導者。我們的競爭優勢在於 MineAnyBuild 的全面性、可擴展性，以及我們對開放世界 AI 代理的深入理解。", "audio": "audios/2505.20148v1.mp3", "timestamp": "2025-05-27T11:12:19.354862"}
{"query": "Foundation Model", "id": "2505.20003v1", "url": "http://arxiv.org/abs/2505.20003v1", "title": "TabPFN: One Model to Rule Them All?", "summary": "Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a\ntransformer-based deep learning model for regression and classification on\ntabular data, which they claim \"outperforms all previous methods on datasets\nwith up to 10,000 samples by a wide margin, using substantially less training\ntime.\" Furthermore, they have called TabPFN a \"foundation model\" for tabular\ndata, as it can support \"data generation, density estimation, learning reusable\nembeddings and fine-tuning\". If these statements are well-supported, TabPFN may\nhave the potential to supersede existing modeling approaches on a wide range of\nstatistical tasks, mirroring a similar revolution in other areas of artificial\nintelligence that began with the advent of large language models. In this\npaper, we provide a tailored explanation of how TabPFN works for a statistics\naudience, by emphasizing its interpretation as approximate Bayesian inference.\nWe also provide more evidence of TabPFN's \"foundation model\" capabilities: We\nshow that an out-of-the-box application of TabPFN vastly outperforms\nspecialized state-of-the-art methods for semi-supervised parameter estimation,\nprediction under covariate shift, and heterogeneous treatment effect\nestimation. We further show that TabPFN can outperform LASSO at sparse\nregression and can break a robustness-efficiency trade-off in classification.\nAll experiments can be reproduced using the code provided at\nhttps://github.com/qinglong-tian/tabpfn_study\n(https://github.com/qinglong-tian/tabpfn_study).", "authors": ["Qiong Zhang", "Yan Shuo Tan", "Qinglong Tian", "Pengfei Li"], "published_date": "2025-05-26", "title_zh": "TabPFN：一統天下的模型？", "summary_zh": "Hollmann等人(Nature 637 (2025) 319-326) 發表了 TabPFN，一個基於 Transformer 的深度學習模型，用於表格數據的迴歸和分類。他們聲稱 TabPFN 在 10,000 個樣本以下的數據集上，顯著超越了以往所有方法，且訓練時間更短。他們更稱 TabPFN 為表格數據的「基礎模型」，能支援數據生成、密度估計、學習可重用的嵌入以及微調。本論文針對統計受眾，深入解釋 TabPFN 的運作原理，並強調其作為近似貝葉斯推斷的解釋。我們也提供了更多證據證明 TabPFN 的「基礎模型」能力：TabPFN 的開箱即用應用程序，在半監督參數估計、協變量偏移下的預測和異質處理效應估計方面，遠遠優於專門的最新方法。我們還表明，TabPFN 在稀疏迴歸方面可以勝過 LASSO，並且可以在分類中打破穩健性與效率的權衡。", "applications": ["**個人化醫療診斷：** 醫生可以快速使用病人的少量臨床數據，透過 TabPFN 建立個人化的疾病風險預測模型，輔助診斷和治療方案制定。", "**金融風險評估：** 金融機構可以利用 TabPFN 根據客戶的財務數據，快速建立信用評估模型，更精準地評估貸款風險，優化信貸決策。", "**零售業客戶行為分析：** 零售商可以運用 TabPFN 分析少量客戶購買紀錄和偏好數據，預測客戶的下一次購買行為，提供個人化的商品推薦和促銷活動，提升銷售額。"], "pitch": "TabPFN是一個顛覆性的表格數據AI模型，其優異的性能和廣泛的應用潛力使其成為一個極具吸引力的投資標的。它解決了傳統機器學習模型在小數據集上表現不佳的問題，並大幅縮短了模型訓練時間。我們可以將TabPFN應用於醫療、金融、零售等各個行業，提供更精準的預測和決策支持，創造巨大的商業價值。此外，TabPFN作為一個“基礎模型”，具有數據生成、密度估計等能力，可以為更多AI應用的開發提供基礎設施，進一步擴展其商業潛力。憑藉其領先的技術和廣闊的市場前景，TabPFN有望成為下一個AI獨角獸。", "audio": "audios/2505.20003v1.mp3", "timestamp": "2025-05-27T11:12:37.939081"}
{"query": "Diffusion Model", "id": "2505.20131v1", "url": "http://arxiv.org/abs/2505.20131v1", "title": "MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning", "summary": "Molecular editing aims to modify a given molecule to optimize desired\nchemical properties while preserving structural similarity. However, current\napproaches typically rely on string-based or continuous representations, which\nfail to adequately capture the discrete, graph-structured nature of molecules,\nresulting in limited structural fidelity and poor controllability. In this\npaper, we propose MolEditRL, a molecular editing framework that explicitly\nintegrates structural constraints with precise property optimization.\nSpecifically, MolEditRL consists of two stages: (1) a discrete graph diffusion\nmodel pretrained to reconstruct target molecules conditioned on source\nstructures and natural language instructions; (2) an editing-aware\nreinforcement learning fine-tuning stage that further enhances property\nalignment and structural preservation by explicitly optimizing editing\ndecisions under graph constraints. For comprehensive evaluation, we construct\nMolEdit-Instruct, the largest and most property-rich molecular editing dataset,\ncomprising 3 million diverse examples spanning single- and multi-property tasks\nacross 10 chemical attributes. Experimental results demonstrate that MolEditRL\nsignificantly outperforms state-of-the-art methods in both property\noptimization accuracy and structural fidelity, achieving a 74\\% improvement in\nediting success rate while using 98\\% fewer parameters.", "authors": ["Yuanxin Zhuang", "Dazhong Shen", "Ying Sun"], "published_date": "2025-05-26", "title_zh": "MolEditRL：透過離散擴散與強化學習進行結構保持的分子編輯", "summary_zh": "MolEditRL 是一種新的分子編輯框架，它利用離散圖擴散模型預訓練，然後透過強化學習微調，能夠在修改分子以優化化學性質的同時，保持其結構相似性。 這個方法優於現有的方法，因為它更準確地捕捉了分子的圖結構本質，從而實現了更好的結構保真度和可控性。 實驗結果顯示，MolEditRL 在屬性優化準確性和結構保真度方面都顯著超越了最先進的方法。", "applications": ["**藥物開發：** 在已知藥物結構的基礎上，對分子進行微小的修改，以提高藥效、降低副作用或改善藥代動力學性質。", "**材料科學：** 設計具有特定物理和化學性質的新材料，例如更高性能的聚合物、更有效的催化劑或更穩定的電池材料。", "**農業化學：** 研發更安全、更有效的農藥和肥料，同時保持對環境的友好性。"], "pitch": "MolEditRL 代表了分子設計領域的重大突破。 我們提出的技術能以更精確、更高效的方式編輯分子結構，同時保持結構的完整性。 這解決了傳統方法在結構保真度上的瓶頸，並為藥物開發、材料科學和農業化學等領域開闢了新的可能性。 MolEditRL 在屬性優化和結構保真度方面超越了現有技術，並且在參數效率上表現出色。 藉由大幅減少研發時程和降低成本， MolEditRL 擁有巨大的商業潛力，能夠加速新藥物和材料的上市，並在多個產業帶來革命性的改變。 我們相信 MolEditRL 將成為分子設計的產業標準，並為投資者帶來可觀的回報。", "audio": "audios/2505.20131v1.mp3", "timestamp": "2025-05-27T11:12:52.015365"}
{"query": "AI", "id": "2505.20246v1", "url": "http://arxiv.org/abs/2505.20246v1", "title": "On Path to Multimodal Historical Reasoning: HistBench and HistAgent", "summary": "Recent advances in large language models (LLMs) have led to remarkable\nprogress across domains, yet their capabilities in the humanities, particularly\nhistory, remain underexplored. Historical reasoning poses unique challenges for\nAI, involving multimodal source interpretation, temporal inference, and\ncross-linguistic analysis. While general-purpose agents perform well on many\nexisting benchmarks, they lack the domain-specific expertise required to engage\nwith historical materials and questions. To address this gap, we introduce\nHistBench, a new benchmark of 414 high-quality questions designed to evaluate\nAI's capacity for historical reasoning and authored by more than 40 expert\ncontributors. The tasks span a wide range of historical problems-from factual\nretrieval based on primary sources to interpretive analysis of manuscripts and\nimages, to interdisciplinary challenges involving archaeology, linguistics, or\ncultural history. Furthermore, the benchmark dataset spans 29 ancient and\nmodern languages and covers a wide range of historical periods and world\nregions. Finding the poor performance of LLMs and other agents on HistBench, we\nfurther present HistAgent, a history-specific agent equipped with carefully\ndesigned tools for OCR, translation, archival search, and image understanding\nin History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of\n27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online\nsearch and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%)\nand Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These\nresults highlight the limitations of existing LLMs and generalist agents and\ndemonstrate the advantages of HistAgent for historical reasoning.", "authors": ["Jiahao Qiu", "Fulian Xiao", "Yimin Wang", "Yuchen Mao", "Yijia Chen", "Xinzhe Juan", "Siran Wang", "Xuan Qi", "Tongcheng Zhang", "Zixin Yao", "Jiacheng Guo", "Yifu Lu", "Charles Argon", "Jundi Cui", "Daixin Chen", "Junran Zhou", "Shuyao Zhou", "Zhanpeng Zhou", "Ling Yang", "Shilong Liu", "Hongru Wang", "Kaixuan Huang", "Xun Jiang", "Yuming Cao", "Yue Chen", "Yunfei Chen", "Zhengyi Chen", "Ruowei Dai", "Mengqiu Deng", "Jiye Fu", "Yunting Gu", "Zijie Guan", "Zirui Huang", "Xiaoyan Ji", "Yumeng Jiang", "Delong Kong", "Haolong Li", "Jiaqi Li", "Ruipeng Li", "Tianze Li", "Zhuoran Li", "Haixia Lian", "Mengyue Lin", "Xudong Liu", "Jiayi Lu", "Jinghan Lu", "Wanyu Luo", "Ziyue Luo", "Zihao Pu", "Zhi Qiao", "Ruihuan Ren", "Liang Wan", "Ruixiang Wang", "Tianhui Wang", "Yang Wang", "Zeyu Wang", "Zihua Wang", "Yujia Wu", "Zhaoyi Wu", "Hao Xin", "Weiao Xing", "Ruojun Xiong", "Weijie Xu", "Yao Shu", "Xiao Yao", "Xiaorui Yang", "Yuchen Yang", "Nan Yi", "Jiadong Yu", "Yangyuxuan Yu", "Huiting Zeng", "Danni Zhang", "Yunjie Zhang", "Zhaoyu Zhang", "Zhiheng Zhang", "Xiaofeng Zheng", "Peirong Zhou", "Linyan Zhong", "Xiaoyin Zong", "Ying Zhao", "Zhenxin Chen", "Lin Ding", "Xiaoyu Gao", "Bingbing Gong", "Yichao Li", "Yang Liao", "Guang Ma", "Tianyuan Ma", "Xinrui Sun", "Tianyi Wang", "Han Xia", "Ruobing Xian", "Gen Ye", "Tengfei Yu", "Wentao Zhang", "Yuxi Wang", "Xi Gao", "Mengdi Wang"], "published_date": "2025-05-26", "title_zh": "邁向多模態歷史推理之路：HistBench 與 HistAgent", "summary_zh": "這篇論文介紹了一個新的歷史推理基準測試HistBench，以及一個專門為歷史研究設計的AI智能體HistAgent。現有的大型語言模型(LLMs)在歷史領域的表現不佳，因為歷史研究涉及多模態資料的解讀、時間推論和跨語言分析。HistBench包含414個高品質問題，涵蓋從文獻檢索到圖像分析等多種歷史問題，並包含29種古今語言。論文作者發現，現有的LLMs和通用智能體在HistBench上的表現很差，因此開發了HistAgent，它配備了OCR、翻譯、檔案搜索和圖像理解等專門工具。基於GPT-4o的HistAgent在HistBench上的準確率顯著優於其他LLMs和通用智能體，證明了專門設計的歷史智能體的優勢。", "applications": ["**歷史研究輔助工具：** 協助歷史學家進行文獻資料搜尋、翻譯古文獻、圖像分析等工作，加速研究進程並提供新的研究視角。", "**文化遺產保護與教育：** 透過分析歷史文獻和文物圖像，重建歷史場景和文化背景，提升博物館展覽的互動性和教育價值，並為文化遺產的數位保存提供更精確的分析。", "**個人歷史探索與學習：** 使用者可以利用這個技術探索家族歷史、解讀古老照片或文獻，更深入地了解自己的根源和歷史事件。"], "pitch": "HistAgent解決了大型語言模型在歷史研究領域表現不足的問題，透過專門的工具和資料集，顯著提升了AI在歷史推理方面的能力。這是一個具有巨大商業潛力的技術，可以應用於歷史研究、文化遺產保護和教育等領域。我們可以將HistAgent打包成B2B服務，提供給大學、博物館和研究機構，作為他們歷史研究和教學的強大輔助工具。同時，也可以開發面向C端消費者的個人歷史探索應用，滿足人們對於了解自身歷史和文化的興趣。考慮到全球範圍內歷史研究和文化遺產保護的巨大市場，HistAgent有望成為歷史領域AI應用的領導者，帶來豐厚的回報。", "audio": "audios/2505.20246v1.mp3", "timestamp": "2025-05-27T12:27:27.072976"}
{"query": "Foundation Model", "id": "2505.20202v1", "url": "http://arxiv.org/abs/2505.20202v1", "title": "PathBench: A comprehensive comparison benchmark for pathology foundation models towards precision oncology", "summary": "The emergence of pathology foundation models has revolutionized computational\nhistopathology, enabling highly accurate, generalized whole-slide image\nanalysis for improved cancer diagnosis, and prognosis assessment. While these\nmodels show remarkable potential across cancer diagnostics and prognostics,\ntheir clinical translation faces critical challenges including variability in\noptimal model across cancer types, potential data leakage in evaluation, and\nlack of standardized benchmarks. Without rigorous, unbiased evaluation, even\nthe most advanced PFMs risk remaining confined to research settings, delaying\ntheir life-saving applications. Existing benchmarking efforts remain limited by\nnarrow cancer-type focus, potential pretraining data overlaps, or incomplete\ntask coverage. We present PathBench, the first comprehensive benchmark\naddressing these gaps through: multi-center in-hourse datasets spanning common\ncancers with rigorous leakage prevention, evaluation across the full clinical\nspectrum from diagnosis to prognosis, and an automated leaderboard system for\ncontinuous model assessment. Our framework incorporates large-scale data,\nenabling objective comparison of PFMs while reflecting real-world clinical\ncomplexity. All evaluation data comes from private medical providers, with\nstrict exclusion of any pretraining usage to avoid data leakage risks. We have\ncollected 15,888 WSIs from 8,549 patients across 10 hospitals, encompassing\nover 64 diagnosis and prognosis tasks. Currently, our evaluation of 19 PFMs\nshows that Virchow2 and H-Optimus-1 are the most effective models overall. This\nwork provides researchers with a robust platform for model development and\noffers clinicians actionable insights into PFM performance across diverse\nclinical scenarios, ultimately accelerating the translation of these\ntransformative technologies into routine pathology practice.", "authors": ["Jiabo Ma", "Yingxue Xu", "Fengtao Zhou", "Yihui Wang", "Cheng Jin", "Zhengrui Guo", "Jianfeng Wu", "On Ki Tang", "Huajun Zhou", "Xi Wang", "Luyang Luo", "Zhengyu Zhang", "Du Cai", "Zizhao Gao", "Wei Wang", "Yueping Liu", "Jiankun He", "Jing Cui", "Zhenhui Li", "Jing Zhang", "Feng Gao", "Xiuming Zhang", "Li Liang", "Ronald Cheong Kin Chan", "Zhe Wang", "Hao Chen"], "published_date": "2025-05-26", "title_zh": "PathBench：一個針對病理學基礎模型，以邁向精準腫瘤學的全面性比較基準", "summary_zh": "病理學基礎模型正改變病理影像分析，提升癌症診斷與預後評估。然而，模型在不同癌症種類的最佳選擇差異大、評估中可能存在資料洩漏、缺乏標準化基準等挑戰阻礙其臨床應用。PathBench透過涵蓋多種常見癌症的多中心內部資料集、嚴格的資料洩漏防範、涵蓋診斷到預後的完整臨床評估，以及自動化的排行榜系統，來填補現有基準的不足。PathBench 使用來自私立醫療機構的大規模資料進行客觀比較，反應真實臨床複雜性，並避免資料洩漏風險。目前評估了 19 個模型，Virchow2 和 H-Optimus-1 表現最佳。PathBench 為研究人員提供模型開發的強大平台，也為臨床醫師提供 PFM 在不同臨床情境下的可行見解，加速這些變革性技術轉化為常規病理學實踐。", "applications": ["**遠距病理診斷諮詢：** 在醫療資源匱乏地區，專家可利用 PathBench 評估過的最佳模型，遠端協助基層醫師進行病理切片分析，提升診斷準確性，節省患者就醫時間和成本。", "**個人化癌症治療方案推薦：** 根據 PathBench 的評估結果，針對特定癌症種類和患者特性，推薦最適合的病理學基礎模型，輔助醫生制定更精準的個人化治療方案，提高治療效果。", "**新藥研發加速：** 藥廠可使用 PathBench 評估的病理學基礎模型，快速分析大量病理切片資料，識別潛在的藥物作用靶點，加速新藥研發進程，降低研發成本。"], "pitch": "PathBench 是病理學基礎模型領域的 Rosetta Stone，解決了模型評估標準化的痛點，加速 AI 病理學的臨床落地。我們提供一個客觀、全面的基準平台，讓醫療機構、藥廠、研究機構都能夠輕鬆比較、選擇最適合自身需求的模型。這不僅能提升診斷準確性、優化治療方案，還能加速新藥研發。PathBench 的商業價值體現在降低醫療成本、提高醫療效率、改善患者預後等方面。透過提供評估服務、模型諮詢服務、以及與藥廠合作進行新藥研發，PathBench 有潛力成為精準腫瘤學領域的領頭羊，創造巨大的市場價值。", "audio": "audios/2505.20202v1.mp3", "timestamp": "2025-05-27T12:27:44.084125"}
{"query": "Diffusion Model", "id": "2505.20171v1", "url": "http://arxiv.org/abs/2505.20171v1", "title": "Long-Context State-Space Video World Models", "summary": "Video diffusion models have recently shown promise for world modeling through\nautoregressive frame prediction conditioned on actions. However, they struggle\nto maintain long-term memory due to the high computational cost associated with\nprocessing extended sequences in attention layers. To overcome this limitation,\nwe propose a novel architecture leveraging state-space models (SSMs) to extend\ntemporal memory without compromising computational efficiency. Unlike previous\napproaches that retrofit SSMs for non-causal vision tasks, our method fully\nexploits the inherent advantages of SSMs in causal sequence modeling. Central\nto our design is a block-wise SSM scanning scheme, which strategically trades\noff spatial consistency for extended temporal memory, combined with dense local\nattention to ensure coherence between consecutive frames. We evaluate the\nlong-term memory capabilities of our model through spatial retrieval and\nreasoning tasks over extended horizons. Experiments on Memory Maze and\nMinecraft datasets demonstrate that our approach surpasses baselines in\npreserving long-range memory, while maintaining practical inference speeds\nsuitable for interactive applications.", "authors": ["Ryan Po", "Yotam Nitzan", "Richard Zhang", "Berlin Chen", "Tri Dao", "Eli Shechtman", "Gordon Wetzstein", "Xun Huang"], "published_date": "2025-05-26", "title_zh": "長上下文狀態空間影片世界模型", "summary_zh": "現有的影片擴散模型在世界建模方面表現出潛力，透過以動作為條件的自迴歸幀預測來實現。然而，由於注意力層處理長序列的計算成本高昂，它們難以維持長期記憶。為了解決這個限制，我們提出了一種新的架構，利用狀態空間模型（SSM）來擴展時間記憶，而不會犧牲計算效率。與先前為非因果視覺任務改裝SSM的方法不同，我們的方法充分利用了SSM在因果序列建模中的固有優勢。我們設計的核心是一種塊狀SSM掃描方案，它策略性地權衡空間一致性以獲得更長的時間記憶，並結合密集的局部注意力來確保連續幀之間的一致性。我們通過對長範圍的空間檢索和推理任務來評估我們模型的長期記憶能力。在Memory Maze和Minecraft數據集上的實驗表明，我們的方法在保持遠程記憶方面超越了基線，同時保持了適用於交互式應用程序的實際推論速度。", "applications": ["**更逼真的遊戲AI：** 遊戲中的NPC可以根據更長時間的記憶來規劃行動，例如記得玩家之前在哪裡藏過，並進行相應的搜索，提供更真實、更具挑戰性的遊戲體驗。", "**自動駕駛的長期規劃：** 自動駕駛系統可以利用長期記憶預測其他車輛或行人的移動軌跡，從而避免潛在的危險，並更安全地規劃行駛路線，例如記住特定路口過去的交通流量模式。", "**機器人輔助的長期照護：** 照護機器人可以記住病患的習慣和喜好，並根據這些記憶提供個性化的照護服務，例如在病人平時喝藥的時間提醒他們服藥。"], "pitch": "我們正在開發一種突破性的影片世界模型，它利用狀態空間模型克服了現有技術在長期記憶方面的限制。我們的技術能夠以更高的效率和更低的成本，對長序列影片進行推理和預測。這項技術的潛在商業價值巨大，可以應用於遊戲、自動駕駛、機器人、智慧城市等眾多領域。我們相信，我們的技術將徹底改變機器感知世界的方式，並為下一代智能系統奠定基礎。特別是，我們在保持推論速度的前提下，提升了長期記憶的性能，這使其特別適合交互式應用，例如在虛擬實境和增強現實環境中提供更流暢、更自然的互動體驗。我們正在尋找投資者，共同將這項技術推向市場，並共同分享由此帶來的巨大收益。", "audio": "audios/2505.20171v1.mp3", "timestamp": "2025-05-27T12:28:00.796151"}
{"query": "AI", "id": "2505.20148v1", "url": "http://arxiv.org/abs/2505.20148v1", "title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents", "summary": "Spatial Planning is a crucial part in the field of spatial intelligence,\nwhich requires the understanding and planning about object arrangements in\nspace perspective. AI agents with the spatial planning ability can better adapt\nto various real-world applications, including robotic manipulation, automatic\nassembly, urban planning etc. Recent works have attempted to construct\nbenchmarks for evaluating the spatial intelligence of Multimodal Large Language\nModels (MLLMs). Nevertheless, these benchmarks primarily focus on spatial\nreasoning based on typical Visual Question-Answering (VQA) forms, which suffers\nfrom the gap between abstract spatial understanding and concrete task\nexecution. In this work, we take a step further to build a comprehensive\nbenchmark called MineAnyBuild, aiming to evaluate the spatial planning ability\nof open-world AI agents in the Minecraft game. Specifically, MineAnyBuild\nrequires an agent to generate executable architecture building plans based on\nthe given multi-modal human instructions. It involves 4,000 curated spatial\nplanning tasks and also provides a paradigm for infinitely expandable data\ncollection by utilizing rich player-generated content. MineAnyBuild evaluates\nspatial planning through four core supporting dimensions: spatial\nunderstanding, spatial reasoning, creativity, and spatial commonsense. Based on\nMineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based\nagents, revealing the severe limitations but enormous potential in their\nspatial planning abilities. We believe our MineAnyBuild will open new avenues\nfor the evaluation of spatial intelligence and help promote further development\nfor open-world AI agents capable of spatial planning.", "authors": ["Ziming Wei", "Bingqian Lin", "Zijian Jiao", "Yunshuang Nie", "Liang Ma", "Yuecheng Liu", "Yuzheng Zhuang", "Xiaodan Liang"], "published_date": "2025-05-26", "title_zh": "MineAnyBuild：開放世界AI代理空間規劃能力基準測試", "summary_zh": "這篇論文介紹了一個名為MineAnyBuild的全新基準測試，用來評估AI代理在開放世界環境（Minecraft）中的空間規劃能力。現有的基準測試多集中在視覺問答形式的空間推理，與實際任務執行存在差距。MineAnyBuild要求AI根據多模態指令生成可執行的建築規劃，涵蓋空間理解、空間推理、創造力和空間常識四個面向。研究團隊利用該基準測試評估了現有基於MLLM的代理，發現它們在空間規劃能力方面存在局限，但也展示了巨大的潛力。MineAnyBuild旨在推動開放世界AI代理在空間規劃能力方面的發展。", "applications": ["**智慧家庭設計：** AI根據使用者描述和偏好，自動生成住宅內部設計方案，並考慮到空間利用率、動線規劃和家具擺放等因素。", "**城市規劃模擬：** 在城市規劃初期，AI可以根據人口密度、交通流量、環境影響等數據，生成多種城市規劃方案，並進行模擬評估，幫助規劃師做出更優決策。", "**虛擬實境遊戲關卡設計：** AI可以自動生成多樣化的VR遊戲關卡，包含複雜的建築結構和環境佈局，為玩家提供更豐富的遊戲體驗。"], "pitch": "我們正在開發MineAnyBuild，一個針對開放世界AI代理的空間規劃能力基準測試。現有市場缺乏有效的評估工具來衡量AI在複雜環境中的空間規劃能力，這阻礙了相關技術的發展。MineAnyBuild填補了這個空白，可以精準評估AI的空間理解、推理、創造力和常識。其潛在商業價值體現在以下幾個方面：\n\n*   **加速AI開發：** 提供標準化的評估工具，幫助開發者更高效地訓練和優化AI模型。\n*   **提升產品競爭力：** 使AI代理在智慧家居、自動駕駛、機器人等領域更具競爭力，能更好地理解和適應真實世界。\n*   **開創新的應用場景：** 促進AI在城市規劃、遊戲開發、建築設計等領域的應用創新，帶來顛覆性的解決方案。\n\n我們相信，MineAnyBuild將成為空間智能領域的重要推動力量，為AI技術的發展帶來巨大的商業價值。", "audio": "audios/2505.20148v1.mp3", "timestamp": "2025-05-27T13:27:59.637373"}
{"query": "Foundation Model", "id": "2505.20003v1", "url": "http://arxiv.org/abs/2505.20003v1", "title": "TabPFN: One Model to Rule Them All?", "summary": "Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a\ntransformer-based deep learning model for regression and classification on\ntabular data, which they claim \"outperforms all previous methods on datasets\nwith up to 10,000 samples by a wide margin, using substantially less training\ntime.\" Furthermore, they have called TabPFN a \"foundation model\" for tabular\ndata, as it can support \"data generation, density estimation, learning reusable\nembeddings and fine-tuning\". If these statements are well-supported, TabPFN may\nhave the potential to supersede existing modeling approaches on a wide range of\nstatistical tasks, mirroring a similar revolution in other areas of artificial\nintelligence that began with the advent of large language models. In this\npaper, we provide a tailored explanation of how TabPFN works for a statistics\naudience, by emphasizing its interpretation as approximate Bayesian inference.\nWe also provide more evidence of TabPFN's \"foundation model\" capabilities: We\nshow that an out-of-the-box application of TabPFN vastly outperforms\nspecialized state-of-the-art methods for semi-supervised parameter estimation,\nprediction under covariate shift, and heterogeneous treatment effect\nestimation. We further show that TabPFN can outperform LASSO at sparse\nregression and can break a robustness-efficiency trade-off in classification.\nAll experiments can be reproduced using the code provided at\nhttps://github.com/qinglong-tian/tabpfn_study\n(https://github.com/qinglong-tian/tabpfn_study).", "authors": ["Qiong Zhang", "Yan Shuo Tan", "Qinglong Tian", "Pengfei Li"], "published_date": "2025-05-26", "title_zh": "TabPFN：一個模型統治一切？", "summary_zh": "Hollmann 等人在《自然》雜誌上發表了一篇論文，介紹了 TabPFN，這是一種基於 Transformer 的深度學習模型，用於處理表格數據的迴歸和分類問題。他們聲稱 TabPFN 在數據集大小最多為 10,000 個樣本的情況下，性能遠遠超過所有先前的方法，並且訓練時間顯著縮短。他們還稱 TabPFN 為表格數據的“基礎模型”，因為它可以支持數據生成、密度估計、學習可重用的嵌入和微調。本文通過強調其作為近似貝葉斯推斷的解釋，為統計學界的受眾提供了 TabPFN 工作原理的量身定制的解釋。我們還提供了更多 TabPFN “基礎模型”能力的證據：我們證明，TabPFN 的開箱即用應用程序遠遠優於半監督參數估計、協變量偏移下的預測和異質處理效應估計方面的專業最先進方法。我們進一步證明，TabPFN 在稀疏迴歸方面可以勝過 LASSO，並且可以打破分類中的魯棒性-效率權衡。", "applications": ["**個人化醫療診斷：** 基於病患的少量病歷數據（例如血液檢查結果、家族病史等），快速且準確地預測患者患某種疾病的風險，並提供初步的治療建議，幫助醫生更有效地做出決策。", "**金融信貸風險評估：** 根據個人有限的信用歷史、社經背景等數據，快速評估貸款申請人的信用風險，協助銀行或金融機構做出更精準的放貸決策，降低壞帳率。", "**市場行銷活動優化：** 在市場活動初期，基於少量用戶數據（例如點擊行為、瀏覽記錄等），快速預測用戶對不同廣告或產品的偏好，從而實現更有效的個性化推薦，提高轉換率。"], "pitch": "想像一下，你只需要一個模型，就能處理所有表格數據相關的預測問題，而且性能遠超現有方法，訓練速度還更快！TabPFN 正是這樣一款革命性的產品。它不僅在小數據集上表現出色，還能勝任數據生成、嵌入學習等更複雜的任務。我們可以利用 TabPFN 打造下一代的 AI 解決方案，大幅降低開發成本，加速產品上市時間。例如，在金融、醫療等行業，TabPFN 可以顯著提高預測準確性，降低風險，帶來巨大的商業價值。我們相信 TabPFN 將會成為表格數據領域的基礎設施，重塑數據科學的格局。現在投資，你將有機會參與這場革命，共同分享 TabPFN 所帶來的巨大潛力。", "audio": "audios/2505.20003v1.mp3", "timestamp": "2025-05-27T13:28:20.665120"}
{"query": "Diffusion Model", "id": "2505.20131v1", "url": "http://arxiv.org/abs/2505.20131v1", "title": "MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning", "summary": "Molecular editing aims to modify a given molecule to optimize desired\nchemical properties while preserving structural similarity. However, current\napproaches typically rely on string-based or continuous representations, which\nfail to adequately capture the discrete, graph-structured nature of molecules,\nresulting in limited structural fidelity and poor controllability. In this\npaper, we propose MolEditRL, a molecular editing framework that explicitly\nintegrates structural constraints with precise property optimization.\nSpecifically, MolEditRL consists of two stages: (1) a discrete graph diffusion\nmodel pretrained to reconstruct target molecules conditioned on source\nstructures and natural language instructions; (2) an editing-aware\nreinforcement learning fine-tuning stage that further enhances property\nalignment and structural preservation by explicitly optimizing editing\ndecisions under graph constraints. For comprehensive evaluation, we construct\nMolEdit-Instruct, the largest and most property-rich molecular editing dataset,\ncomprising 3 million diverse examples spanning single- and multi-property tasks\nacross 10 chemical attributes. Experimental results demonstrate that MolEditRL\nsignificantly outperforms state-of-the-art methods in both property\noptimization accuracy and structural fidelity, achieving a 74\\% improvement in\nediting success rate while using 98\\% fewer parameters.", "authors": ["Yuanxin Zhuang", "Dazhong Shen", "Ying Sun"], "published_date": "2025-05-26", "title_zh": "MolEditRL：透過離散擴散與強化學習實現結構保留的分子編輯", "summary_zh": "這篇論文提出一個名為 MolEditRL 的新分子編輯框架。現有方法通常難以兼顧結構相似性與化學性質優化。MolEditRL 結合了離散圖擴散模型和強化學習，一方面確保修改後的分子結構與原始分子相似，另一方面又可以精準地調整化學性質。研究團隊構建了一個大型分子編輯數據集 MolEdit-Instruct，包含300萬個範例。實驗結果顯示，MolEditRL 在性質優化和結構保真度上都顯著優於現有技術，編輯成功率提高了 74%，同時參數減少了 98%。", "applications": ["**新藥開發：** 針對已知的藥物分子進行微調，優化藥效、降低副作用，快速開發新藥。", "**材料科學：** 調整材料分子的結構，提升材料的穩定性、導電性等關鍵性能，開發新型材料。", "**農業化學：** 修改農藥分子，提高農藥的專一性和降解速度，減少對環境的影響，研發更安全高效的農藥。"], "pitch": "MolEditRL 代表了分子編輯領域的重大突破，透過結合離散擴散模型與強化學習，顯著提升了分子編輯的效率和精確度。其在藥物開發、材料科學、農業化學等領域擁有巨大的應用潛力。與現有方法相比，MolEditRL 在提升成功率的同時，大幅降低了計算成本。我們相信，MolEditRL 將加速新材料和新藥的發現，並為相關產業帶來顛覆性的變革。數據集的公開也將推動學術界和產業界的共同發展。此技術的商業價值體現在更快的研發速度、更低的研發成本和更廣闊的市場前景，是極具潛力的投資標的。", "audio": "audios/2505.20131v1.mp3", "timestamp": "2025-05-27T13:28:34.958615"}
{"query": "AI", "id": "2505.20246v1", "url": "http://arxiv.org/abs/2505.20246v1", "title": "On Path to Multimodal Historical Reasoning: HistBench and HistAgent", "summary": "Recent advances in large language models (LLMs) have led to remarkable\nprogress across domains, yet their capabilities in the humanities, particularly\nhistory, remain underexplored. Historical reasoning poses unique challenges for\nAI, involving multimodal source interpretation, temporal inference, and\ncross-linguistic analysis. While general-purpose agents perform well on many\nexisting benchmarks, they lack the domain-specific expertise required to engage\nwith historical materials and questions. To address this gap, we introduce\nHistBench, a new benchmark of 414 high-quality questions designed to evaluate\nAI's capacity for historical reasoning and authored by more than 40 expert\ncontributors. The tasks span a wide range of historical problems-from factual\nretrieval based on primary sources to interpretive analysis of manuscripts and\nimages, to interdisciplinary challenges involving archaeology, linguistics, or\ncultural history. Furthermore, the benchmark dataset spans 29 ancient and\nmodern languages and covers a wide range of historical periods and world\nregions. Finding the poor performance of LLMs and other agents on HistBench, we\nfurther present HistAgent, a history-specific agent equipped with carefully\ndesigned tools for OCR, translation, archival search, and image understanding\nin History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of\n27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online\nsearch and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%)\nand Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These\nresults highlight the limitations of existing LLMs and generalist agents and\ndemonstrate the advantages of HistAgent for historical reasoning.", "authors": ["Jiahao Qiu", "Fulian Xiao", "Yimin Wang", "Yuchen Mao", "Yijia Chen", "Xinzhe Juan", "Siran Wang", "Xuan Qi", "Tongcheng Zhang", "Zixin Yao", "Jiacheng Guo", "Yifu Lu", "Charles Argon", "Jundi Cui", "Daixin Chen", "Junran Zhou", "Shuyao Zhou", "Zhanpeng Zhou", "Ling Yang", "Shilong Liu", "Hongru Wang", "Kaixuan Huang", "Xun Jiang", "Yuming Cao", "Yue Chen", "Yunfei Chen", "Zhengyi Chen", "Ruowei Dai", "Mengqiu Deng", "Jiye Fu", "Yunting Gu", "Zijie Guan", "Zirui Huang", "Xiaoyan Ji", "Yumeng Jiang", "Delong Kong", "Haolong Li", "Jiaqi Li", "Ruipeng Li", "Tianze Li", "Zhuoran Li", "Haixia Lian", "Mengyue Lin", "Xudong Liu", "Jiayi Lu", "Jinghan Lu", "Wanyu Luo", "Ziyue Luo", "Zihao Pu", "Zhi Qiao", "Ruihuan Ren", "Liang Wan", "Ruixiang Wang", "Tianhui Wang", "Yang Wang", "Zeyu Wang", "Zihua Wang", "Yujia Wu", "Zhaoyi Wu", "Hao Xin", "Weiao Xing", "Ruojun Xiong", "Weijie Xu", "Yao Shu", "Xiao Yao", "Xiaorui Yang", "Yuchen Yang", "Nan Yi", "Jiadong Yu", "Yangyuxuan Yu", "Huiting Zeng", "Danni Zhang", "Yunjie Zhang", "Zhaoyu Zhang", "Zhiheng Zhang", "Xiaofeng Zheng", "Peirong Zhou", "Linyan Zhong", "Xiaoyin Zong", "Ying Zhao", "Zhenxin Chen", "Lin Ding", "Xiaoyu Gao", "Bingbing Gong", "Yichao Li", "Yang Liao", "Guang Ma", "Tianyuan Ma", "Xinrui Sun", "Tianyi Wang", "Han Xia", "Ruobing Xian", "Gen Ye", "Tengfei Yu", "Wentao Zhang", "Yuxi Wang", "Xi Gao", "Mengdi Wang"], "published_date": "2025-05-26", "title_zh": "邁向多模態歷史推理之路：HistBench與HistAgent", "summary_zh": "這篇論文介紹了HistBench，一個專門為評估AI歷史推理能力設計的基準測試集，包含414個高品質問題，涵蓋多種歷史領域，並涉及多種語言。研究發現，現有的通用型大型語言模型(LLM)在HistBench上的表現不佳。為此，研究團隊開發了HistAgent，一個專為歷史研究設計的AI智能體，配備OCR、翻譯、檔案搜索和圖像理解等工具。HistAgent在HistBench上的表現明顯優於通用型LLM和其他智能體，凸顯了專門設計的AI智能體在歷史推理方面的優勢。", "applications": ["**歷史文獻數位化與研究加速：**幫助歷史學家快速分析大量文獻資料，包含手稿、古籍、圖片等，自動翻譯不同語言的資料，並提供歷史事件的關聯性分析，節省研究時間。", "**博物館導覽與互動體驗：**開發智能博物館導覽App，根據用戶興趣提供客製化的歷史故事，結合AR/VR技術，讓用戶身歷其境地體驗歷史場景，並回答用戶提出的歷史問題。", "**歷史教育遊戲與模擬：**創建沉浸式的歷史教育遊戲，讓學生扮演歷史人物，參與歷史事件，通過遊戲互動學習歷史知識，並培養他們的批判性思維能力。"], "pitch": "HistBench和HistAgent的出現，標誌著AI在歷史研究領域邁出了重要一步。HistBench提供了一個標準化的評估平台，可以客觀衡量AI的歷史推理能力。HistAgent則展示了專門設計的AI智能體在處理複雜歷史問題方面的巨大潛力。考慮到全球歷史文獻的數位化需求，以及民眾對於歷史知識的濃厚興趣，HistAgent具有巨大的商業潛力。我們可以將HistAgent應用於文獻數位化服務、歷史教育產品、博物館智能導覽系統等領域，並通過訂閱服務或授權模式獲利。更長遠來看，HistAgent可以成為一個強大的知識引擎，為學術研究、文化傳播和商業應用提供支持。我們正在尋找投資者，共同開發和推廣HistAgent，打造歷史AI領域的領導者。", "audio": "audios/2505.20246v1.mp3", "timestamp": "2025-05-27T14:13:25.741762"}
{"query": "Foundation Model", "id": "2505.20202v1", "url": "http://arxiv.org/abs/2505.20202v1", "title": "PathBench: A comprehensive comparison benchmark for pathology foundation models towards precision oncology", "summary": "The emergence of pathology foundation models has revolutionized computational\nhistopathology, enabling highly accurate, generalized whole-slide image\nanalysis for improved cancer diagnosis, and prognosis assessment. While these\nmodels show remarkable potential across cancer diagnostics and prognostics,\ntheir clinical translation faces critical challenges including variability in\noptimal model across cancer types, potential data leakage in evaluation, and\nlack of standardized benchmarks. Without rigorous, unbiased evaluation, even\nthe most advanced PFMs risk remaining confined to research settings, delaying\ntheir life-saving applications. Existing benchmarking efforts remain limited by\nnarrow cancer-type focus, potential pretraining data overlaps, or incomplete\ntask coverage. We present PathBench, the first comprehensive benchmark\naddressing these gaps through: multi-center in-hourse datasets spanning common\ncancers with rigorous leakage prevention, evaluation across the full clinical\nspectrum from diagnosis to prognosis, and an automated leaderboard system for\ncontinuous model assessment. Our framework incorporates large-scale data,\nenabling objective comparison of PFMs while reflecting real-world clinical\ncomplexity. All evaluation data comes from private medical providers, with\nstrict exclusion of any pretraining usage to avoid data leakage risks. We have\ncollected 15,888 WSIs from 8,549 patients across 10 hospitals, encompassing\nover 64 diagnosis and prognosis tasks. Currently, our evaluation of 19 PFMs\nshows that Virchow2 and H-Optimus-1 are the most effective models overall. This\nwork provides researchers with a robust platform for model development and\noffers clinicians actionable insights into PFM performance across diverse\nclinical scenarios, ultimately accelerating the translation of these\ntransformative technologies into routine pathology practice.", "authors": ["Jiabo Ma", "Yingxue Xu", "Fengtao Zhou", "Yihui Wang", "Cheng Jin", "Zhengrui Guo", "Jianfeng Wu", "On Ki Tang", "Huajun Zhou", "Xi Wang", "Luyang Luo", "Zhengyu Zhang", "Du Cai", "Zizhao Gao", "Wei Wang", "Yueping Liu", "Jiankun He", "Jing Cui", "Zhenhui Li", "Jing Zhang", "Feng Gao", "Xiuming Zhang", "Li Liang", "Ronald Cheong Kin Chan", "Zhe Wang", "Hao Chen"], "published_date": "2025-05-26", "title_zh": "PathBench：針對精準腫瘤學的病理學基礎模型全面比較基準", "summary_zh": "病理學基礎模型正在改變計算組織病理學，提升癌症診斷和預後評估的準確性和通用性。然而，不同癌症類型最佳模型的多樣性、評估中潛在的數據洩漏以及缺乏標準化基準等問題阻礙了它們的臨床應用。PathBench 是一個綜合性基準，它通過涵蓋常見癌症的多中心內部數據集，並嚴格防止數據洩漏，以及評估從診斷到預後的完整臨床譜系，來彌補這些差距。PathBench 提供了一個客觀的模型比較平台，反映了真實世界的臨床複雜性，包含來自 10 家醫院的 15,888 張 WSI 和 8,549 名患者的數據，涵蓋 64 個診斷和預後任務。目前評估了 19 個模型，Virchow2 和 H-Optimus-1 整體表現最佳。PathBench 為研究人員提供了一個強大的模型開發平台，並為臨床醫生提供了 PFM 在不同臨床場景中的性能資訊，最終加速了這些變革性技術在常規病理學實踐中的應用。", "applications": ["遠程病理學診斷：在病理學家稀缺的地區，利用 PathBench 優化過的模型，提供遠程精準的癌症診斷和預後評估。", "個性化癌症治療方案推薦：基於 PathBench 的評估結果，針對不同患者的癌症類型和病理特徵，推薦最佳的治療方案。", "病理學工作流程自動化：利用 PathBench 認證的 PFM，自動篩選高風險病例，減少病理學家的工作量，提高診斷效率。"], "pitch": "PathBench 是一個革命性的病理學基礎模型評估平台，它解決了目前癌症診斷和預後領域的關鍵痛點：缺乏標準化和客觀的評估基準。通過提供多中心、多癌種、嚴格防洩漏的數據集以及全面的評估流程，PathBench 能夠幫助醫療機構快速識別和部署最適合其需求的 AI 模型，大幅提高診斷準確性和效率，降低醫療成本，並最終改善患者的治療效果。我們的商業模式將包括提供評估報告、模型優化服務和合作開發個性化醫療解決方案。在這個 AI 醫療的藍海市場，PathBench 將成為推動精準腫瘤學發展的關鍵引擎，具有巨大的商業潛力和社會價值。", "audio": "audios/2505.20202v1.mp3", "timestamp": "2025-05-27T14:13:41.424533"}
{"query": "Diffusion Model", "id": "2505.20171v1", "url": "http://arxiv.org/abs/2505.20171v1", "title": "Long-Context State-Space Video World Models", "summary": "Video diffusion models have recently shown promise for world modeling through\nautoregressive frame prediction conditioned on actions. However, they struggle\nto maintain long-term memory due to the high computational cost associated with\nprocessing extended sequences in attention layers. To overcome this limitation,\nwe propose a novel architecture leveraging state-space models (SSMs) to extend\ntemporal memory without compromising computational efficiency. Unlike previous\napproaches that retrofit SSMs for non-causal vision tasks, our method fully\nexploits the inherent advantages of SSMs in causal sequence modeling. Central\nto our design is a block-wise SSM scanning scheme, which strategically trades\noff spatial consistency for extended temporal memory, combined with dense local\nattention to ensure coherence between consecutive frames. We evaluate the\nlong-term memory capabilities of our model through spatial retrieval and\nreasoning tasks over extended horizons. Experiments on Memory Maze and\nMinecraft datasets demonstrate that our approach surpasses baselines in\npreserving long-range memory, while maintaining practical inference speeds\nsuitable for interactive applications.", "authors": ["Ryan Po", "Yotam Nitzan", "Richard Zhang", "Berlin Chen", "Tri Dao", "Eli Shechtman", "Gordon Wetzstein", "Xun Huang"], "published_date": "2025-05-26", "title_zh": "長上下文狀態空間視訊世界模型", "summary_zh": "現有的視訊擴散模型在透過自迴歸框架預測來實現世界建模方面表現出潛力，但由於注意力層處理長序列的高計算成本，難以維持長期記憶。為了解決這個限制，我們提出了一種新穎的架構，利用狀態空間模型 (SSM) 來擴展時間記憶，且不影響計算效率。與先前為非因果視覺任務改造SSM的方法不同，我們的方法充分利用了SSM在因果序列建模中的固有優勢。 我們設計的核心是一個分塊式SSM掃描方案，它策略性地以空間一致性換取更長的時間記憶，並結合密集的局部注意力以確保連續幀之間的連貫性。我們透過空間檢索和在更長範圍內進行推理的任務來評估我們模型的長期記憶能力。在Memory Maze和Minecraft數據集上的實驗表明，我們的方法在保持長程記憶方面優於基準模型，同時保持了適用於互動式應用程式的實用推論速度。", "applications": ["**更逼真的遊戲 AI：** 讓遊戲中的 NPC 能夠記住玩家過去的行為，並根據長期互動歷史做出更智能、更人性化的反應，例如記住玩家幫助過它，並在之後回報。", "**智慧型家庭監控：** 建立一個能理解環境長期變化的智慧監控系統，例如能辨識長時間沒出現的家庭成員，或偵測到異常的活動模式（例如反覆進出房間，且每次都攜帶特定物品）。", "**自動駕駛汽車的預測性駕駛：** 讓自動駕駛汽車能夠根據過去的交通模式和駕駛行為，預測其他車輛或行人的下一步行動，提高駕駛安全性並優化路線規劃。"], "pitch": "我們正在開發下一代視訊理解技術，它超越了現有的模型，能夠理解和預測更長的時間跨度。這項技術的基礎是我們獨特的長上下文狀態空間視訊世界模型，它解決了現有模型在處理長期依賴關係時遇到的計算瓶頸。我們的模型在遊戲AI、智慧監控和自動駕駛等領域具有巨大的商業潛力。試想一下，一個可以真正理解環境並預測事件的世界模型，將如何徹底改變這些行業。我們尋求投資，以加速我們的研發工作，並將這項突破性的技術推向市場，成為視訊智能領域的領導者。", "audio": "audios/2505.20171v1.mp3", "timestamp": "2025-05-27T14:14:07.213438"}
{"query": "AI", "id": "2505.20148v1", "url": "http://arxiv.org/abs/2505.20148v1", "title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents", "summary": "Spatial Planning is a crucial part in the field of spatial intelligence,\nwhich requires the understanding and planning about object arrangements in\nspace perspective. AI agents with the spatial planning ability can better adapt\nto various real-world applications, including robotic manipulation, automatic\nassembly, urban planning etc. Recent works have attempted to construct\nbenchmarks for evaluating the spatial intelligence of Multimodal Large Language\nModels (MLLMs). Nevertheless, these benchmarks primarily focus on spatial\nreasoning based on typical Visual Question-Answering (VQA) forms, which suffers\nfrom the gap between abstract spatial understanding and concrete task\nexecution. In this work, we take a step further to build a comprehensive\nbenchmark called MineAnyBuild, aiming to evaluate the spatial planning ability\nof open-world AI agents in the Minecraft game. Specifically, MineAnyBuild\nrequires an agent to generate executable architecture building plans based on\nthe given multi-modal human instructions. It involves 4,000 curated spatial\nplanning tasks and also provides a paradigm for infinitely expandable data\ncollection by utilizing rich player-generated content. MineAnyBuild evaluates\nspatial planning through four core supporting dimensions: spatial\nunderstanding, spatial reasoning, creativity, and spatial commonsense. Based on\nMineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based\nagents, revealing the severe limitations but enormous potential in their\nspatial planning abilities. We believe our MineAnyBuild will open new avenues\nfor the evaluation of spatial intelligence and help promote further development\nfor open-world AI agents capable of spatial planning.", "authors": ["Ziming Wei", "Bingqian Lin", "Zijian Jiao", "Yunshuang Nie", "Liang Ma", "Yuecheng Liu", "Yuzheng Zhuang", "Xiaodan Liang"], "published_date": "2025-05-26", "title_zh": "MineAnyBuild：開放世界 AI 代理的空間規劃能力基準測試", "summary_zh": "這篇論文提出了一個名為 MineAnyBuild 的新基準測試，用於評估開放世界 AI 代理在 Minecraft 遊戲中的空間規劃能力。 現有的測試主要集中在視覺問答形式的空間推理，與實際任務執行存在差距。 MineAnyBuild 旨在填補這一空白，要求 AI 代理根據多模態的人類指令生成可執行的建築規劃。 它包含 4,000 個精選的空間規劃任務，並且可以透過豐富的玩家生成內容無限擴展資料集。 該基準測試從空間理解、空間推理、創造力和空間常識四個核心維度評估空間規劃能力。 研究人員使用 MineAnyBuild 對現有的基於多模態大型語言模型的代理進行了全面評估，揭示了它們在空間規劃能力方面的嚴重局限性，但也看到了巨大的潛力。 這個基準測試有望促進具有空間規劃能力的開放世界 AI 代理的進一步發展。", "applications": ["**智慧家庭設計助手：** 讓使用者透過語音或文字指令，快速生成客製化的房屋設計方案，包含家具擺放、動線規劃等，並能模擬實際居住體驗。", "**建築工地自動化規劃：** AI 代理可以根據建築藍圖和施工環境，自動規劃建材堆放位置、機械設備路線，優化施工流程，提高效率並降低成本。", "**遊戲關卡設計工具：** 協助遊戲開發者快速生成多樣化的遊戲關卡地圖，並能根據遊戲機制自動調整地形和物件佈局，節省開發時間。"], "pitch": "MineAnyBuild 基準測試為評估和提升 AI 的空間規劃能力提供了一個強大的工具。 想像一下，我們擁有一項技術，能讓 AI 理解、推理、創造並應用空間常識，從而自動設計建築、規劃城市、優化物流，甚至協助製造業進行精密組裝。 MineAnyBuild 的獨特之處在於其基於 Minecraft 的開放世界環境，提供了無限的可能性和擴展性。 我們正處於空間智能 AI 的早期階段，而 MineAnyBuild 將成為加速其發展的催化劑。 透過與遊戲開發商、建築公司、物流企業等合作，我們可以將這項技術應用到各行各業，創造巨大的商業價值。 我們的目標是打造一個領先的空間智能 AI 平台，為未來世界的建設和發展做出貢獻。 現在投資，將共同塑造空間智能的未來！", "audio": "audios/2505.20148v1.mp3", "timestamp": "2025-05-27T15:14:50.369373"}
{"query": "Foundation Model", "id": "2505.20003v1", "url": "http://arxiv.org/abs/2505.20003v1", "title": "TabPFN: One Model to Rule Them All?", "summary": "Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a\ntransformer-based deep learning model for regression and classification on\ntabular data, which they claim \"outperforms all previous methods on datasets\nwith up to 10,000 samples by a wide margin, using substantially less training\ntime.\" Furthermore, they have called TabPFN a \"foundation model\" for tabular\ndata, as it can support \"data generation, density estimation, learning reusable\nembeddings and fine-tuning\". If these statements are well-supported, TabPFN may\nhave the potential to supersede existing modeling approaches on a wide range of\nstatistical tasks, mirroring a similar revolution in other areas of artificial\nintelligence that began with the advent of large language models. In this\npaper, we provide a tailored explanation of how TabPFN works for a statistics\naudience, by emphasizing its interpretation as approximate Bayesian inference.\nWe also provide more evidence of TabPFN's \"foundation model\" capabilities: We\nshow that an out-of-the-box application of TabPFN vastly outperforms\nspecialized state-of-the-art methods for semi-supervised parameter estimation,\nprediction under covariate shift, and heterogeneous treatment effect\nestimation. We further show that TabPFN can outperform LASSO at sparse\nregression and can break a robustness-efficiency trade-off in classification.\nAll experiments can be reproduced using the code provided at\nhttps://github.com/qinglong-tian/tabpfn_study\n(https://github.com/qinglong-tian/tabpfn_study).", "authors": ["Qiong Zhang", "Yan Shuo Tan", "Qinglong Tian", "Pengfei Li"], "published_date": "2025-05-26", "title_zh": "TabPFN：一統江湖的模型？", "summary_zh": "TabPFN是一個基於Transformer的深度學習模型，專為表格數據設計，據稱在1萬筆資料以下的數據集上，其效能遠勝過以往所有方法，且訓練時間大幅縮短。更厲害的是，它被視為表格數據的「基礎模型」，能支援數據生成、密度估計、可重複使用的嵌入和微調等功能。本文深入探討TabPFN的運作原理，並展示其作為「基礎模型」的能力：在半監督參數估計、共變量轉移下的預測，以及異質性處理效應估計等任務中，TabPFN無需特別調整，就能超越專門的頂尖方法。此外，它在稀疏迴歸中能勝過LASSO，並打破分類中的穩健性-效率權衡。所有實驗皆可透過提供的程式碼重現。", "applications": ["**個人化醫療診斷：** 醫生可以快速分析患者的病歷資料（例如：血液檢查、症狀等），預測患者罹患特定疾病的風險，並推薦最適合的治療方案，即使數據量不大也能有較高的準確性。", "**金融風險評估：** 銀行可以利用TabPFN分析客戶的財務數據（例如：信用評分、收入、消費習慣等），預測客戶的違約風險，從而更有效地管理信貸風險。", "**智慧農業優化：** 農民可以利用TabPFN分析農田的土壤、天氣、作物生長等數據，預測作物的產量，並優化灌溉、施肥等措施，提高農業生產效率。"], "pitch": "TabPFN是表格數據領域的遊戲規則改變者，它不僅效能卓越，且無需大量資料就能取得顯著成果。其基礎模型的特性，使其在多個領域具有廣闊的應用前景，例如個人化醫療、金融風險管理、智慧農業等。對於那些面臨資料稀缺、需要快速原型驗證，以及追求高效能的企業來說，TabPFN提供了一個極具吸引力的解決方案。這是一個潛力無限的投資機會，我們將打造基於TabPFN的垂直應用平台，解決各行業的痛點，成為表格數據領域的領導者。", "audio": "audios/2505.20003v1.mp3", "timestamp": "2025-05-27T15:15:07.852715"}
{"query": "Diffusion Model", "id": "2505.20131v1", "url": "http://arxiv.org/abs/2505.20131v1", "title": "MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning", "summary": "Molecular editing aims to modify a given molecule to optimize desired\nchemical properties while preserving structural similarity. However, current\napproaches typically rely on string-based or continuous representations, which\nfail to adequately capture the discrete, graph-structured nature of molecules,\nresulting in limited structural fidelity and poor controllability. In this\npaper, we propose MolEditRL, a molecular editing framework that explicitly\nintegrates structural constraints with precise property optimization.\nSpecifically, MolEditRL consists of two stages: (1) a discrete graph diffusion\nmodel pretrained to reconstruct target molecules conditioned on source\nstructures and natural language instructions; (2) an editing-aware\nreinforcement learning fine-tuning stage that further enhances property\nalignment and structural preservation by explicitly optimizing editing\ndecisions under graph constraints. For comprehensive evaluation, we construct\nMolEdit-Instruct, the largest and most property-rich molecular editing dataset,\ncomprising 3 million diverse examples spanning single- and multi-property tasks\nacross 10 chemical attributes. Experimental results demonstrate that MolEditRL\nsignificantly outperforms state-of-the-art methods in both property\noptimization accuracy and structural fidelity, achieving a 74\\% improvement in\nediting success rate while using 98\\% fewer parameters.", "authors": ["Yuanxin Zhuang", "Dazhong Shen", "Ying Sun"], "published_date": "2025-05-26", "title_zh": "MolEditRL：基於離散擴散與強化學習的結構保持型分子編輯", "summary_zh": "現有分子編輯方法難以兼顧化學性質優化與結構相似性保持。本論文提出 MolEditRL 框架，透過離散圖擴散模型和強化學習微調，在結構約束下精確優化分子性質。MolEditRL 模型首先利用離散圖擴散模型學習在給定原始結構和自然語言指令下重建目標分子，然後透過強化學習微調，進一步提升性質對齊與結構保持能力。實驗結果顯示，MolEditRL 在性質優化準確性和結構保真度方面均優於現有技術，編輯成功率提升 74%，同時參數數量減少 98%。", "applications": ["**藥物發現：** 基於已知藥物結構，快速高效地調整分子結構以提高藥效、降低副作用，或改善藥物代謝動力學。", "**材料設計：** 修改現有材料的分子結構，以優化其特定物理或化學性質，例如提高導電性、增強強度或改善耐腐蝕性。", "**農業化學品開發：** 編輯農藥或肥料的分子結構，以提高其有效性、選擇性或環境友好性。"], "pitch": "MolEditRL 解決了分子編輯領域中結構保持與性質優化難以兼顧的痛點，在藥物、材料和農業化學等領域具有巨大的商業潛力。其核心優勢在於其高效性（參數少）、準確性（成功率高）以及對結構的精確控制，這使得其在發現新型化合物和優化現有化合物方面具有顯著的優勢。透過 MolEditRL，可以大幅縮短研發週期，降低研發成本，並加速創新化合物的商業化進程。我們認為，MolEditRL 有望成為下一代分子設計的重要基礎設施，為化學、生物和材料科學帶來顛覆性變革，具有極高的投資價值。", "audio": "audios/2505.20131v1.mp3", "timestamp": "2025-05-27T15:15:21.735087"}
{"query": "AI", "id": "2505.20246v1", "url": "http://arxiv.org/abs/2505.20246v1", "title": "On Path to Multimodal Historical Reasoning: HistBench and HistAgent", "summary": "Recent advances in large language models (LLMs) have led to remarkable\nprogress across domains, yet their capabilities in the humanities, particularly\nhistory, remain underexplored. Historical reasoning poses unique challenges for\nAI, involving multimodal source interpretation, temporal inference, and\ncross-linguistic analysis. While general-purpose agents perform well on many\nexisting benchmarks, they lack the domain-specific expertise required to engage\nwith historical materials and questions. To address this gap, we introduce\nHistBench, a new benchmark of 414 high-quality questions designed to evaluate\nAI's capacity for historical reasoning and authored by more than 40 expert\ncontributors. The tasks span a wide range of historical problems-from factual\nretrieval based on primary sources to interpretive analysis of manuscripts and\nimages, to interdisciplinary challenges involving archaeology, linguistics, or\ncultural history. Furthermore, the benchmark dataset spans 29 ancient and\nmodern languages and covers a wide range of historical periods and world\nregions. Finding the poor performance of LLMs and other agents on HistBench, we\nfurther present HistAgent, a history-specific agent equipped with carefully\ndesigned tools for OCR, translation, archival search, and image understanding\nin History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of\n27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online\nsearch and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%)\nand Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These\nresults highlight the limitations of existing LLMs and generalist agents and\ndemonstrate the advantages of HistAgent for historical reasoning.", "authors": ["Jiahao Qiu", "Fulian Xiao", "Yimin Wang", "Yuchen Mao", "Yijia Chen", "Xinzhe Juan", "Siran Wang", "Xuan Qi", "Tongcheng Zhang", "Zixin Yao", "Jiacheng Guo", "Yifu Lu", "Charles Argon", "Jundi Cui", "Daixin Chen", "Junran Zhou", "Shuyao Zhou", "Zhanpeng Zhou", "Ling Yang", "Shilong Liu", "Hongru Wang", "Kaixuan Huang", "Xun Jiang", "Yuming Cao", "Yue Chen", "Yunfei Chen", "Zhengyi Chen", "Ruowei Dai", "Mengqiu Deng", "Jiye Fu", "Yunting Gu", "Zijie Guan", "Zirui Huang", "Xiaoyan Ji", "Yumeng Jiang", "Delong Kong", "Haolong Li", "Jiaqi Li", "Ruipeng Li", "Tianze Li", "Zhuoran Li", "Haixia Lian", "Mengyue Lin", "Xudong Liu", "Jiayi Lu", "Jinghan Lu", "Wanyu Luo", "Ziyue Luo", "Zihao Pu", "Zhi Qiao", "Ruihuan Ren", "Liang Wan", "Ruixiang Wang", "Tianhui Wang", "Yang Wang", "Zeyu Wang", "Zihua Wang", "Yujia Wu", "Zhaoyi Wu", "Hao Xin", "Weiao Xing", "Ruojun Xiong", "Weijie Xu", "Yao Shu", "Xiao Yao", "Xiaorui Yang", "Yuchen Yang", "Nan Yi", "Jiadong Yu", "Yangyuxuan Yu", "Huiting Zeng", "Danni Zhang", "Yunjie Zhang", "Zhaoyu Zhang", "Zhiheng Zhang", "Xiaofeng Zheng", "Peirong Zhou", "Linyan Zhong", "Xiaoyin Zong", "Ying Zhao", "Zhenxin Chen", "Lin Ding", "Xiaoyu Gao", "Bingbing Gong", "Yichao Li", "Yang Liao", "Guang Ma", "Tianyuan Ma", "Xinrui Sun", "Tianyi Wang", "Han Xia", "Ruobing Xian", "Gen Ye", "Tengfei Yu", "Wentao Zhang", "Yuxi Wang", "Xi Gao", "Mengdi Wang"], "published_date": "2025-05-26", "title_zh": "邁向多模態歷史推理之路：HistBench與HistAgent", "summary_zh": "大型語言模型（LLMs）在各領域取得顯著進展，但在人文學科，尤其是歷史領域的能力仍未被充分探索。歷史推理對AI提出了獨特的挑戰，包括多模態來源解讀、時間推理和跨語言分析。為此，我們推出HistBench，一個包含414個高質量問題的基準測試，旨在評估AI的歷史推理能力。此外，我們還介紹了HistAgent，一個專為歷史領域設計的智能體，配備OCR、翻譯、檔案搜索和圖像理解等工具。實驗表明，HistAgent在HistBench上的表現明顯優於通用LLMs和智能體，證明了針對歷史領域的AI設計的優勢。", "applications": ["**歷史文獻研究助手：** 提供學者快速查詢、翻譯和分析歷史文獻的功能，加速研究進程，降低語言門檻。", "**互動式歷史學習平台：** 結合遊戲化設計，讓學生能透過與HistAgent互動，更深入地理解歷史事件和人物。", "**文化遺產保護與數位化：** 利用HistAgent的圖像理解和語言分析能力，協助修復、翻譯和詮釋古文物及文獻，保存珍貴的歷史文化遺產。"], "pitch": "HistAgent是第一個專為歷史領域設計的多模態AI智能體，透過HistBench基準測試驗證了其優於通用LLMs的歷史推理能力。這意味著我們掌握了歷史研究領域的關鍵技術，潛在商業價值巨大。HistAgent可以授權給學術機構、博物館、文化遺產組織，甚至應用於教育市場，提供更精準、更深入的歷史研究和學習體驗。 我們將持續開發HistAgent，拓展其在歷史事件預測、文物真偽鑑定等方面的應用，搶佔歷史AI市場的先機，打造下一個AI獨角獸。", "audio": "audios/2505.20246v1.mp3", "timestamp": "2025-05-27T16:17:38.830070"}
{"query": "Foundation Model", "id": "2505.20202v1", "url": "http://arxiv.org/abs/2505.20202v1", "title": "PathBench: A comprehensive comparison benchmark for pathology foundation models towards precision oncology", "summary": "The emergence of pathology foundation models has revolutionized computational\nhistopathology, enabling highly accurate, generalized whole-slide image\nanalysis for improved cancer diagnosis, and prognosis assessment. While these\nmodels show remarkable potential across cancer diagnostics and prognostics,\ntheir clinical translation faces critical challenges including variability in\noptimal model across cancer types, potential data leakage in evaluation, and\nlack of standardized benchmarks. Without rigorous, unbiased evaluation, even\nthe most advanced PFMs risk remaining confined to research settings, delaying\ntheir life-saving applications. Existing benchmarking efforts remain limited by\nnarrow cancer-type focus, potential pretraining data overlaps, or incomplete\ntask coverage. We present PathBench, the first comprehensive benchmark\naddressing these gaps through: multi-center in-hourse datasets spanning common\ncancers with rigorous leakage prevention, evaluation across the full clinical\nspectrum from diagnosis to prognosis, and an automated leaderboard system for\ncontinuous model assessment. Our framework incorporates large-scale data,\nenabling objective comparison of PFMs while reflecting real-world clinical\ncomplexity. All evaluation data comes from private medical providers, with\nstrict exclusion of any pretraining usage to avoid data leakage risks. We have\ncollected 15,888 WSIs from 8,549 patients across 10 hospitals, encompassing\nover 64 diagnosis and prognosis tasks. Currently, our evaluation of 19 PFMs\nshows that Virchow2 and H-Optimus-1 are the most effective models overall. This\nwork provides researchers with a robust platform for model development and\noffers clinicians actionable insights into PFM performance across diverse\nclinical scenarios, ultimately accelerating the translation of these\ntransformative technologies into routine pathology practice.", "authors": ["Jiabo Ma", "Yingxue Xu", "Fengtao Zhou", "Yihui Wang", "Cheng Jin", "Zhengrui Guo", "Jianfeng Wu", "On Ki Tang", "Huajun Zhou", "Xi Wang", "Luyang Luo", "Zhengyu Zhang", "Du Cai", "Zizhao Gao", "Wei Wang", "Yueping Liu", "Jiankun He", "Jing Cui", "Zhenhui Li", "Jing Zhang", "Feng Gao", "Xiuming Zhang", "Li Liang", "Ronald Cheong Kin Chan", "Zhe Wang", "Hao Chen"], "published_date": "2025-05-26", "title_zh": "PathBench：針對精準腫瘤學，病理學基礎模型之全面性比較基準", "summary_zh": "病理學基礎模型正在改變計算病理學，為癌症診斷和預後評估提供更準確、更通用的全玻片影像分析。然而，這些模型的臨床應用面臨挑戰，像是不同癌症的最佳模型不同、評估中潛在的數據洩漏，以及缺乏標準化的基準。PathBench 旨在解決這些問題，提供一個全面的基準平台，包含多中心、涵蓋多種常見癌症的內部數據集，嚴格防止數據洩漏，並評估從診斷到預後的完整臨床範疇。PathBench 包含大規模數據，可以客觀地比較病理學基礎模型，同時反映真實世界的臨床複雜性。目前已評估 19 個模型，結果顯示 Virchow2 和 H-Optimus-1 整體表現最佳。PathBench 為研究人員提供了一個強大的模型開發平台，並為臨床醫生提供關於病理學基礎模型在各種臨床場景中表現的可行見解，最終加速這些轉型技術轉化為常規病理學實踐。", "applications": ["**個性化癌症治療建議：** 根據病理切片影像，PathBench 能夠協助醫生選擇最適合患者的治療方案，提升治療效果。", "**自動化病理分析工作流程：** PathBench 能夠自動識別癌症類型和嚴重程度，大幅縮短病理分析的時間，減輕病理醫生的工作負擔。", "**遠端病理諮詢平台：** PathBench 能夠讓遠端病理專家更精準地評估病理切片，提升偏遠地區的醫療水平。"], "pitch": "各位投資人，癌症診斷領域正迎來革命性的變革！PathBench 為病理學基礎模型提供了一個標準化、客觀的評估平台，加速了 AI 技術在癌症診斷和預後預測上的應用。我們的數據來自真實醫療機構，有效避免數據洩漏，確保評估結果的可靠性。想像一下，一個能夠精準識別癌症類型、預測預後並推薦最佳治療方案的 AI 系統，將大幅提升癌症患者的生存率和生活品質。PathBench 將成為病理學基礎模型的黃金標準，吸引模型開發者、醫療機構和藥廠的廣泛採用。我們尋求投資，以擴大數據規模、拓展模型評估種類，並打造一個連接模型開發者和醫療機構的商業生態系統，共同推動精準腫瘤學的發展，創造巨大的商業價值。", "audio": "audios/2505.20202v1.mp3", "timestamp": "2025-05-27T16:18:00.923412"}
{"query": "Diffusion Model", "id": "2505.20171v1", "url": "http://arxiv.org/abs/2505.20171v1", "title": "Long-Context State-Space Video World Models", "summary": "Video diffusion models have recently shown promise for world modeling through\nautoregressive frame prediction conditioned on actions. However, they struggle\nto maintain long-term memory due to the high computational cost associated with\nprocessing extended sequences in attention layers. To overcome this limitation,\nwe propose a novel architecture leveraging state-space models (SSMs) to extend\ntemporal memory without compromising computational efficiency. Unlike previous\napproaches that retrofit SSMs for non-causal vision tasks, our method fully\nexploits the inherent advantages of SSMs in causal sequence modeling. Central\nto our design is a block-wise SSM scanning scheme, which strategically trades\noff spatial consistency for extended temporal memory, combined with dense local\nattention to ensure coherence between consecutive frames. We evaluate the\nlong-term memory capabilities of our model through spatial retrieval and\nreasoning tasks over extended horizons. Experiments on Memory Maze and\nMinecraft datasets demonstrate that our approach surpasses baselines in\npreserving long-range memory, while maintaining practical inference speeds\nsuitable for interactive applications.", "authors": ["Ryan Po", "Yotam Nitzan", "Richard Zhang", "Berlin Chen", "Tri Dao", "Eli Shechtman", "Gordon Wetzstein", "Xun Huang"], "published_date": "2025-05-26", "title_zh": "長上下文狀態空間影片世界模型", "summary_zh": "影片擴散模型在透過基於動作條件的自迴歸幀預測實現世界建模方面展現了希望。然而，由於在注意力層中處理擴展序列的高計算成本，它們難以維持長期記憶。為了解決這個限制，我們提出了一種利用狀態空間模型（SSM）來擴展時間記憶而不損害計算效率的新架構。與先前為非因果視覺任務改造SSM的方法不同，我們的方法充分利用了SSM在因果序列建模中的固有優勢。我們設計的核心是塊狀SSM掃描方案，它策略性地用空間一致性換取擴展的時間記憶，並結合密集的局部注意力以確保連續幀之間的連貫性。我們通過在擴展範圍內的空間檢索和推理任務來評估我們模型的長期記憶能力。在 Memory Maze 和 Minecraft 數據集上的實驗表明，我們的方案在保持長程記憶方面超越了基線，同時保持了適用於互動式應用程序的實用推理速度。", "applications": ["**自動駕駛模擬訓練：** 模擬器可以生成更長更真實的駕駛場景，讓自動駕駛算法在接近真實世界的環境中學習和優化，減少真實道路測試的成本和風險。", "**遊戲AI開發：** 讓遊戲中的非玩家角色（NPC）擁有更強的記憶力和推理能力，可以根據玩家過去的行為和遊戲世界的變化做出更複雜、更真實的反應，提升遊戲的沉浸感和挑戰性。", "**影片編輯與生成：** 根據少量素材自動生成更長、更連貫的影片內容，例如根據幾分鐘的片段生成完整的電影預告片，或根據文字描述生成動畫短片。"], "pitch": "我們開發了一種革新性的影片世界模型，解決了現有模型在處理長序列時的記憶瓶頸。透過創新的狀態空間模型架構，我們的模型能夠更有效地學習和預測長期時間序列，為自動駕駛模擬、遊戲 AI 和影片生成等領域帶來顛覆性的應用。我們的技術不僅提升了模型的預測準確性，還保持了實時推理速度，使其適用於互動式應用。這代表著我們能賦能更智能、更具適應性的 AI 系統，在上述應用場景中創造巨大的商業價值。 想像一下，能以更低的成本訓練更安全的自動駕駛系統，或打造更沉浸式、更具吸引力的遊戲體驗。 我們的模型是構建下一代智能系統的關鍵基石，具有巨大的市場潛力。", "audio": "audios/2505.20171v1.mp3", "timestamp": "2025-05-27T16:18:17.518849"}
{"query": "AI", "id": "2505.20148v1", "url": "http://arxiv.org/abs/2505.20148v1", "title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents", "summary": "Spatial Planning is a crucial part in the field of spatial intelligence,\nwhich requires the understanding and planning about object arrangements in\nspace perspective. AI agents with the spatial planning ability can better adapt\nto various real-world applications, including robotic manipulation, automatic\nassembly, urban planning etc. Recent works have attempted to construct\nbenchmarks for evaluating the spatial intelligence of Multimodal Large Language\nModels (MLLMs). Nevertheless, these benchmarks primarily focus on spatial\nreasoning based on typical Visual Question-Answering (VQA) forms, which suffers\nfrom the gap between abstract spatial understanding and concrete task\nexecution. In this work, we take a step further to build a comprehensive\nbenchmark called MineAnyBuild, aiming to evaluate the spatial planning ability\nof open-world AI agents in the Minecraft game. Specifically, MineAnyBuild\nrequires an agent to generate executable architecture building plans based on\nthe given multi-modal human instructions. It involves 4,000 curated spatial\nplanning tasks and also provides a paradigm for infinitely expandable data\ncollection by utilizing rich player-generated content. MineAnyBuild evaluates\nspatial planning through four core supporting dimensions: spatial\nunderstanding, spatial reasoning, creativity, and spatial commonsense. Based on\nMineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based\nagents, revealing the severe limitations but enormous potential in their\nspatial planning abilities. We believe our MineAnyBuild will open new avenues\nfor the evaluation of spatial intelligence and help promote further development\nfor open-world AI agents capable of spatial planning.", "authors": ["Ziming Wei", "Bingqian Lin", "Zijian Jiao", "Yunshuang Nie", "Liang Ma", "Yuecheng Liu", "Yuzheng Zhuang", "Xiaodan Liang"], "published_date": "2025-05-26", "title_zh": "MineAnyBuild：開放世界AI代理空間規劃能力基準測試", "summary_zh": "這篇論文提出了一個名為MineAnyBuild的基準測試，用於評估AI代理在Minecraft遊戲中基於多模態人類指令生成可執行建築規劃的能力。目前的AI模型在空間理解、推理、創造力和常識等方面仍存在嚴重局限性，但具有巨大的潛力。MineAnyBuild提供4000個精選任務，並可無限擴充數據集，旨在推動開放世界AI代理的空間規劃能力發展。", "applications": ["**智慧家居設計助手：** 根據用戶語音或圖像描述，自動生成客製化家居設計方案，並預覽實際效果，省去人工設計的繁瑣過程。", "**自動化倉儲管理系統：** 根據貨物種類和數量，自動規劃倉庫的最佳擺放位置，提高空間利用率和揀貨效率。", "**城市規劃模擬器：** 基於人口、交通、環境等多因素數據，模擬城市發展的不同情景，幫助城市規劃者做出更科學的決策。"], "pitch": "MineAnyBuild不僅是一個基準測試，更是一個加速開放世界AI發展的催化劑。我們解決的是空間智能這一關鍵瓶頸，AI能夠理解、推理並創造性地規劃空間，其應用潛力無限。想像一下，AI能根據客戶需求自動設計房子、優化工廠布局、甚至規劃整個城市的交通。MineAnyBuild為這些願景提供了實現的基礎。初期，我們可以將此技術應用於建築設計、倉儲物流等領域，通過授權、SaaS模式獲利。長遠來看，我們將建立一個開放的AI空間規劃平台，吸引更多開發者參與，形成強大的生態系統，成為空間智能領域的領頭羊。這個市場的規模巨大，前景無限。", "audio": "audios/2505.20148v1.mp3", "timestamp": "2025-05-27T17:13:05.944841"}
{"query": "Foundation Model", "id": "2505.20003v1", "url": "http://arxiv.org/abs/2505.20003v1", "title": "TabPFN: One Model to Rule Them All?", "summary": "Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a\ntransformer-based deep learning model for regression and classification on\ntabular data, which they claim \"outperforms all previous methods on datasets\nwith up to 10,000 samples by a wide margin, using substantially less training\ntime.\" Furthermore, they have called TabPFN a \"foundation model\" for tabular\ndata, as it can support \"data generation, density estimation, learning reusable\nembeddings and fine-tuning\". If these statements are well-supported, TabPFN may\nhave the potential to supersede existing modeling approaches on a wide range of\nstatistical tasks, mirroring a similar revolution in other areas of artificial\nintelligence that began with the advent of large language models. In this\npaper, we provide a tailored explanation of how TabPFN works for a statistics\naudience, by emphasizing its interpretation as approximate Bayesian inference.\nWe also provide more evidence of TabPFN's \"foundation model\" capabilities: We\nshow that an out-of-the-box application of TabPFN vastly outperforms\nspecialized state-of-the-art methods for semi-supervised parameter estimation,\nprediction under covariate shift, and heterogeneous treatment effect\nestimation. We further show that TabPFN can outperform LASSO at sparse\nregression and can break a robustness-efficiency trade-off in classification.\nAll experiments can be reproduced using the code provided at\nhttps://github.com/qinglong-tian/tabpfn_study\n(https://github.com/qinglong-tian/tabpfn_study).", "authors": ["Qiong Zhang", "Yan Shuo Tan", "Qinglong Tian", "Pengfei Li"], "published_date": "2025-05-26", "title_zh": "TabPFN：一統江湖的模型？", "summary_zh": "近期研究發表了一種名為 TabPFN 的基於 Transformer 的深度學習模型，用於處理表格數據的回歸和分類問題。研究聲稱，TabPFN 在樣本數少於 10,000 的數據集上，顯著優於所有先前的模型，且訓練時間更短。研究人員更將 TabPFN 視為表格數據的“基礎模型”，具備數據生成、密度估計、可重用嵌入學習和微調等能力。本文解釋了 TabPFN 的工作原理，並強調其作為近似貝葉斯推斷的解釋。此外，本文也提供了更多證據證明 TabPFN 作為“基礎模型”的能力，表明 TabPFN 在半監督參數估計、協變量偏移下的預測和異質處理效應估計方面，優於專業的最新方法。研究還表明 TabPFN 在稀疏回歸方面優於 LASSO，並且可以打破分類中的穩健性-效率權衡。", "applications": ["**醫療診斷輔助：** 利用病人病歷數據（如實驗室數值、症狀描述）快速預測疾病風險或提供診斷建議，無需針對不同疾病訓練不同模型。", "**金融信貸評估：** 透過客戶的財務數據和個人資料，快速評估其信貸風險，並提供個性化的貸款方案，大幅降低風險評估成本。", "**電商商品推薦：** 根據用戶過去的購買紀錄、瀏覽行為和人口統計數據，進行個性化的商品推薦，提升轉換率和用戶滿意度。"], "pitch": "TabPFN 正在重新定義表格數據的建模方式，其在小數據集上的卓越性能和通用性使其成為一個極具潛力的“基礎模型”。想像一下，無需為每個特定的表格數據集訓練不同的模型，只需一個 TabPFN 就能應付各種任務。這意味著更快的模型開發速度、更低的維護成本和更廣泛的應用範圍。我們可以將 TabPFN 應用於醫療、金融、零售等各個行業，為其提供更精準的預測和更優化的決策支持。其商業價值體現在：\n\n* **顯著降低建模成本：** 通用模型減少了模型開發和維護的支出。\n* **加速產品上市時間：** 快速部署模型，更快地推出新產品和服務。\n* **提高決策效率：** 更準確的預測可以幫助企業做出更明智的決策。\n* **開闢新市場：** 在小數據集場景下的優異表現，為在數據稀缺領域的應用打開了機會。\n\n我們相信 TabPFN 有機會成為表格數據領域的領導者，並為投資者帶來豐厚的回報。", "audio": "audios/2505.20003v1.mp3", "timestamp": "2025-05-27T17:13:30.852361"}
{"query": "Diffusion Model", "id": "2505.20131v1", "url": "http://arxiv.org/abs/2505.20131v1", "title": "MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning", "summary": "Molecular editing aims to modify a given molecule to optimize desired\nchemical properties while preserving structural similarity. However, current\napproaches typically rely on string-based or continuous representations, which\nfail to adequately capture the discrete, graph-structured nature of molecules,\nresulting in limited structural fidelity and poor controllability. In this\npaper, we propose MolEditRL, a molecular editing framework that explicitly\nintegrates structural constraints with precise property optimization.\nSpecifically, MolEditRL consists of two stages: (1) a discrete graph diffusion\nmodel pretrained to reconstruct target molecules conditioned on source\nstructures and natural language instructions; (2) an editing-aware\nreinforcement learning fine-tuning stage that further enhances property\nalignment and structural preservation by explicitly optimizing editing\ndecisions under graph constraints. For comprehensive evaluation, we construct\nMolEdit-Instruct, the largest and most property-rich molecular editing dataset,\ncomprising 3 million diverse examples spanning single- and multi-property tasks\nacross 10 chemical attributes. Experimental results demonstrate that MolEditRL\nsignificantly outperforms state-of-the-art methods in both property\noptimization accuracy and structural fidelity, achieving a 74\\% improvement in\nediting success rate while using 98\\% fewer parameters.", "authors": ["Yuanxin Zhuang", "Dazhong Shen", "Ying Sun"], "published_date": "2025-05-26", "title_zh": "MolEditRL：透過離散擴散與強化學習實現結構保留的分子編輯", "summary_zh": "MolEditRL是一種新型的分子編輯框架，它結合了離散圖擴散模型和強化學習，旨在優化分子特性並同時保留其結構相似性。與傳統方法不同，MolEditRL直接處理分子的圖結構，提升了結構保真度和可控性。它包含兩個階段：首先，使用離散圖擴散模型，根據原始結構和自然語言指令重建目標分子；然後，利用編輯感知的強化學習進行微調，在圖約束下優化編輯決策。研究團隊創建了一個包含300萬個範例的大型數據集MolEdit-Instruct用於評估。實驗結果表明，MolEditRL在特性優化和結構保真度方面顯著優於現有技術，編輯成功率提高了74%，同時使用的參數減少了98%。", "applications": ["**藥物發現加速：** 針對已知的候選藥物，快速迭代修改分子結構，以提高其藥效、降低副作用，或改善藥物動力學特性，縮短藥物開發週期。", "**新型材料設計：** 在現有材料的基礎上，編輯分子結構，設計出具有更優異物理或化學性質的新型材料，例如更高效率的太陽能電池材料或更強韌的塑料。", "**個性化化學品定製：** 根據特定需求（例如：香味、色素、溶劑），編輯分子結構，定製具有特定功能的化學品，滿足不同行業的個性化需求。"], "pitch": "MolEditRL 代表著藥物和材料發現的重大突破。其獨特的基於圖的離散擴散與強化學習的結合，克服了傳統方法的局限性，實現了前所未有的結構控制和特性優化。這意味著更快的藥物發現週期、更有效的材料設計，以及更高成功率。透過 MolEditRL，我們可以大幅降低研發成本，加速產品上市時間，並創造出具有卓越性能的新型化學品和材料。其創新的參數效率更進一步證明了其商業可行性。我們相信 MolEditRL 具備顛覆市場的潛力，將成為化學、製藥和材料科學領域的遊戲規則改變者。 投資 MolEditRL，就是投資於未來，投資於更快速、更有效、更智能的分子設計。", "audio": "audios/2505.20131v1.mp3", "timestamp": "2025-05-27T17:13:55.301034"}
{"query": "AI", "id": "2505.20246v1", "url": "http://arxiv.org/abs/2505.20246v1", "title": "On Path to Multimodal Historical Reasoning: HistBench and HistAgent", "summary": "Recent advances in large language models (LLMs) have led to remarkable\nprogress across domains, yet their capabilities in the humanities, particularly\nhistory, remain underexplored. Historical reasoning poses unique challenges for\nAI, involving multimodal source interpretation, temporal inference, and\ncross-linguistic analysis. While general-purpose agents perform well on many\nexisting benchmarks, they lack the domain-specific expertise required to engage\nwith historical materials and questions. To address this gap, we introduce\nHistBench, a new benchmark of 414 high-quality questions designed to evaluate\nAI's capacity for historical reasoning and authored by more than 40 expert\ncontributors. The tasks span a wide range of historical problems-from factual\nretrieval based on primary sources to interpretive analysis of manuscripts and\nimages, to interdisciplinary challenges involving archaeology, linguistics, or\ncultural history. Furthermore, the benchmark dataset spans 29 ancient and\nmodern languages and covers a wide range of historical periods and world\nregions. Finding the poor performance of LLMs and other agents on HistBench, we\nfurther present HistAgent, a history-specific agent equipped with carefully\ndesigned tools for OCR, translation, archival search, and image understanding\nin History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of\n27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online\nsearch and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%)\nand Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These\nresults highlight the limitations of existing LLMs and generalist agents and\ndemonstrate the advantages of HistAgent for historical reasoning.", "authors": ["Jiahao Qiu", "Fulian Xiao", "Yimin Wang", "Yuchen Mao", "Yijia Chen", "Xinzhe Juan", "Siran Wang", "Xuan Qi", "Tongcheng Zhang", "Zixin Yao", "Jiacheng Guo", "Yifu Lu", "Charles Argon", "Jundi Cui", "Daixin Chen", "Junran Zhou", "Shuyao Zhou", "Zhanpeng Zhou", "Ling Yang", "Shilong Liu", "Hongru Wang", "Kaixuan Huang", "Xun Jiang", "Yuming Cao", "Yue Chen", "Yunfei Chen", "Zhengyi Chen", "Ruowei Dai", "Mengqiu Deng", "Jiye Fu", "Yunting Gu", "Zijie Guan", "Zirui Huang", "Xiaoyan Ji", "Yumeng Jiang", "Delong Kong", "Haolong Li", "Jiaqi Li", "Ruipeng Li", "Tianze Li", "Zhuoran Li", "Haixia Lian", "Mengyue Lin", "Xudong Liu", "Jiayi Lu", "Jinghan Lu", "Wanyu Luo", "Ziyue Luo", "Zihao Pu", "Zhi Qiao", "Ruihuan Ren", "Liang Wan", "Ruixiang Wang", "Tianhui Wang", "Yang Wang", "Zeyu Wang", "Zihua Wang", "Yujia Wu", "Zhaoyi Wu", "Hao Xin", "Weiao Xing", "Ruojun Xiong", "Weijie Xu", "Yao Shu", "Xiao Yao", "Xiaorui Yang", "Yuchen Yang", "Nan Yi", "Jiadong Yu", "Yangyuxuan Yu", "Huiting Zeng", "Danni Zhang", "Yunjie Zhang", "Zhaoyu Zhang", "Zhiheng Zhang", "Xiaofeng Zheng", "Peirong Zhou", "Linyan Zhong", "Xiaoyin Zong", "Ying Zhao", "Zhenxin Chen", "Lin Ding", "Xiaoyu Gao", "Bingbing Gong", "Yichao Li", "Yang Liao", "Guang Ma", "Tianyuan Ma", "Xinrui Sun", "Tianyi Wang", "Han Xia", "Ruobing Xian", "Gen Ye", "Tengfei Yu", "Wentao Zhang", "Yuxi Wang", "Xi Gao", "Mengdi Wang"], "published_date": "2025-05-26", "title_zh": "邁向多模態歷史推理：HistBench與HistAgent", "summary_zh": "這篇論文介紹了HistBench，一個專為評估AI歷史推理能力而設計的高質量基準測試集，包含414個問題，涵蓋多種歷史領域和語言。研究發現，通用大型語言模型在HistBench上的表現不佳，於是論文提出了HistAgent，一個專為歷史研究設計的AI代理，配備了OCR、翻譯、文檔搜尋和圖像理解等工具。實驗結果表明，HistAgent在歷史推理方面明顯優於通用大型語言模型。", "applications": ["**客製化歷史學習體驗：**想像一個能根據你的興趣和學習進度，提供相關文獻、圖像和解釋的AI歷史導師。你可以問它任何關於歷史的問題，它會利用HistAgent提供的工具，從各種來源找出答案，並以易於理解的方式呈現給你。", "**協助學者進行歷史研究：** 歷史學家可以利用HistAgent快速搜尋和分析大量的歷史文獻，例如古籍、手稿和照片。這能幫助他們發現新的證據，驗證假設，並加速研究進程。", "**博物館互動式體驗：** 將HistAgent整合到博物館的導覽系統中，遊客可以透過提問來深入了解展品的歷史背景。例如，遊客可以詢問某件文物是如何製作的、它的文化意義是什麼，以及它在當時社會中的用途。"], "pitch": "HistBench與HistAgent的創新結合，解決了通用AI在歷史領域的推理短板，開啟了歷史研究和教育的全新可能性。HistAgent的商業潛力巨大，涵蓋了教育、研究、文化機構等多個領域。透過將HistAgent打造成一個可擴展的雲端服務平台，我們可以向學校、博物館、研究機構和個人提供客製化的歷史推理解決方案。此外，還可以將HistAgent的技術授權給其他AI公司，共同推動多模態歷史推理的發展。我們相信，HistAgent將成為歷史領域的遊戲規則改變者，帶來可觀的商業回報。", "audio": "audios/2505.20246v1.mp3", "timestamp": "2025-05-27T18:17:49.599454"}
{"query": "Foundation Model", "id": "2505.20202v1", "url": "http://arxiv.org/abs/2505.20202v1", "title": "PathBench: A comprehensive comparison benchmark for pathology foundation models towards precision oncology", "summary": "The emergence of pathology foundation models has revolutionized computational\nhistopathology, enabling highly accurate, generalized whole-slide image\nanalysis for improved cancer diagnosis, and prognosis assessment. While these\nmodels show remarkable potential across cancer diagnostics and prognostics,\ntheir clinical translation faces critical challenges including variability in\noptimal model across cancer types, potential data leakage in evaluation, and\nlack of standardized benchmarks. Without rigorous, unbiased evaluation, even\nthe most advanced PFMs risk remaining confined to research settings, delaying\ntheir life-saving applications. Existing benchmarking efforts remain limited by\nnarrow cancer-type focus, potential pretraining data overlaps, or incomplete\ntask coverage. We present PathBench, the first comprehensive benchmark\naddressing these gaps through: multi-center in-hourse datasets spanning common\ncancers with rigorous leakage prevention, evaluation across the full clinical\nspectrum from diagnosis to prognosis, and an automated leaderboard system for\ncontinuous model assessment. Our framework incorporates large-scale data,\nenabling objective comparison of PFMs while reflecting real-world clinical\ncomplexity. All evaluation data comes from private medical providers, with\nstrict exclusion of any pretraining usage to avoid data leakage risks. We have\ncollected 15,888 WSIs from 8,549 patients across 10 hospitals, encompassing\nover 64 diagnosis and prognosis tasks. Currently, our evaluation of 19 PFMs\nshows that Virchow2 and H-Optimus-1 are the most effective models overall. This\nwork provides researchers with a robust platform for model development and\noffers clinicians actionable insights into PFM performance across diverse\nclinical scenarios, ultimately accelerating the translation of these\ntransformative technologies into routine pathology practice.", "authors": ["Jiabo Ma", "Yingxue Xu", "Fengtao Zhou", "Yihui Wang", "Cheng Jin", "Zhengrui Guo", "Jianfeng Wu", "On Ki Tang", "Huajun Zhou", "Xi Wang", "Luyang Luo", "Zhengyu Zhang", "Du Cai", "Zizhao Gao", "Wei Wang", "Yueping Liu", "Jiankun He", "Jing Cui", "Zhenhui Li", "Jing Zhang", "Feng Gao", "Xiuming Zhang", "Li Liang", "Ronald Cheong Kin Chan", "Zhe Wang", "Hao Chen"], "published_date": "2025-05-26", "title_zh": "PathBench：針對精準腫瘤學之病理學基礎模型的全面比較基準測試", "summary_zh": "病理學基礎模型（PFMs）的出現為計算病理學帶來了革命，能夠針對全玻片影像進行高精度、通用性的分析，從而改善癌症診斷和預後評估。然而，這些模型在臨床轉譯方面面臨重大挑戰，包括不同癌症類型中最佳模型的變異性、評估中潛在的數據洩漏以及缺乏標準化的基準測試。為了克服這些障礙，我們推出了PathBench，這是第一個全面的基準測試，通過以下方式解決這些差距：涵蓋常見癌症的多中心內部數據集，嚴格的洩漏預防措施，涵蓋從診斷到預後的完整臨床範圍的評估，以及用於持續模型評估的自動化排行榜系統。我們的框架整合了大規模數據，可以客觀地比較PFMs，同時反映真實世界的臨床複雜性。評估數據來自私人醫療機構，嚴格排除任何預訓練的使用，以避免數據洩漏風險。我們收集了來自10家醫院的8,549名患者的15,888張WSI，涵蓋了64項診斷和預後任務。目前，我們對19個PFM的評估表明，Virchow2和H-Optimus-1是總體上最有效的模型。這項工作為研究人員提供了一個強大的模型開發平台，並為臨床醫生提供了關於PFM在不同臨床場景中表現的可操作見解，最終加速了這些變革性技術轉化為常規病理學實踐。", "applications": ["**遠程病理診斷：** 在醫療資源匱乏地區，PathBench驗證過的PFM模型可以輔助病理學家進行遠程診斷，提升診斷效率和準確性，使更多患者受益。", "**個性化癌症治療方案：** 利用PFM分析病理切片，可以更精準地預測患者對不同治療方案的反應，為醫生制定個性化的癌症治療方案提供依據，提高治療成功率。", "**藥物研發加速：** 藥廠可以利用PathBench來評估不同PFM在分析腫瘤微環境、預測藥物反應等方面的能力，從而加速新藥研發流程，降低研發成本。"], "pitch": "PathBench是一個改變癌症診斷和治療的平台。現今病理學基礎模型雖有潛力，但缺乏客觀的比較標準。PathBench提供了一個經過嚴格驗證、無數據洩漏疑慮的基準測試平台，讓醫院、藥廠和AI開發者能客觀評估和選擇最適合的病理學模型。這不僅能大幅提升診斷準確性，實現更精準的個人化治療，還能加速藥物研發進程。我們的商業模式包括提供付費的基準測試服務、模型評估報告以及數據分析諮詢。PathBench的潛在客戶涵蓋醫院、研究機構、製藥公司以及醫療AI開發商，市場規模龐大。我們尋求投資者共同打造這個革命性的平台，將AI的力量帶入病理學，改善全球癌症患者的生活。", "audio": "audios/2505.20202v1.mp3", "timestamp": "2025-05-27T18:18:21.234774"}
{"query": "Diffusion Model", "id": "2505.20171v1", "url": "http://arxiv.org/abs/2505.20171v1", "title": "Long-Context State-Space Video World Models", "summary": "Video diffusion models have recently shown promise for world modeling through\nautoregressive frame prediction conditioned on actions. However, they struggle\nto maintain long-term memory due to the high computational cost associated with\nprocessing extended sequences in attention layers. To overcome this limitation,\nwe propose a novel architecture leveraging state-space models (SSMs) to extend\ntemporal memory without compromising computational efficiency. Unlike previous\napproaches that retrofit SSMs for non-causal vision tasks, our method fully\nexploits the inherent advantages of SSMs in causal sequence modeling. Central\nto our design is a block-wise SSM scanning scheme, which strategically trades\noff spatial consistency for extended temporal memory, combined with dense local\nattention to ensure coherence between consecutive frames. We evaluate the\nlong-term memory capabilities of our model through spatial retrieval and\nreasoning tasks over extended horizons. Experiments on Memory Maze and\nMinecraft datasets demonstrate that our approach surpasses baselines in\npreserving long-range memory, while maintaining practical inference speeds\nsuitable for interactive applications.", "authors": ["Ryan Po", "Yotam Nitzan", "Richard Zhang", "Berlin Chen", "Tri Dao", "Eli Shechtman", "Gordon Wetzstein", "Xun Huang"], "published_date": "2025-05-26", "title_zh": "長脈絡狀態空間影片世界模型", "summary_zh": "影片擴散模型在透過自迴歸幀預測建構世界模型方面展現了潛力。然而，由於注意力層處理長序列所需的計算成本高昂，它們難以維持長期記憶。為了解決這個限制，我們提出一種新穎的架構，利用狀態空間模型（SSM）來擴展時間記憶，同時又不影響計算效率。與以往將SSM改造用於非因果視覺任務的方法不同，我們的方法充分利用了SSM在因果序列建模方面的內在優勢。我們設計的核心是一種分塊式SSM掃描方案，它策略性地犧牲了空間一致性來換取更長的時間記憶，並結合密集局部注意力來確保連續幀之間的連貫性。我們通過在長時間範圍內進行空間檢索和推理任務來評估我們模型的長期記憶能力。在Memory Maze和Minecraft數據集上的實驗表明，我們的方法在保持長程記憶方面超越了基準線，同時保持了適用於互動應用程式的實際推論速度。", "applications": ["**更真實的遊戲AI：** 讓遊戲中的NPC（非玩家角色）能夠記住玩家過去的行為，並根據這些記憶做出更智能、更自然的反應，例如記得玩家幫助過它，下次遇到時會更友善。", "**個性化影片推薦：** 分析使用者長時間的影片觀看習慣，不僅僅是最近幾分鐘，而是幾小時甚至幾天的觀看紀錄，更精準地預測使用者接下來會想看什麼類型的影片。", "**智能家居環境感知：** 讓機器人能夠理解並記住家居環境的長期變化，例如家人習慣將鑰匙放在哪個地方，或者哪扇窗戶經常被打開，从而更好地提供帮助，例如提醒用户关窗。"], "pitch": "我們開發了一種突破性的影片世界模型，它能像人一樣擁有長期記憶，理解並預測複雜的影片場景。與傳統模型相比，我們的技術在保持記憶的同時，大幅降低了計算成本，使其更易於部署於各種應用中。想像一下，一個能夠學習並記住你所有遊戲習慣的AI助手，或者一個能夠理解你長期觀影偏好的影片推薦系統。我們的技術擁有巨大的商業潛力，可應用於遊戲、娛樂、機器人技術、安防監控等領域。我們正在尋找投資者，共同將這項技術推向市場，重新定義人機互動的未來。", "audio": "audios/2505.20171v1.mp3", "timestamp": "2025-05-27T18:18:48.063382"}
{"query": "AI", "id": "2505.20148v1", "url": "http://arxiv.org/abs/2505.20148v1", "title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents", "summary": "Spatial Planning is a crucial part in the field of spatial intelligence,\nwhich requires the understanding and planning about object arrangements in\nspace perspective. AI agents with the spatial planning ability can better adapt\nto various real-world applications, including robotic manipulation, automatic\nassembly, urban planning etc. Recent works have attempted to construct\nbenchmarks for evaluating the spatial intelligence of Multimodal Large Language\nModels (MLLMs). Nevertheless, these benchmarks primarily focus on spatial\nreasoning based on typical Visual Question-Answering (VQA) forms, which suffers\nfrom the gap between abstract spatial understanding and concrete task\nexecution. In this work, we take a step further to build a comprehensive\nbenchmark called MineAnyBuild, aiming to evaluate the spatial planning ability\nof open-world AI agents in the Minecraft game. Specifically, MineAnyBuild\nrequires an agent to generate executable architecture building plans based on\nthe given multi-modal human instructions. It involves 4,000 curated spatial\nplanning tasks and also provides a paradigm for infinitely expandable data\ncollection by utilizing rich player-generated content. MineAnyBuild evaluates\nspatial planning through four core supporting dimensions: spatial\nunderstanding, spatial reasoning, creativity, and spatial commonsense. Based on\nMineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based\nagents, revealing the severe limitations but enormous potential in their\nspatial planning abilities. We believe our MineAnyBuild will open new avenues\nfor the evaluation of spatial intelligence and help promote further development\nfor open-world AI agents capable of spatial planning.", "authors": ["Ziming Wei", "Bingqian Lin", "Zijian Jiao", "Yunshuang Nie", "Liang Ma", "Yuecheng Liu", "Yuzheng Zhuang", "Xiaodan Liang"], "published_date": "2025-05-26", "title_zh": "MineAnyBuild：開放世界AI代理的空間規劃基準測試", "summary_zh": "空間規劃是空間智能的關鍵一環，涉及對物體空間排列的理解和規劃。本研究提出了一個名為MineAnyBuild的綜合基準測試，旨在評估開放世界AI代理在Minecraft遊戲中的空間規劃能力。MineAnyBuild要求AI代理根據多模態的人類指令生成可執行的建築設計方案，包含四個核心維度：空間理解、空間推理、創造力以及空間常識。透過對現有基於MLLM的代理進行全面評估，揭示了它們在空間規劃能力上的嚴重局限，但也展示了巨大的潛力。這項研究旨在為空間智能的評估開闢新途徑，並促進具備空間規劃能力的開放世界AI代理的進一步發展。", "applications": ["**智慧家居設計助手：** 根據使用者的語音指令和偏好，自動生成室內佈局方案，並提供3D預覽，讓使用者輕鬆打造理想的居家環境。", "**城市規劃模擬器：** 基於AI的城市規劃工具，能根據人口密度、交通流量、環境影響等因素，自動生成最佳的城市佈局方案，協助政府和開發商做出更明智的決策。", "**機器人倉庫優化：** 機器人透過MineAnyBuild的空間規劃能力，可以自主優化倉庫貨架的排列方式，提升存儲效率和揀貨速度，大幅降低倉儲成本。"], "pitch": "MineAnyBuild解決了AI在複雜環境下進行空間規劃的關鍵瓶頸。我們提供的基準測試和評估框架，將加速開發出更智能、更具創造力的AI代理，這些代理能夠應用於智慧家居、城市規劃、物流倉儲等多個高潛力領域。我們的競爭優勢在於，MineAnyBuild不只評估空間推理，更重視實際執行能力，讓AI真正理解並應用空間資訊。透過授權MineAnyBuild基準測試以及合作開發特定行業的空間規劃AI解決方案，我們有信心能為投資者帶來顯著的投資回報，並引領下一代空間智能AI的發展。", "audio": "audios/2505.20148v1.mp3", "timestamp": "2025-05-27T19:11:11.367078"}
{"query": "Foundation Model", "id": "2505.20003v1", "url": "http://arxiv.org/abs/2505.20003v1", "title": "TabPFN: One Model to Rule Them All?", "summary": "Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a\ntransformer-based deep learning model for regression and classification on\ntabular data, which they claim \"outperforms all previous methods on datasets\nwith up to 10,000 samples by a wide margin, using substantially less training\ntime.\" Furthermore, they have called TabPFN a \"foundation model\" for tabular\ndata, as it can support \"data generation, density estimation, learning reusable\nembeddings and fine-tuning\". If these statements are well-supported, TabPFN may\nhave the potential to supersede existing modeling approaches on a wide range of\nstatistical tasks, mirroring a similar revolution in other areas of artificial\nintelligence that began with the advent of large language models. In this\npaper, we provide a tailored explanation of how TabPFN works for a statistics\naudience, by emphasizing its interpretation as approximate Bayesian inference.\nWe also provide more evidence of TabPFN's \"foundation model\" capabilities: We\nshow that an out-of-the-box application of TabPFN vastly outperforms\nspecialized state-of-the-art methods for semi-supervised parameter estimation,\nprediction under covariate shift, and heterogeneous treatment effect\nestimation. We further show that TabPFN can outperform LASSO at sparse\nregression and can break a robustness-efficiency trade-off in classification.\nAll experiments can be reproduced using the code provided at\nhttps://github.com/qinglong-tian/tabpfn_study\n(https://github.com/qinglong-tian/tabpfn_study).", "authors": ["Qiong Zhang", "Yan Shuo Tan", "Qinglong Tian", "Pengfei Li"], "published_date": "2025-05-26", "title_zh": "TabPFN：一統江湖的模型？", "summary_zh": "TabPFN 是一個基於 Transformer 的深度學習模型，專為表格資料的迴歸和分類而設計。研究聲稱它在最多 10,000 個樣本的資料集上，超越了所有先前的模型，並且訓練時間更短。作者認為 TabPFN 是一個表格資料的「基礎模型」，能夠支援資料生成、密度估計、學習可重複使用的嵌入和微調。 本研究透過近似貝氏推論的角度解釋 TabPFN 的運作機制，並進一步驗證其「基礎模型」的能力，證明 TabPFN 在半監督參數估計、共變量偏移下的預測和異質性處理效應估計等方面，明顯優於專門的先進方法。 它甚至能在稀疏迴歸中超越 LASSO，並打破分類中的穩健性與效率之間的權衡。", "applications": ["**醫療診斷輔助：** 根據病患的病歷資料（如：年齡、性別、症狀、檢驗結果）快速準確地預測患病機率，輔助醫生做出更精準的診斷。", "**金融風險評估：** 分析客戶的財務資料（如：收入、負債、信用評分）來評估貸款風險，以便銀行和金融機構做出更明智的貸款決策。", "**產品推薦系統：** 根據用戶的歷史購買紀錄、瀏覽行為和人口統計資訊，更精準地推薦用戶可能感興趣的商品，提升銷售額和用戶滿意度。"], "pitch": "TabPFN 是一個革命性的表格資料分析工具，它以更少的時間和資源，就能在各種應用場景中提供超越傳統模型的精準預測。想像一下，它能大幅降低企業在模型訓練和部署上的成本，同時提升決策的準確性，進而創造巨大的商業價值。 從金融、醫療到零售，各行各業都能受益於 TabPFN 的強大能力。 它不僅能優化現有的工作流程，還能開闢全新的商業模式。 作為一個表格資料的「基礎模型」，TabPFN 的潛力無限，是 AI 領域下一個引爆點。 我們相信，投資 TabPFN，就是投資未來。", "audio": "audios/2505.20003v1.mp3", "timestamp": "2025-05-27T19:11:44.608978"}
{"query": "Diffusion Model", "id": "2505.20131v1", "url": "http://arxiv.org/abs/2505.20131v1", "title": "MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning", "summary": "Molecular editing aims to modify a given molecule to optimize desired\nchemical properties while preserving structural similarity. However, current\napproaches typically rely on string-based or continuous representations, which\nfail to adequately capture the discrete, graph-structured nature of molecules,\nresulting in limited structural fidelity and poor controllability. In this\npaper, we propose MolEditRL, a molecular editing framework that explicitly\nintegrates structural constraints with precise property optimization.\nSpecifically, MolEditRL consists of two stages: (1) a discrete graph diffusion\nmodel pretrained to reconstruct target molecules conditioned on source\nstructures and natural language instructions; (2) an editing-aware\nreinforcement learning fine-tuning stage that further enhances property\nalignment and structural preservation by explicitly optimizing editing\ndecisions under graph constraints. For comprehensive evaluation, we construct\nMolEdit-Instruct, the largest and most property-rich molecular editing dataset,\ncomprising 3 million diverse examples spanning single- and multi-property tasks\nacross 10 chemical attributes. Experimental results demonstrate that MolEditRL\nsignificantly outperforms state-of-the-art methods in both property\noptimization accuracy and structural fidelity, achieving a 74\\% improvement in\nediting success rate while using 98\\% fewer parameters.", "authors": ["Yuanxin Zhuang", "Dazhong Shen", "Ying Sun"], "published_date": "2025-05-26", "title_zh": "MolEditRL：透過離散擴散與強化學習實現結構保留的分子編輯", "summary_zh": "MolEditRL是一種新的分子編輯框架，旨在修改現有分子以優化其化學性質，同時保留原始結構的相似性。它結合了離散圖擴散模型和強化學習，能更好地處理分子結構的圖形本質，從而實現更高的結構保真度和可控性。該方法透過兩個階段運作：首先利用圖擴散模型根據原始結構和自然語言指令重建目標分子；然後使用強化學習微調，透過圖形約束顯式優化編輯決策，進一步提高性質對齊和結構保留。實驗證明，MolEditRL在性質優化準確性和結構保真度方面都優於現有技術，编辑成功率提高了74％，同時參數使用量減少了98％。", "applications": ["**藥物發現加速：** 根據已知的藥物分子結構，優化其藥效、降低副作用，快速生成更具潛力的候選藥物。", "**材料設計創新：** 修改現有材料的分子結構，提升其導電性、耐熱性或機械強度，設計出更優異的新型材料。", "**農藥改良與環境保護：** 編輯現有農藥分子結構，提高其殺蟲效果，同時降低對環境和人體的毒性，開發更安全的農藥。"], "pitch": "MolEditRL是分子編輯領域的一項突破性技術，它以更高效和精確的方式修改分子結構，同時最大限度地保留原有結構的特性。我們解決了傳統方法在處理分子圖形結構方面的局限性，实现了高达74%的编辑成功率提升以及98%的参数减少，这是一个巨大的飞跃。这意味着更快的药物发现周期，更高效的材料设计和更环保的化学品开发。我们的技术拥有巨大的商业潜力，特别是在制药、材料科学和农业化工等领域。我们正在寻找战略合作伙伴，共同将MolEditRL商业化，抢占市场先机，彻底改变这些行业的研发模式，并创造显著的经济和社会价值。", "audio": "audios/2505.20131v1.mp3", "timestamp": "2025-05-27T19:13:22.263626"}
{"query": "AI", "id": "2505.20148v1", "url": "http://arxiv.org/abs/2505.20148v1", "title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents", "summary": "Spatial Planning is a crucial part in the field of spatial intelligence,\nwhich requires the understanding and planning about object arrangements in\nspace perspective. AI agents with the spatial planning ability can better adapt\nto various real-world applications, including robotic manipulation, automatic\nassembly, urban planning etc. Recent works have attempted to construct\nbenchmarks for evaluating the spatial intelligence of Multimodal Large Language\nModels (MLLMs). Nevertheless, these benchmarks primarily focus on spatial\nreasoning based on typical Visual Question-Answering (VQA) forms, which suffers\nfrom the gap between abstract spatial understanding and concrete task\nexecution. In this work, we take a step further to build a comprehensive\nbenchmark called MineAnyBuild, aiming to evaluate the spatial planning ability\nof open-world AI agents in the Minecraft game. Specifically, MineAnyBuild\nrequires an agent to generate executable architecture building plans based on\nthe given multi-modal human instructions. It involves 4,000 curated spatial\nplanning tasks and also provides a paradigm for infinitely expandable data\ncollection by utilizing rich player-generated content. MineAnyBuild evaluates\nspatial planning through four core supporting dimensions: spatial\nunderstanding, spatial reasoning, creativity, and spatial commonsense. Based on\nMineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based\nagents, revealing the severe limitations but enormous potential in their\nspatial planning abilities. We believe our MineAnyBuild will open new avenues\nfor the evaluation of spatial intelligence and help promote further development\nfor open-world AI agents capable of spatial planning.", "authors": ["Ziming Wei", "Bingqian Lin", "Zijian Jiao", "Yunshuang Nie", "Liang Ma", "Yuecheng Liu", "Yuzheng Zhuang", "Xiaodan Liang"], "published_date": "2025-05-26", "title_zh": "MineAnyBuild：開放世界AI代理的空間規劃基準測試", "summary_zh": "空間規劃在空間智能領域至關重要。這篇論文提出了一個名為MineAnyBuild的綜合基準測試，旨在評估開放世界AI代理在Minecraft遊戲中的空間規劃能力。MineAnyBuild要求AI代理根據多模態的人工指令生成可執行的建築規劃。它涵蓋4,000個空間規劃任務，並提供了一種通過利用豐富的玩家生成內容來無限擴展數據收集的範例。該基準測試通過空間理解、空間推理、創造力和空間常識四個核心維度來評估空間規劃能力。透過對現有基於多模態大型語言模型的代理進行評估，揭示了它們在空間規劃能力方面的嚴重限制和巨大潛力。", "applications": ["**智慧家居設計助手：** 讓使用者透過語音或文字描述理想的房間佈局和裝飾風格，AI自動生成可視化的設計方案，並提供家具擺放建議，甚至可以生成Minecraft風格的示意圖方便使用者理解。", "**城市規劃模擬器：** 城市規劃師可以使用AI來模擬不同建築物和基礎設施的擺放位置對交通、人口流動和資源分配的影響，從而找到最佳的城市發展方案。使用者可以輸入文字描述，例如“增加公園綠地”、“改善交通擁堵”，AI則自動生成方案並提供評估報告。", "**機器人倉庫管理：** 讓機器人能夠理解倉庫的空間結構，並根據指令自主規劃貨物的存放和取貨路徑，提高倉庫的運營效率。可以應用於電商物流、工廠物料管理等場景。"], "pitch": "MineAnyBuild這個基準測試代表了AI空間智能領域的突破。它不僅提供了一個客觀評估AI空間規劃能力的平台，更為開發具備實用價值的空間智能應用奠定了基礎。試想一下，一個能理解並執行複雜空間指令的AI，可以徹底改變建築設計、城市規劃、物流管理等行業。我們相信，基於MineAnyBuild的研究和開發，將催生出無數創新型產品和服務，帶來巨大的商業價值和社會效益。現在投資，就能搶佔先機，成為空間智能革命的領跑者！", "audio": "audios/2505.20148v1.mp3", "timestamp": "2025-05-27T20:15:37.960959"}
{"query": "Foundation Model", "id": "2505.20003v1", "url": "http://arxiv.org/abs/2505.20003v1", "title": "TabPFN: One Model to Rule Them All?", "summary": "Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a\ntransformer-based deep learning model for regression and classification on\ntabular data, which they claim \"outperforms all previous methods on datasets\nwith up to 10,000 samples by a wide margin, using substantially less training\ntime.\" Furthermore, they have called TabPFN a \"foundation model\" for tabular\ndata, as it can support \"data generation, density estimation, learning reusable\nembeddings and fine-tuning\". If these statements are well-supported, TabPFN may\nhave the potential to supersede existing modeling approaches on a wide range of\nstatistical tasks, mirroring a similar revolution in other areas of artificial\nintelligence that began with the advent of large language models. In this\npaper, we provide a tailored explanation of how TabPFN works for a statistics\naudience, by emphasizing its interpretation as approximate Bayesian inference.\nWe also provide more evidence of TabPFN's \"foundation model\" capabilities: We\nshow that an out-of-the-box application of TabPFN vastly outperforms\nspecialized state-of-the-art methods for semi-supervised parameter estimation,\nprediction under covariate shift, and heterogeneous treatment effect\nestimation. We further show that TabPFN can outperform LASSO at sparse\nregression and can break a robustness-efficiency trade-off in classification.\nAll experiments can be reproduced using the code provided at\nhttps://github.com/qinglong-tian/tabpfn_study\n(https://github.com/qinglong-tian/tabpfn_study).", "authors": ["Qiong Zhang", "Yan Shuo Tan", "Qinglong Tian", "Pengfei Li"], "published_date": "2025-05-26", "title_zh": "TabPFN：一模型統治天下？", "summary_zh": "TabPFN是一個基於Transformer的深度學習模型，適用於表格數據的迴歸和分類。論文聲稱它在最多10,000個樣本的數據集上，性能遠超以往所有方法，且訓練時間顯著縮短。作者認為TabPFN是表格數據的「基礎模型」，能支持數據生成、密度估計、學習可重用的嵌入和微調。該論文強調TabPFN作為近似貝葉斯推論的解釋，並提供更多證據證明其基礎模型能力：在半監督參數估計、協變量偏移下的預測以及異質處理效應估計方面，TabPFN大幅超越了專業的頂尖方法。此外，TabPFN在稀疏迴歸方面優於LASSO，並能打破分類中的穩健性-效率權衡。", "applications": ["**醫療診斷：** 根據病患的病史、檢查結果等表格數據，快速準確地預測疾病風險或診斷結果，協助醫生做出更明智的決策。", "**金融風控：** 基於客戶的個人資料、交易紀錄等表格數據，評估信用風險、預測欺詐行為，提高貸款審批效率和降低壞帳率。", "**電商推薦：** 根據用戶的瀏覽歷史、購買紀錄等表格數據，個性化推薦商品，提高用戶的購物體驗和銷售額。"], "pitch": "TabPFN 代表了表格數據分析的重大突破，是一個可以大幅簡化機器學習流程，並提高模型效能的基礎模型。想像一下，企業無需耗費大量資源訓練客製化模型，就能快速解決各種預測問題，從風險評估到客戶行為分析。TabPFN 的泛用性與卓越效能，使其具備顛覆傳統表格數據分析市場的潛力，我們相信它能夠為企業帶來顯著的成本節省、效率提升和競爭優勢。我們的目標是將 TabPFN 打造成表格數據領域的 OpenAI，建立一個圍繞 TabPFN 的生態系統，提供易於使用的 API、預訓練模型和行業解決方案，最終實現商業價值最大化。", "audio": "audios/2505.20003v1.mp3", "timestamp": "2025-05-27T20:16:02.796554"}
{"query": "Diffusion Model", "id": "2505.20131v1", "url": "http://arxiv.org/abs/2505.20131v1", "title": "MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning", "summary": "Molecular editing aims to modify a given molecule to optimize desired\nchemical properties while preserving structural similarity. However, current\napproaches typically rely on string-based or continuous representations, which\nfail to adequately capture the discrete, graph-structured nature of molecules,\nresulting in limited structural fidelity and poor controllability. In this\npaper, we propose MolEditRL, a molecular editing framework that explicitly\nintegrates structural constraints with precise property optimization.\nSpecifically, MolEditRL consists of two stages: (1) a discrete graph diffusion\nmodel pretrained to reconstruct target molecules conditioned on source\nstructures and natural language instructions; (2) an editing-aware\nreinforcement learning fine-tuning stage that further enhances property\nalignment and structural preservation by explicitly optimizing editing\ndecisions under graph constraints. For comprehensive evaluation, we construct\nMolEdit-Instruct, the largest and most property-rich molecular editing dataset,\ncomprising 3 million diverse examples spanning single- and multi-property tasks\nacross 10 chemical attributes. Experimental results demonstrate that MolEditRL\nsignificantly outperforms state-of-the-art methods in both property\noptimization accuracy and structural fidelity, achieving a 74\\% improvement in\nediting success rate while using 98\\% fewer parameters.", "authors": ["Yuanxin Zhuang", "Dazhong Shen", "Ying Sun"], "published_date": "2025-05-26", "title_zh": "MolEditRL：基於離散擴散與強化學習的結構保留型分子編輯", "summary_zh": "MolEditRL 是一種新的分子編輯框架，旨在修改現有分子以優化其化學特性，同時保持結構相似性。它利用離散圖擴散模型和強化學習，克服了傳統方法在捕捉分子離散結構和控制精確編輯方面的局限性。MolEditRL 在化學特性優化和結構保真度方面都顯著優於現有技術，參數使用量更少，編輯成功率更高。研究團隊並創建了一個大型分子編輯數據集 MolEdit-Instruct，用於全面評估。", "applications": ["**藥物發現加速：** 基於已有藥物結構，快速優化藥物活性、選擇性和生物利用度，縮短藥物開發週期。", "**材料科學設計：** 針對特定應用場景，例如太陽能電池或新型塑膠，設計具有優異性能的分子結構。", "**化學反應優化：** 改造反應物分子結構，提高化學反應的產率和選擇性，降低生產成本。"], "pitch": "MolEditRL 解決了分子編輯領域長期存在的結構保真度和精確控制難題。其基於離散擴散和強化學習的創新架構，在藥物發現、材料科學等領域具有巨大的應用潛力，能夠大幅加速新分子和材料的設計與開發。相較於現有方法，MolEditRL 參數使用量更少，性能更優越，意味著更高的效率和更低的計算成本。考慮到藥物研發和材料開發市場的龐大規模，MolEditRL 技術的商業價值巨大，尤其是在專利保護和商業化合作方面具有顯著優勢，有望成為化學資訊學和人工智能驅動的創新平台。", "audio": "audios/2505.20131v1.mp3", "timestamp": "2025-05-27T20:16:23.614538"}
{"query": "AI", "id": "2505.20246v1", "url": "http://arxiv.org/abs/2505.20246v1", "title": "On Path to Multimodal Historical Reasoning: HistBench and HistAgent", "summary": "Recent advances in large language models (LLMs) have led to remarkable\nprogress across domains, yet their capabilities in the humanities, particularly\nhistory, remain underexplored. Historical reasoning poses unique challenges for\nAI, involving multimodal source interpretation, temporal inference, and\ncross-linguistic analysis. While general-purpose agents perform well on many\nexisting benchmarks, they lack the domain-specific expertise required to engage\nwith historical materials and questions. To address this gap, we introduce\nHistBench, a new benchmark of 414 high-quality questions designed to evaluate\nAI's capacity for historical reasoning and authored by more than 40 expert\ncontributors. The tasks span a wide range of historical problems-from factual\nretrieval based on primary sources to interpretive analysis of manuscripts and\nimages, to interdisciplinary challenges involving archaeology, linguistics, or\ncultural history. Furthermore, the benchmark dataset spans 29 ancient and\nmodern languages and covers a wide range of historical periods and world\nregions. Finding the poor performance of LLMs and other agents on HistBench, we\nfurther present HistAgent, a history-specific agent equipped with carefully\ndesigned tools for OCR, translation, archival search, and image understanding\nin History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of\n27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online\nsearch and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%)\nand Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These\nresults highlight the limitations of existing LLMs and generalist agents and\ndemonstrate the advantages of HistAgent for historical reasoning.", "authors": ["Jiahao Qiu", "Fulian Xiao", "Yimin Wang", "Yuchen Mao", "Yijia Chen", "Xinzhe Juan", "Siran Wang", "Xuan Qi", "Tongcheng Zhang", "Zixin Yao", "Jiacheng Guo", "Yifu Lu", "Charles Argon", "Jundi Cui", "Daixin Chen", "Junran Zhou", "Shuyao Zhou", "Zhanpeng Zhou", "Ling Yang", "Shilong Liu", "Hongru Wang", "Kaixuan Huang", "Xun Jiang", "Yuming Cao", "Yue Chen", "Yunfei Chen", "Zhengyi Chen", "Ruowei Dai", "Mengqiu Deng", "Jiye Fu", "Yunting Gu", "Zijie Guan", "Zirui Huang", "Xiaoyan Ji", "Yumeng Jiang", "Delong Kong", "Haolong Li", "Jiaqi Li", "Ruipeng Li", "Tianze Li", "Zhuoran Li", "Haixia Lian", "Mengyue Lin", "Xudong Liu", "Jiayi Lu", "Jinghan Lu", "Wanyu Luo", "Ziyue Luo", "Zihao Pu", "Zhi Qiao", "Ruihuan Ren", "Liang Wan", "Ruixiang Wang", "Tianhui Wang", "Yang Wang", "Zeyu Wang", "Zihua Wang", "Yujia Wu", "Zhaoyi Wu", "Hao Xin", "Weiao Xing", "Ruojun Xiong", "Weijie Xu", "Yao Shu", "Xiao Yao", "Xiaorui Yang", "Yuchen Yang", "Nan Yi", "Jiadong Yu", "Yangyuxuan Yu", "Huiting Zeng", "Danni Zhang", "Yunjie Zhang", "Zhaoyu Zhang", "Zhiheng Zhang", "Xiaofeng Zheng", "Peirong Zhou", "Linyan Zhong", "Xiaoyin Zong", "Ying Zhao", "Zhenxin Chen", "Lin Ding", "Xiaoyu Gao", "Bingbing Gong", "Yichao Li", "Yang Liao", "Guang Ma", "Tianyuan Ma", "Xinrui Sun", "Tianyi Wang", "Han Xia", "Ruobing Xian", "Gen Ye", "Tengfei Yu", "Wentao Zhang", "Yuxi Wang", "Xi Gao", "Mengdi Wang"], "published_date": "2025-05-26", "title_zh": "邁向多模態歷史推理之路：HistBench與HistAgent", "summary_zh": "本研究指出，目前大型語言模型在歷史領域的推理能力仍有不足。為此，研究團隊推出了HistBench，一個包含414個高品質問題的歷史推理基準測試，涵蓋多模態資料解讀、時間推理及跨語言分析等挑戰。同時，團隊開發了HistAgent，一個專為歷史研究設計的AI代理，配備OCR、翻譯、檔案檢索及圖像理解等工具。實驗結果顯示，HistAgent在HistBench上的表現顯著優於通用型語言模型和AI代理，突顯了專用工具在歷史研究中的優勢。", "applications": ["**博物館導覽 App：** 結合 HistAgent 技術，讓遊客透過掃描文物照片或聆聽解說，就能即時獲得更深入的歷史背景資訊，甚至可以讓 AI 回答遊客提出的相關問題，提升參觀體驗。", "**歷史文獻研究助理：** HistAgent 可以幫助歷史學家快速檢索、翻譯和分析大量的歷史文獻資料，大幅提升研究效率。例如，自動辨識手寫古籍、翻譯不同語言的史料、或分析圖像中的歷史物件。", "**歷史教育遊戲與模擬：** 利用 HistAgent 的歷史推理能力，開發更具互動性和挑戰性的歷史教育遊戲。玩家可以扮演歷史人物，透過與 AI 互動，模擬歷史事件，學習歷史知識。"], "pitch": "我們開發的 HistAgent 是首個專為歷史研究設計的 AI 代理，解決了通用型語言模型在歷史領域推理能力不足的問題。HistBench 作為業界首創的歷史推理基準測試，將成為評估和提升 AI 歷史理解能力的重要工具。HistAgent 應用廣泛，從博物館導覽、學術研究到教育娛樂，都具備顛覆傳統模式的潛力。基於 HistAgent 的技術授權和客製化服務，將為我們帶來可觀的營收，同時，HistBench 的數據集也具備極高的價值，可以出售或用於訓練更強大的 AI 模型。我們相信，HistAgent 將引領歷史研究進入一個全新的數位化時代，成為歷史領域 AI 應用的領導者。", "audio": "audios/2505.20246v1.mp3", "timestamp": "2025-05-27T21:13:39.106521"}
{"query": "Foundation Model", "id": "2505.20202v1", "url": "http://arxiv.org/abs/2505.20202v1", "title": "PathBench: A comprehensive comparison benchmark for pathology foundation models towards precision oncology", "summary": "The emergence of pathology foundation models has revolutionized computational\nhistopathology, enabling highly accurate, generalized whole-slide image\nanalysis for improved cancer diagnosis, and prognosis assessment. While these\nmodels show remarkable potential across cancer diagnostics and prognostics,\ntheir clinical translation faces critical challenges including variability in\noptimal model across cancer types, potential data leakage in evaluation, and\nlack of standardized benchmarks. Without rigorous, unbiased evaluation, even\nthe most advanced PFMs risk remaining confined to research settings, delaying\ntheir life-saving applications. Existing benchmarking efforts remain limited by\nnarrow cancer-type focus, potential pretraining data overlaps, or incomplete\ntask coverage. We present PathBench, the first comprehensive benchmark\naddressing these gaps through: multi-center in-hourse datasets spanning common\ncancers with rigorous leakage prevention, evaluation across the full clinical\nspectrum from diagnosis to prognosis, and an automated leaderboard system for\ncontinuous model assessment. Our framework incorporates large-scale data,\nenabling objective comparison of PFMs while reflecting real-world clinical\ncomplexity. All evaluation data comes from private medical providers, with\nstrict exclusion of any pretraining usage to avoid data leakage risks. We have\ncollected 15,888 WSIs from 8,549 patients across 10 hospitals, encompassing\nover 64 diagnosis and prognosis tasks. Currently, our evaluation of 19 PFMs\nshows that Virchow2 and H-Optimus-1 are the most effective models overall. This\nwork provides researchers with a robust platform for model development and\noffers clinicians actionable insights into PFM performance across diverse\nclinical scenarios, ultimately accelerating the translation of these\ntransformative technologies into routine pathology practice.", "authors": ["Jiabo Ma", "Yingxue Xu", "Fengtao Zhou", "Yihui Wang", "Cheng Jin", "Zhengrui Guo", "Jianfeng Wu", "On Ki Tang", "Huajun Zhou", "Xi Wang", "Luyang Luo", "Zhengyu Zhang", "Du Cai", "Zizhao Gao", "Wei Wang", "Yueping Liu", "Jiankun He", "Jing Cui", "Zhenhui Li", "Jing Zhang", "Feng Gao", "Xiuming Zhang", "Li Liang", "Ronald Cheong Kin Chan", "Zhe Wang", "Hao Chen"], "published_date": "2025-05-26", "title_zh": "PathBench：針對精準腫瘤學的病理學基礎模型之全面比較基準", "summary_zh": "病理學基礎模型(PFM)正改變計算病理學領域，能進行高準確、廣泛的整張玻片影像分析，改善癌症診斷和預後評估。然而，不同癌症類型最佳模型的可變性、評估中的潛在數據洩漏以及缺乏標準化基準等挑戰阻礙了PFM的臨床轉譯。PathBench是一個全面基準，透過多中心內部數據集、涵蓋診斷到預後的完整臨床範圍評估，以及自動化排行榜系統來解決這些問題。PathBench使用來自多家醫療機構的大規模數據，並嚴格排除用於預訓練的數據，以避免數據洩漏。目前評估了19個PFM，結果顯示Virchow2和H-Optimus-1是整體表現最佳的模型。PathBench為研究人員提供了一個強大的模型開發平台，並為臨床醫生提供有關PFM在不同臨床情境中表現的可操作見解，最終加速了這些變革性技術在常規病理學實踐中的應用。", "applications": ["**遠距病理診斷：** 在醫療資源匱乏地區，利用PFM分析數位病理切片，協助病理學家進行遠端診斷，提高診斷效率和準確性。", "**個性化治療方案推薦：** 基於PFM分析結果，預測患者對不同治療方案的反應，為腫瘤科醫生提供更精準的個性化治療方案建議，提高治療成功率。", "**藥物研發加速：** 利用PFM對大量病理影像進行分析，篩選出對特定藥物反應更敏感的腫瘤細胞類型，加速新藥研發進程，降低研發成本。"], "pitch": "PathBench是一個革命性的平台，它解決了精準腫瘤學中病理學基礎模型(PFM)應用的關鍵瓶頸。現有PFM缺乏標準化的、公正的評估基準，阻礙了它們的臨床應用。PathBench提供了一個全面、嚴謹的評估平台，擁有大規模、高質量的多中心病理數據，並有效避免了數據洩漏風險。透過PathBench，研究機構和醫療機構可以客觀地比較和選擇最適合其需求的PFM，從而加速癌症診斷、預後和治療的進步。我們的商業模式包括向製藥公司、醫療器械公司和醫療機構提供PathBench的使用許可，以及提供基於PathBench數據的諮詢服務。我們相信PathBench將成為精準腫瘤學領域的黃金標準，具有巨大的市場潛力，並能顯著改善患者的治療效果。", "audio": "audios/2505.20202v1.mp3", "timestamp": "2025-05-27T21:14:34.698025"}
{"query": "Diffusion Model", "id": "2505.20171v1", "url": "http://arxiv.org/abs/2505.20171v1", "title": "Long-Context State-Space Video World Models", "summary": "Video diffusion models have recently shown promise for world modeling through\nautoregressive frame prediction conditioned on actions. However, they struggle\nto maintain long-term memory due to the high computational cost associated with\nprocessing extended sequences in attention layers. To overcome this limitation,\nwe propose a novel architecture leveraging state-space models (SSMs) to extend\ntemporal memory without compromising computational efficiency. Unlike previous\napproaches that retrofit SSMs for non-causal vision tasks, our method fully\nexploits the inherent advantages of SSMs in causal sequence modeling. Central\nto our design is a block-wise SSM scanning scheme, which strategically trades\noff spatial consistency for extended temporal memory, combined with dense local\nattention to ensure coherence between consecutive frames. We evaluate the\nlong-term memory capabilities of our model through spatial retrieval and\nreasoning tasks over extended horizons. Experiments on Memory Maze and\nMinecraft datasets demonstrate that our approach surpasses baselines in\npreserving long-range memory, while maintaining practical inference speeds\nsuitable for interactive applications.", "authors": ["Ryan Po", "Yotam Nitzan", "Richard Zhang", "Berlin Chen", "Tri Dao", "Eli Shechtman", "Gordon Wetzstein", "Xun Huang"], "published_date": "2025-05-26", "title_zh": "長上下文狀態空間影片世界模型", "summary_zh": "現有的影片擴散模型在透過自迴歸幀預測來進行世界建模方面展現了潛力，但由於注意力層處理長序列時計算成本過高，難以維持長期記憶。為了解決這個限制，我們提出了一種新穎的架構，利用狀態空間模型（SSM）來擴展時間記憶，同時又不影響計算效率。與先前為非因果視覺任務改造SSM的方法不同，我們的方法充分利用了SSM在因果序列建模方面的固有優勢。我們設計的核心是一個分塊SSM掃描方案，它有策略地犧牲了空間一致性來換取更長的時間記憶，並結合密集的局部注意力來確保連續幀之間的一致性。我們通過在延伸的時間範圍內進行空間檢索和推理任務來評估模型的長期記憶能力。在Memory Maze和Minecraft數據集上的實驗表明，我們的方法在保持遠程記憶方面超越了基線，同時保持了適用於互動應用程序的實用推理速度。", "applications": ["**更真實的遊戲AI：** 遊戲AI可以利用這種長期記憶能力，記住玩家過去的行為和遊戲世界的變化，做出更複雜和自然的反應，提供更具沉浸感的遊戲體驗。", "**智慧型影片編輯：** 影片編輯軟體可以使用這種模型來預測影片中接下來的場景，幫助編輯者快速找到需要的片段，並實現更智能化的剪輯和特效添加。", "**自動駕駛輔助系統：** 自動駕駛汽車可以利用這種模型來預測行車環境中的長期變化，例如其他車輛的移動軌跡或行人的行為，從而做出更安全和有效的駕駛決策。"], "pitch": "想像一下，一個AI能像人類一樣記住過去發生的事情，並根據長期記憶進行預測和決策。我們開發的長上下文狀態空間影片世界模型，正是讓AI擁有這種能力的關鍵技術。它解決了現有影片模型在處理長序列時的記憶瓶頸，顯著提升了AI在複雜環境下的感知和推理能力。這項技術在遊戲、影片編輯、自動駕駛等領域具有廣闊的商業前景，能催生出更智能、更高效的應用。我們的模型不僅提升了性能，更降低了計算成本，使其更易於部署和商業化。我們相信，這項技術將引領新一代AI應用的發展，為我們的投資者帶來豐厚的回報。", "audio": "audios/2505.20171v1.mp3", "timestamp": "2025-05-27T21:15:07.603550"}
{"query": "AI", "id": "2505.20148v1", "url": "http://arxiv.org/abs/2505.20148v1", "title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents", "summary": "Spatial Planning is a crucial part in the field of spatial intelligence,\nwhich requires the understanding and planning about object arrangements in\nspace perspective. AI agents with the spatial planning ability can better adapt\nto various real-world applications, including robotic manipulation, automatic\nassembly, urban planning etc. Recent works have attempted to construct\nbenchmarks for evaluating the spatial intelligence of Multimodal Large Language\nModels (MLLMs). Nevertheless, these benchmarks primarily focus on spatial\nreasoning based on typical Visual Question-Answering (VQA) forms, which suffers\nfrom the gap between abstract spatial understanding and concrete task\nexecution. In this work, we take a step further to build a comprehensive\nbenchmark called MineAnyBuild, aiming to evaluate the spatial planning ability\nof open-world AI agents in the Minecraft game. Specifically, MineAnyBuild\nrequires an agent to generate executable architecture building plans based on\nthe given multi-modal human instructions. It involves 4,000 curated spatial\nplanning tasks and also provides a paradigm for infinitely expandable data\ncollection by utilizing rich player-generated content. MineAnyBuild evaluates\nspatial planning through four core supporting dimensions: spatial\nunderstanding, spatial reasoning, creativity, and spatial commonsense. Based on\nMineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based\nagents, revealing the severe limitations but enormous potential in their\nspatial planning abilities. We believe our MineAnyBuild will open new avenues\nfor the evaluation of spatial intelligence and help promote further development\nfor open-world AI agents capable of spatial planning.", "authors": ["Ziming Wei", "Bingqian Lin", "Zijian Jiao", "Yunshuang Nie", "Liang Ma", "Yuecheng Liu", "Yuzheng Zhuang", "Xiaodan Liang"], "published_date": "2025-05-26", "title_zh": "MineAnyBuild：開放世界AI代理空間規劃能力基準測試", "summary_zh": "空間規劃是空間智能的關鍵組成部分，需要理解並規劃物體在空間中的排列。本研究提出一個名為 MineAnyBuild 的綜合基準測試，旨在評估開放世界 AI 代理在 Minecraft 遊戲中的空間規劃能力。MineAnyBuild 要求代理根據給定的多模態人類指令生成可執行的架構建構計畫。它包含 4,000 個精心策劃的空間規劃任務，並提供了一個通過利用豐富的玩家生成內容無限擴展數據收集的範例。MineAnyBuild 通過四個核心支持維度評估空間規劃：空間理解、空間推理、創造力以及空間常識。基於 MineAnyBuild，我們對現有的基於 MLLM 的代理進行了全面評估，揭示了它們在空間規劃能力方面的嚴重局限性和巨大潛力。我們相信 MineAnyBuild 將為空間智能的評估開闢新途徑，並有助於促進能夠進行空間規劃的開放世界 AI 代理的進一步發展。", "applications": ["**智慧家居設計助手：** 讓使用者用口語描述想要建造的房間或建築風格，AI 就能自動生成詳細的設計圖和建材清單，甚至模擬建造過程。", "**機器人倉庫佈局優化：** 針對特定的倉庫空間和貨物種類，AI 能夠自動規劃最有效率的貨架擺放和機器人移動路線，提升倉庫的運營效率。", "**城市規劃模擬：** 根據人口增長、交通流量和環境因素等數據，AI 能夠模擬不同城市規劃方案的長期影響，幫助決策者做出更明智的選擇。"], "pitch": "MineAnyBuild 是一個革命性的空間智能評估平台，它利用 Minecraft 遊戲的無限可能性，為開放世界 AI 代理的空間規劃能力建立了嚴謹且可擴展的基準測試。目前 AI 在空間規劃方面能力不足，這限制了其在許多領域的應用。MineAnyBuild 提供了一個框架，能夠加速相關演算法的開發，並通過其四個核心評估維度(空間理解、推理、創造力、常識) 確保 AI 具備真正實用的空間智能。想像一下，一個能夠理解人類指令並自動設計建築、優化倉庫或規劃城市的 AI，其潛在市場規模巨大，涵蓋建築、物流、城市規劃等多個產業。我們相信，MineAnyBuild 將是未來空間智能發展的基石，具有極高的投資價值。", "audio": "audios/2505.20148v1.mp3", "timestamp": "2025-05-27T22:13:29.432677"}
{"query": "Foundation Model", "id": "2505.20003v1", "url": "http://arxiv.org/abs/2505.20003v1", "title": "TabPFN: One Model to Rule Them All?", "summary": "Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a\ntransformer-based deep learning model for regression and classification on\ntabular data, which they claim \"outperforms all previous methods on datasets\nwith up to 10,000 samples by a wide margin, using substantially less training\ntime.\" Furthermore, they have called TabPFN a \"foundation model\" for tabular\ndata, as it can support \"data generation, density estimation, learning reusable\nembeddings and fine-tuning\". If these statements are well-supported, TabPFN may\nhave the potential to supersede existing modeling approaches on a wide range of\nstatistical tasks, mirroring a similar revolution in other areas of artificial\nintelligence that began with the advent of large language models. In this\npaper, we provide a tailored explanation of how TabPFN works for a statistics\naudience, by emphasizing its interpretation as approximate Bayesian inference.\nWe also provide more evidence of TabPFN's \"foundation model\" capabilities: We\nshow that an out-of-the-box application of TabPFN vastly outperforms\nspecialized state-of-the-art methods for semi-supervised parameter estimation,\nprediction under covariate shift, and heterogeneous treatment effect\nestimation. We further show that TabPFN can outperform LASSO at sparse\nregression and can break a robustness-efficiency trade-off in classification.\nAll experiments can be reproduced using the code provided at\nhttps://github.com/qinglong-tian/tabpfn_study\n(https://github.com/qinglong-tian/tabpfn_study).", "authors": ["Qiong Zhang", "Yan Shuo Tan", "Qinglong Tian", "Pengfei Li"], "published_date": "2025-05-26", "title_zh": "TabPFN：一統天下的模型？", "summary_zh": "近期研究發表了一款名為 TabPFN 的基於 Transformer 的深度學習模型，用於表格數據的迴歸和分類。據稱，在最多一萬個樣本的數據集上，TabPFN 的效能大幅超越所有先前的方法，且訓練時間更短。作者更稱其為表格數據的「基礎模型」，可支援數據生成、密度估計、學習可重用的嵌入以及微調等功能。本文針對統計領域的受眾，以近似貝氏推論的角度，詳細解釋了 TabPFN 的運作原理，並提供了更多證據來支持其「基礎模型」的潛力：TabPFN 在半監督參數估計、協變量偏移下的預測和異質性處理效應估計等任務中，超越了最先進的專用方法。此外，TabPFN 在稀疏迴歸方面優於 LASSO，並打破了分類中的穩健性-效率權衡。", "applications": ["**金融風險評估：** TabPFN 可以快速準確地評估貸款申請者的違約風險，甚至在數據量不足的情況下也能提供可靠的判斷，幫助銀行更有效地控制信貸風險。", "**個性化醫療：**基於患者的病歷數據，TabPFN 可以預測患者對不同治療方案的反應，幫助醫生制定更精準的治療方案，提高治療效果。", "**智能製造：** TabPFN 可以分析工廠的生產數據，預測設備的故障風險，優化生產流程，提高生產效率，減少資源浪費。"], "pitch": "TabPFN 是一款顛覆性的表格數據分析模型，擁有媲美大型語言模型的潛力，能在各行各業釋放數據價值。它的核心優勢在於無需大量的訓練數據就能取得卓越的性能，解決了傳統機器學習模型在數據稀缺場景下的困境。想像一下，無需耗費巨資收集海量數據，就能立即獲得高度準確的預測和洞察，這將大幅降低企業的AI應用門檻，加速各行各業的智能化轉型。TabPFN 的應用範圍廣泛，從金融、醫療到製造業，都存在巨大的商業價值。我們相信，TabPFN 將成為企業在數據驅動決策方面不可或缺的工具，而我們將致力於將 TabPFN 的潛力最大化，打造一個基於 TabPFN 的生態系統，為客戶提供全面的解決方案，搶佔市場先機。", "audio": "audios/2505.20003v1.mp3", "timestamp": "2025-05-27T22:14:19.743655"}
{"query": "Diffusion Model", "id": "2505.20131v1", "url": "http://arxiv.org/abs/2505.20131v1", "title": "MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning", "summary": "Molecular editing aims to modify a given molecule to optimize desired\nchemical properties while preserving structural similarity. However, current\napproaches typically rely on string-based or continuous representations, which\nfail to adequately capture the discrete, graph-structured nature of molecules,\nresulting in limited structural fidelity and poor controllability. In this\npaper, we propose MolEditRL, a molecular editing framework that explicitly\nintegrates structural constraints with precise property optimization.\nSpecifically, MolEditRL consists of two stages: (1) a discrete graph diffusion\nmodel pretrained to reconstruct target molecules conditioned on source\nstructures and natural language instructions; (2) an editing-aware\nreinforcement learning fine-tuning stage that further enhances property\nalignment and structural preservation by explicitly optimizing editing\ndecisions under graph constraints. For comprehensive evaluation, we construct\nMolEdit-Instruct, the largest and most property-rich molecular editing dataset,\ncomprising 3 million diverse examples spanning single- and multi-property tasks\nacross 10 chemical attributes. Experimental results demonstrate that MolEditRL\nsignificantly outperforms state-of-the-art methods in both property\noptimization accuracy and structural fidelity, achieving a 74\\% improvement in\nediting success rate while using 98\\% fewer parameters.", "authors": ["Yuanxin Zhuang", "Dazhong Shen", "Ying Sun"], "published_date": "2025-05-26", "title_zh": "MolEditRL：透過離散擴散與強化學習實現結構保留的分子編輯", "summary_zh": "MolEditRL 是一種新的分子編輯方法，它結合了離散擴散模型和強化學習，能夠在修改分子以優化其化學性質的同時，更好地保留原始分子的結構。傳統方法通常使用字串或連續表示法，無法充分捕捉分子圖形的離散特性，導致結構保真度不足和可控性差。MolEditRL 通過預訓練的圖形擴散模型（用於結構重建）和強化學習微調（用於屬性優化和結構保持）來解決這個問題。研究團隊還創建了一個大型的分子編輯數據集 MolEdit-Instruct，證明 MolEditRL 在屬性優化準確性和結構保真度方面都顯著優於現有技術。", "applications": ["**藥物發現：** 在已知藥物分子的基礎上，修改分子結構以提高藥效、降低副作用，或延長藥物半衰期，創造更有效的候選藥物。", "**材料科學：** 針對特定材料（如聚合物、奈米材料）的分子結構進行微調，以改善其物理、化學或機械性能，例如提高強度、耐熱性或導電性。", "**化學品優化：** 針對特定工業化學品，通過分子編輯來改善其生產效率、降低成本或提高安全性，例如優化催化劑的活性或降低有害物質的生成。"], "pitch": "MolEditRL 代表了分子編輯領域的重大突破，通過結合離散擴散模型和強化學習，它能夠在優化化學性質的同時，保持卓越的結構保真度。這解決了傳統方法的關鍵瓶頸，為藥物發現、材料科學和化學品優化等領域打開了新的可能性。其顯著優於現有技術的性能，加上更少的參數需求，意味著更快的研發週期和更低的計算成本。我們相信 MolEditRL 具有巨大的商業價值，可以幫助企業加速創新，開發更高效、更安全、更具競爭力的產品。其在藥物發現領域的潛力尤其引人注目，可以顯著降低新藥開發的成本和時間，為人類健康帶來福音。基於 MolEditRL 的平台可以提供結構導向的分子編輯服務，或作為強大的研發工具，賦能各行業的創新團隊。", "audio": "audios/2505.20131v1.mp3", "timestamp": "2025-05-27T22:15:19.376214"}
{"query": "AI", "id": "2505.20246v1", "url": "http://arxiv.org/abs/2505.20246v1", "title": "On Path to Multimodal Historical Reasoning: HistBench and HistAgent", "summary": "Recent advances in large language models (LLMs) have led to remarkable\nprogress across domains, yet their capabilities in the humanities, particularly\nhistory, remain underexplored. Historical reasoning poses unique challenges for\nAI, involving multimodal source interpretation, temporal inference, and\ncross-linguistic analysis. While general-purpose agents perform well on many\nexisting benchmarks, they lack the domain-specific expertise required to engage\nwith historical materials and questions. To address this gap, we introduce\nHistBench, a new benchmark of 414 high-quality questions designed to evaluate\nAI's capacity for historical reasoning and authored by more than 40 expert\ncontributors. The tasks span a wide range of historical problems-from factual\nretrieval based on primary sources to interpretive analysis of manuscripts and\nimages, to interdisciplinary challenges involving archaeology, linguistics, or\ncultural history. Furthermore, the benchmark dataset spans 29 ancient and\nmodern languages and covers a wide range of historical periods and world\nregions. Finding the poor performance of LLMs and other agents on HistBench, we\nfurther present HistAgent, a history-specific agent equipped with carefully\ndesigned tools for OCR, translation, archival search, and image understanding\nin History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of\n27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online\nsearch and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%)\nand Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These\nresults highlight the limitations of existing LLMs and generalist agents and\ndemonstrate the advantages of HistAgent for historical reasoning.", "authors": ["Jiahao Qiu", "Fulian Xiao", "Yimin Wang", "Yuchen Mao", "Yijia Chen", "Xinzhe Juan", "Siran Wang", "Xuan Qi", "Tongcheng Zhang", "Zixin Yao", "Jiacheng Guo", "Yifu Lu", "Charles Argon", "Jundi Cui", "Daixin Chen", "Junran Zhou", "Shuyao Zhou", "Zhanpeng Zhou", "Ling Yang", "Shilong Liu", "Hongru Wang", "Kaixuan Huang", "Xun Jiang", "Yuming Cao", "Yue Chen", "Yunfei Chen", "Zhengyi Chen", "Ruowei Dai", "Mengqiu Deng", "Jiye Fu", "Yunting Gu", "Zijie Guan", "Zirui Huang", "Xiaoyan Ji", "Yumeng Jiang", "Delong Kong", "Haolong Li", "Jiaqi Li", "Ruipeng Li", "Tianze Li", "Zhuoran Li", "Haixia Lian", "Mengyue Lin", "Xudong Liu", "Jiayi Lu", "Jinghan Lu", "Wanyu Luo", "Ziyue Luo", "Zihao Pu", "Zhi Qiao", "Ruihuan Ren", "Liang Wan", "Ruixiang Wang", "Tianhui Wang", "Yang Wang", "Zeyu Wang", "Zihua Wang", "Yujia Wu", "Zhaoyi Wu", "Hao Xin", "Weiao Xing", "Ruojun Xiong", "Weijie Xu", "Yao Shu", "Xiao Yao", "Xiaorui Yang", "Yuchen Yang", "Nan Yi", "Jiadong Yu", "Yangyuxuan Yu", "Huiting Zeng", "Danni Zhang", "Yunjie Zhang", "Zhaoyu Zhang", "Zhiheng Zhang", "Xiaofeng Zheng", "Peirong Zhou", "Linyan Zhong", "Xiaoyin Zong", "Ying Zhao", "Zhenxin Chen", "Lin Ding", "Xiaoyu Gao", "Bingbing Gong", "Yichao Li", "Yang Liao", "Guang Ma", "Tianyuan Ma", "Xinrui Sun", "Tianyi Wang", "Han Xia", "Ruobing Xian", "Gen Ye", "Tengfei Yu", "Wentao Zhang", "Yuxi Wang", "Xi Gao", "Mengdi Wang"], "published_date": "2025-05-26", "title_zh": "邁向多模態歷史推理之路：HistBench與HistAgent", "summary_zh": "本研究發現，大型語言模型 (LLMs) 在歷史領域表現不足，原因在於歷史推理需要多模態資料解讀、時間推論和跨語言分析等專業能力。為解決此問題，我們創建了 HistBench，一個包含 414 個高品質歷史問題的基準測試，旨在評估 AI 的歷史推理能力。此外，我們開發了 HistAgent，一個專為歷史領域設計的智能體，配備 OCR、翻譯、文獻檢索和圖像理解等工具。實驗結果顯示，HistAgent 在 HistBench 上的表現明顯優於通用型 LLMs 和智能體，證明了專為歷史領域設計的 AI 具有顯著優勢。", "applications": ["數位化歷史研究：協助學者快速分析大量歷史文獻、手稿、照片等資料，加速研究進程並發現新的歷史觀點。", "互動式歷史學習：創建更沉浸式、互動式的歷史學習體驗，學生可與 AI 智能體互動，探索歷史事件、人物和文化。", "文化遺產保護與修復：輔助歷史文物修復師進行文物分析、解讀和重建，提供更精準的修復方案。"], "pitch": "HistBench和HistAgent代表了AI在歷史領域的重大突破。我們正在構建一個能夠真正理解、分析和推理歷史的AI，這開啟了巨大的商業潛力。 HistBench作為一個標準化的評估平台，將引領歷史AI的發展方向，而HistAgent則證明了專用型AI在特定領域的優勢。其商業價值體現在：1)提供領先的歷史研究工具，面向學術機構和歷史研究者，收取訂閱費用；2)賦能教育機構，打造互動式的歷史學習產品，提升教學效果和學生參與度；3)為文化遺產保護機構提供專業的AI解決方案，提升工作效率和修復品質。我們相信，HistBench和HistAgent將引領歷史AI的浪潮，並在學術、教育和文化遺產領域創造巨大的價值。", "audio": "audios/2505.20246v1.mp3", "timestamp": "2025-05-27T23:13:34.109414"}
{"query": "Foundation Model", "id": "2505.20202v1", "url": "http://arxiv.org/abs/2505.20202v1", "title": "PathBench: A comprehensive comparison benchmark for pathology foundation models towards precision oncology", "summary": "The emergence of pathology foundation models has revolutionized computational\nhistopathology, enabling highly accurate, generalized whole-slide image\nanalysis for improved cancer diagnosis, and prognosis assessment. While these\nmodels show remarkable potential across cancer diagnostics and prognostics,\ntheir clinical translation faces critical challenges including variability in\noptimal model across cancer types, potential data leakage in evaluation, and\nlack of standardized benchmarks. Without rigorous, unbiased evaluation, even\nthe most advanced PFMs risk remaining confined to research settings, delaying\ntheir life-saving applications. Existing benchmarking efforts remain limited by\nnarrow cancer-type focus, potential pretraining data overlaps, or incomplete\ntask coverage. We present PathBench, the first comprehensive benchmark\naddressing these gaps through: multi-center in-hourse datasets spanning common\ncancers with rigorous leakage prevention, evaluation across the full clinical\nspectrum from diagnosis to prognosis, and an automated leaderboard system for\ncontinuous model assessment. Our framework incorporates large-scale data,\nenabling objective comparison of PFMs while reflecting real-world clinical\ncomplexity. All evaluation data comes from private medical providers, with\nstrict exclusion of any pretraining usage to avoid data leakage risks. We have\ncollected 15,888 WSIs from 8,549 patients across 10 hospitals, encompassing\nover 64 diagnosis and prognosis tasks. Currently, our evaluation of 19 PFMs\nshows that Virchow2 and H-Optimus-1 are the most effective models overall. This\nwork provides researchers with a robust platform for model development and\noffers clinicians actionable insights into PFM performance across diverse\nclinical scenarios, ultimately accelerating the translation of these\ntransformative technologies into routine pathology practice.", "authors": ["Jiabo Ma", "Yingxue Xu", "Fengtao Zhou", "Yihui Wang", "Cheng Jin", "Zhengrui Guo", "Jianfeng Wu", "On Ki Tang", "Huajun Zhou", "Xi Wang", "Luyang Luo", "Zhengyu Zhang", "Du Cai", "Zizhao Gao", "Wei Wang", "Yueping Liu", "Jiankun He", "Jing Cui", "Zhenhui Li", "Jing Zhang", "Feng Gao", "Xiuming Zhang", "Li Liang", "Ronald Cheong Kin Chan", "Zhe Wang", "Hao Chen"], "published_date": "2025-05-26", "title_zh": "PathBench：針對精準腫瘤學的病理學基礎模型綜合比較基準", "summary_zh": "PathBench是一個全面的病理學基礎模型基準測試平台，旨在解決目前模型在癌症診斷、預後評估等方面存在的問題，例如：不同癌症類型最佳模型差異、評估中潛在的數據洩漏，以及缺乏標準化基準。 PathBench透過包含多中心、涵蓋常見癌症的數據集，並嚴格防止數據洩漏，且評估範圍涵蓋從診斷到預後的完整臨床範圍，以及自動化的排行榜系統，來提供客觀、全面的模型比較。 PathBench的目標是加速這些轉型技術在常規病理學實踐中的應用。", "applications": ["**個性化癌症治療方案推薦：** 醫生可以根據PathBench的評估結果，針對特定癌症類型和患者特徵，選擇最有效的病理學基礎模型，輔助制定更精準的治療方案。", "**遠程病理診斷輔助：** 在病理學家資源匱乏的地區，可以使用PathBench驗證過的模型，進行遠程病理診斷，提高診斷效率和準確性。", "**藥物研發加速：** 製藥公司可以使用PathBench來評估不同模型在識別藥物敏感性和耐藥性方面的能力，從而加速藥物研發進程。"], "pitch": "PathBench解決了病理學基礎模型商業化的關鍵痛點：缺乏標準化、可信賴的評估基準。我們提供一個獨立、客觀的平台，讓醫院、藥廠和AI公司能有效評估不同模型的性能，降低採用風險，加速臨床應用。 我們透過獨家取得的多中心病理數據和嚴格的洩漏防護機制，確保評估結果的真實性和可靠性。 PathBench不僅能提高癌症診斷和治療的精準度，還能加速藥物研發，具有巨大的市場潛力，有望成為病理AI領域的黃金標準。 我們將以SaaS模式提供服務，並根據數據量和功能模組進行收費，預計將在未來三年內實現快速增長。", "audio": "audios/2505.20202v1.mp3", "timestamp": "2025-05-27T23:14:19.799856"}
{"query": "Diffusion Model", "id": "2505.20171v1", "url": "http://arxiv.org/abs/2505.20171v1", "title": "Long-Context State-Space Video World Models", "summary": "Video diffusion models have recently shown promise for world modeling through\nautoregressive frame prediction conditioned on actions. However, they struggle\nto maintain long-term memory due to the high computational cost associated with\nprocessing extended sequences in attention layers. To overcome this limitation,\nwe propose a novel architecture leveraging state-space models (SSMs) to extend\ntemporal memory without compromising computational efficiency. Unlike previous\napproaches that retrofit SSMs for non-causal vision tasks, our method fully\nexploits the inherent advantages of SSMs in causal sequence modeling. Central\nto our design is a block-wise SSM scanning scheme, which strategically trades\noff spatial consistency for extended temporal memory, combined with dense local\nattention to ensure coherence between consecutive frames. We evaluate the\nlong-term memory capabilities of our model through spatial retrieval and\nreasoning tasks over extended horizons. Experiments on Memory Maze and\nMinecraft datasets demonstrate that our approach surpasses baselines in\npreserving long-range memory, while maintaining practical inference speeds\nsuitable for interactive applications.", "authors": ["Ryan Po", "Yotam Nitzan", "Richard Zhang", "Berlin Chen", "Tri Dao", "Eli Shechtman", "Gordon Wetzstein", "Xun Huang"], "published_date": "2025-05-26", "title_zh": "長上下文狀態空間影片世界模型", "summary_zh": "本研究提出了一種新的影片世界模型架構，它利用狀態空間模型（SSM）來克服傳統影片擴散模型在處理長序列時遇到的長期記憶問題。傳統模型受限於注意力層的運算成本，難以維持長時間的記憶。此架構採用區塊式SSM掃描機制，在空間一致性和擴展時間記憶之間取得平衡，並結合密集局部注意力以確保連續幀之間的連貫性。實驗結果顯示，該模型在記憶迷宮和Minecraft數據集上，於保持長期記憶方面超越了基準模型，同時保持了適用於互動式應用程式的實用推理速度。", "applications": ["**更真實的遊戲AI：** 讓遊戲中的非玩家角色 (NPC) 能記住玩家過去的行為和選擇，從而做出更個性化和更具策略性的反應，提升遊戲體驗。", "**智能家庭助手：** 家庭助手可以理解並記住使用者長期的習慣和偏好，例如每天的例行公事、喜歡的音樂類型等，提供更精確和個人化的服務。", "**自動駕駛的預測能力：** 汽車能更準確地預測其他車輛或行人的行為，例如根據過去的駕駛模式判斷前車是否可能變換車道，提高駕駛安全性。"], "pitch": "我們解決了影片世界模型在處理長時序資料時的關鍵瓶頸，提供了一種更高效、更精準的長期記憶解決方案。目前市場上，基於AI的應用正朝著更智能、更個性化的方向發展。我們的技術能夠賦能遊戲、自動駕駛、智能家居等眾多領域，提升使用者體驗和安全，並降低運算成本。想像一下，一個能記住你所有偏好的智能助手，一個能預測危險的自動駕駛系統，以及一個能根據你的遊戲風格調整難度的AI對手。這些都是我們技術的商業潛力。我們需要資金來擴大模型規模，探索更多應用場景，並建立一支強大的工程團隊，將這項技術推向市場，成為下一代AI的基礎。", "audio": "audios/2505.20171v1.mp3", "timestamp": "2025-05-27T23:15:01.703934"}
{"query": "AI", "id": "2505.20148v2", "url": "http://arxiv.org/abs/2505.20148v2", "title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents", "summary": "Spatial Planning is a crucial part in the field of spatial intelligence,\nwhich requires the understanding and planning about object arrangements in\nspace perspective. AI agents with the spatial planning ability can better adapt\nto various real-world applications, including robotic manipulation, automatic\nassembly, urban planning etc. Recent works have attempted to construct\nbenchmarks for evaluating the spatial intelligence of Multimodal Large Language\nModels (MLLMs). Nevertheless, these benchmarks primarily focus on spatial\nreasoning based on typical Visual Question-Answering (VQA) forms, which suffers\nfrom the gap between abstract spatial understanding and concrete task\nexecution. In this work, we take a step further to build a comprehensive\nbenchmark called MineAnyBuild, aiming to evaluate the spatial planning ability\nof open-world AI agents in the Minecraft game. Specifically, MineAnyBuild\nrequires an agent to generate executable architecture building plans based on\nthe given multi-modal human instructions. It involves 4,000 curated spatial\nplanning tasks and also provides a paradigm for infinitely expandable data\ncollection by utilizing rich player-generated content. MineAnyBuild evaluates\nspatial planning through four core supporting dimensions: spatial\nunderstanding, spatial reasoning, creativity, and spatial commonsense. Based on\nMineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based\nagents, revealing the severe limitations but enormous potential in their\nspatial planning abilities. We believe our MineAnyBuild will open new avenues\nfor the evaluation of spatial intelligence and help promote further development\nfor open-world AI agents capable of spatial planning.", "authors": ["Ziming Wei", "Bingqian Lin", "Zijian Jiao", "Yunshuang Nie", "Liang Ma", "Yuecheng Liu", "Yuzheng Zhuang", "Xiaodan Liang"], "published_date": "2025-05-26", "title_zh": "MineAnyBuild：開放世界AI代理空間規劃能力評估基準", "summary_zh": "本研究推出了一個名為MineAnyBuild的基準測試，旨在評估開放世界AI代理在Minecraft遊戲中的空間規劃能力。MineAnyBuild要求AI根據多模態指令生成可執行的建築設計方案。它包含4000個精心設計的空間規劃任務，並提供了一個可無限擴展的數據收集範例。透過空間理解、推理、創造力和常識四個維度評估AI的空間規劃能力。 評估結果顯示，現有的基於多模態大型語言模型（MLLM）的代理在空間規劃方面存在嚴重的局限性，但同時也展現了巨大的潛力。MineAnyBuild有望開闢空間智慧評估的新途徑，並促進具備空間規劃能力的開放世界AI代理的進一步發展。", "applications": ["**智慧家居設計助手：** 讓AI根據使用者的語音和視覺需求，自動生成家居裝修設計方案，並在虛擬環境中預覽。", "**自動倉儲管理：** 根據倉庫空間和貨物特性，AI自動優化貨物擺放位置，提高倉儲效率並減少人工成本。", "**建築工程模擬：** AI根據建築藍圖和周邊環境，模擬建築物的建造過程，預先發現潛在問題並優化施工方案。"], "pitch": "我們打造了 MineAnyBuild，一個針對開放世界AI的空間規劃能力評估基準。想像一下，一個能理解使用者指令，在 Minecraft 這樣複雜環境裡自主完成建築設計的AI。這不僅僅是遊戲，更是現實世界空間智能的雛形。現有AI在這方面仍有巨大進步空間，而 MineAnyBuild 能有效驅動技術發展，應用於智慧家居、倉儲物流、建築工程等多個領域，大幅提升效率並降低成本。這是一個潛力無限的藍海市場，我們期待投資者共同參與，引領空間智能的未來！", "audio": "audios/2505.20148v2.mp3", "timestamp": "2025-05-28T01:05:10.045992"}
{"query": "Foundation Model", "id": "2505.20003v1", "url": "http://arxiv.org/abs/2505.20003v1", "title": "TabPFN: One Model to Rule Them All?", "summary": "Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a\ntransformer-based deep learning model for regression and classification on\ntabular data, which they claim \"outperforms all previous methods on datasets\nwith up to 10,000 samples by a wide margin, using substantially less training\ntime.\" Furthermore, they have called TabPFN a \"foundation model\" for tabular\ndata, as it can support \"data generation, density estimation, learning reusable\nembeddings and fine-tuning\". If these statements are well-supported, TabPFN may\nhave the potential to supersede existing modeling approaches on a wide range of\nstatistical tasks, mirroring a similar revolution in other areas of artificial\nintelligence that began with the advent of large language models. In this\npaper, we provide a tailored explanation of how TabPFN works for a statistics\naudience, by emphasizing its interpretation as approximate Bayesian inference.\nWe also provide more evidence of TabPFN's \"foundation model\" capabilities: We\nshow that an out-of-the-box application of TabPFN vastly outperforms\nspecialized state-of-the-art methods for semi-supervised parameter estimation,\nprediction under covariate shift, and heterogeneous treatment effect\nestimation. We further show that TabPFN can outperform LASSO at sparse\nregression and can break a robustness-efficiency trade-off in classification.\nAll experiments can be reproduced using the code provided at\nhttps://github.com/qinglong-tian/tabpfn_study\n(https://github.com/qinglong-tian/tabpfn_study).", "authors": ["Qiong Zhang", "Yan Shuo Tan", "Qinglong Tian", "Pengfei Li"], "published_date": "2025-05-26", "title_zh": "TabPFN：一個模型統治所有模型？", "summary_zh": "TabPFN 是一個基於 Transformer 的深度學習模型，專門用於處理表格數據的迴歸和分類任務。研究人員聲稱，在樣本數少於 10,000 的數據集中，它在性能和訓練速度上都遠遠超過了以往的所有方法。更重要的是，他們將 TabPFN 視為表格數據的「基礎模型」，認為它具備數據生成、密度估計、學習可重用嵌入和微調等能力。本文進一步解釋了 TabPFN 的運作原理，並強調了其作為近似貝葉斯推斷的解釋。同時，論文也提供了更多證據來支持 TabPFN 作為「基礎模型」的能力，證明它在半監督參數估計、協變量漂移下的預測以及異質性處理效應估計等任務中，都顯著優於專門的、最先進的方法。此外，TabPFN 在稀疏迴歸方面也能超越 LASSO，並打破分類中的穩健性-效率權衡。 所有實驗都可以使用提供的程式碼重現。", "applications": ["**個人化醫療診斷：** 基於患者病歷、基因數據等表格數據，快速準確地預測疾病風險，並推薦個性化的治療方案，無需大量歷史數據訓練專門模型。", "**金融信貸評估：** 根據用戶的信用歷史、收入、消費習慣等表格數據，快速評估貸款申請人的信用風險，避免傳統模型需要大量數據和人工特徵工程的局限。", "**智慧農業決策：** 結合土壤成分、氣象數據、作物生長情況等表格數據，預測作物產量、病蟲害風險，為農民提供精準的灌溉、施肥建議，提升農業生產效率。"], "pitch": "TabPFN 的出現將顛覆表格數據的應用格局。它不僅能大幅提升模型訓練效率和預測精度，更重要的是，它作為一個通用的「基礎模型」，可以廣泛應用於各行各業，無需為每個特定任務都從頭訓練模型。想像一下，一個模型就能解決醫療、金融、農業等多個領域的數據分析問題，這將極大降低數據應用的門檻和成本。投資 TabPFN 相關技術，意味著擁抱一個潛力巨大的市場，特別是在缺乏大量數據的領域，TabPFN 的優勢將更加明顯。 我們相信，TabPFN 將引領表格數據分析的下一個革命，成為AI領域的下一個獨角獸。", "audio": "audios/2505.20003v1.mp3", "timestamp": "2025-05-28T01:05:26.363177"}
{"query": "Diffusion Model", "id": "2505.20131v1", "url": "http://arxiv.org/abs/2505.20131v1", "title": "MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning", "summary": "Molecular editing aims to modify a given molecule to optimize desired\nchemical properties while preserving structural similarity. However, current\napproaches typically rely on string-based or continuous representations, which\nfail to adequately capture the discrete, graph-structured nature of molecules,\nresulting in limited structural fidelity and poor controllability. In this\npaper, we propose MolEditRL, a molecular editing framework that explicitly\nintegrates structural constraints with precise property optimization.\nSpecifically, MolEditRL consists of two stages: (1) a discrete graph diffusion\nmodel pretrained to reconstruct target molecules conditioned on source\nstructures and natural language instructions; (2) an editing-aware\nreinforcement learning fine-tuning stage that further enhances property\nalignment and structural preservation by explicitly optimizing editing\ndecisions under graph constraints. For comprehensive evaluation, we construct\nMolEdit-Instruct, the largest and most property-rich molecular editing dataset,\ncomprising 3 million diverse examples spanning single- and multi-property tasks\nacross 10 chemical attributes. Experimental results demonstrate that MolEditRL\nsignificantly outperforms state-of-the-art methods in both property\noptimization accuracy and structural fidelity, achieving a 74\\% improvement in\nediting success rate while using 98\\% fewer parameters.", "authors": ["Yuanxin Zhuang", "Dazhong Shen", "Ying Sun"], "published_date": "2025-05-26", "title_zh": "MolEditRL：透過離散擴散與強化學習實現結構保持的分子編輯", "summary_zh": "MolEditRL是一個新型分子編輯框架，旨在修改現有分子以優化其化學性質，同時保持結構相似性。它結合了離散圖擴散模型和強化學習，克服了傳統方法在捕捉分子離散和圖結構的不足。第一階段使用圖擴散模型，根據原始結構和自然語言指令重建目標分子。第二階段通過強化學習微調，在圖結構約束下優化編輯決策，進一步提升性質對齊和結構保持。通過大規模分子編輯數據集MolEdit-Instruct的驗證，MolEditRL在性質優化和結構保真度方面均優於現有技術，編輯成功率提升74%，參數數量減少98%。", "applications": ["**藥物發現優化：**針對已知的藥物分子，利用MolEditRL快速找到具有更好藥效、更低副作用或更好溶解性的類似物，加速新藥開發流程。", "**材料科學設計：**修改現有材料的分子結構，以改善其物理或化學性質，例如提高導電性、增強耐腐蝕性或改善光學特性，從而設計出更高效能的材料。", "**個性化化學品定製：**根據特定需求（例如，特定香味、特定顏色或特定化學反應活性）修改現有化學分子，為客戶提供個性化的化學品設計服務。"], "pitch": "MolEditRL 是一個革命性的分子編輯平台，解決了傳統方法在保持結構完整性方面的瓶頸。我們利用離散擴散和強化學習，實現了對分子性質的精準調控，同時保證了分子結構的穩定性。這使得我們能夠更高效、更可靠地設計出具有目標性質的新分子。我們已驗證了MolEditRL在藥物發現和材料科學領域的巨大潛力，尤其是在加速藥物優化和新材料開發方面具有顯著優勢。相較於現有技術，MolEditRL在成功率和效率上都有大幅提升，參數減少也意味著更低的運算成本。我們相信，MolEditRL將成為未來化學和材料研究的重要工具，具有巨大的市場潛力和商業價值，尤其是在快速迭代的藥物開發和高性能材料設計領域。", "audio": "audios/2505.20131v1.mp3", "timestamp": "2025-05-28T01:05:40.343155"}
{"query": "AI", "id": "2505.21500v1", "url": "http://arxiv.org/abs/2505.21500v1", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "published_date": "2025-05-27", "title_zh": "ViewSpatial-Bench：評估視覺-語言模型中多視角空間定位能力", "summary_zh": "本研究指出，現有的視覺-語言模型 (VLMs) 在理解和推理視覺內容方面表現出色，但在需要跨視角理解和空間推理的任務中仍然面臨重大挑戰。它們擅長以自我為中心的空間推理（從相機的角度出發），但當需要採用另一個實體的空間參考框架時，無法推廣到他者中心視角。為此，我們推出了 ViewSpatial-Bench，這是第一個全面的基準測試，專門用於多視角空間定位識別評估，涵蓋五種不同的任務類型，並由自動化的 3D 註釋管線提供精確的方向標籤。對多種 VLM 在 ViewSpatial-Bench 上的全面評估顯示，存在顯著的性能差距：模型在相機視角任務中表現出合理的性能，但在從人的角度進行推理時，準確性會降低。通過在我們的多視角空間數據集上微調 VLM，我們在各項任務中的整體性能提高了 46.24%，突出了我們方法的有效性。這項工作為具體化人工智慧系統中的空間智能建立了一個關鍵基準，並提供了經驗證據，證明建模 3D 空間關係可以提高 VLM 相應的空間理解能力。", "applications": ["**協助視障人士導航：** 透過理解不同視角的空間關係，系統可以為視障人士提供更精確的導航指示，例如「從你的左前方5公尺處，有一個垃圾桶」。", "**遠程協作機器人：** 讓遠程操作人員可以更容易地控制機器人完成任務，即使操作人員看不到機器人所處的環境，也能透過機器人的視覺系統理解環境的空間關係，例如「幫我把桌子上的紅色杯子，移到花瓶的右邊」。", "**遊戲中的 AI 角色：** 提升遊戲中 AI 角色的智慧程度，使其能更好地理解遊戲環境，並做出更逼真的行為，例如一個 AI 士兵能判斷「敵人在建築物的二樓，必須從哪個角度攻擊才能命中」。"], "pitch": "我們開發的 ViewSpatial-Bench，是評估和提升視覺-語言模型空間理解能力的黃金標準。目前市場上缺乏有效評估模型跨視角空間推理能力的工具。ViewSpatial-Bench 不僅能精準評估模型，更提供數據集進行微調，顯著提升模型性能。這項技術在導航、遠程協作機器人和遊戲等領域擁有廣闊的應用前景。想像一下，一個能真正理解人類意圖，並能以人類視角思考的 AI 系統，其商業價值不可估量。我們的目標是將 ViewSpatial-Bench 打造成行業標準，並將微調後的模型應用於各行各業，成為引領下一代 AI 發展的關鍵技術。", "audio": "audios/2505.21500v1.mp3", "timestamp": "2025-05-28T03:09:35.180671"}
{"query": "Diffusion Model", "id": "2505.21488v1", "url": "http://arxiv.org/abs/2505.21488v1", "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "summary": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "published_date": "2025-05-27", "title_zh": "果斷決策：雜訊誘導式佈局用於多主體生成", "summary_zh": "現有的文字生成圖像擴散模型在生成多個不同主體時面臨挑戰，複雜提示詞容易導致主體洩漏，造成數量、屬性和視覺特徵的不準確。 本論文提出一種新方法，從初始雜訊中預測與提示詞對齊的空間佈局，並在去噪過程中不斷精煉。 這種雜訊誘導式佈局避免了與外部強加佈局的衝突，更好地保留了模型先驗知識。 透過小型神經網路預測和精煉演變中的雜訊誘導式佈局，確保主體間清晰邊界，同時保持一致性。實驗結果表明，這種雜訊對齊策略比現有的佈局引導技術，在文字-圖像對齊和更穩定的多主體生成方面表現更好，同時保留了模型原始分佈的豐富多樣性。", "applications": ["**個性化商品設計：** 使用者可以透過文字描述，精準控制圖像中多個元素（例如：客製化手機殼，要求上面有貓咪、狗狗和鮮花的特定位置排列），自動生成符合需求的設計圖，不再需要繁瑣的手動調整。", "**故事繪本創作：** 作者可以藉由文字敘述，輕鬆生成包含多個角色和場景的繪本插圖，每個角色都能依照故事設定出現在適當的位置，大幅降低繪製成本和時間。", "**虛擬實境場景設計：** 設計師可以快速建立包含多個物件的虛擬實境場景，例如：一個房間裡有桌子、椅子、書架，並且精確指定它們的位置和屬性，加速VR/AR內容的開發流程。"], "pitch": "我們正在開發一項突破性的AI技術，能更精準地根據文字描述生成包含多個主體的圖像，解決現有模型容易出現的主體混淆和佈局失控問題。 我們的雜訊誘導式佈局方法，能有效提升生成圖像的準確性和真實感，同時保持模型的多樣性。 這項技術在個性化商品設計、教育娛樂內容創作、虛擬實境開發等領域具有廣闊的應用前景。 透過我們的平台，使用者能夠以更低的成本、更高的效率創造出令人驚艷的視覺內容。 我們相信，這項技術將顛覆傳統的圖像生成模式，開啟一個全新的創意時代。 我們尋求種子輪投資，用於加速技術開發和市場推廣，將這項創新技術推向更廣闊的舞台，打造圖像生成領域的新標準。", "audio": "audios/2505.21488v1.mp3", "timestamp": "2025-05-28T03:10:10.669392"}
{"query": "AI", "id": "2505.21500v1", "url": "http://arxiv.org/abs/2505.21500v1", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "published_date": "2025-05-27", "title_zh": "ViewSpatial-Bench：評估視覺語言模型中多視角空間定位能力", "summary_zh": "視覺語言模型（VLMs）在理解和推理視覺內容方面表現出色，但在需要跨視角理解和空間推理的任務中仍然面臨挑戰。我們發現一個關鍵限制：目前的VLMs主要擅長以自我為中心的空間推理（從相機的角度），但在要求採用另一個實體的空間參考框架時，無法推廣到他心視角。我們推出了ViewSpatial-Bench，這是第一個全面的基準測試，專門用於在五種不同的任務類型中進行多視角空間定位識別評估，並由自動化的3D標註管道提供支援，該管道生成精確的方向標籤。對ViewSpatial-Bench上各種VLMs的全面評估顯示出顯著的性能差異：模型在相機視角任務上表現出合理的性能，但在從人的角度進行推理時，準確性降低。通過在我們的多視角空間數據集上微調VLMs，我們在各項任務上的整體性能提高了46.24％，突出了我們方法的有效性。我們的工作為具身AI系統中的空間智能建立了一個關鍵基準，並提供了經驗證據，表明對3D空間關係建模可以增強VLMs相應的空間理解能力。", "applications": ["**虛擬實境導覽與互動：** 在VR遊戲或教學應用中，讓玩家或使用者可以透過自然語言指令，從NPC或其他角色的角度理解環境，例如指示：「從守衛的視角，告訴我門在哪裡？」。", "**輔助駕駛與自動駕駛：** 提升自動駕駛系統對行人或其他車輛意圖的理解。例如，系統可以從行人的視角判斷其是否會闖紅燈，或者從另一輛車的駕駛員視角預測其變換車道的可能性。", "**遠程協作機器人：** 在遠程維修、手術或救援等場景中，操作員可以透過機器人的多個視角，更好地理解現場環境，並發出更精確的指令。例如，手術機器人可以透過外科醫生的角度進行操作，提高精準度。"], "pitch": "ViewSpatial-Bench解決了視覺語言模型在跨視角空間推理上的重大瓶頸，這是實現真正智能AI的關鍵一步。我們提供了一個標準化的評估平台，並證明了通過精確的數據微調可以顯著提升模型性能。這項技術的潛在商業價值體現在以下幾個方面：\n\n1. **提升現有AI產品的競爭力：** 可以作為一個嵌入式組件，提升現有自動駕駛系統、機器人、VR/AR應用等對空間環境的理解和互動能力，從而在市場上脫穎而出。\n2. **開創新的應用場景：** 實現更自然、更直觀的人機互動，例如，允許用戶通過語言從不同視角理解和操作智能設備，開創全新的應用場景。\n3. **數據即資產：** 我們建立的自動化3D標註管道和多視角空間數據集本身就具有很高的商業價值，可以出售或授權給其他AI公司或研究機構。\n\n總而言之，ViewSpatial-Bench不僅是一個評估基準，更是一個實現更高層次空間智能的基礎設施，具有巨大的商業潛力，值得投資。", "audio": "audios/2505.21500v1.mp3", "timestamp": "2025-05-28T04:18:41.801614"}
{"query": "Foundation Model", "id": "2505.21432v1", "url": "http://arxiv.org/abs/2505.21432v1", "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model", "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.", "authors": ["Haoming Song", "Delin Qu", "Yuanqi Yao", "Qizhi Chen", "Qi Lv", "Yiwen Tang", "Modi Shi", "Guanghui Ren", "Maoqing Yao", "Bin Zhao", "Dong Wang", "Xuelong Li"], "published_date": "2025-05-27", "title_zh": "Hume：在視覺-語言-動作模型中引入系統二思維", "summary_zh": "Hume 是一種新型的視覺-語言-動作（VLA）模型，它模仿人類處理複雜任務時的緩慢思考過程（系統二思維）。Hume 通過價值導向的思考方式和級聯動作去噪技術，提升機器人在複雜環境下的精巧控制能力。它由兩個系統組成：系統二負責價值導向的思考，評估不同動作選項的價值並選擇最佳方案；系統一則是一個輕量級的反應式策略，根據系統二選擇的動作，進行實時、流暢的動作控制。實驗證明，Hume 在多個模擬和真實機器人場景中都優於現有的 VLA 模型。", "applications": ["**精準醫療手術機器人：** Hume 可以讓手術機器人在複雜的體內環境中做出更精確、更安全的決策，例如在狹小的空間中避開關鍵血管，進行精細的組織切除。", "**家庭服務型機器人：** Hume 可以幫助機器人理解更複雜的指令，例如『把廚房桌上的紅蘋果放進冰箱』，並在執行過程中根據環境變化做出調整，避免碰撞或損壞物品。", "**自動化倉儲物流機器人：** Hume 可以讓機器人在擁擠的倉庫中更有效地搬運貨物，避免碰撞和貨物損壞，並根據實時交通狀況優化路線。"], "pitch": "Hume 是一種革命性的機器人控制技術，它通過模仿人類的緩慢思考模式，賦予機器人更強的決策能力和更精確的動作控制。我們相信 Hume 有巨大的商業潛力，可以應用於醫療、服務、物流等多個領域，大幅提升生產效率和服務質量。我們的核心競爭力在於領先的價值導向思考機制和級聯動作去噪技術，這使得 Hume 在複雜環境下的表現遠超現有方案。我們正在尋求投資，以加速 Hume 的商業化落地，並將其打造為下一代機器人控制平台的領頭羊。", "audio": "audios/2505.21432v1.mp3", "timestamp": "2025-05-28T04:18:54.557160"}
{"query": "Diffusion Model", "id": "2505.21488v1", "url": "http://arxiv.org/abs/2505.21488v1", "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "summary": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "published_date": "2025-05-27", "title_zh": "果斷決策：噪音誘導的多主體生成佈局", "summary_zh": "現有的文字生成圖像擴散模型在生成多個不同主體時面臨挑戰。複雜的提示詞容易導致主體洩漏，造成數量、屬性和視覺特徵上的不準確。為了防止主體洩漏，需要知道每個主體的空間位置。目前的方法通常使用外部佈局控制來提供這些空間位置。然而，強制執行預設的佈局經常會與採樣初始噪音所決定的固有佈局相衝突，導致與模型先驗知識的不一致。本文提出了一種新方法，可以預測與提示詞對齊的空間佈局，該佈局源自初始噪音，並在去噪過程中不斷完善。透過依賴這種噪音誘導的佈局，我們避免了與外部強加的佈局衝突，並更好地保留了模型的先驗知識。我們的方法採用一個小型神經網路，在每個去噪步驟中預測和完善不斷演變的噪音誘導佈局，確保主體之間有清晰的邊界，同時保持一致性。實驗結果表明，與現有的佈局引導技術相比，這種噪音對齊策略可以實現更好的文本圖像對齊和更穩定的多主體生成，同時保留了模型原始分佈的豐富多樣性。", "applications": ["**個性化兒童繪本生成:** 父母輸入一段故事，系統自動生成包含多個角色的插圖，每個角色在畫面中的位置和互動關係由AI根據故事內容和噪音誘導自動調整，生成獨一無二的繪本。", "**廣告設計自動化:** 行銷人員輸入產品描述和目標客群，系統生成包含多個產品和人物的廣告素材，AI自動佈局，突出產品賣點，吸引目標客群的目光。", "**遊戲場景快速原型設計:** 遊戲設計師輸入場景描述，系統快速生成包含多個元素（建築、角色、道具）的遊戲場景概念圖，AI根據場景描述和噪音誘導生成符合邏輯且視覺上吸引人的佈局，加速遊戲開發流程。"], "pitch": "我們開發了一種革命性的AI技術，能夠根據文字描述生成包含多個主體的圖像，且避免了傳統方法中常見的主體洩漏問題。透過噪音誘導佈局，我們的技術能生成更準確、更自然的圖像，同時保留了模型原有的創造力。這項技術的商業價值巨大，可以廣泛應用於內容創作、廣告設計、遊戲開發等領域，降低製作成本，提高生產效率，並創造更具吸引力的視覺內容。想象一下，一鍵生成個性化兒童繪本，快速設計出吸引眼球的廣告，或者高效地搭建遊戲場景原型。這不僅僅是技術，更是一個巨大的市場機會，我們正在尋找投資者，一起抓住這個時代的風口，引領AI圖像生成的新紀元。", "audio": "audios/2505.21488v1.mp3", "timestamp": "2025-05-28T04:19:13.025328"}
{"query": "AI", "id": "2505.21500v1", "url": "http://arxiv.org/abs/2505.21500v1", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "published_date": "2025-05-27", "title_zh": "ViewSpatial-Bench：評估視覺語言模型中多視角空間定位能力", "summary_zh": "現有的視覺語言模型（VLMs）在理解和推理視覺內容方面表現出色，但在需要跨視點理解和空間推理的任務中仍面臨挑戰。主要問題是，目前的VLMs擅長以自我為中心的空間推理（從相機視角），但在需要採用其他實體的空間參考系時，無法推廣到以他人為中心的視角。 我們推出了ViewSpatial-Bench，這是第一個專門用於多視角空間定位識別評估的綜合基準，涵蓋五種不同的任務類型，並由自動化的3D註釋流程支援，可產生精確的方向標籤。對各種VLMs在ViewSpatial-Bench上的全面評估表明，存在顯著的性能差距：模型在相機視角任務中表現出合理的性能，但在從人類視角進行推理時，準確性會降低。 通過在我們的多視角空間數據集上對VLMs進行微調，我們在各項任務中的總體性能提高了46.24％，突顯了我們方法的有效性。 我們的研究為具體化人工智能系統中的空間智能建立了一個關鍵基準，並提供了經驗證據，表明建模3D空間關係可以增強VLMs相應的空間理解能力。", "applications": ["**智慧居家導航：** 幫助視障人士使用智慧眼鏡或手機，不僅能告知前方有障礙物，更能提供更精確的空間定位資訊，例如：『前方三點鐘方向，距離兩公尺處有一張椅子，椅背朝向您』，避免碰撞。", "**遠程協作機器人控制：** 工程師可以使用遠端操控的機器人，在危險或難以到達的環境中進行檢查或維修。透過增強機器人的空間理解能力，工程師可以從機器人的視角，更準確地控制機器人的動作，例如：『將機械臂伸向左前方的紅色閥門』，提高工作效率和安全性。", "**增強現實（AR）遊戲：** 開發者可以利用更精確的空間定位資訊，創造更具沉浸感的AR遊戲體驗。例如：玩家可以在真實的房間中，與虛擬角色進行互動，角色可以根據玩家的位置和視角，做出更自然的反應，例如：『怪物會躲到桌子後面，從右側探出頭來』。"], "pitch": "ViewSpatial-Bench 代表了視覺語言模型（VLMs）空間智能領域的重大突破。 目前的 VLM 在空間理解方面存在明顯的盲點，特別是在跨視角推理時。 我們的基準和微調方法填補了這個空白，使 VLM 能夠更有效地理解 3D 空間。 潛在的商業價值體現在多個領域：首先，能大幅提升機器人導航的精準度，應用於倉儲、物流和自動駕駛；其次，可改善遠程協作工具的效率，加速工業維護和建築設計；最後，能夠賦能新一代的 AR/VR 應用，創造更沉浸式的遊戲和教育體驗。 我們的技術不僅提高了 VLM 的性能，還開闢了全新的市場機會。 透過與機器人、AR/VR 公司以及醫療保健機構合作，我們有機會在數十億美元的市場中佔據領先地位。 我們正在尋求投資，以加速技術開發，擴大數據集規模，並建立合作夥伴關係，將 ViewSpatial-Bench 商業化，成為 VLM 空間智能的黃金標準。", "audio": "audios/2505.21500v1.mp3", "timestamp": "2025-05-28T05:19:27.398425"}
{"query": "Foundation Model", "id": "2505.21432v1", "url": "http://arxiv.org/abs/2505.21432v1", "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model", "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.", "authors": ["Haoming Song", "Delin Qu", "Yuanqi Yao", "Qizhi Chen", "Qi Lv", "Yiwen Tang", "Modi Shi", "Guanghui Ren", "Maoqing Yao", "Bin Zhao", "Dong Wang", "Xuelong Li"], "published_date": "2025-05-27", "title_zh": "Hume：在視覺-語言-動作模型中引入系統二思維", "summary_zh": "這篇論文介紹了Hume，一個結合視覺、語言和動作(VLA)的模型，它模仿人類的慢思考方式（系統二思維）來提升機器人在物理世界中的操作能力。Hume透過價值導向的思考，在多個可能的動作選項中選擇最佳方案，然後利用快速反應的系統一來執行精細的動作控制。實驗結果顯示，Hume在模擬和真實機器人環境中都超越了現有的VLA模型。", "applications": ["**智慧型家庭助手：** Hume可以讓機器人更理解複雜的指令，例如『把紅色的蘋果從冰箱裡拿出來，放在綠色的碗旁邊』，並在執行前先思考最佳路徑，避免碰撞或其他錯誤。", "**自動化倉儲物流：** 在複雜的倉儲環境中，機器人需要能夠識別不同的物品、規劃最佳的搬運路線，並且靈活地處理各種突發狀況。Hume 的系統二思維可以幫助機器人做出更明智的決策。", "**精準醫療手術：** 在微創手術中，醫生需要控制精密的機械手臂。Hume 可以協助醫生更精確地執行手術動作，降低手術風險，並提高手術成功率。"], "pitch": "Hume 是下一代機器人控制系統的基石，它通過模擬人類的思考方式，賦予機器人更強大的理解和決策能力。想像一下，一個能夠像人類一樣思考和行動的機器人，將會對製造業、物流業、醫療保健等領域帶來顛覆性的變革。Hume 不僅提高了機器人的精度和效率，更重要的是，它降低了對機器人編程和維護的需求。我們相信，Hume 有潜力成為一個百億美元級別的技術平台，重新定義人機協作的未來。", "audio": "audios/2505.21432v1.mp3", "timestamp": "2025-05-28T05:19:40.704312"}
{"query": "Diffusion Model", "id": "2505.21488v1", "url": "http://arxiv.org/abs/2505.21488v1", "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "summary": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "published_date": "2025-05-27", "title_zh": "果斷決策：噪聲誘導佈局用於多主體生成", "summary_zh": "現有的文本到圖像擴散模型在生成多個不同主體時仍面臨挑戰。複雜的提示詞常常導致主體洩漏，造成數量、屬性和視覺特徵上的錯誤。防止主體間洩漏需要了解每個主體的空間位置。我們提出一種新方法，預測一個與提示詞對齊的空間佈局，該佈局源自初始噪聲，並在去噪過程中不斷完善。通過依賴這種噪聲誘導佈局，我們避免了與外部強加佈局的衝突，並更好地保留了模型的先驗知識。實驗結果表明，與現有的佈局引導技術相比，這種噪聲對齊策略實現了更好的文本-圖像對齊和更穩定的多主體生成，同時保留了模型原始分佈的豐富多樣性。", "applications": ["**創意廣告設計：** 廣告公司可利用該技術快速生成包含多個不同產品的廣告圖片，例如：同時展示新款手機、耳機和智能手錶的圖片，且各產品之間的空間關係和特徵清晰明確，節省設計師的時間和精力。", "**遊戲場景設計：** 遊戲開發者可以使用該技術快速生成包含多個不同角色的遊戲場景圖，例如：一張圖片中包含英雄、怪物和背景元素，且角色和環境之間具有良好的互動和空間關係，提升遊戲開發效率。", "**個性化禮品定制：** 用戶可以輸入包含多個主體的描述性文字，例如“一隻貓和一隻狗在海灘上玩耍”，然後生成獨特的個性化圖片，用於製作手機殼、抱枕、馬克杯等禮品。"], "pitch": "我們正在開發一種突破性的圖像生成技術，它能根據文字提示，生成包含多個獨立主體的圖片，並有效避免主體間的干擾和洩漏。與傳統方法相比，我們的技術能生成更準確、更逼真且更具創意的圖像。想象一下，你能輕易生成包含多種產品的引人注目的廣告素材，或者快速創建複雜的遊戲場景。我們的技術擁有廣闊的商業前景，涵蓋廣告、遊戲、電商、教育和個性化定制等多個領域。 我們正在尋求[金額]的種子輪融資，用於擴大研發團隊，提升模型性能，並拓展市場渠道。我們相信，我們的技術將徹底改變圖像生成行業，並為投資者帶來豐厚的回報。", "audio": "audios/2505.21488v1.mp3", "timestamp": "2025-05-28T05:19:58.060840"}
{"query": "AI", "id": "2505.21500v1", "url": "http://arxiv.org/abs/2505.21500v1", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "published_date": "2025-05-27", "title_zh": "ViewSpatial-Bench：評估視覺-語言模型中的多視角空間定位能力", "summary_zh": "視覺-語言模型(VLMs)在理解視覺內容方面表現出色，但在跨視角理解和空間推理方面仍然存在挑戰。現有的VLMs擅長以自我為中心的空間推理（從相機角度），但在需要採用其他實體的空間參考框架時，無法泛化到以客觀為中心的視角。我們推出了ViewSpatial-Bench，這是第一個全面的基準測試，專為跨五種不同任務類型的多視角空間定位識別評估而設計，並由自動化的3D註釋管道支持，該管道生成精確的方向標籤。在ViewSpatial-Bench上對各種VLMs的全面評估揭示了顯著的性能差異：模型在相機視角任務上表現出合理的性能，但在從人類視角進行推理時，準確性會降低。通過在我們的多視角空間數據集上微調VLMs，我們在各項任務中實現了46.24%的總體性能提升，突顯了我們方法的有效性。我們的研究為具身AI系統中的空間智能建立了一個關鍵的基準，並提供了經驗證據，證明建模3D空間關係可以增強VLMs相應的空間理解能力。", "applications": ["**智慧家庭導航:** 讓機器人助理能夠理解人類的指令，例如「把遙控器從沙發左邊的茶几上拿過來」，即使機器人不在同一個位置也能正確執行。", "**AR/VR遊戲體驗:** 提供更逼真的遊戲互動，玩家可以與虛擬環境中的物體進行更自然的互動，例如指示虛擬角色「把劍從架子最上面的右邊拿下來」。", "**遠程協作與指導:** 專家可以透過遠程視角指導現場人員進行維修或操作，例如「把紅色電線從接線盒的右下角拆下來」，即使專家不在現場也能精準指示。"], "pitch": "ViewSpatial-Bench 為視覺-語言模型解決了空間理解的關鍵瓶頸，使其能夠理解和推理不同視角下的空間關係。這將為具身AI、機器人技術、AR/VR等領域帶來革命性的應用。我們的基準測試和微調方法能夠顯著提升模型的空間理解能力，創造更智能、更人性化的AI互動體驗。想像一下，一個機器人可以理解複雜的空間指令，一個AR應用可以提供精準的空間指導，一個虛擬助手可以完全理解你的位置和意圖。這不僅能提升效率，更能創造全新的商業模式。我們的團隊已經證實了技術的可行性，並且擁有了先發優勢，現在我們需要資金加速產品化，搶佔市場先機，成為未來空間智能領域的領導者。", "audio": "audios/2505.21500v1.mp3", "timestamp": "2025-05-28T06:20:29.155172"}
{"query": "Foundation Model", "id": "2505.21432v1", "url": "http://arxiv.org/abs/2505.21432v1", "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model", "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.", "authors": ["Haoming Song", "Delin Qu", "Yuanqi Yao", "Qizhi Chen", "Qi Lv", "Yiwen Tang", "Modi Shi", "Guanghui Ren", "Maoqing Yao", "Bin Zhao", "Dong Wang", "Xuelong Li"], "published_date": "2025-05-27", "title_zh": "Hume: 在視覺-語言-行動模型中引入系統二思維", "summary_zh": "當處理複雜任務時，人類在實際行動前會進行緩慢的思考（系統二思維）。這種思維模式最近在提升大型語言模型（LLMs）解決數位領域的複雜任務方面取得了顯著進展。然而，對於與物理世界互動的機器人基礎模型來說，緩慢思考的潛力仍未被充分探索。本研究提出Hume，一個雙系統視覺-語言-行動（VLA）模型，具有價值引導的系統二思維和級聯動作降噪，旨在探索視覺-語言-行動模型在靈巧機器人控制中類似人類的思考能力。Hume的系統二透過擴展視覺-語言-行動模型主幹，並加入一個新的價值查詢頭來實現價值引導的思考，該價值查詢頭用於估計預測動作的狀態-動作價值。價值引導的思考透過重複採樣多個動作候選，並根據狀態-動作價值選擇其中一個來進行。Hume的系統一是一個輕量級的反應式視覺運動策略，它接收系統二選擇的動作，並執行級聯動作降噪以進行靈巧的機器人控制。在部署時，系統二以低頻率執行價值引導的思考，而系統一非同步地接收系統二選擇的動作候選，並即時預測流暢的動作。實驗結果表明，Hume在多個模擬基準測試和真實機器人部署中，都優於現有的最先進的視覺-語言-行動模型。", "applications": ["**精準農業：** 機器人可以分析農作物圖像和環境數據，以決定最佳的收割時間和方式，例如：輕柔地摘取成熟的番茄，避免損壞其他果實。", "**醫療手術輔助：** 機器人可以協助外科醫生進行精細手術，例如：在狹窄的空間內準確地縫合血管，減少對周圍組織的損傷。", "**居家照護：** 機器人可以協助老年人或殘疾人士完成日常生活任務，例如：安全地從冰箱取出特定物品，並小心地遞給他們。"], "pitch": "Hume 是一個突破性的 VLA 模型，它將人類的『系統二思維』引入機器人控制領域，使其能夠像人類一樣思考和行動。與傳統機器人相比，Hume 具備更強的泛化能力和決策能力，能夠適應複雜和不確定的環境。我們相信，Hume 將引領機器人技術進入一個新的時代，在智能製造、醫療健康、居家服務等領域具有巨大的商業潛力。我們的團隊擁有深厚的機器人、視覺、語言和人工智慧背景，並且已經在真實機器人平台上驗證了 Hume 的有效性。我們正在尋求種子輪融資，以加速 Hume 的商業化進程，搶佔市場先機，成為行業領導者。", "audio": "audios/2505.21432v1.mp3", "timestamp": "2025-05-28T06:20:59.692565"}
{"query": "Diffusion Model", "id": "2505.21488v1", "url": "http://arxiv.org/abs/2505.21488v1", "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "summary": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "published_date": "2025-05-27", "title_zh": "果斷決策：雜訊誘導式佈局用於多主體生成", "summary_zh": "現有的文字生成圖像擴散模型在生成多個不同主體時仍然面臨挑戰。複雜的提示詞容易導致主體洩漏，造成數量、屬性和視覺特徵的不準確。防止主體洩漏需要了解每個主體的空間位置。近期的方法通過外部佈局控制提供這些空間位置。然而，強制執行這種預定的佈局往往與採樣的初始雜訊所決定的固有佈局相衝突，導致與模型的先驗知識不一致。本文介紹了一種新方法，它預測與提示詞對齊的空間佈局，該佈局來自初始雜訊，並在整個去噪過程中進行完善。通過依賴這種雜訊誘導式佈局，我們避免了與外部強加的佈局發生衝突，並更好地保留了模型的先驗知識。我們的方法採用一個小型神經網路來預測和完善每個去噪步驟中不斷演變的雜訊誘導式佈局，確保主體之間有清晰的邊界，同時保持一致性。實驗結果表明，與現有的佈局引導技術相比，這種雜訊對齊策略实现了更好的文字圖像對齊和更穩定的多主體生成，同時保留了模型原始分佈的豐富多樣性。", "applications": ["**客製化兒童故事書生成器：** 家長可以輸入文字描述，例如「一隻戴著紅帽的小貓和一隻抱著藍色玩具熊的小狗在花園裡玩耍」，系統自動生成符合描述且主體清晰分明的圖像，製作獨一無二的兒童故事書。", "**虛擬試穿/試用場景設計：** 用戶可以描述多件商品（例如不同款式的帽子、眼鏡、衣服），系統生成一個人物試穿這些商品的效果圖，確保每件商品的位置和屬性都正確顯示，提供更真實的線上購物體驗。", "**廣告素材自動生成：** 廣告商可以輸入產品描述和想要的場景（例如「一個紅色跑車停在陽光明媚的海灘旁，背景是蔚藍的大海和棕櫚樹」），系統自動生成高質量廣告素材，節省設計成本和時間。"], "pitch": "我們正在開發一種突破性的文字生成圖像技術，解決了現有模型在處理多主體生成時的精度問題。我們的雜訊誘導式佈局方法，能夠確保生成的圖像不僅符合文本描述，而且主體之間的邊界清晰、屬性準確，大大提升了圖像的真實感和可用性。這項技術的潛在商業價值巨大，可以廣泛應用於兒童內容創作、電商、廣告等領域。我們相信，透過這項技術，我們能夠幫助企業降低設計成本、提高內容產出效率，並為用戶帶來更個性化、更逼真的視覺體驗。我們的團隊擁有深厚的AI研發背景，並已取得顯著的實驗成果。我們正在尋求種子輪投資，以加速產品開發和市場推廣，目標是成為文字生成圖像領域的領先者。", "audio": "audios/2505.21488v1.mp3", "timestamp": "2025-05-28T06:21:33.434507"}
{"query": "AI", "id": "2505.21500v1", "url": "http://arxiv.org/abs/2505.21500v1", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "published_date": "2025-05-27", "title_zh": "ViewSpatial-Bench：評估視覺-語言模型中多視角空間定位能力", "summary_zh": "現今的視覺-語言模型（VLMs）在理解視覺內容方面表現出色，但在跨視角理解和空間推理方面仍存在挑戰。這些模型擅長以相機視角進行空間推理，但在需要採用其他實體的空間參考系（非自我中心視角）時表現不佳。我們推出了 ViewSpatial-Bench，這是第一個專門針對多視角空間定位識別評估的綜合基準，包含五種不同的任務類型，並採用自動化的3D標註流程來生成精確的方向標籤。對多種VLMs的全面評估顯示，模型在相機視角任務中表現尚可，但在以人類視角進行推理時準確性降低。通過在我們的多視角空間數據集上微調VLMs，我們在各項任務中實現了 46.24% 的整體性能提升，證明了我們方法的有效性。這項工作為具身AI系統中的空間智能建立了一個重要的基準，並提供了經驗證據表明，對3D空間關係進行建模可以提高VLMs的空間理解能力。", "applications": ["**導航輔助：**協助視障人士或在複雜環境中導航，不僅提供方向，更能以使用者的視角描述周圍環境，例如：「從你的左前方三公尺處，有一張長椅」。", "**虛擬助手：**讓虛擬助手能更準確地理解指令，例如：「把桌子上的紅蘋果從你的右邊移到我的左邊」，並能根據使用者視角調整機器人動作。", "**遊戲AI：**創建更逼真且具有策略性的遊戲AI，讓AI能從不同角色視角分析戰局，制定更有效的戰術，不再只是單純的預設路徑或行為。"], "pitch": "ViewSpatial-Bench解決了視覺-語言模型在多視角空間推理上的關鍵瓶頸，這項技術的突破將大幅提升具身AI系統的智能化水平。我們的基準數據集和微調方法，已經證明了性能的顯著提升，預示著在導航、虛擬助手、遊戲AI等領域擁有巨大的商業潛力。我們可以將這項技術授權給相關企業，或開發更智能的機器人解決方案，搶佔市場先機。我們的競爭優勢在於擁有領先的多視角空間理解能力，以及可規模化的3D標註流程。我們正在尋求種子輪投資，以加速技術商業化，並構建下一代更智能的具身AI應用。", "audio": "audios/2505.21500v1.mp3", "timestamp": "2025-05-28T07:13:48.825078"}
{"query": "Foundation Model", "id": "2505.21432v1", "url": "http://arxiv.org/abs/2505.21432v1", "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model", "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.", "authors": ["Haoming Song", "Delin Qu", "Yuanqi Yao", "Qizhi Chen", "Qi Lv", "Yiwen Tang", "Modi Shi", "Guanghui Ren", "Maoqing Yao", "Bin Zhao", "Dong Wang", "Xuelong Li"], "published_date": "2025-05-27", "title_zh": "Hume：在視覺-語言-動作模型中引入系統二思維", "summary_zh": "人類在執行複雜物理任務前會先進行緩慢的思考。這種思考模式最近在增強大型語言模型（LLMs）解決數位領域的複雜任務方面取得了顯著進展。然而，對於與現實世界互動的機器人基礎模型來說，緩慢思考的潛力尚未得到充分開發。本研究提出 Hume：一種雙系統視覺-語言-動作（VLA）模型，它具有價值引導的系統二思維和級聯動作去噪，探索視覺-語言-動作模型在靈巧機器人控制方面的人類式思考能力。Hume 的系統二透過擴展具有新型價值查詢頭的視覺-語言-動作模型骨幹來估計預測動作的狀態-動作價值，從而實現價值引導的思考。價值引導的思考透過重複採樣多個動作候選者並根據狀態-動作價值選擇一個來進行。Hume 的系統一是一種輕量級的反應式視覺運動策略，它採用系統二選擇的動作並執行級聯動作去噪，以實現靈巧的機器人控制。在部署時，系統二以低頻率進行價值引導的思考，而系統一非同步地接收系統二選擇的動作候選者並即時預測流暢的動作。我們證明，Hume 在多個模擬基準測試和真實機器人部署中優於現有的最先進的視覺-語言-動作模型。", "applications": ["**精密手術輔助機器人：** Hume 能讓機器人更精確地執行手術，透過系統二的思考，機器人能評估各種動作的風險和效益，減少手術失誤。", "**危險環境下的拆彈機器人：** 拆彈任務極其複雜且危險。Hume 提供的緩慢思考能力可讓機器人在評估各種拆彈方案後，選擇最佳方案，最大限度地降低爆炸風險。", "**複雜零件的自動組裝機器人：** 在製造業中，組裝複雜零件需要精確的動作和細緻的判斷。Hume 能夠讓機器人學習並執行這些組裝任務，提高生產效率和產品品質。"], "pitch": "Hume 突破了機器人領域的認知瓶頸，其模仿人類雙系統思考模式的VLA模型，賦予了機器人更強大的決策能力和對複雜環境的適應性。這不僅在靈巧控制方面超越了現有技術，更開創了機器人智能的新方向。從手術輔助到危險任務，Hume 的應用潛力巨大。我們相信，Hume 將成為下一代機器人的核心技術，顛覆傳統產業，創造巨大的商業價值。初期市場可鎖定高端製造業和醫療保健領域，長期目標是將Hume模型廣泛應用於各類智能機器人，打造一個更智能、更高效的未來。", "audio": "audios/2505.21432v1.mp3", "timestamp": "2025-05-28T07:14:06.479484"}
{"query": "Diffusion Model", "id": "2505.21488v1", "url": "http://arxiv.org/abs/2505.21488v1", "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "summary": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "published_date": "2025-05-27", "title_zh": "果斷決策：用於多主體生成的噪音誘導式佈局", "summary_zh": "現有的文字轉圖像擴散模型在生成多個不同主體方面仍面臨挑戰。複雜的提示詞經常導致主體洩漏，造成數量、屬性和視覺特徵的不準確。防止主體間洩漏需要了解每個主體的空間位置。最近的方法通過外部佈局控制提供這些空間位置。然而，強制執行這種規定的佈局經常與採樣的初始噪音所決定的固有佈局相衝突，導致與模型的先驗知識不一致。在這項工作中，我們提出了一種新方法，該方法預測與提示詞對齊的空間佈局，該佈局源自初始噪音，並在整個去噪過程中對其進行優化。通過依賴這種噪音誘導式佈局，我們避免了與外部強加佈局的衝突，並更好地保留了模型的先驗知識。我們的模型採用一個小型神經網路來預測和優化每個去噪步驟中不斷演變的噪音誘導式佈局，確保主體之間清晰的邊界，同時保持一致性。實驗結果表明，與現有的佈局引導技術相比，這種噪音對齊策略實現了更好的文本-圖像對齊和更穩定的多主體生成，同時保留了模型原始分佈的豐富多樣性。", "applications": ["**客製化商品設計：** 讓使用者輸入文字描述，例如「桌上有三顆紅蘋果，旁邊放著一本書」，系統就能自動生成符合描述的圖片，並可進一步應用在手機殼、T恤等客製化商品上。", "**廣告素材快速生成：** 廣告公司可以使用這項技術，根據不同產品和行銷活動的文字描述，快速生成多種不同佈局和主體的廣告圖片，提高廣告素材的製作效率。", "**兒童故事書插圖自動生成：** 作者只需要專注於撰寫故事，系統就可以自動根據故事內容生成對應的插圖，降低製作成本和時間，讓更多人能夠出版自己的故事。"], "pitch": "我們解決了文字生成圖像領域中的一個核心問題：多主體生成時的主體洩漏問題。通過創新地利用噪音誘導式佈局，我們不僅提高了生成圖像的品質和準確性，更簡化了生成流程。這項技術具有巨大的商業潛力，可以廣泛應用於客製化商品、廣告素材生成、遊戲開發、以及教育等領域。我們的競爭優勢在於無需外部佈局控制，更好地保留了模型的先驗知識，從而產生更自然、更符合使用者期望的圖像。我們正在尋求投資，以加速技術研發和市場推廣，目標是成為文字生成圖像領域的領先者，為各行業帶來革命性的創新。", "audio": "audios/2505.21488v1.mp3", "timestamp": "2025-05-28T07:14:24.388380"}
{"query": "AI", "id": "2505.21500v1", "url": "http://arxiv.org/abs/2505.21500v1", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "published_date": "2025-05-27", "title_zh": "ViewSpatial-Bench：評估視覺語言模型中的多視角空間定位能力", "summary_zh": "現有的視覺語言模型 (VLMs) 在理解和推理視覺內容方面表現出色，但在需要跨視角理解和空間推理的任務中仍然存在挑戰。我們的研究發現，目前的VLMs主要擅長以自我為中心的空間推理（從相機的視角），但當需要採用另一個實體的空間參考框架時，無法推廣到以他人為中心的視角。為了解決這個問題，我們推出了ViewSpatial-Bench，這是第一個專為多視角空間定位識別評估而設計的綜合基準，涵蓋五種不同的任務類型，並以自動化的3D註釋流程提供精確的方向標籤。通過對ViewSpatial-Bench上多種VLMs的全面評估，我們發現了一個顯著的性能差異：模型在相機視角的任務中表現出合理的性能，但在從人的視角進行推理時，準確性會降低。通過在我們的多視角空間數據集上對VLMs進行微調，我們在各項任務中的整體性能提高了46.24%，突出了我們方法的有效性。我們的研究建立了一個用於具體化AI系統中空間智能的關鍵基準，並提供了經驗證據表明，建模3D空間關係可以增強VLMs相應的空間理解能力。", "applications": ["導航輔助：幫助視障人士理解周圍環境，例如：「前方五公尺有一張椅子，靠左邊」。可以將手機或穿戴裝置的鏡頭資訊，轉換為空間理解，並用語音描述，解決他們在導航上遇到的困難。", "遠程協作：用於遠程機器人操控，例如：工程師遠程遙控機器人維修設備，機器人可以理解工程師的指令：「將紅色扳手從工作台左側拿起，放到設備右側」，提升維修效率。", "遊戲AI：提升遊戲中AI角色的空間感知能力和互動性，例如：AI隊友可以根據玩家的視角和指令做出更合理的戰術決策：「敵人躲在掩體後方，我從左邊繞過去，你負責火力壓制」。"], "pitch": "我們開發了ViewSpatial-Bench，這是業界首個專注於提升視覺語言模型多視角空間理解能力的基準。現有模型在處理以他人視角為中心的空間任務時表現不佳，這限制了它們在導航、機器人、AR/VR等多個領域的應用。我們的基準和微調方法能有效提升模型在這些任務上的表現，解決了關鍵痛點。商業價值體現在：1) 提升相關產品的可用性和用戶體驗；2) 為相關領域的創新應用提供技術基礎；3) 數據集本身可以商業授權。我們正在尋求種子輪投資，以進一步擴大數據集規模、開發更高效的微調算法，並與合作夥伴探索商業化機會，例如與導航設備廠商、機器人公司、遊戲開發商等合作。", "audio": "audios/2505.21500v1.mp3", "timestamp": "2025-05-28T09:15:07.369830"}
{"query": "Foundation Model", "id": "2505.21432v1", "url": "http://arxiv.org/abs/2505.21432v1", "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model", "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.", "authors": ["Haoming Song", "Delin Qu", "Yuanqi Yao", "Qizhi Chen", "Qi Lv", "Yiwen Tang", "Modi Shi", "Guanghui Ren", "Maoqing Yao", "Bin Zhao", "Dong Wang", "Xuelong Li"], "published_date": "2025-05-27", "title_zh": "Hume：在視覺-語言-行動模型中引入系統二思維", "summary_zh": "Hume 是一種新的視覺-語言-行動 (VLA) 模型，它模擬人類在複雜任務中先進行慢速思考再採取行動的方式，稱為「系統二思維」。Hume 透過價值導向的思考，讓機器人能夠更靈巧地控制動作。它包含兩個系統：系統二負責重複取樣多個行動方案，並根據狀態-行動價值選擇最佳方案；系統一則是一個輕量級的反應式視覺運動策略，負責接收系統二選定的行動，並進行級聯行動去噪，以實現精確控制。實驗證明，Hume 在模擬和真實機器人環境中都優於現有的 VLA 模型。", "applications": ["**精準醫療手術輔助：** Hume 可應用於輔助醫生進行精準醫療手術，例如微創手術或使用機器人手臂進行複雜操作。系統二負責規劃手術步驟，系統一負責精準控制機器人手臂，提高手術成功率和降低風險。", "**自動化倉庫揀貨與包裝：** Hume 可用於自動化倉庫環境，讓機器人能夠自主揀選、分類和包裝商品。系統二負責理解訂單和規劃路線，系統一負責精準抓取和放置商品，提高效率和降低人力成本。", "**危險環境探索與救援：** Hume 可部署在危險環境中，例如核洩漏現場或地震災區，讓機器人能夠自主探索、收集信息和執行救援任務。系統二負責評估風險和規劃路線，系統一負責靈活控制機器人移動和操作，保障人員安全。"], "pitch": "Hume 模型解決了機器人在複雜環境中行動決策的關鍵問題，通過模擬人類的慢速思考模式，顯著提升了機器人的操作精度和效率。這項技術在醫療、物流、安防等領域具有廣闊的應用前景。我們的商業價值在於：\n\n*   **技術領先性：** Hume 模型超越了現有 VLA 模型的性能，具有明顯的技術優勢。\n*   **市場需求：** 各行業對自動化和機器人控制的需求不斷增長，Hume 模型滿足了市場對更智能、更靈活的機器人控制解決方案的需求。\n*   **商業模式：** 可以通過提供 Hume 模型授權、定制化解決方案或 SaaS 服務等方式實現商業化。\n*   **退出策略：** 潛在的退出策略包括被大型科技公司收購，或通過 IPO 進行公開募股。\n\n我們相信，Hume 模型具有顛覆機器人控制領域的潛力，能為投資者帶來豐厚的回報。", "audio": "audios/2505.21432v1.mp3", "timestamp": "2025-05-28T09:15:26.558172"}
{"query": "Diffusion Model", "id": "2505.21488v1", "url": "http://arxiv.org/abs/2505.21488v1", "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "summary": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "published_date": "2025-05-27", "title_zh": "果斷決策：噪音誘導式佈局用於多主體生成", "summary_zh": "現有的文字生成圖像模型在生成多個不同主體時面臨挑戰，複雜的提示詞容易導致主體洩漏，造成數量、屬性和視覺特徵上的不準確。為了避免主體洩漏，需要了解每個主體的空間位置。本研究提出一種新方法，從初始噪音中預測並優化與提示詞對齊的空間佈局。這種噪音誘導式佈局避免了與外部強加佈局的衝突，更好地保留了模型的先驗知識。該方法使用一個小型神經網路在每個去噪步驟預測和優化噪音誘導式佈局，確保主體之間的清晰邊界，同時保持一致性。實驗結果表明，與現有的佈局引導技術相比，這種噪音對齊策略實現了更好的文本圖像對齊和更穩定的多主體生成，同時保留了模型原始分佈的豐富多樣性。", "applications": ["**客製化兒童讀物：**根據文字描述自動生成包含多個角色和場景的兒童讀物插圖，每個角色個性鮮明，互動自然。", "**廣告素材自動生成：**根據廣告文案自動生成包含多個商品和人物的廣告素材，確保商品位置合理，人物互動符合廣告主題。", "**電影分鏡圖快速生成：**編劇可以輸入簡單的文字描述，快速生成多個角色在不同場景下的分鏡圖，幫助視覺化劇本，加速電影製作流程。"], "pitch": "想像一下，一個AI可以根據您的文字描述，精準地創造出包含多個角色和元素的圖像，並且每個角色都栩栩如生，互動自然。這項技術解決了現有AI圖像生成的一個重大瓶頸——多主體生成時的混亂和不準確。我們的噪音誘導式佈局方法，就像一位經驗豐富的導演，確保每個角色都在正確的位置，並且和諧地融入畫面。其商業價值巨大：1. 加速內容創作：從廣告素材到遊戲美術，都能大幅提升生產力。2. 降低設計成本：無需昂貴的專業設計師，即可快速生成高質量的視覺內容。3. 開啟全新應用場景：客製化內容、教育工具、娛樂體驗等，想像空間無限。 我們相信，這項技術將徹底改變內容創作的方式，成為新一代視覺AI的基石，值得投資者重點關注。", "audio": "audios/2505.21488v1.mp3", "timestamp": "2025-05-28T09:15:44.484952"}
{"query": "AI", "id": "2505.21500v1", "url": "http://arxiv.org/abs/2505.21500v1", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "published_date": "2025-05-27", "title_zh": "ViewSpatial-Bench：評估視覺語言模型中的多視角空間定位能力", "summary_zh": "現有的視覺語言模型(VLMs)雖然在理解視覺內容方面表現出色，但在需要跨視點理解和空間推理的任務中仍存在挑戰。主要問題是它們擅長以自我為中心的空間推理（即從相機角度），但當需要採用其他實體的空間參考系時，就無法推廣到以他人為中心的視角。我們推出了ViewSpatial-Bench，這是首個全面性的基準測試，專為多視點空間定位識別評估而設計，涵蓋五種不同的任務類型，並由自動化的3D標註流程提供精確的方向標籤。對不同VLMs在ViewSpatial-Bench上的評估顯示出顯著的性能差距：模型在相機視角任務上表現良好，但在從人類視角進行推理時，準確性會降低。通過在我們的多視角空間數據集上微調VLMs，我們在各項任務中的總體性能提高了46.24%，突顯了我們方法的有效性。這項工作為具體化AI系統中的空間智能建立了關鍵基準，並提供了經驗證據，表明對3D空間關係進行建模可以增強VLMs的空間理解能力。", "applications": ["**提升機器人導航能力：** 讓機器人能像人類一樣理解不同角度的空間關係，例如「請幫我把桌子左邊的書拿過來」，即使機器人不在桌子左邊也能正確執行。", "**改善虛擬實境(VR)和擴增實境(AR)體驗：** 在VR/AR環境中，能讓使用者以更自然的視角與虛擬對象互動，例如AR教學APP能根據使用者位置，提供更精準的組裝步驟指示。", "**輔助視障人士導航：** 利用多視角空間推理，開發能理解周遭環境的智慧輔助工具，例如語音導航系統能更準確地描述前方物體的位置和方向，協助視障人士安全移動。"], "pitch": "我們打造的ViewSpatial-Bench及相關技術，解決了視覺語言模型在跨視角空間推理方面的關鍵瓶頸。目前市場上缺乏能有效理解多視角空間關係的AI方案，導致機器人在複雜環境下的應用受限，VR/AR體驗不夠自然，視障輔助技術仍不完善。我們的技術能顯著提升VLMs的空間智能，帶來以下商業價值：\n\n1.  **提高機器人智能化程度：** 賦予機器人更強大的空間認知能力，擴展應用場景，例如物流、倉儲、安防等。\n2.  **優化VR/AR用戶體驗：** 打造更沉浸、更自然的互動體驗，提升VR/AR產品的競爭力，吸引更多用戶。\n3.  **拓展無障礙科技市場：** 開發創新的輔助工具，幫助視障人士更好地融入社會，具有巨大的社會價值和潛在市場。\n\n透過投資我們的技術，您將能搶佔市場先機，成為空間智能AI領域的領頭羊，並獲得豐厚的回報。", "audio": "audios/2505.21500v1.mp3", "timestamp": "2025-05-28T10:16:00.930955"}
{"query": "Foundation Model", "id": "2505.21432v1", "url": "http://arxiv.org/abs/2505.21432v1", "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model", "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.", "authors": ["Haoming Song", "Delin Qu", "Yuanqi Yao", "Qizhi Chen", "Qi Lv", "Yiwen Tang", "Modi Shi", "Guanghui Ren", "Maoqing Yao", "Bin Zhao", "Dong Wang", "Xuelong Li"], "published_date": "2025-05-27", "title_zh": "Hume：在視覺-語言-動作模型中引入系統二思維", "summary_zh": "這篇論文介紹了Hume，一個結合視覺、語言和動作(VLA)的模型，它模仿人類在複雜任務中先緩慢思考再行動的模式。Hume透過價值導向的系統二思維，結合了評估動作價值的機制，並利用級聯動作降噪技術，提升機器人在物理世界中的靈巧控制能力。實驗證明，Hume在模擬和真實機器人部署中，都優於現有的VLA模型。", "applications": ["**家庭服務機器人：** 讓機器人能更聰明地完成複雜的家務，例如：精準地將碗盤放入洗碗機，或是在擁擠的廚房中安全地準備食材。", "**醫療手術輔助：** 協助醫生進行精細的手術操作，系統二的思考可以幫助機器人分析手術步驟，減少失誤風險。", "**工廠自動化：** 應用於需要高度靈巧性的生產線上，例如：組裝精密的電子元件，或處理易碎的物品。"], "pitch": "Hume是基於大型語言模型的具身智慧革命性突破，透過模仿人類系統二的思考方式，賦予機器人更強大的決策能力和靈巧性。這解決了目前機器人在複雜環境下表現不佳的核心問題。其潛在商業價值巨大：它可以應用於家庭服務、醫療、製造等各個領域，大幅提升生產效率，降低人力成本，並創造全新的服務模式。我們相信Hume有潛力成為未來智慧機器人的核心技術，帶來數十億美元的市場機會。投資Hume，就是投資未來具身智慧的領導地位。", "audio": "audios/2505.21432v1.mp3", "timestamp": "2025-05-28T10:16:14.752386"}
{"query": "Diffusion Model", "id": "2505.21488v1", "url": "http://arxiv.org/abs/2505.21488v1", "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "summary": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "published_date": "2025-05-27", "title_zh": "果斷決策：雜訊誘導式佈局用於多主體生成", "summary_zh": "現有的文字生成圖像模型在生成多個不同主體時面臨挑戰。複雜的提示詞經常導致主體洩漏，造成數量、屬性和視覺特徵上的不準確。為了避免主體間的洩漏，需要知道每個主體的空間位置。雖然有方法透過外部佈局控制來提供這些空間位置，但強制執行預先設定的佈局往往與採樣初始雜訊所決定的自然佈局相衝突，導致與模型先驗知識的不對齊。本研究提出一種新方法，預測一個與提示詞對齊的空間佈局，該佈局源自初始雜訊，並在整個去噪過程中進行改進。透過依賴這種雜訊誘導式佈局，我們可以避免與外部強加的佈局發生衝突，並更好地保留模型的先驗知識。我們的模型使用一個小型神經網絡，在每個去噪步驟預測和改進不斷演變的雜訊誘導式佈局，確保主體之間清晰的邊界，同時保持一致性。實驗結果表明，與現有的佈局引導技術相比，這種雜訊對齊策略實現了更好的文字圖像對齊和更穩定的多主體生成，同時保留了模型原始分佈的豐富多樣性。", "applications": ["**客製化故事書插圖：** 父母可以輸入一段包含多個角色的故事，系統自動生成精美的插圖，每個角色都清晰可見，且場景佈局符合故事描述。", "**虛擬時尚設計：** 設計師可以輸入描述服裝設計和人物的提示詞，系統自動生成模特穿著該服裝的圖像，並能精確控制模特的位置和姿勢，以便更好地展示設計效果。", "**廣告素材生成：** 廣告商可以輸入包含多個產品和目標受眾的提示詞，系統自動生成吸引眼球的廣告圖像，清晰地展示所有產品，並確保它們在畫面中的佈局符合廣告策略。"], "pitch": "想像一下，能讓文字生成圖像的效率和精準度提升到一個全新的層次。我們的技術不僅能生成圖像，更能精確控制畫面中多個主體的佈局，解決了現有模型常見的主體混淆問題。這意味著更棒的內容創作工具，不論是個人使用者還是大型企業，都能輕鬆打造更具創意、更符合需求的視覺內容。在龐大的AI圖像生成市場中，我們的技術能提供更精確的控制和更穩定的效果，具備顯著的競爭優勢，潛力巨大。我們正在顛覆AI圖像生成產業，為所有人帶來更便捷、更強大的創意表達方式。", "audio": "audios/2505.21488v1.mp3", "timestamp": "2025-05-28T10:16:31.972760"}
{"query": "AI", "id": "2505.21500v1", "url": "http://arxiv.org/abs/2505.21500v1", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "published_date": "2025-05-27", "title_zh": "ViewSpatial-Bench：評估視覺-語言模型中多視角空間定位能力", "summary_zh": "視覺-語言模型在理解和推理視覺內容方面展現了卓越的能力，但在需要跨視點理解和空間推理的任務中仍然存在重大挑戰。目前，視覺-語言模型主要擅長以自我為中心的空間推理（從相機的角度），但當需要採用另一個實體的空間參考系時，無法推廣到以他人為中心的視角。我們推出 ViewSpatial-Bench，這是第一個全面的基準測試，專門用於跨五種不同任務類型的多視點空間定位識別評估，並由自動化的 3D 註釋流程提供支持，該流程可生成精確的方向標籤。對 ViewSpatial-Bench 上多種視覺-語言模型的全面評估顯示出顯著的性能差異：模型在相機視角任務上表現出合理的性能，但在從人類視角進行推理時準確性降低。通過在我們的多視角空間數據集上微調視覺-語言模型，我們在各項任務中的總體性能提高了 46.24%，突出了我們方法的有效性。我們的工作為具體化 AI 系統中的空間智能建立了一個關鍵基準，並提供了經驗證據表明，對 3D 空間關係進行建模可以提高視覺-語言模型相應的空間理解能力。", "applications": ["**導航輔助：** 改善自動駕駛汽車或機器人在複雜環境中的導航能力，使其能夠理解其他行人或車輛的視角，避免碰撞並做出更合理的決策。例如，讓自動駕駛車理解行人想過馬路的意圖（即使行人沒有明確指示）。", "**遠程協作：** 在遠程醫療或遠程維修中，讓專家能夠透過機器人的視角，精確地指導現場人員完成任務，即便專家和現場人員所看到的場景有所不同。例如，醫生可以從手術機器人的角度，指導護士在手術室中的下一步操作。", "**遊戲AI：** 創造更逼真和更具策略性的遊戲AI角色，使其能夠根據玩家或其他角色的視角，做出更智能的反應和決策。例如，讓遊戲中的敵人能夠判斷玩家是否在觀察自己，從而採取不同的行動。"], "pitch": "ViewSpatial-Bench解決了視覺-語言模型在空間推理和多視角理解方面的瓶頸，這對於打造真正智能的AI系統至關重要。我們的基準測試和微調方法，能夠大幅提升模型在現實世界場景中的表現，例如自動駕駛、機器人輔助和擴增實境。市場機會巨大，我們能將AI的能力從單純的視覺識別，提升到能夠理解和模擬複雜空間關係的程度。透過與汽車製造商、機器人公司和遊戲開發商的合作，我們可以快速將這項技術商業化，並在具體化AI領域佔據領先地位。這項技術將會是下一代具身智慧的基石，潛在市場規模極為可觀。", "audio": "audios/2505.21500v1.mp3", "timestamp": "2025-05-28T11:12:48.909510"}
{"query": "Foundation Model", "id": "2505.21432v1", "url": "http://arxiv.org/abs/2505.21432v1", "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model", "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.", "authors": ["Haoming Song", "Delin Qu", "Yuanqi Yao", "Qizhi Chen", "Qi Lv", "Yiwen Tang", "Modi Shi", "Guanghui Ren", "Maoqing Yao", "Bin Zhao", "Dong Wang", "Xuelong Li"], "published_date": "2025-05-27", "title_zh": "Hume: 在視覺-語言-動作模型中引入系統二思考", "summary_zh": "Hume 是一種新型的視覺-語言-動作 (VLA) 模型，它模仿人類在處理複雜任務時的慢思考模式。該模型結合了價值導向的系統二思考和級聯動作降噪，以提升機器人在複雜環境中的控制能力。Hume 的系統二透過評估預測動作的價值來進行價值導向思考，並從多個候選動作中選擇最佳的。系統一則是一個輕量級的反應式視覺運動策略，負責對系統二選擇的動作進行降噪，並實時執行精細的機器人控制。實驗結果表明，Hume 在模擬和真實機器人部署中均優於現有的 VLA 模型。", "applications": ["**智能家居助手:** 讓機器人能更聰明地完成複雜的家務，例如更精確地清潔、整理，並能根據使用者偏好做出調整，而不僅僅是執行預設程序。", "**手術輔助機器人:** Hume 可以協助醫生進行更精確、更安全的微創手術，系統二思考可以幫助機器人評估不同手術方案的風險和收益，並選擇最佳方案。", "**高危險環境作業:**  在核電站維護、災害救援等高危險環境中，Hume 可以讓機器人遠程執行複雜任務，減少人員暴露在危險環境中的風險。"], "pitch": "Hume 模型突破了傳統機器人控制的局限，引入了人類般的思考模式，使得機器人能夠在複雜、多變的環境中做出更明智的決策。其核心價值在於提升了機器人的智能化程度和適應能力，使其能夠應用於更廣泛的領域，例如智能製造、醫療保健、以及高危險環境作業等。通過授權許可Hume 模型以及提供基于Hume 的解決方案，我们能夠快速搶佔市場先機，並在快速成長的機器人智能化領域中建立領導地位。未来的发展方向包括模型轻量化，降低硬件门槛，使其能够应用于消费级机器人产品，从而实现更广阔的市场潜力。", "audio": "audios/2505.21432v1.mp3", "timestamp": "2025-05-28T11:13:04.060134"}
{"query": "Diffusion Model", "id": "2505.21488v1", "url": "http://arxiv.org/abs/2505.21488v1", "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "summary": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "published_date": "2025-05-27", "title_zh": "果斷決策：雜訊誘導式佈局用於多主體生成", "summary_zh": "現有的文字生成圖像模型在生成多個不同主體時面臨挑戰，複雜的提示詞容易導致主體洩漏，造成數量、屬性和視覺特徵的不準確。為了避免主體間的洩漏，需要了解每個主體的空間位置。現有方法透過外部佈局控制提供這些空間位置，但強制執行預先設定的佈局往往會與採樣初始雜訊所決定的固有佈局產生衝突，導致與模型的先驗知識不一致。本研究提出一種新方法，從初始雜訊中預測與提示詞對齊的空間佈局，並在去噪過程中不斷完善。透過依賴這種雜訊誘導式佈局，避免與外部強加的佈局產生衝突，並更好地保留模型的先驗知識。我們的方法使用一個小型神經網路來預測和完善每個去噪步驟中不斷演變的雜訊誘導式佈局，確保主體之間清晰的邊界，同時保持一致性。實驗結果表明，與現有的佈局引導技術相比，這種雜訊對齊策略實現了更好的文字圖像對齊和更穩定的多主體生成，同時保留了模型原始分佈的豐富多樣性。", "applications": ["**個性化兒童故事書生成：** 家長可以輸入文字描述，生成包含多個不同角色且佈局合理的兒童故事插圖，例如「一個小女孩和一隻小狗在公園裡玩耍」。", "**虛擬時尚設計：** 設計師可以描述不同服裝在模特身上的位置和呈現方式，生成具有專業水準的時尚產品展示圖，例如「一件紅色連衣裙和一條黑色皮帶，搭配一個手提包」。", "**遊戲場景快速設計：** 遊戲開發者可以通過文字描述快速生成包含多個物體的遊戲場景素材，例如「一個城堡、一個森林和一條河流」。"], "pitch": "我們解決了文字生成圖像領域的一個關鍵痛點：多主體生成時的主體洩漏問題。我們的雜訊誘導式佈局方法，相較於傳統的佈局控制方法，能顯著提升生成圖像的質量、一致性和多樣性，同時避免了與模型先驗知識的衝突。這項技術的商業價值體現在以下幾個方面：\n\n*   **大幅提升現有圖像生成平台的用戶體驗：** 將我們的技術整合到現有的AI繪圖平台上，可以提供更精準、更可控的多主體圖像生成能力，吸引更多付費用戶。\n*   **開拓新的商業應用場景：** 從個性化內容創作到專業設計輔助，我們的技術可以應用於多個垂直領域，創造新的商業模式和市場機會。\n*   **提高圖像生成效率，降低成本：** 我們的方法能更有效地利用現有模型的能力，減少生成錯誤圖像的數量，從而降低計算成本。\n\n我們相信這項技術具備顛覆現有圖像生成市場的潛力，能夠為投資者帶來豐厚的回報。", "audio": "audios/2505.21488v1.mp3", "timestamp": "2025-05-28T11:13:24.132203"}
{"query": "AI", "id": "2505.21500v1", "url": "http://arxiv.org/abs/2505.21500v1", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "published_date": "2025-05-27", "title_zh": "ViewSpatial-Bench：評估視覺語言模型中的多視角空間定位能力", "summary_zh": "這篇論文提出了一個新的評估基準 ViewSpatial-Bench，專門用來測試視覺語言模型(VLM)在不同視角下的空間定位能力。研究發現，現有的VLM雖然在以相機視角為主的空間推理表現不錯，但在需要從其他實體（例如人）的視角進行空間推理時，能力明顯下降。透過在多視角空間數據集上對VLM進行微調，研究團隊成功提升了模型在相關任務上的表現，增幅達46.24%。這項研究為具身人工智慧系統的空間智能建立了一個重要的基準，並證明建模3D空間關係能夠提升VLM的空間理解能力。", "applications": ["**自動駕駛導航：** 讓自動駕駛汽車不僅能理解自身的周圍環境，還能理解其他行人或車輛的視角和意圖，例如，預測行人是否能看到來車，从而做出更安全的決策。", "**虛擬實境(VR)協作：** 在VR環境中，不同使用者能以各自的視角觀察和互動，提升協作效率。例如，建築師在VR中協同設計，需要理解彼此對於同一建築元素的視角差異。", "**遠程醫療診斷：** 醫生可以通過遙控機器人觀察病人，並以病人的視角進行判斷，例如，判斷病人是否能清楚看到藥瓶上的標籤，從而更好地評估病人的生活自理能力。"], "pitch": "ViewSpatial-Bench解決了視覺語言模型在多視角空間理解上的重大瓶頸，這是具身人工智慧發展的關鍵一步。其商業價值體現在多個領域：首先，能大幅提升自動駕駛的安全性與可靠性，搶佔市場先機；其次，能優化VR/AR的互動體驗，開拓更廣闊的應用場景；再者，能賦能智能機器人更強的環境感知能力，提高其在複雜環境中的適應性。通過授權ViewSpatial-Bench基準和相關模型技術，我們可以與汽車製造商、VR/AR開發商和機器人公司建立戰略合作，快速將技術商業化，並建立行業標準。更重要的是，此項技術是實現真正自主智能體(Autonomous Agent)的基礎，具備巨大的長期增長潛力。", "audio": "audios/2505.21500v1.mp3", "timestamp": "2025-05-28T12:27:36.591037"}
{"query": "Foundation Model", "id": "2505.21432v1", "url": "http://arxiv.org/abs/2505.21432v1", "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model", "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.", "authors": ["Haoming Song", "Delin Qu", "Yuanqi Yao", "Qizhi Chen", "Qi Lv", "Yiwen Tang", "Modi Shi", "Guanghui Ren", "Maoqing Yao", "Bin Zhao", "Dong Wang", "Xuelong Li"], "published_date": "2025-05-27", "title_zh": "Hume: 在視覺-語言-動作模型中引入系統二思維", "summary_zh": "這篇論文介紹了Hume，一個視覺-語言-動作(VLA)模型，它模仿人類在執行複雜任務前進行的「慢思考」（系統二思維）。Hume利用一個價值引導的系統二來評估不同動作方案的價值，並選擇最佳方案。然後，一個輕量級的系統一負責執行選定的動作，並進行實時的動作去噪。實驗證明Hume在多個模擬和真實機器人部署環境中超越了現有的VLA模型。", "applications": ["**自動化烹飪助手:** 機器人可以利用Hume來分析食譜，理解不同烹飪步驟的意義，並根據食材狀況和烹飪進度，做出更精確和靈活的動作，例如控制火候、調整配料比例等，從而提升烹飪品質和效率。", "**精準醫療手術輔助:** 在手術過程中，Hume可以輔助醫生進行複雜的手術操作。通過分析手術影像和預測可能的風險，Hume可以幫助醫生選擇最佳的手術方案，並實時調整手術動作，從而提高手術的成功率和安全性。", "**智能倉儲揀選:** 機器人可以使用Hume來理解訂單需求和貨物擺放位置，並根據貨物的重量、形狀和穩定性，選擇最佳的抓取和放置方式，從而提高揀選效率和減少貨物損壞。"], "pitch": "Hume突破了現有機器人模型的限制，透過引入系統二思維，大幅提升了機器人在複雜環境中的決策能力和動作精準度。這項技術具有廣泛的應用前景，尤其在自動化、醫療、物流等領域。我們相信，Hume將成為下一代機器人平台的核心技術，為各行各業帶來革命性的效率提升和成本降低。相較於市面上缺乏思考能力的機器人，Hume更具備商業價值，有望成為業界領導者。", "audio": "audios/2505.21432v1.mp3", "timestamp": "2025-05-28T12:27:49.576625"}
{"query": "Diffusion Model", "id": "2505.21488v1", "url": "http://arxiv.org/abs/2505.21488v1", "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "summary": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "published_date": "2025-05-27", "title_zh": "果斷決策：用於多主體生成的雜訊誘導佈局", "summary_zh": "現有的文字生成圖像擴散模型在生成多個不同主體時面臨挑戰。複雜的提示詞經常導致主體洩漏，造成數量、屬性和視覺特徵的不準確。防止主體間的洩漏需要了解每個主體的空間位置。最近的方法通過外部佈局控制提供這些空間位置，但強制執行預定的佈局通常與採樣的初始雜訊所決定的固有佈局相衝突，導致與模型的先驗知識不一致。本文提出一種新方法，預測與提示詞對齊、源自初始雜訊的空間佈局，並在去噪過程中不斷完善。通過依賴這種雜訊誘導佈局，我們避免了與外部強制佈局的衝突，並更好地保留了模型的先驗知識。我們的方法採用一個小型神經網路來預測和完善每個去噪步驟中不斷演變的雜訊誘導佈局，確保主體之間清晰的邊界，同時保持一致性。實驗結果表明，與現有的佈局引導技術相比，這種雜訊對齊策略實現了改進的文字圖像對齊和更穩定的多主體生成，同時保留了模型原始分佈的豐富多樣性。", "applications": ["**客製化兒童讀物生成器：** 家長可以輸入簡單的描述，例如“穿著紅色外套的小女孩和一隻棕色小狗在公園裡玩耍”，系統就能根據描述生成多張圖畫，讓孩子選擇，最終製作成一本獨一無二的兒童故事書。", "**廣告素材自動生成：** 廣告公司可以輸入多個產品和場景描述，例如“一瓶汽水，一個漢堡，在海灘上，陽光普照”，AI就能自動生成多個包含所有元素的廣告圖片，減少設計師的工作量並提高效率。", "**虛擬房間設計助手：** 使用者可以輸入家具和房間描述，例如“一張沙發，兩把椅子，一張咖啡桌，現代風格客廳”，系統就能生成多個不同的房間佈局方案，方便使用者進行選擇和調整，最終實現虛擬的室內設計體驗。"], "pitch": "這個團隊解決了文字生成圖像領域的一個核心痛點：如何穩定且準確地生成包含多個主體的圖像。他們提出的雜訊誘導佈局方法，有效避免了主體洩漏和與模型先驗知識的衝突，提高了圖像生成的質量和穩定性。這個技術具有廣泛的應用前景，例如客製化內容生成、廣告素材自動化和虛擬現實等。其商業價值體現在：1. 降低內容生產成本，提高效率；2. 創造更個性化和吸引人的用戶體驗；3. 在快速增長的AI生成內容市場中佔據領先地位。團隊需要進一步優化模型，並探索更多商業應用場景，例如與遊戲公司合作開發遊戲素材生成工具，或與電商平台合作提供商品圖片自動生成服務。潛在的投資回報非常可觀。", "audio": "audios/2505.21488v1.mp3", "timestamp": "2025-05-28T12:28:08.038672"}
{"query": "AI", "id": "2505.21500v1", "url": "http://arxiv.org/abs/2505.21500v1", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "published_date": "2025-05-27", "title_zh": "ViewSpatial-Bench：評估視覺語言模型中的多視角空間定位", "summary_zh": "現有的視覺語言模型（VLMs）在理解和推理視覺內容方面表現出色，但在跨視角理解和空間推理方面仍然面臨重大挑戰。 我們的研究發現，現有VLMs主要擅長以自我為中心的空間推理（從相機的角度），但當需要採用另一個實體的空間參考框架時，無法推廣到以他人為中心的視角。 我們推出了ViewSpatial-Bench，這是第一個全面的基準測試，專門用於評估跨五種不同任務類型的多視角空間定位識別，並由自動化的3D註釋管道支持，該管道可生成精確的方向標籤。 對ViewSpatial-Bench上各種VLMs的綜合評估顯示出顯著的性能差異：模型在相機視角任務中表現出合理的性能，但在從人類視角進行推理時，準確性降低。 通過在我們的多視角空間數據集上微調VLMs，我們在各個任務中的總體性能提高了46.24％，突出了我們方法的有效性。 我們的研究為具身AI系統中的空間智能建立了一個關鍵基準，並提供了經驗證據，表明對3D空間關係建模可增強VLMs的相應空間理解能力。", "applications": ["**提升自動駕駛導航能力：** 讓自動駕駛車輛不僅理解自身位置，還能理解行人或其他車輛的視角，從而更安全地預測其行為。", "**改善虛擬實境/擴增實境體驗：** 讓VR/AR應用程式理解使用者及虛擬物件之間的空間關係，並根據使用者視角進行調整，提供更沉浸式的互動體驗。", "**協助機器人進行協作任務：** 讓機器人理解人類的指示，並能從人類的視角去執行任務，例如「把紅色積木放在你右邊的藍色積木的上面」。"], "pitch": "ViewSpatial-Bench解決了視覺語言模型在多視角空間推理上的關鍵痛點，這直接限制了它們在自動駕駛、VR/AR和機器人等領域的應用。我們的基準測試和改進方法，能夠顯著提升模型在這些真實世界場景中的表現。我們提供的資料集和模型微調方法，能加速相關領域的技術突破。想像一下，一個能真正理解人類意圖並能完美協作的機器人助手，或者一個能根據不同視角安全導航的自動駕駛系統，這些都將成為可能。ViewSpatial-Bench不僅是一個技術工具，更是開啟全新商業機會的鑰匙，例如更精準的定位服務、更智慧的機器人解決方案，以及更沉浸式的VR/AR體驗。我們尋求資金，以擴大資料集規模，開發更先進的模型架構，並將技術商業化，最終將空間智能帶入每一個角落。", "audio": "audios/2505.21500v1.mp3", "timestamp": "2025-05-28T13:28:29.997247"}
{"query": "Foundation Model", "id": "2505.21432v1", "url": "http://arxiv.org/abs/2505.21432v1", "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model", "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.", "authors": ["Haoming Song", "Delin Qu", "Yuanqi Yao", "Qizhi Chen", "Qi Lv", "Yiwen Tang", "Modi Shi", "Guanghui Ren", "Maoqing Yao", "Bin Zhao", "Dong Wang", "Xuelong Li"], "published_date": "2025-05-27", "title_zh": "休姆：在視覺-語言-動作模型中引入系統二思維", "summary_zh": "本研究提出一個名為「休姆」的視覺-語言-動作（VLA）模型，旨在讓機器人像人類一樣，在執行複雜任務前進行「慢思考」（系統二思維）。休姆模型透過價值引導的系統二思維和級聯動作降噪技術，提升機器人在真實世界中的靈巧控制能力。系統二藉由估算預測動作的狀態-動作價值，進行價值引導的思考，從多個候選動作中選擇最佳方案。系統一則是一個輕量級的反應式視覺運動策略，負責接收系統二選定的動作，並進行級聯動作降噪，以實現靈巧的機器人控制。實驗結果顯示，休姆模型在多個模擬基準測試和真實機器人部署中，均優於現有的最先進VLA模型。", "applications": ["**精準醫療手術輔助：** 休姆模型可以協助醫生進行微創手術，透過視覺和語言理解手術流程，並根據價值引導選擇最佳的器械操作，提升手術精度和安全性。", "**複雜環境下的自動化倉儲：** 在擁擠且動態的倉儲環境中，休姆模型可以幫助機器人識別貨物、規劃路徑，並安全準確地抓取和放置物品，提高倉儲效率。", "**家庭服務型機器人：** 休姆模型可以應用於家庭服務型機器人，使其能夠理解用戶指令、辨識環境，並執行複雜的家務任務，例如整理房間、準備食物等。"], "pitch": "休姆模型是機器人控制領域的突破性技術，它賦予機器人類似人類的「慢思考」能力，使其能夠更智能、更安全地執行複雜任務。這種技術在醫療、倉儲、家庭服務等領域具有廣闊的應用前景。透過授權、訂閱制AI服務、軟硬體整合方案等商業模式，我們能夠為客戶提供高效、可靠的機器人解決方案，大幅降低人力成本、提高生產效率，並創造可觀的商業價值。我們相信，休姆模型將引領機器人技術的下一個發展階段，為各行各業帶來顛覆性的變革，是值得投資的明日之星。", "audio": "audios/2505.21432v1.mp3", "timestamp": "2025-05-28T13:28:53.777646"}
{"query": "Diffusion Model", "id": "2505.21488v1", "url": "http://arxiv.org/abs/2505.21488v1", "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "summary": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "published_date": "2025-05-27", "title_zh": "果斷決策：雜訊誘導式多主體生成佈局", "summary_zh": "現有的文字生成圖像擴散模型在生成多個不同主體時面臨挑戰。複雜的提示詞容易導致主體洩漏，造成數量、屬性和視覺特徵的不準確。防止主體間洩漏需要了解每個主體的空間位置。雖然現有方法透過外部佈局控制提供空間位置，但強制執行預先設定的佈局往往與採樣的初始雜訊所決定的內在佈局相衝突，導致與模型的先驗知識不一致。本研究提出一種新方法，基於初始雜訊預測與提示詞對齊的空間佈局，並在去噪過程中不斷優化。透過依賴這種雜訊誘導的佈局，我們避免了與外部強加佈局的衝突，更好地保留了模型的先驗知識。我們的方法使用一個小型神經網路來預測和優化每個去噪步驟中不斷演變的雜訊誘導佈局，確保主體之間的清晰邊界，同時保持一致性。實驗結果表明，與現有的佈局引導技術相比，這種雜訊對齊策略實現了更好的文字-圖像對齊和更穩定的多主體生成，同時保留了模型原始分佈的豐富多樣性。", "applications": ["**客製化童書繪製：** 父母可以透過文字描述，例如「一隻藍色小狗在花園裡追逐一隻紅色蝴蝶，背景是黃色的房子」，快速生成客製化的童書插圖，孩子們可以參與設計，激發創造力。", "**廣告素材自動生成：** 行銷團隊可以利用這項技術，快速生成多樣化的產品廣告圖片，例如「一位穿著紅色外套的模特兒站在台北101前，背景是藍天白雲」，大幅縮短廣告素材的製作時間，並降低成本。", "**遊戲美術資源快速迭代：** 遊戲開發者可以透過文字描述，快速生成遊戲中的角色、場景和道具，例如「一位手持長劍的騎士站在城堡門前，背景是夕陽」，加速遊戲美術資源的迭代速度，並探索更多設計可能性。"], "pitch": "想像一下，只需輸入一句話，就能創造出栩栩如生的圖像，而且每個圖像都完美呈現你想要的多個主體和場景！我們的創新技術解決了AI圖像生成領域的關鍵痛點：多主體間的干擾和佈局控制的衝突。這項技術的核心優勢在於，它能讓AI真正理解並遵循你的意圖，生成高度精準、風格多樣的圖像，同時避免了傳統方法中常見的模糊和錯誤。這不僅能大幅提升圖像生成的效率和品質，更能開啟全新的商業模式。我們可以將這項技術授權給廣告公司、遊戲開發商、教育機構和電商平台，幫助他們快速生成高品質的視覺內容，降低製作成本，並創造更具吸引力的用戶體驗。更進一步，我們還可以開發客製化的圖像生成API，讓企業能夠將這項技術無縫整合到現有的工作流程中。這個市場潛力巨大，我們相信這項技術將會徹底改變人們創造和消費視覺內容的方式。", "audio": "audios/2505.21488v1.mp3", "timestamp": "2025-05-28T13:29:16.581868"}
{"query": "AI", "id": "2505.21500v1", "url": "http://arxiv.org/abs/2505.21500v1", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "published_date": "2025-05-27", "title_zh": "ViewSpatial-Bench：評估視覺語言模型中的多視角空間定位能力", "summary_zh": "視覺語言模型 (VLMs) 在理解和推理視覺內容方面展現了卓越的能力，但在需要跨視角理解和空間推理的任務中仍然存在重大挑戰。 我們發現一個關鍵限制：目前的 VLM 主要擅長以自我為中心的空間推理（從相機的角度），但當需要採用另一個實體的空間參考框架時，它們無法推廣到以他人為中心的視角。 我們推出了 ViewSpatial-Bench，這是第一個全面的基準，專門用於跨五種不同任務類型的多視角空間定位識別評估，並由自動化的 3D 註釋流程提供支持，該流程生成精確的方向標籤。 對 ViewSpatial-Bench 上多種 VLM 的全面評估揭示了顯著的性能差異：模型在相機視角任務中表現出合理的性能，但在從人類視角進行推理時，準確性降低。 通過在我們的多視角空間數據集上微調 VLM，我們在各項任務中的整體性能提高了 46.24%，突出了我們方法的有效性。 我們的工作為具身人工智慧系統中的空間智能建立了一個重要的基準，並提供了經驗證據，證明建模 3D 空間關係可以提高 VLM 相應的空間理解能力。", "applications": ["導航輔助系統：讓視障人士或不熟悉環境的人可以透過語音指令，要求系統從特定人物或物體的角度提供方向資訊，例如：「從郵筒的角度，商店在哪裡？」", "虛擬實境 (VR) 教學：用於模擬訓練，讓使用者可以從不同角色的視角學習操作流程，例如：從護士的角度觀察手術室環境，學習手術器械的位置。", "機器人協助組裝：讓機器人能理解人類的空間指令，並根據人類的角度調整其操作，例如：「把螺絲刀放在我左邊的零件上」。"], "pitch": "ViewSpatial-Bench 解決了 VLM 在跨視角空間理解上的關鍵瓶頸，這對下一代具身人工智慧至關重要。 我們提供的基準測試和數據集，能大幅提升機器人、VR/AR 應用、以及任何需要機器理解人類空間意圖的場景的性能。 我們已證明微調後性能提升 46%，這代表巨大的商業潛力，可以授權給需要更強大空間推理能力的企業，例如：自動駕駛、無人機送貨、智能家居、以及工業自動化等領域。 我們尋求投資以擴大數據集規模、開發更有效的微調技術，並將 ViewSpatial-Bench 商業化，使其成為行業標準，從而佔據具身人工智慧空間理解的領導地位。", "audio": "audios/2505.21500v1.mp3", "timestamp": "2025-05-28T14:14:43.618238"}
{"query": "Foundation Model", "id": "2505.21432v1", "url": "http://arxiv.org/abs/2505.21432v1", "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model", "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.", "authors": ["Haoming Song", "Delin Qu", "Yuanqi Yao", "Qizhi Chen", "Qi Lv", "Yiwen Tang", "Modi Shi", "Guanghui Ren", "Maoqing Yao", "Bin Zhao", "Dong Wang", "Xuelong Li"], "published_date": "2025-05-27", "title_zh": "休謨：在視覺-語言-動作模型中引入系統二思維", "summary_zh": "人類在執行複雜的實體世界任務前，會先進行深思熟慮。這種思考模式，最近在提升大型語言模型（LLMs）解決數位領域的複雜任務上取得了顯著進展。然而，對於與實體世界互動的機器人基礎模型而言，深思熟慮的潛力尚未被充分挖掘。本研究提出「休謨」（Hume）：一個雙系統的視覺-語言-動作（VLA）模型，結合價值導向的系統二思維和級聯動作去噪，探索視覺-語言-動作模型在靈巧機器人控制中的類人思考能力。「休謨」的系統二透過擴展VLA模型骨幹，增加一個新穎的價值查詢頭（value-query head）來估計預測動作的狀態-動作價值，從而實現價值導向的思考。價值導向的思考透過重複抽樣多個動作候選，並根據狀態-動作價值選擇一個動作來實現。「休謨」的系統一是一個輕量級的反應式視覺運動策略，它接收系統二選擇的動作，並執行級聯動作去噪以進行靈巧的機器人控制。在部署時，系統二以低頻率執行價值導向的思考，而系統一則非同步地接收系統二選擇的動作候選，並即時預測流暢的動作。實驗表明，「休謨」在多個模擬基準測試和真實機器人部署中，都優於現有的最先進的視覺-語言-動作模型。", "applications": ["**精準醫療手術輔助：** 機器人手術醫生在進行複雜手術時，系統可以根據病患的即時影像、病歷資料和手術目標，提供最佳的手術路徑和操作建議，降低手術風險並提高成功率。", "**高精度工業組裝：** 在高度自動化的工廠中，機器人可以根據產品設計圖和組裝步驟，靈活地完成複雜的零件組裝任務，減少人工干預，提高生產效率和產品良率。", "**複雜環境下的災害救援：** 在地震、火災等災害現場，機器人可以利用視覺和語言理解能力，自主規劃救援路徑，並靈巧地搬運瓦礫、打開障礙物，協助搜救人員尋找受困者。"], "pitch": "Hume代表了機器人控制領域的一大飛躍，它將人類的深度思考模式融入機器人系統，賦予機器人更強大的解決複雜問題的能力。其雙系統架構不僅提升了機器人的靈活性和精確性，更使其能夠適應多變的現實環境。從醫療、工業到災害救援，Hume的應用前景廣闊。我們相信，Hume技術將引領下一代智能機器人的發展，帶來巨大的商業價值。投資Hume，就是投資機器人控制的未來，擁抱無限的可能性。", "audio": "audios/2505.21432v1.mp3", "timestamp": "2025-05-28T14:15:04.266306"}
{"query": "Diffusion Model", "id": "2505.21488v1", "url": "http://arxiv.org/abs/2505.21488v1", "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "summary": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "published_date": "2025-05-27", "title_zh": "果斷決策：噪音誘導的多主體生成佈局", "summary_zh": "現有的文字生成圖像擴散模型在生成多個不同的主體時面臨挑戰。複雜的提示詞常常導致主體洩漏，造成數量、屬性和視覺特徵上的不準確。防止主體間洩漏需要了解每個主體的空間位置。近期的方法透過外部佈局控制來提供這些空間位置。然而，強制執行這樣的佈局通常與採樣初始噪音所決定的固有佈局相衝突，導致與模型的先驗知識不一致。這篇論文提出了一種新方法，可以預測一個與提示詞對齊的空間佈局，該佈局源自初始噪音，並在去噪過程中進行精煉。透過依賴於這種噪音誘導的佈局，我們避免了與外部強加佈局的衝突，並更好地保留了模型的先驗知識。我們的模型使用一個小型神經網絡來預測和精煉每個去噪步驟中不斷演變的噪音誘導佈局，確保主體之間有清晰的邊界，同時保持一致性。實驗結果表明，與現有的佈局引導技術相比，這種噪音對齊策略實現了更好的文本圖像對齊和更穩定的多主體生成，同時保留了模型原始分佈的豐富多樣性。", "applications": ["**客製化童書插圖生成：** 家長可以輸入文字描述，讓系統自動生成包含多個角色和場景的童書插圖，每個角色都有清晰的區分和互動。", "**遊戲場景設計：** 遊戲開發者可以快速生成包含多個物件和角色的遊戲場景，並精確控制每個物件和角色的位置和關係，加速遊戲開發流程。", "**電商產品展示圖片生成：** 電商業者可以根據商品描述自動生成包含多個商品和模特的展示圖片，並確保每個商品都清晰可見，提高產品吸引力。"], "pitch": "想像一下，我們能夠根據簡單的文字描述，自動生成包含多個主體、視覺效果一致且精準的圖像。這項技術能解決現有AI繪圖模型在處理複雜提示詞時，容易產生主體混淆和佈局失真的問題。我們的噪音誘導佈局方法，能讓AI更精準地控制圖像中各個元素的空間關係，大幅提升生成圖像的品質和可控性。從電商產品展示、客製化內容生成到遊戲開發，這項技術擁有廣泛的應用前景。我們不僅能降低創作門檻，更能創造全新的商業模式。例如，我們可以提供API服務，讓企業輕鬆生成高品質的產品宣傳圖片，或是打造一個AI輔助的童書創作平台。這項技術的潛在市場規模巨大，絕對值得投資。", "audio": "audios/2505.21488v1.mp3", "timestamp": "2025-05-28T14:15:24.122267"}
{"query": "AI", "id": "2505.21500v1", "url": "http://arxiv.org/abs/2505.21500v1", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "published_date": "2025-05-27", "title_zh": "ViewSpatial-Bench：評估視覺-語言模型中的多視角空間定位", "summary_zh": "現有的視覺-語言模型(VLMs)在理解和推理視覺內容方面表現出色，但在需要跨視角理解和空間推理的任務中仍然存在挑戰。 我們發現VLMs主要擅長以自我為中心的空間推理（從相機的角度），但在需要採用另一個實體的空間參考框架時，無法推廣到以他人為中心的視角。 我們推出了ViewSpatial-Bench，這是第一個全面的基準測試，專門用於跨五種不同任務類型的多視角空間定位識別評估，並由自動化的3D註釋管道提供支持，該管道可生成精確的方向標籤。 對ViewSpatial-Bench上各種VLMs的全面評估顯示出顯著的性能差異：模型在相機視角任務上表現出合理的性能，但在從人的角度進行推理時，準確性會降低。 通過在我們的多視角空間數據集上微調VLMs，我們在各項任務中的總體性能提高了46.24％，突顯了我們方法的有效性。 我們的研究為具體化AI系統中的空間智能建立了一個重要的基準，並提供了經驗證據，表明建模3D空間關係可以增強VLMs相應的空間理解能力。", "applications": ["**導航輔助系統：** 應用於機器人或無人機，使其能夠根據使用者的指令，例如「將紅色盒子搬到桌子的左邊」，從不同視角理解並執行任務，即使機器人的視角與指令發出者的視角不同。", "**虛擬實境/擴增實境遊戲：** 在遊戲中，玩家可以與AI角色互動，這些角色能夠理解玩家的指示，並根據玩家的視角進行操作，例如「幫我把櫃子裡的槍拿過來」，提升遊戲的沉浸感和互動性。", "**遠端協助和維修：** 技術人員可以遠端指導現場人員進行設備維修，通過共享視覺資訊，讓AI系統輔助技術人員理解現場人員的視角，並提供更精確的指導，例如「將紅色的電線接到接線盒右邊的第三個端子上」。"], "pitch": "ViewSpatial-Bench定義了下一代空間智慧VLMs的標準，而我們的微調方法更是將這些模型的能力大幅提升。想像一下：具備真正空間理解能力的機器人助手、高度沉浸式的遊戲體驗、以及更高效的遠端協作。 這是一個龐大的市場機會，涵蓋了機器人、遊戲、製造業、醫療保健等多個領域。 我們正在尋求投資，以擴展我們的數據集、進一步優化我們的模型，並將ViewSpatial-Bench打造成業界的黃金標準，從而搶佔先機，成為空間智慧VLMs領域的領導者，並打造多項具有巨大商業價值的應用。", "audio": "audios/2505.21500v1.mp3", "timestamp": "2025-05-28T15:13:35.205403"}
{"query": "Foundation Model", "id": "2505.21432v1", "url": "http://arxiv.org/abs/2505.21432v1", "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model", "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.", "authors": ["Haoming Song", "Delin Qu", "Yuanqi Yao", "Qizhi Chen", "Qi Lv", "Yiwen Tang", "Modi Shi", "Guanghui Ren", "Maoqing Yao", "Bin Zhao", "Dong Wang", "Xuelong Li"], "published_date": "2025-05-27", "title_zh": "Hume: 在視覺-語言-動作模型中引入系統二思維", "summary_zh": "這篇論文介紹了Hume，一個整合視覺、語言和動作的雙系統模型，旨在讓機器人像人類一樣，在執行複雜任務前先進行深入思考（系統二思維）。Hume利用價值導向的思考方式，評估不同動作方案的價值，然後選擇最佳方案。同時，Hume還包含一個快速反應的系統一，負責將選定的動作方案轉化為精確的機器人控制指令。實驗證明，Hume在模擬和真實機器人環境中都優於現有的視覺-語言-動作模型。", "applications": ["**家庭服務機器人：** 讓機器人在執行複雜家務時，例如整理雜亂的廚房，能夠先評估不同整理方案的效率和安全性，再採取行動，避免誤傷或造成損壞。", "**醫療手術機器人：** 協助醫生進行高精度手術，機器人可以預先評估不同手術路徑的風險和效益，降低手術失誤的可能性，提升手術成功率。", "**自動化倉儲管理：** 讓倉儲機器人在揀貨和搬運過程中，能夠快速評估貨物的位置、重量和周圍環境，選擇最優路線，提高效率並避免碰撞。"], "pitch": "Hume 模型代表著機器人控制領域的一大突破，它將人類的思考模式融入到機器人系統中，使其能夠更安全、更有效率地完成複雜任務。其商業價值在於：1. **性能提升：** Hume 能夠顯著提高機器人的任務執行能力，降低錯誤率，從而降低運營成本。2. **應用廣泛：** 從家庭服務、醫療保健到工業自動化，Hume 都有廣泛的應用前景，市場潛力巨大。3. **技術壁壘：** Hume 融合了多項創新技術，具有較高的技術壁壘，先發優勢明顯。 我們相信，投資 Hume 能夠在快速增長的機器人市場中佔據領先地位，獲得豐厚的回報。", "audio": "audios/2505.21432v1.mp3", "timestamp": "2025-05-28T15:13:52.853496"}
{"query": "Diffusion Model", "id": "2505.21488v1", "url": "http://arxiv.org/abs/2505.21488v1", "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "summary": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "published_date": "2025-05-27", "title_zh": "果斷決策：雜訊誘導的多主體生成佈局", "summary_zh": "現有的文字生成圖像擴散模型在生成多個獨立主體時面臨挑戰。複雜的提示詞容易導致主體洩漏，造成數量、屬性及視覺特徵上的不準確。為了避免主體洩漏，需要了解每個主體的空間位置。過去的方法使用外部佈局控制來提供這些空間位置，但強制執行預先設定的佈局往往與初始雜訊所決定的內在佈局衝突，導致與模型的先驗知識不一致。本文提出一種新方法，從初始雜訊中預測與提示詞對齊的空間佈局，並在去噪過程中不斷完善。透過依賴這種雜訊誘導的佈局，我們避免了與外部強加佈局的衝突，並更好地保留了模型的先驗知識。我們的方法使用一個小型神經網路來預測和完善每個去噪步驟中不斷演進的雜訊誘導佈局，確保主體之間有清晰的邊界，同時保持一致性。實驗結果表明，與現有的佈局引導技術相比，這種雜訊對齊策略實現了更好的文字圖像對齊和更穩定的多主體生成，同時保留了模型原始分佈的豐富多樣性。", "applications": ["**客製化童話故事書生成器：** 父母可以輸入簡單的文字描述，例如『一隻快樂的小熊坐在草地上，旁邊有一顆蘋果樹』，系統就能生成符合描述的精美插圖，並且確保小熊、草地和蘋果樹的位置關係正確自然，讓孩子們擁有獨一無二的客製化故事書。", "**虛擬產品設計輔助工具：** 設計師可以透過文字描述來生成產品的視覺呈現。例如，描述『一個現代風格的沙發，旁邊有一個圓形茶几和一盞落地燈』，系統可以快速生成不同風格的佈局，協助設計師快速迭代設計概念，並減少人工繪製的時間和成本。", "**遊戲場景自動生成：** 遊戲開發者可以使用文字描述來生成遊戲場景。例如，『一個被遺棄的城市廢墟，有幾輛生鏽的汽車和一間破敗的教堂』，系統能生成符合描述的場景，並確保物件的位置關係合理，大幅降低遊戲場景製作的門檻和時間。"], "pitch": "我們開發了一項突破性的AI技術，可以根據文字描述生成高品質、多主體的圖像，解決了現有AI繪圖工具在處理複雜場景時容易出現的'主體洩漏'問題。我們的核心創新在於利用雜訊誘導的佈局，確保圖像中的每個元素都能準確、自然地呈現。這項技術擁有廣闊的商業應用前景，包括客製化內容生成、產品設計輔助、遊戲開發等。我們相信，透過這項技術，我們可以賦能創作者，降低創作門檻，並為用戶提供更豐富、更個性化的內容體驗。我們正在尋找投資者，共同將這項技術推向市場，打造下一代的AI圖像生成平台。", "audio": "audios/2505.21488v1.mp3", "timestamp": "2025-05-28T15:14:15.144403"}
{"query": "AI", "id": "2505.21500v1", "url": "http://arxiv.org/abs/2505.21500v1", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "published_date": "2025-05-27", "title_zh": "ViewSpatial-Bench：評估視覺語言模型中多視角空間定位能力", "summary_zh": "視覺語言模型（VLMs）在理解和推理視覺內容方面表現出色，但在需要跨視角理解和空間推理的任務中仍存在挑戰。目前VLMs主要擅長以自我為中心的空間推理（從相機視角），但難以推廣到以他人視角為參考的客觀視角。我們提出了ViewSpatial-Bench，這是一個全面的基準測試，專為跨五種不同任務類型的多視角空間定位識別評估而設計，並配有自動化的3D註釋流程，可以生成精確的方向標籤。對ViewSpatial-Bench上各種VLMs的綜合評估表明，性能存在顯著差異：模型在相機視角任務上表現尚可，但在從人視角進行推理時準確性降低。通過在我們的多視角空間數據集上微調VLMs，我們在各項任務中實現了46.24%的整體性能提升，突顯了我們方法的有效性。我們的工作為具身人工智慧系統的空間智能建立了一個關鍵基準，並提供了經驗證據，證明對3D空間關係進行建模可以增強VLMs相應的空間理解能力。", "applications": ["智能家居：讓機器人能根據住戶的指示，從使用者的角度理解物品位置，例如：『幫我從你右邊的桌子上拿杯水』，即便機器人的相對位置與使用者不同。", "自動駕駛：提升自動駕駛系統對行人意圖的理解能力。例如，判斷行人是否打算從車輛左側穿越馬路，從而做出更安全的決策。", "醫療輔助：輔助醫生進行手術規劃，透過不同視角的影像，更精準的定位病灶位置，提升手術成功率與降低風險。"], "pitch": "ViewSpatial-Bench 及其相關研究為具身人工智慧開啟了全新的可能性，解決了視覺語言模型在跨視角空間推理上的關鍵瓶頸。我們打造的基準測試與微調技術，能顯著提升模型在理解人類視角下的空間關係的能力。這項技術可廣泛應用於智能家居、自動駕駛、醫療輔助等領域，形成巨大的市場潛力。想像一下，具備卓越空間推理能力的機器人管家，或是能夠預測行人行為的智慧汽車，這都將改變我們與世界互動的方式。我們的團隊擁有領先的技術實力與獨家的數據資源，我們相信 ViewSpatial-Bench 將成為具身人工智慧領域的黃金標準，並為投資者帶來豐厚的回報。", "audio": "audios/2505.21500v1.mp3", "timestamp": "2025-05-28T16:15:56.008810"}
{"query": "Foundation Model", "id": "2505.21432v1", "url": "http://arxiv.org/abs/2505.21432v1", "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model", "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.", "authors": ["Haoming Song", "Delin Qu", "Yuanqi Yao", "Qizhi Chen", "Qi Lv", "Yiwen Tang", "Modi Shi", "Guanghui Ren", "Maoqing Yao", "Bin Zhao", "Dong Wang", "Xuelong Li"], "published_date": "2025-05-27", "title_zh": "Hume：在視覺-語言-動作模型中引入系統二思維", "summary_zh": "這篇論文介紹了Hume，一個結合了視覺、語言和動作（VLA）的模型，它模仿人類在執行複雜任務前進行的深度思考（系統二思維）。Hume透過評估不同動作選項的價值來指導其決策，並利用一個輕量級的系統一模型來執行細緻的動作控制。實驗證明，Hume在模擬和真實機器人環境中都優於現有的VLA模型。", "applications": ["**家用機器人：** Hume可以讓機器人更好地理解指令並更精確地完成複雜的家務，例如收拾房間、準備晚餐，或是照顧老人小孩，且能針對突發狀況做出更合理的判斷。", "**工業自動化：** Hume可以應用於複雜的製造流程，例如精密的零件組裝、質量檢測，或是處理不規則形狀的物體，提高生產效率和降低錯誤率。", "**醫療手術輔助：** Hume可以協助醫生進行微創手術，透過視覺識別和價值判斷，提供更精確的動作建議和風險評估，提高手術成功率。"], "pitch": "Hume代表了機器人領域的一項突破，它賦予機器人類似人類的深度思考能力，使其能夠應對更複雜、更具挑戰性的任務。我們相信Hume有潛力顛覆現有的機器人應用模式，從工廠自動化到醫療保健，提供更安全、更高效、更靈活的解決方案。我們的技術優勢在於創新的價值導向的系統二思維和級聯動作降噪，能夠顯著提升機器人的決策能力和動作精確度。我們正在尋找投資，以加速Hume的商業化，並將其推向更廣闊的市場，成為引領下一代機器人發展的領導者。", "audio": "audios/2505.21432v1.mp3", "timestamp": "2025-05-28T16:16:17.444773"}
{"query": "Diffusion Model", "id": "2505.21488v1", "url": "http://arxiv.org/abs/2505.21488v1", "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "summary": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "published_date": "2025-05-27", "title_zh": "果斷決策：噪音誘導的多主體生成佈局", "summary_zh": "現有的文字生成圖像擴散模型在生成多個不同主體時仍然面臨挑戰，複雜的提示詞經常導致主體洩漏，造成數量、屬性和視覺特徵上的不準確。為了解決這個問題，本文提出了一種新的方法，從初始噪音中預測一個與提示詞對齊的空間佈局，並在去噪過程中不斷優化。透過依賴這個噪音誘導的佈局，避免了與外部強制佈局的衝突，更好地保留了模型先驗。實驗結果表明，這種噪音對齊策略在多主體生成方面，比現有的佈局引導技術實現了更好的文字圖像對齊和更穩定的效果，同時保留了模型原始分佈的豐富多樣性。", "applications": ["**虛擬試衣間:** 用戶可以輸入文字描述，例如“藍色牛仔外套和紅色條紋襯衫的女性”，AI可以根據用戶的身材數據生成逼真的試穿圖像，避免服裝之間的遮擋和混淆。", "**兒童故事書插畫:** 家長或教師輸入故事內容，AI可以根據不同角色和場景描述，生成多主體、清晰生動的插畫，大幅降低繪製成本和時間。", "**廣告設計:** 廣告商可以輸入產品描述和目標人群，AI可以自動生成包含多個產品或人物的廣告圖像，快速測試不同視覺方案，提高廣告效果。"], "pitch": "我們開發了一種突破性的AI圖像生成技術，能夠根據文字描述精準生成包含多個主體的圖像，解決了傳統模型容易出現的主體混淆問題。這項技術的核心優勢在於其獨特的噪音誘導佈局方法，能夠確保圖像內容與文字描述高度一致，同時保留了AI模型原有的創造力。我們的技術有巨大的商業潛力，可以應用於電商、娛樂、教育、廣告等多個領域，例如虛擬試穿、插畫製作、廣告設計等。我們尋求種子輪融資，以加速產品開發和市場拓展，目標是成為下一代AI圖像生成領域的領導者，徹底顛覆現有的視覺內容創作方式。", "audio": "audios/2505.21488v1.mp3", "timestamp": "2025-05-28T16:16:48.770521"}
{"query": "AI", "id": "2505.21500v1", "url": "http://arxiv.org/abs/2505.21500v1", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "published_date": "2025-05-27", "title_zh": "ViewSpatial-Bench：評估視覺語言模型中的多視角空間定位能力", "summary_zh": "現今的視覺語言模型（VLMs）在理解視覺內容方面表現出色，但在跨視角理解和空間推理方面仍存在挑戰。 主要問題在於，現有VLMs擅長以自我為中心的空間推理（相機視角），但當需要採用其他實體的空間參考框架時，便無法推廣到他者視角。 我們推出了ViewSpatial-Bench，這是第一個專為多視角空間定位識別評估而設計的綜合基準，涵蓋五種不同的任務類型，並由自動化的3D標註流程提供精確的方向標籤。 對各種VLMs在ViewSpatial-Bench上的全面評估顯示，模型在相機視角任務上表現尚可，但在從人類視角進行推理時，準確度會降低。 通過在我們的多視角空間數據集上微調VLMs，我們在各項任務中實現了46.24%的整體性能提升，突顯了我們方法的有效性。 我們的研究建立了一個用於具身AI系統空間智能的關鍵基準，並提供了經驗證據表明，對3D空間關係進行建模可以增強VLMs相應的空間理解能力。", "applications": ["**智慧導航：** 協助視障人士或不熟悉環境的人，提供更精確、更易懂的導航指示，例如：「從你的角度看，紅色的建築物在你右手邊5公尺處。」", "**遠程協助：** 在需要遠程技術人員協助排除故障的場景，例如維修複雜設備，技術人員可透過他人（非相機）的視角，更清楚地了解現場狀況，並給予更有效的指示。", "**遊戲開發：** 開發更具沉浸感和真實感的遊戲體驗，讓遊戲角色能根據玩家的視角或遊戲中其他角色的視角，做出更合理的反應和互動。"], "pitch": "ViewSpatial-Bench 解決了視覺語言模型在空間推理方面的關鍵痛點，為具身AI和相關應用打開了新的可能性。 我們提供的數據集和基準測試，能有效提升模型在多視角空間理解上的能力，這對於導航、遠程協助、遊戲等領域至關重要。 透過授權 ViewSpatial-Bench 相關技術或以此為基礎開發應用，我們能夠搶佔市場先機，打造更智能、更人性化的 AI 解決方案， 創造龐大的商業價值。", "audio": "audios/2505.21500v1.mp3", "timestamp": "2025-05-28T17:12:54.814311"}
{"query": "Foundation Model", "id": "2505.21432v1", "url": "http://arxiv.org/abs/2505.21432v1", "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model", "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.", "authors": ["Haoming Song", "Delin Qu", "Yuanqi Yao", "Qizhi Chen", "Qi Lv", "Yiwen Tang", "Modi Shi", "Guanghui Ren", "Maoqing Yao", "Bin Zhao", "Dong Wang", "Xuelong Li"], "published_date": "2025-05-27", "title_zh": "休謨：在視覺-語言-動作模型中引入系統二思維", "summary_zh": "面對複雜任務時，人類會先進行深思熟慮再採取行動。這種「慢思考」模式最近在增強大型語言模型（LLM）解決數位領域複雜問題方面取得了顯著進展。然而，對於與物理世界互動的機器人基礎模型來說，慢思考的潛力尚未得到充分發揮。本研究提出「休謨 (Hume)」，這是一個雙系統視覺-語言-動作 (VLA) 模型，結合了價值導向的系統二思維和級聯動作去噪，旨在探索視覺-語言-動作模型在靈巧機器人控制中類似人類的思考能力。休謨的系統二通過擴展視覺-語言-動作模型骨幹網絡，並添加一個新的價值查詢頭，來估計預測動作的狀態-動作價值，從而實現價值導向的思考。這種思考方式通過重複抽樣多個動作候選並根據狀態-動作價值選擇一個來進行。休謨的系統一是一個輕量級的反應式視覺運動策略，它接收系統二選擇的動作，並執行級聯動作去噪以實現靈巧的機器人控制。在部署時，系統二以低頻率進行價值導向的思考，而系統一則異步地接收系統二選擇的動作候選，並實時預測流暢的動作。實驗證明，休謨在多個模擬基準和真實機器人部署中均優於現有的最先進視覺-語言-動作模型。", "applications": ["**高精度手術機器人：** 休謨系統二的深思熟慮能力可以讓手術機器人在高風險手術中做出更精準和安全的決策，例如避開重要血管或神經，減少手術失誤。", "**複雜環境下的倉儲物流機器人：** 在擁擠且不斷變化的倉庫環境中，休謨系統可以幫助機器人更有效地規劃路徑，避免碰撞，並以更智能的方式揀選和搬運貨物。", "**家庭服務機器人：** 休謨賦予服務機器人處理複雜家務的能力，例如整理凌亂的房間，為客人準備精緻的餐點，甚至在緊急情況下做出正確判斷。"], "pitch": "休謨是下一代機器人控制的核心技術。我們將大型語言模型的慢思考能力引入視覺-語言-動作模型，顯著提升了機器人在複雜環境中的靈活性和智能性。相比現有方案，休謨在決策精度和控制流暢性方面均有顯著優勢。我們的商業價值體現在：一，授權技術給醫療、物流、製造等行業的機器人廠商，提升其產品競爭力；二，針對特定行業痛點，開發定制化的機器人解決方案，例如高精度手術機器人或智能倉儲機器人。休謨將重新定義人機協作，開啟機器人應用的新紀元，帶來巨大的市場潛力。", "audio": "audios/2505.21432v1.mp3", "timestamp": "2025-05-28T17:13:18.026922"}
{"query": "Diffusion Model", "id": "2505.21488v1", "url": "http://arxiv.org/abs/2505.21488v1", "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "summary": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "published_date": "2025-05-27", "title_zh": "果斷決策：噪音誘導式佈局用於多主體生成", "summary_zh": "現有的文字生成圖片模型在生成多個不同主體時面臨挑戰，複雜的提示詞容易導致主體洩漏，造成數量、屬性和視覺特徵上的錯誤。本論文提出一種新方法，從初始噪音中預測並精煉與提示詞對齊的空間佈局，在降噪過程中不斷調整。這種噪音誘導式佈局避免了與外部強加佈局的衝突，更好地保留了模型的先驗知識，並提升了文字與圖像的對齊程度，實現更穩定的多主體生成。", "applications": ["**廣告素材自動生成：** 根據產品描述和目標客群，自動生成包含多個產品展示位置和人物互動的廣告圖片，提高廣告素材的製作效率。", "**故事繪本自動生成：** 輸入故事內容，自動生成帶有多個角色互動場景的繪本插圖，方便兒童文學創作。", "**虛擬商品展示：** 針對電商平台，自動生成多個商品組合的展示圖片，展示不同商品搭配效果，吸引消費者。"], "pitch": "各位投資人，我們正處於AI圖像生成的黃金時代，但現有技術在生成多個主體時仍然存在問題。我們的『噪音誘導式佈局』技術，能有效解決這一痛點，提供更準確、更穩定的多主體圖像生成能力。想像一下，廣告公司不再需要花費大量時間和金錢聘請攝影師，而是可以使用我們的技術快速生成高品質的廣告素材；兒童文學作家可以輕鬆地將他們的創意轉化為精美的繪本插圖。我們的技術具有廣泛的應用前景，從廣告、教育到電商，都存在巨大的市場需求。我們相信，『噪音誘導式佈局』將成為下一代圖像生成技術的關鍵組件，為各行各業帶來革命性的變化。我們需要您的資金支持，加速產品開發和市場推廣，共同把握這個千載難逢的商業機會！", "audio": "audios/2505.21488v1.mp3", "timestamp": "2025-05-28T17:13:39.451247"}
{"query": "AI", "id": "2505.21500v1", "url": "http://arxiv.org/abs/2505.21500v1", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "published_date": "2025-05-27", "title_zh": "ViewSpatial-Bench：評估視覺語言模型中的多視角空間定位能力", "summary_zh": "現今的視覺語言模型（VLMs）在理解和推理視覺內容方面表現出色，但在需要跨視點理解和空間推理的任務中仍存在挑戰。主要問題是：VLMs擅長以相機視角進行空間推理，但無法推廣到需要從其他實體的視角進行推理的任務。為此，我們推出了ViewSpatial-Bench，這是一個專為多視點空間定位識別評估而設計的綜合基準，涵蓋五種不同任務類型，並配備自動化的3D標註流程，生成精確的方向標籤。對多個VLMs的評估顯示，模型在相機視角的任務中表現尚可，但在以人為視角進行推理時準確性降低。通過在我們的多視角空間數據集上微調VLMs，我們在所有任務中實現了46.24%的性能提升，證明了我們方法的有效性。這項工作為具身AI系統的空間智能建立了一個關鍵基準，並提供了經驗證據，證明建模3D空間關係可以增強VLMs的空間理解能力。", "applications": ["**導航輔助：** 讓視障人士可以更容易透過語音指令與手機App，了解周遭環境中物體的相對位置與方向，例如：「前方十點鐘方向有一張椅子，請避開。」", "**協作機器人：** 指導工廠或倉庫中的機器人理解人類的指示，從不同角度或位置找到目標物體，例如：「從你的左邊開始算起，第二個箱子裡面有螺絲。」", "**AR/VR遊戲：** 創造更具沉浸感的AR/VR體驗，讓玩家可以透過自然語言與虛擬世界中的物件互動，例如：「把桌子上的紅蘋果放到你右手邊的櫃子上。」"], "pitch": "ViewSpatial-Bench 解決了視覺語言模型在多視角空間推理上的關鍵瓶頸，使其能夠更準確地理解和操作真實世界。想像一下，有了這項技術，具身智能系統將不再只看到 '它們自己' 的視角，而是能夠像人類一樣，理解 '你的'、'我的'、'他的' 視角。這將解鎖數十億美元的市場機會，從個人導航輔助到複雜的工業自動化，甚至到新一代的AR/VR沉浸式體驗。我們的基準測試和數據集讓開發者可以快速迭代和改進模型，而我們在微調後的46%性能提升證明了其商業可行性。我們正在尋找投資者，一起打造空間智能的未來，讓機器真正理解我們的世界。", "audio": "audios/2505.21500v1.mp3", "timestamp": "2025-05-28T18:18:02.572609"}
{"query": "Foundation Model", "id": "2505.21432v1", "url": "http://arxiv.org/abs/2505.21432v1", "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model", "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.", "authors": ["Haoming Song", "Delin Qu", "Yuanqi Yao", "Qizhi Chen", "Qi Lv", "Yiwen Tang", "Modi Shi", "Guanghui Ren", "Maoqing Yao", "Bin Zhao", "Dong Wang", "Xuelong Li"], "published_date": "2025-05-27", "title_zh": "Hume：在視覺-語言-動作模型中引入系統二思維", "summary_zh": "論文提出Hume，一個雙系統視覺-語言-動作(VLA)模型，模擬人類在複雜任務中先深思熟慮再行動的模式。Hume結合價值導向的系統二思維和級聯動作去噪，提升機器人在現實世界中的精巧操控能力。系統二透過價值查詢頭評估行動價值，重複取樣並選擇最佳行動；系統一則是一個輕量級的反應式視覺運動策略，對系統二選擇的動作進行去噪，實時預測流暢動作。實驗證明，Hume在多個模擬和真實機器人部署中優於現有的VLA模型。", "applications": ["**手術機器人輔助：** Hume可以應用於手術機器人，幫助醫生更精準、更安全地完成複雜手術，例如微創手術，透過系統二的思考評估不同操作方案的風險和收益，再由系統一執行精準動作。", "**自動駕駛汽車：** 在複雜的交通狀況下，Hume可以提升自動駕駛汽車的決策能力。系統二可以分析周圍環境，預測潛在風險，並規劃最佳路線，系統一則負責實時控制車輛，避免碰撞。", "**家庭服務機器人：** Hume可以賦予家庭服務機器人更強的自主性和適應性。例如，機器人可以根據主人指令，分析廚房環境，選擇合適的工具，並安全地完成烹飪任務，系統二思考步驟，系統一執行細節動作。"], "pitch": "Hume突破了傳統機器人控制的限制，透過模擬人類的雙系統思維，為機器人賦予了更強的自主性和決策能力。其價值導向的深度思考和實時動作執行，使其在複雜環境中更具優勢。從手術機器人、自動駕駛到家庭服務，Hume的應用前景廣闊。我們相信，Hume將引領下一代機器人技術的發展，創造巨大的商業價值。投資Hume，就是投資未來智能化時代的基石。", "audio": "audios/2505.21432v1.mp3", "timestamp": "2025-05-28T18:18:19.035767"}
{"query": "Diffusion Model", "id": "2505.21488v1", "url": "http://arxiv.org/abs/2505.21488v1", "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "summary": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "published_date": "2025-05-27", "title_zh": "果斷決策：噪聲誘導的多主體生成佈局", "summary_zh": "現有的文本到圖像擴散模型在生成多個不同主體時，常常遇到主體洩漏的問題，導致數量、屬性和視覺特徵不準確。本文提出一種新的方法，從初始噪聲中預測與提示詞對齊的空間佈局，並在去噪過程中不斷優化。透過這種噪聲誘導的佈局，避免了與外部強制佈局的衝突，更好地保留了模型的先驗知識。實驗結果表明，這種噪聲對齊策略在保持模型原始分佈多樣性的同時，實現了更好的文本圖像對齊和更穩定的多主體生成。", "applications": ["**兒童繪本製作：** 根據文本描述自動生成包含多個角色和場景的插圖，簡化繪本製作流程，降低成本。", "**廣告設計：** 快速生成具有特定佈局和人物的廣告素材，例如：『一個男人在海灘上享受陽光，旁邊有一隻開心的拉布拉多犬』，提高設計效率，拓展創意空間。", "**遊戲開發：** 基於文字描述生成多個不同角色和背景的遊戲素材，例如：『兩個精靈在森林裡尋找寶藏』，快速原型設計和迭代，降低美術成本。"], "pitch": "我們解決了文生圖模型在處理多主體生成時的關鍵痛點：主體洩漏和佈局不協調。我們的噪聲誘導佈局技術能更準確地生成包含多個不同主體的圖像，且保持高度一致性和多樣性。這為圖像生成領域帶來了革命性的進展，極大地拓展了文生圖技術的應用範圍。想像一下，設計師、內容創作者，甚至遊戲開發者，都能透過簡單的文字描述，快速生成高質量的多主體圖像。這將大幅提高生產力，降低成本，並激發無限的創意可能性。我們相信，我們的技術擁有巨大的市場潛力，可以應用於廣告、教育、娛樂等多個行業，並帶來豐厚的回報。我們正在尋找投資者，共同打造一個基於文本生成圖像的全新生態系統，引領下一代視覺內容創作。", "audio": "audios/2505.21488v1.mp3", "timestamp": "2025-05-28T18:18:35.601134"}
{"query": "AI", "id": "2505.21500v1", "url": "http://arxiv.org/abs/2505.21500v1", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "published_date": "2025-05-27", "title_zh": "ViewSpatial-Bench：評估視覺語言模型中的多視角空間定位能力", "summary_zh": "現有的視覺語言模型（VLMs）在理解和推理視覺內容方面表現出色，但在需要跨視點理解和空間推理的任務中仍然面臨挑戰。主要問題是VLMs擅長以自我為中心的空間推理（從相機視角），但當需要採用另一實體的空間參考系時，無法推廣到以他人為中心的視角。我們推出了ViewSpatial-Bench，這是首個專為多視點空間定位識別評估而設計的綜合基準測試，涵蓋五種不同的任務類型，並由自動化的3D標註管道提供支援，以生成精確的方向標籤。對不同VLMs在ViewSpatial-Bench上的全面評估顯示，存在顯著的性能差異：模型在相機視角任務上表現合理，但在從人類視角進行推理時，準確性降低。透過在我們的多視角空間數據集上微調VLMs，我們在各項任務上實現了46.24%的總體性能提升，突顯了我們方法的有效性。我們的工作為具體化AI系統中的空間智能建立了一個關鍵基準，並提供了經驗證據，證明建模3D空間關係可以增強VLMs相應的空間理解能力。", "applications": ["**改善視障人士的導航系統：** 透過讓模型理解不同視角下的空間關係，可以開發更精確的導航系統，協助視障人士在複雜環境中安全移動。例如，系統可以告訴使用者：“從你朋友的角度來看，垃圾桶在你的左前方”。", "**提升遊戲AI的協作能力：** 使遊戲中的AI角色能够理解玩家的視角和指令，并作出更符合玩家期望的反應。例如，在戰術射擊遊戲中，隊友AI可以根據玩家的指示“從我的視角繞後”，並有效執行任務。", "**優化機器人操作任務的執行：** 在需要機器人與人類協作的環境中，例如倉庫或工廠，讓機器人能夠理解人類的操作意圖和空間參考，可以提高工作效率和安全性。例如，人類說：“把那箱子從你的右邊搬到我的左邊”，機器人能够精準理解並執行。"], "pitch": "ViewSpatial-Bench解決了視覺語言模型在理解跨視點空間關係方面的核心瓶頸，這是一個價值數十億美元的市場痛點。我們的基準測試和數據集將成為訓練下一代具體化AI系統的黃金標準，這些系統能夠在複雜的、以人為中心的環境中進行準確的空間推理。透過授權許可數據集、提供模型微調服務，以及與機器人、自動駕駛和AR/VR等領域的企業合作，我們有潛力建立一個龐大且可持續的商業模式。我們46.24%的性能提升證明了我們方法的有效性，並為我們在空間智能領域的領先地位奠定了基礎。我們正在建立一個將深刻影響人機互動未來的關鍵技術。", "audio": "audios/2505.21500v1.mp3", "timestamp": "2025-05-28T19:11:43.424926"}
{"query": "Foundation Model", "id": "2505.21432v1", "url": "http://arxiv.org/abs/2505.21432v1", "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model", "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.", "authors": ["Haoming Song", "Delin Qu", "Yuanqi Yao", "Qizhi Chen", "Qi Lv", "Yiwen Tang", "Modi Shi", "Guanghui Ren", "Maoqing Yao", "Bin Zhao", "Dong Wang", "Xuelong Li"], "published_date": "2025-05-27", "title_zh": "Hume：在視覺-語言-動作模型中引入系統二思維", "summary_zh": "這篇論文介紹了Hume，一個結合視覺、語言和動作的雙系統模型，旨在讓機器人像人類一樣思考後再行動。Hume模擬了人類的「系統二」慢思考模式，透過評估不同行動選項的價值，來指導機器人選擇最佳行動。同時，Hume使用「系統一」的快速反應系統，將系統二選定的行動方案進行精確調整，以實現靈巧的機器人控制。實驗證明，Hume在模擬和真實環境中都優於現有的視覺-語言-動作模型。", "applications": ["**自動化手術：** Hume可以幫助機器人進行更精準、更安全的微創手術。透過評估手術步驟的風險和收益，Hume能引導機器人選擇最佳的手術路徑和操作方式，降低手術風險。", "**複雜組裝作業：** 在製造業中，Hume可以賦予機器人更強的解決問題能力，使其能夠處理更複雜、多變的組裝任務。例如，組裝需要高度靈巧性的精密儀器。", "**家庭服務機器人：** Hume可以提升家庭服務機器人的智能水平，使其能夠更好地理解人類指令、規劃任務步驟，並安全有效地執行各種家務，例如精細物品的整理和清潔。"], "pitch": "Hume 代表了機器人控制領域的重大突破，它通過引入類似人類的決策過程，極大地提升了機器人在複雜環境中的表現。其潛在的商業價值體現在：\n\n1. **更高的自動化效率：** Hume 使得機器人能夠處理過去需要人工干預的任務，從而提高生產效率，降低勞動成本。\n2. **更廣泛的應用場景：** Hume 的靈活性和智能性使其能夠應用於醫療、製造、服務等多個行業，市場前景廣闊。\n3. **領先的技術優勢：** Hume 的雙系統架構和價值導向的決策機制，使其在競爭中具有明顯優勢，有機會成為行業標準。\n4. **降低開發和維護成本：** 一旦Hume的模型訓練完成，未來只需要對其進行少量調整，便可應用到不同的任務中，降低了開發和維護成本。\n\n因此，投資 Hume 具有極高的回報潛力，有望在智能機器人時代佔據領先地位。", "audio": "audios/2505.21432v1.mp3", "timestamp": "2025-05-28T19:12:13.584912"}
{"query": "Diffusion Model", "id": "2505.21488v1", "url": "http://arxiv.org/abs/2505.21488v1", "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "summary": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "published_date": "2025-05-27", "title_zh": "果斷決策：基於噪音誘導佈局的多主體生成", "summary_zh": "現有的文生圖擴散模型在生成多個獨立主體時面臨挑戰。複雜的提示詞經常導致主體洩漏，造成數量、屬性和視覺特徵的不準確。防止主體洩漏需要了解每個主體的空間位置。雖然有些方法通過外部佈局控制提供這些空間位置，但強制執行預先設定的佈局往往與初始噪音決定的固有佈局相衝突，導致與模型的先驗知識不一致。本研究提出一種新方法，從初始噪音中預測並在去噪過程中不斷優化與提示詞對齊的空間佈局。通過依賴這種噪音誘導佈局，我們避免了與外部強加佈局的衝突，更好地保留了模型的先驗知識。我們使用一個小型神經網路在每個去噪步驟預測和優化不斷演變的噪音誘導佈局，確保主體之間的清晰邊界，同時保持一致性。實驗結果表明，與現有的佈局引導技術相比，這種噪音對齊策略在文本圖像對齊方面表現更好，並能生成更穩定的多主體圖像，同時保留了模型原始分佈的豐富多樣性。", "applications": ["**電商商品圖自動生成：** 用戶提供簡單的描述（例如：「紅色沙發旁邊的白色落地燈」），系統自動生成精確且風格多樣的商品展示圖，無需專業攝影。", "**故事繪本自動生成：** 根據文字故事自動生成配圖，確保每個角色在不同場景中保持一致的視覺特徵和正確的空間位置，簡化兒童繪本製作流程。", "**廣告素材快速生成：** 輸入廣告文案，快速生成多樣化的廣告圖片，圖片包含不同的元素和人物，且元素之間的位置關係符合廣告創意要求，提高廣告素材的製作效率。"], "pitch": "我們正在解決文生圖領域的一個核心痛點：多主體圖像生成的精度和一致性。我們的技術基於噪音誘導佈局，能有效防止主體洩漏，生成更準確、更逼真的圖像。相較於現有技術，我們的方案在文本圖像對齊方面表現更佳，同時保留了生成模型的多樣性。這意味著更強大的商業潛力，尤其是在電商、教育和廣告等領域。想像一下，電商平台能自動生成高質量的商品圖，廣告公司能快速迭代創意素材，教育機構能輕鬆製作個性化的學習內容。我們的技術將大幅降低圖像製作的成本，提高效率，並創造新的商業模式。我們正在尋找投資者，一起將這項創新技術推向市場，引領下一代圖像生成革命。", "audio": "audios/2505.21488v1.mp3", "timestamp": "2025-05-28T19:12:43.976376"}
{"query": "AI", "id": "2505.21500v1", "url": "http://arxiv.org/abs/2505.21500v1", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "published_date": "2025-05-27", "title_zh": "ViewSpatial-Bench：評估視覺語言模型中多視角空間定位", "summary_zh": "現有的視覺語言模型（VLMs）在理解視覺內容方面表現出色，但在需要跨視角理解和空間推理的任務中仍然存在挑戰。ViewSpatial-Bench是一個專門設計的基準測試，用於評估VLM在五種不同任務類型中的多視角空間定位識別能力。測試結果表明，模型在相機視角下的表現尚可，但在從人類視角進行推理時準確性明顯下降。通過在多視角空間數據集上微調VLM，整體性能提高了46.24%，證明了該方法的有效性。這項工作為具身AI系統的空間智能建立了一個關鍵基準，並提供了經驗證據，表明建模3D空間關係可以提高VLM的空間理解能力。", "applications": ["**智慧導航/輔助駕駛：** 根據乘客的指令（例如：“從你的角度，前面第二個路口左轉”）來理解和執行導航指令，而非僅僅依靠地圖或車載鏡頭的視角。", "**遠程協助：** 專家可以透過AR/VR介面，以使用者的視角來引導使用者完成複雜的任務，例如修理機器、安裝設備等，並確保雙方對空間位置的理解一致。", "**智能家居控制：** 使用者可以更自然地透過語音指令控制智能家居設備，例如：“把客廳沙發右邊的檯燈關掉”，而無需提供精確的位置描述或預先設定的場景。"], "pitch": "ViewSpatial-Bench解決了視覺語言模型在多視角空間理解上的關鍵瓶頸，這對具身AI和人機協作至關重要。我們提供了一個標準化的基準測試，並展示了透過特定數據集微調可以顯著提升模型性能。 投資ViewSpatial-Bench相關技術，將能大幅提升AI在導航、遠程協助、智能家居等領域的應用價值，打造更直觀、更自然的用戶體驗。我們將持續完善基準測試，並開發更有效的模型訓練方法，助力企業開發出更強大的、能夠真正理解人類意圖的AI產品，搶佔市場先機。其商業價值體現在更智能、更個性化的使用者體驗，以及更高效、更安全的AI應用上，具有巨大的市場潛力。", "audio": "audios/2505.21500v1.mp3", "timestamp": "2025-05-28T20:16:01.080202"}
{"query": "Foundation Model", "id": "2505.21432v1", "url": "http://arxiv.org/abs/2505.21432v1", "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model", "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.", "authors": ["Haoming Song", "Delin Qu", "Yuanqi Yao", "Qizhi Chen", "Qi Lv", "Yiwen Tang", "Modi Shi", "Guanghui Ren", "Maoqing Yao", "Bin Zhao", "Dong Wang", "Xuelong Li"], "published_date": "2025-05-27", "title_zh": "Hume：在視覺-語言-動作模型中引入系統二思考", "summary_zh": "Hume 是一種新的視覺-語言-動作 (VLA) 模型，旨在模仿人類在執行複雜任務前的慢思考過程。它結合了基於價值的系統二思考和級聯動作去噪，提升了機器人在物理世界中的靈巧控制能力。Hume 包含兩個系統：系統一是一個輕量級的反應式視覺運動策略，系統二則透過價值查詢頭估算狀態-動作值，重複取樣多個動作候選並選擇最佳方案。實驗證明，Hume 在多個模擬基準測試和真實機器人部署中，優於現有的最先進 VLA 模型。", "applications": ["**智慧家居幫手：** Hume 可以讓機器人更聰明地執行複雜的家務任務，例如整理凌亂的廚房，不僅知道如何擺放物品，還能根據物體價值 (例如易碎性、使用頻率) 做出更合理的決策。", "**工業自動化：** Hume 可以應用於需要高度靈巧性的工業場景，例如組裝複雜零件或處理易損壞的產品，提升生產效率和降低損壞率。", "**遠程醫療手術：** Hume 有助於開發更精確的遠程手術機器人，讓醫生能夠遠程執行複雜的手術，擴大醫療資源的覆蓋範圍。"], "pitch": "Hume 代表了機器人控制領域的重大突破，它讓機器人不僅能執行動作，還能像人類一樣進行思考和決策。透過模擬人類的系統二思考，Hume 顯著提升了機器人在複雜環境中的表現。這項技術在智慧家居、工業自動化和醫療保健等領域具有巨大的商業潛力。相較於現有的基於深度學習的機器人控制方案，Hume 具備更高的穩定性、適應性和可解釋性，這將使其成為市場上更具競爭力的產品。我們相信，Hume 將引領機器人技術的下一個時代，為我們的投資者帶來豐厚的回報。", "audio": "audios/2505.21432v1.mp3", "timestamp": "2025-05-28T20:16:54.065413"}
{"query": "Diffusion Model", "id": "2505.21488v1", "url": "http://arxiv.org/abs/2505.21488v1", "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "summary": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "published_date": "2025-05-27", "title_zh": "果斷決策：雜訊誘導式多主體生成排版", "summary_zh": "現有的文字轉圖像擴散模型在生成多個不同主體時面臨挑戰。複雜的提示詞容易導致主體洩漏，造成數量、屬性和視覺特徵上的不準確。為避免主體洩漏，需要掌握每個主體的空間位置。現有方法透過外部排版控制提供這些空間位置，但強制規定的排版常與初始雜訊決定的內在排版衝突，導致與模型先驗知識不符。本研究提出一種新方法，預測與提示詞對齊、源自初始雜訊的空間排版，並在去噪過程中逐步完善。透過依賴這種雜訊誘導式排版，避免了與外部強加排版的衝突，更好地保留了模型的先驗知識。本方法採用一個小型神經網路，預測並完善每個去噪步驟中不斷演變的雜訊誘導式排版，確保主體之間有清晰的邊界，同時保持一致性。實驗結果表明，與現有的排版引導技術相比，這種雜訊對齊策略实现了更好的文本圖像對齊和更穩定的多主體生成，同時保留了模型原始分佈的豐富多樣性。", "applications": ["**客製化電子商務商品圖片：** 商家可以根據文字描述快速生成包含多個不同商品的精美圖片，例如描述「一個紅色背包，旁邊有一雙藍色運動鞋，背景是海灘」，用於商品展示和廣告宣傳。", "**個性化故事繪本創作：** 父母或教育工作者可以根據兒童的故事描述生成包含多個角色的插圖，例如描述「一隻小熊在森林裡遇到了三隻小兔子」，快速製作個性化繪本，激發兒童的想像力。", "**建築設計概念視覺化：** 建築師可以通過文字描述建築的結構和周圍環境，生成包含建築主體以及周圍景觀的示意圖，例如描述「一棟現代風格的別墅，帶有游泳池，周圍環繞著綠樹」，快速驗證設計概念並與客戶溝通。"], "pitch": "解決了文字生成圖像領域中多主體生成的一大痛點：主體洩漏和不一致性。現有技術依賴外部排版，效果不佳且干擾模型本身的能力。我們的技術利用雜訊誘導排版，更好地與模型本身的能力融合，生成更準確、更美觀、更可控的多主體圖像。想像一下，一個只需文字描述就能生成高質量產品圖的電商平台，或者一個讓每個人都能輕鬆創作個性化故事繪本的工具。這項技術的商業價值巨大，應用廣泛，從電商、教育到設計，都有潛力顛覆現有的內容創作模式，創造全新的商業機會。", "audio": "audios/2505.21488v1.mp3", "timestamp": "2025-05-28T20:17:43.148042"}
{"query": "AI", "id": "2505.21500v1", "url": "http://arxiv.org/abs/2505.21500v1", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "published_date": "2025-05-27", "title_zh": "ViewSpatial-Bench：評估視覺語言模型中的多視角空間定位能力", "summary_zh": "現有的視覺語言模型（VLMs）在理解視覺內容方面表現出色，但在跨視點理解和空間推理方面仍存在挑戰。 主要問題是，VLMs擅長以相機視角為中心的自我中心空間推理，但在需要採用另一個實體的空間參考系時，無法推廣到以物體為中心的空間推理。 我們推出了ViewSpatial-Bench，這是第一個全面的基準測試，專為跨五種不同任務類型的多視點空間定位識別評估而設計，並由自動化的3D註釋管道支持，該管道可生成精確的方向標籤。 在ViewSpatial-Bench上對各種VLMs的全面評估表明，性能存在顯著差異：模型在相機視角的任務中表現出合理的性能，但在從人類視角進行推理時，準確性會降低。 通過在我們的多視角空間數據集上微調VLMs，我們在各項任務中的總體性能提高了46.24％，突顯了我們方法的有效性。 我們的研究為具體化AI系統中的空間智能建立了一個關鍵基準，並提供了經驗證據，表明建模3D空間關係可以增強VLMs相應的空間理解能力。", "applications": ["**遠程協助維修：** 工廠技術人員可以穿戴AR設備，讓遠端專家通過VLMs看到技術人員的視角，並指示他們『從你的右邊拿起扳手』，VLMs能理解技術人員的右邊，而非專家自己的右邊，從而提供更精確的指導。", "**機器人輔助導盲：** 導盲犬的未來進化版！機器人可以理解視障人士的空間位置，並根據他們的指令導航。例如，『帶我到我右手邊的咖啡廳』，機器人能理解視障人士的空間參考系，避開障礙物，提供更安全的導航。", "**多人協作遊戲：** 在VR/AR遊戲中，玩家可以以不同的視角互動。VLMs能讓遊戲中的NPC理解不同玩家的空間關係，例如，『A玩家請掩護B玩家的左側』，NPC能準確判斷B玩家的左側並提供掩護。"], "pitch": "ViewSpatial-Bench解決了視覺語言模型在空間推理上的瓶頸，尤其是在非自我中心視角下的推理能力。這打開了許多商業應用的大門，從遠程協作到機器人輔助，再到沉浸式遊戲體驗。我們的基準測試和微調方法為VLM在空間智能領域的應用奠定了基礎。投資ViewSpatial-Bench，意味著投資下一代空間智能技術，我們將授權企業開發更智能、更直觀、更具同理心的 AI 解決方案。潛在客戶包括AR/VR設備製造商、機器人開發公司、遊戲工作室、以及需要遠程協助和培訓的企業。先發優勢顯著，市場潛力巨大。", "audio": "audios/2505.21500v1.mp3", "timestamp": "2025-05-28T21:12:59.811305"}
{"query": "Foundation Model", "id": "2505.21432v1", "url": "http://arxiv.org/abs/2505.21432v1", "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model", "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.", "authors": ["Haoming Song", "Delin Qu", "Yuanqi Yao", "Qizhi Chen", "Qi Lv", "Yiwen Tang", "Modi Shi", "Guanghui Ren", "Maoqing Yao", "Bin Zhao", "Dong Wang", "Xuelong Li"], "published_date": "2025-05-27", "title_zh": "休谟：在视觉-语言-动作模型中引入系统2思维", "summary_zh": "本研究提出名为“休谟”（Hume）的双系统视觉-语言-动作（VLA）模型，旨在赋予机器人更像人类的思考能力，提升其在复杂物理环境中的操作能力。休谟模型包含一个价值导向的系统2思考模块和一个级联动作降噪模块。系统2通过评估预测动作的价值来指导思考，重复采样多个候选动作并选择最优者。系统1则是一个轻量级的反应式视觉运动策略，接收系统2选定的动作，并进行级联动作降噪以实现精细的机器人控制。实验结果表明，休谟在多个模拟和真实机器人部署中优于现有的最先进的视觉-语言-动作模型。", "applications": ["**智能厨房助理：** 帮助机器人理解食谱并执行复杂的烹饪任务，例如切割蔬菜、搅拌食材，并能根据食材新鲜度和用户偏好选择最佳操作方案。", "**自动化仓储物流：** 用于机器人拣选、包装和运输易碎或形状不规则的物品，通过评估不同抓取方式的安全性，降低损坏率。", "**远程医疗手术：**  增强远程手术机器人的操作精准度和安全性，医生可以通过视觉反馈评估不同操作步骤的风险，减少手术失误。"], "pitch": "休谟模型代表了机器人智能的重大突破，它赋予机器人类似人类的“慢思考”能力，使其能够处理更复杂、更精细的物理任务。这解决了当前机器人智能在泛化性和鲁棒性方面的瓶颈。我们的核心优势在于独特的双系统架构，将价值评估融入动作选择，并结合了级联动作降噪技术。市场潜力巨大，尤其是在智能制造、医疗健康、物流仓储等领域，这些领域对机器人精细操作和决策能力有着迫切需求。我们正在寻找投资以加速模型优化、扩大应用场景，并构建基于休谟的机器人操作系统，最终目标是引领下一代机器人智能革命。", "audio": "audios/2505.21432v1.mp3", "timestamp": "2025-05-28T21:13:20.891603"}
{"query": "Diffusion Model", "id": "2505.21488v1", "url": "http://arxiv.org/abs/2505.21488v1", "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "summary": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "published_date": "2025-05-27", "title_zh": "果斷決策：雜訊誘導式佈局用於多主體生成", "summary_zh": "現有的文字生成圖片模型在生成多個不同主體時仍存在挑戰。複雜的提示詞經常導致主體洩漏，造成數量、屬性和視覺特徵的不準確。防止主體洩漏需要了解每個主體的空間位置。近期的方法通過外部佈局控制提供這些空間位置。然而，強制執行這種預定的佈局通常與採樣初始雜訊所決定的固有佈局相衝突，導致與模型先驗知識的不一致。本文提出了一種新方法，可以預測與提示詞對齊的空間佈局，該佈局源自初始雜訊，並在整個去噪過程中進行完善。通過依賴這種雜訊誘導式佈局，我們避免了與外部強制佈局的衝突，並更好地保留了模型的先驗知識。我們的方法採用一個小型神經網絡來預測和完善每個去噪步驟中不斷演變的雜訊誘導式佈局，確保主體之間的清晰邊界，同時保持一致性。實驗結果表明，與現有的佈局引導技術相比，這種雜訊對齊策略實現了更好的文本-圖像對齊和更穩定的多主體生成，同時保留了模型原始分佈的豐富多樣性。", "applications": ["**客製化繪本生成:** 家長可以輸入簡單的文字描述，例如「一隻小貓和一隻小狗在公園裡玩耍」，系統就能自動生成繪本插圖，確保小貓和小狗不會混在一起，並且符合小朋友對公園的想像。", "**廣告素材自動生成:** 廣告公司可以快速生成多主體廣告素材，例如「一個男人和一個女人在海灘上喝咖啡，背景是日落」，系統能確保人物比例正確，海灘和日落的風格一致，大幅降低設計成本。", "**電商產品展示圖生成:** 電商平台可以根據產品描述自動生成精美的產品展示圖，例如「一雙紅色運動鞋和一個黑色背包在木質桌面上」，系統能確保鞋子和背包的擺放位置和角度合理，並與背景風格協調。"], "pitch": "想像一下，只需簡單的文字描述，就能創造出栩栩如生、多主體、高品質的圖片，不再受限於模型對複雜提示詞的理解偏差。我們的新技術，利用雜訊誘導式佈局，完美解決了文字生成圖片領域中主體洩漏的難題。這不僅意味著更高的生成品質和更低的錯誤率，更開啟了內容創作的全新可能。從客製化繪本、廣告素材到電商產品展示，應用場景無限廣闊。其潛在商業價值巨大，能顯著降低設計成本，提高創作效率，並賦予創作者更大的自由度和靈活性。這項技術將徹底改變圖片生成領域，為企業和個人帶來前所未有的商業機會。", "audio": "audios/2505.21488v1.mp3", "timestamp": "2025-05-28T21:13:59.303118"}
{"query": "AI", "id": "2505.21500v1", "url": "http://arxiv.org/abs/2505.21500v1", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "published_date": "2025-05-27", "title_zh": "ViewSpatial-Bench：評估視覺語言模型中的多視角空間定位", "summary_zh": "現今的視覺語言模型(VLMs)在理解和推理視覺內容方面表現出色，但在需要跨視角理解和空間推理的任務中仍然面臨挑戰。 我們發現一個關鍵限制：目前的VLMs主要擅長以自我為中心的空間推理（從相機的角度），但當需要採用另一個實體的空間參考系時，它們無法推廣到以他人為中心的視角。 我們推出了ViewSpatial-Bench，這是第一個全面的基準，專門用於跨五種不同任務類型進行多視角空間定位識別評估，並由自動化的3D註釋管道提供支持，該管道生成精確的方向標籤。 對ViewSpatial-Bench上多種VLMs的全面評估揭示了顯著的性能差異：模型在相機視角任務上表現出合理的性能，但在從人的角度進行推理時，準確性會降低。 通過在我們的多視角空間數據集上微調VLMs，我們在各個任務中實現了46.24%的總體性能提升，突顯了我們方法的有效性。 我們的研究為具體化AI系統中的空間智能建立了一個關鍵基準，並提供了經驗證據表明，建模3D空間關係可以增強VLMs的相應空間理解能力。", "applications": ["**導航輔助：** 開發智能導航系統，即使使用者從不同視角描述位置，也能準確理解並引導方向，例如：'我站在書桌旁，請帶我到沙發的左邊。'", "**虛擬實境(VR)遊戲：** 提升VR遊戲的沉浸感，使遊戲角色能夠根據玩家的視角變化做出更真實的反應，例如：NPC會根據玩家的站立位置給予不同的指示或提醒。", "**遠程協作：** 增強遠程協作工具，例如，在工業設計中，工程師可以通過共享的3D模型，從各自不同的視角進行交流和協作，精確定位問題並提出解決方案。"], "pitch": "ViewSpatial-Bench 提供了一個強大的基準測試和改進工具，旨在克服視覺語言模型在多視角空間理解方面的關鍵瓶頸。 這項技術的商業價值體現在以下幾個方面：\n\n*   **智能助手升級：** 顯著提升智能助手的空間感知能力，使其能夠在複雜的環境中更精準地理解用戶指令，實現更自然的互動。\n*   **增強現實(AR)應用：** 為AR應用帶來更強大的空間理解能力，讓虛擬物體能夠更精確地與現實世界互動，例如，在AR購物應用中，用戶可以更準確地模擬家具在自家環境中的擺放效果。\n*   **自主機器人優化：** 提高自主機器人的空間定位和導航能力，使其能夠在複雜環境中更安全、更有效地執行任務，例如，在倉儲物流、無人駕駛等領域。\n\n我們相信，ViewSpatial-Bench 所實現的多視角空間理解能力將成為下一代AI應用的核心競爭力，為各行業帶來巨大的商業機會。 我們正在尋求投資，以加速我們的研究和開發工作，並將這項創新技術推向市場。", "audio": "audios/2505.21500v1.mp3", "timestamp": "2025-05-28T22:13:08.827565"}
{"query": "Foundation Model", "id": "2505.21432v1", "url": "http://arxiv.org/abs/2505.21432v1", "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model", "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.", "authors": ["Haoming Song", "Delin Qu", "Yuanqi Yao", "Qizhi Chen", "Qi Lv", "Yiwen Tang", "Modi Shi", "Guanghui Ren", "Maoqing Yao", "Bin Zhao", "Dong Wang", "Xuelong Li"], "published_date": "2025-05-27", "title_zh": "休謨：在視覺-語言-行動模型中引入系統二思維", "summary_zh": "此論文介紹了Hume，一個具有雙系統的視覺-語言-行動(VLA)模型，旨在讓機器人像人類一樣思考後再行動。Hume模型模擬了人類的「系統二」慢思考，透過價值導向的思考方式，評估並選擇最佳行動方案，再利用「系統一」快速反應的視覺運動策略，精準控制機器人執行複雜任務。實驗證明Hume在模擬和真實機器人環境中，表現優於現有的VLA模型。", "applications": ["**高精度組裝作業：** 在工廠中，Hume模型可以幫助機器人完成精密的零件組裝，例如電子產品製造，避免因操作失誤造成的損壞。", "**危險環境清理：** 在核電廠洩漏或化學品外洩等危險環境中，Hume模型可以引導機器人進行自主清理，降低人員風險。", "**複雜手術輔助：** 在手術過程中，Hume模型可以協助醫生進行精細的操作，例如微創手術，提高手術的精準度和成功率。"], "pitch": "Hume模型是機器人控制領域的突破性技術，透過模擬人類的思考模式，顯著提升了機器人在複雜環境中的適應性和操作精度。想像一下，一個能夠像人類一樣思考並執行任務的機器人，將徹底改變製造業、醫療保健、甚至家庭服務等各個產業。我們相信，Hume模型具有巨大的商業潛力，尤其是在需要高度靈活性和精確度的應用領域。投資Hume，就是投資未來機器人技術的發展，搶佔市場先機，共同打造一個更智慧、更高效的世界。", "audio": "audios/2505.21432v1.mp3", "timestamp": "2025-05-28T22:13:29.587469"}
{"query": "Diffusion Model", "id": "2505.21488v1", "url": "http://arxiv.org/abs/2505.21488v1", "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "summary": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "published_date": "2025-05-27", "title_zh": "果斷決策：噪音誘導式佈局用於多主體生成", "summary_zh": "現有的文字生成圖像擴散模型在生成多個不同主體時仍面臨挑戰。複雜的提示詞經常導致主體洩漏，造成數量、屬性和視覺特徵上的不準確。防止主體間的洩漏需要了解每個主體的空間位置。最近的方法通過外部佈局控制提供這些空間位置。然而，強制執行這種預定的佈局經常與採樣的初始噪音所決定的固有佈局相衝突，導致與模型先驗的不對齊。本研究提出一種新方法，該方法預測與提示詞對齊的空間佈局，該佈局源自初始噪音，並在整個去噪過程中對其進行細化。通過依賴於這種噪音誘導式佈局，我們避免了與外部強加的佈局的衝突，並更好地保留了模型的先驗。我們的方法採用一個小型神經網路來預測和細化每個去噪步驟中不斷演變的噪音誘導式佈局，確保主體之間清晰的邊界，同時保持一致性。實驗結果表明，與現有的佈局引導技術相比，這種噪音對齊策略實現了更好的文本-圖像對齊和更穩定的多主體生成，同時保留了模型原始分佈的豐富多樣性。", "applications": ["**客製化故事書生成：** 家長可以輸入故事情節，系統自動生成包含多個角色和場景的故事書插圖，且角色互動自然。", "**虛擬空間設計預覽：** 設計師可以利用文字描述快速生成多個房間的佈局草圖，包含家具、裝飾品等，並根據噪音誘導的佈局調整，避免風格衝突。", "**廣告素材快速生成：** 行銷人員可以根據產品特性和目標受眾的描述，生成包含多個產品和人物的廣告圖片，大幅縮短設計週期。"], "pitch": "想像一下，一個能夠根據您的描述，精準且毫不費力地生成包含多個不同主體圖像的AI系統。我們解決了現有文字生成圖像模型在處理複雜場景時的『主體洩漏』問題，透過獨特的『噪音誘導式佈局』技術，確保圖像中每個元素的空間位置和關係都完美呈現，更符合自然的視覺邏輯。這項技術不僅能提升圖像品質，更解放了內容創作的生產力。從客製化故事書到虛擬空間設計，再到廣告素材的快速生成，我們的技術應用範圍廣泛。我們的商業價值在於：(1) 顯著提升圖像生成效率，降低創作成本；(2) 提供更精準、更具創意的圖像生成結果，增強用戶體驗；(3) 打造一個全新的圖像創作平台，賦能各行各業。我們正在尋找投資人，一同將這項革命性的技術推向市場，開創圖像生成的新時代。", "audio": "audios/2505.21488v1.mp3", "timestamp": "2025-05-28T22:14:18.483916"}
{"query": "AI", "id": "2505.21500v1", "url": "http://arxiv.org/abs/2505.21500v1", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "published_date": "2025-05-27", "title_zh": "ViewSpatial-Bench：評估視覺語言模型中的多視角空間定位能力", "summary_zh": "現有的視覺語言模型(VLMs)在理解和推理視覺內容方面表現出色，但在需要跨視角理解和空間推理的任務上仍面臨挑戰。它們擅長以相機視角進行空間推理，但無法推廣到以其他實體視角為中心的空間框架。我們推出ViewSpatial-Bench，這是第一個專為多視角空間定位識別評估而設計的綜合基準測試，包含五種不同的任務類型，並由自動化的3D標註流程支援，可生成精確的方向標籤。對各種VLMs的評估顯示，模型在相機視角的任務上表現尚可，但在以人類視角進行推理時，準確性會降低。通過在我們的多視角空間數據集上對VLMs進行微調，我們在各項任務上的總體性能提高了46.24％，突顯了我們方法的有效性。這項工作為具體化人工智能系統中的空間智能建立了一個關鍵的基準，並提供了經驗證據，表明對3D空間關係建模可以增強VLMs的空間理解能力。", "applications": ["**AR導航：** 在AR應用中，模型可以理解使用者和其他物件的相對位置，並提供更精確的導航指引，例如：『走到桌子左邊的那個人那裡』，而不僅僅是『往前走』。", "**機器人輔助：** 機器人可以根據人類的指示，從人類的視角理解場景，並執行更複雜的任務，例如：『把紅色的杯子遞給坐在沙發右邊的人』。", "**智慧家居：** 智慧家居系統可以理解使用者的空間意圖，例如：『關掉臥室燈』，即使使用者不在臥室裡，系統也能根據使用者的位置和視角判斷臥室的方向。"], "pitch": "ViewSpatial-Bench 解決了視覺語言模型在空間推理領域的一大痛點：缺乏跨視角理解能力。我們的基準測試和數據集能顯著提升模型在實際應用中的性能，尤其是在需要人機協作或理解人類意圖的場景中。這項技術的潛在商業價值巨大，可以應用於AR/VR、機器人、智慧家居等多個領域。想像一下，一個能夠真正理解使用者意圖並提供精確空間導航的智慧助手，或是能夠根據人類指令執行複雜任務的機器人夥伴。ViewSpatial-Bench 不僅僅是一個研究工具，更是通往下一代人機交互的關鍵技術，具備極高的投資回報潛力。", "audio": "audios/2505.21500v1.mp3", "timestamp": "2025-05-28T23:14:06.688633"}
{"query": "Foundation Model", "id": "2505.21432v1", "url": "http://arxiv.org/abs/2505.21432v1", "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model", "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.", "authors": ["Haoming Song", "Delin Qu", "Yuanqi Yao", "Qizhi Chen", "Qi Lv", "Yiwen Tang", "Modi Shi", "Guanghui Ren", "Maoqing Yao", "Bin Zhao", "Dong Wang", "Xuelong Li"], "published_date": "2025-05-27", "title_zh": "Hume：在視覺-語言-動作模型中引入系統二思維", "summary_zh": "人類在處理複雜任務時，會先經過深思熟慮再採取行動。這種「慢思考」模式最近在提升大型語言模型（LLMs）解決數位領域的複雜任務方面取得了顯著進展。然而，對於與物理世界互動的機器人基礎模型來說，慢思考的潛力仍有待探索。本研究提出Hume：一個雙系統的視覺-語言-動作（VLA）模型，具有價值引導的系統二思維和級聯動作去噪，旨在探索視覺-語言-動作模型在靈巧機器人控制中類似人類的思考能力。Hume的系統二通過使用新的價值查詢頭來擴展視覺-語言-動作模型骨幹，以估計預測動作的狀態-動作價值，從而實現價值引導的思考。價值引導的思考通過重複採樣多個動作候選者並根據狀態-動作價值選擇一個來進行。Hume的系統一是一個輕量級的反應式視覺運動策略，它接受系統二選擇的動作並執行級聯動作去噪，用於靈巧的機器人控制。在部署時，系統二以低頻率執行價值引導的思考，而系統一異步接收系統二選擇的動作候選者並即時預測流暢的動作。實驗結果表明，Hume在多個模擬基準測試和真實機器人部署中優於現有的最先進的視覺-語言-動作模型。", "applications": ["**自動化手術助手：** Hume可以幫助外科醫生進行更精確的手術操作，例如縫合或切割，透過系統二的思考避免誤操作，系統一則執行平滑的動作。", "**智能製造組裝：** 在複雜的產品組裝線上，Hume可以控制機器人執行精細組裝任務，例如小型電子元件的放置，透過價值判斷避免錯誤組裝，並透過系統一快速調整。", "**家庭服務機器人：** Hume可以讓家庭服務機器人更好地理解人類指令，並執行更複雜的家務任務，例如整理物品、清潔角落等，思考最佳路徑和方法，並確保安全操作。"], "pitch": "Hume 代表了機器人控制領域的重大突破，將人類的思考模式融入機器人系統，實現了更高水平的靈活性和適應性。我們正在開發一個賦予機器人『思考能力』的平台，這將徹底改變機器人產業。相較於現有技術，Hume 擁有以下優勢：\n\n*   **更強大的任務處理能力：** Hume的雙系統架構使其能夠處理更複雜、更具挑戰性的任務，超越了傳統機器人系統的能力。\n*   **更高的精度和安全性：** 價值引導的系統二思維有助於降低錯誤率，提高操作的安全性。\n*   **更快的部署速度：** 系統一的輕量級設計使得 Hume 能夠快速部署到各種機器人平台。\n\n我們尋求投資以加速 Hume 的商業化進程，包括：\n\n*   **擴大研發團隊：** 吸引頂尖人才，進一步提升 Hume 的性能。\n*   **建立商業合作夥伴關係：** 與領先的機器人製造商和行業合作，將 Hume 整合到現有產品中。\n*   **拓展應用領域：** 將 Hume 應用於醫療、製造、物流等各個領域。\n\nHume 的潛在商業價值巨大，我們相信它將成為下一代機器人控制系統的關鍵技術，為投資者帶來豐厚的回報。想象一下，一個可以像人類一樣思考和行動的機器人，它將徹底改變我們的生活和工作方式。這就是 Hume 的願景，也是我們的投資機會。", "audio": "audios/2505.21432v1.mp3", "timestamp": "2025-05-28T23:15:16.780244"}
{"query": "Diffusion Model", "id": "2505.21488v1", "url": "http://arxiv.org/abs/2505.21488v1", "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "summary": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "published_date": "2025-05-27", "title_zh": "果斷決策：用於多主體生成的噪聲誘導布局", "summary_zh": "現有的文字生成圖像模型在生成多個不同主體時仍面臨挑戰，複雜的提示詞容易導致主體洩漏，造成數量、屬性和視覺特徵上的錯誤。本研究提出一種新方法，基於初始噪聲預測與提示詞對齊的空間布局，並在去噪過程中不斷優化。通過利用這種噪聲誘導布局，避免了與外部施加布局的衝突，更好地保留了模型自身的先驗知識。實驗結果表明，與現有的布局引導技術相比，這種噪聲對齊策略能實現更好的文本圖像對齊和更穩定的多主體生成，同時保持模型原始分佈的豐富多樣性。", "applications": ["**個性化兒童繪本生成：** 家長只需提供簡單的故事描述，系統就能自動生成包含多個角色且布局合理的插圖，讓孩子擁有獨一無二的繪本。", "**廣告素材快速生成：** 行銷人員可以輸入產品名稱、目標客群等資訊，系統自動產生多樣化的廣告圖片，包含多個產品和人物，大幅提升設計效率。", "**電影分鏡腳本視覺化：** 導演或編劇可以將分鏡腳本的文字描述輸入系統，系統生成初步的分鏡圖像，方便溝通和視覺化故事情節。"], "pitch": "我們正在開發一項基於AI的多主體圖像生成技術，解決了現有模型容易出現的主體洩漏問題，能夠精確控制圖像中多個主體的數量、屬性和布局。這項技術的核心優勢在於利用噪聲誘導布局，保證圖像的自然性和一致性。我們看到了在個性化內容創作、廣告行銷和影視製作等領域的巨大商業潛力。通過降低圖像生成的門檻、提升創作效率和豐富內容多樣性，我們的技術將顛覆傳統的圖像生成模式，為使用者創造巨大的價值。我們正在尋求投資，以加速產品開發和市場拓展，共同打造下一代圖像生成平台。", "audio": "audios/2505.21488v1.mp3", "timestamp": "2025-05-28T23:15:50.634001"}
{"query": "AI", "id": "2505.21500v1", "url": "http://arxiv.org/abs/2505.21500v1", "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities.", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "published_date": "2025-05-27", "title_zh": "ViewSpatial-Bench：評估視覺語言模型中的多視角空間定位能力", "summary_zh": "現有的視覺語言模型(VLMs)在理解和推理視覺內容方面表現出色，但在需要跨視角理解和空間推理的任務中仍存在挑戰。我們的研究發現，目前的VLMs主要擅長以自我為中心的空間推理(從相機視角)，但當需要採用其他實體的空間參考框架時，無法推廣到以他人為中心的視角。我們推出了ViewSpatial-Bench，這是第一個專門用於多視角空間定位識別評估的綜合基準，包含五種不同的任務類型，並由自動化的3D標註流程支持，生成精確的方向標籤。對各種VLMs在ViewSpatial-Bench上的綜合評估顯示，存在顯著的性能差距：模型在相機視角的任務中表現良好，但在從人的視角進行推理時，準確性會降低。通過在我們的多視角空間數據集上微調VLMs，我們在各項任務中的整體性能提高了46.24%，突顯了我們方法的有效性。我們的研究為具身AI系統中的空間智能建立了一個關鍵的基準，並提供了經驗證據，表明建模3D空間關係可以提高VLMs相應的空間理解能力。", "applications": ["**導航輔助：** 幫助視障人士或在複雜環境（例如大型商場、機場）中行走困難的人們，從他們的視角提供方向指引，並避開障礙物。", "**協作機器人：** 讓機器人能夠理解人類的指令，並根據人類的視角在同一空間中協作，例如，『將那個紅色的工具遞給我，就在你的右手邊』。", "**遊戲AI：** 使遊戲中的非玩家角色(NPC)能夠更真實地理解玩家的意圖和行為，並根據自身的視角做出反應，增強遊戲的沉浸感和互動性。"], "pitch": "ViewSpatial-Bench解決了視覺語言模型在跨視角空間推理方面的瓶頸，這是具身智能發展的關鍵一步。 我們提供了一個標準化的評估基準和數據集，可以加速相關技術的發展，並為開發更智能、更人性化的AI系統提供基礎。其商業價值體現在以下幾個方面：\n\n*   **提升現有VLMs的性能：** 我們的研究表明，通過微調可以顯著提高VLMs的空間推理能力，這對於許多應用程序（如自動駕駛、智能家居）至關重要。\n*   **創造新的商業應用：** 我們提出的應用場景，如導航輔助和協作機器人，具有巨大的市場潛力。\n*   **構建行業標準：** ViewSpatial-Bench有望成為評估空間智能的行業標準，吸引更多研究者和企業參與，形成良性循環。\n\n我們正在尋求投資，以進一步完善ViewSpatial-Bench，擴展數據集規模，並探索更多商業應用，將這項技術推向市場，引領具身智能的發展。", "audio": "audios/2505.21500v1.mp3", "timestamp": "2025-05-29T01:05:28.907620"}
{"query": "Foundation Model", "id": "2505.21432v1", "url": "http://arxiv.org/abs/2505.21432v1", "title": "Hume: Introducing System-2 Thinking in Visual-Language-Action Model", "summary": "Humans practice slow thinking before performing actual actions when handling\ncomplex tasks in the physical world. This thinking paradigm, recently, has\nachieved remarkable advancement in boosting Large Language Models (LLMs) to\nsolve complex tasks in digital domains. However, the potential of slow thinking\nremains largely unexplored for robotic foundation models interacting with the\nphysical world. In this work, we propose Hume: a dual-system\nVision-Language-Action (VLA) model with value-guided System-2 thinking and\ncascaded action denoising, exploring human-like thinking capabilities of\nVision-Language-Action models for dexterous robot control. System 2 of Hume\nimplements value-Guided thinking by extending a Vision-Language-Action Model\nbackbone with a novel value-query head to estimate the state-action value of\npredicted actions. The value-guided thinking is conducted by repeat sampling\nmultiple action candidates and selecting one according to state-action value.\nSystem 1 of Hume is a lightweight reactive visuomotor policy that takes System\n2 selected action and performs cascaded action denoising for dexterous robot\ncontrol. At deployment time, System 2 performs value-guided thinking at a low\nfrequency while System 1 asynchronously receives the System 2 selected action\ncandidate and predicts fluid actions in real time. We show that Hume\noutperforms the existing state-of-the-art Vision-Language-Action models across\nmultiple simulation benchmark and real-robot deployments.", "authors": ["Haoming Song", "Delin Qu", "Yuanqi Yao", "Qizhi Chen", "Qi Lv", "Yiwen Tang", "Modi Shi", "Guanghui Ren", "Maoqing Yao", "Bin Zhao", "Dong Wang", "Xuelong Li"], "published_date": "2025-05-27", "title_zh": "Hume：在視覺-語言-動作模型中引入系統二思考", "summary_zh": "Hume 是一種新的視覺-語言-動作 (VLA) 模型，它模仿人類在複雜任務中使用的「慢思考」模式，也就是所謂的系統二思考。Hume 通過價值引導思考和級聯動作降噪技術，增強了機器人基礎模型與物理世界的互動能力。它包含兩個系統：系統二基於價值引導，通過多次採樣動作候選並根據狀態-動作價值選擇最佳選項來進行思考；系統一則是一個輕量級的反應式視覺運動策略，接收系統二選擇的動作，並進行實時動作降噪，實現精巧的機器人控制。實驗證明，Hume 在模擬和真實機器人環境中都優於現有的 VLA 模型。", "applications": ["**自動化廚房助手：** Hume 可用於機器人廚房助手，精確地執行複雜的烹飪任務，例如切菜、攪拌和配料，提高效率和準確性。", "**高精度手術機器人：** Hume 的精細控制能力使其適用於手術機器人，協助醫生進行高精度操作，例如縫合和組織切除，從而提高手術成功率。", "**危險環境下的操作：** Hume 可以部署在危險環境中，例如核電站或化學品洩漏現場，執行救援、檢測和清理任務，保護人類安全。"], "pitch": "Hume 代表了機器人領域的一個重大突破，它通過引入人類的思考方式，顯著提升了機器人在複雜環境中的操作能力。這種結合了視覺、語言和動作的模型，克服了傳統機器人控制的局限性，為諸如醫療、製造業和危險環境等各行各業開闢了新的商業機會。其潛在的商業價值體現在：1. 提高效率和準確性，降低勞動成本；2. 開發新的應用場景，創造新的市場；3. 提升安全性，減少人為錯誤造成的損失。我們相信，Hume 有潜力成为下一代机器人控制平台的核心技术，并推动机器人技术在各行各业的广泛应用。投資 Hume，就是投資機器人技術的未來。", "audio": "audios/2505.21432v1.mp3", "timestamp": "2025-05-29T01:05:43.249540"}
{"query": "Diffusion Model", "id": "2505.21488v1", "url": "http://arxiv.org/abs/2505.21488v1", "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "summary": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution.", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "published_date": "2025-05-27", "title_zh": "果斷決策：噪聲誘導的多主體生成佈局", "summary_zh": "現有的文本生成圖像擴散模型在生成多個不同主體時面臨挑戰。複雜的提示詞常常導致主體洩漏，造成數量、屬性和視覺特徵上的不準確。防止主體間的洩漏需要了解每個主體的空間位置。一些方法透過外部佈局控制提供這些空間位置。然而，強制執行這種預定的佈局常常與採樣初始噪聲所決定的固有佈局相衝突，導致與模型的先驗知識不一致。本研究提出了一種新方法，該方法預測與提示詞對齊的空間佈局，該佈局源自初始噪聲，並在整個去噪過程中對其進行優化。通過依賴這種噪聲誘導的佈局，我們避免了與外部強加佈局的衝突，並更好地保留了模型的先驗知識。我們的方法採用一個小型神經網絡來預測和優化每個去噪步驟中不斷演變的噪聲誘導佈局，確保主體之間清晰的邊界，同時保持一致性。實驗結果表明，與現有的佈局引導技術相比，這種噪聲對齊策略实现了更好的文本-圖像對齊和更穩定的多主體生成，同時保留了模型原始分佈的豐富多樣性。", "applications": ["**個性化電子賀卡生成：** 用戶輸入描述，例如“生日派對上，一個小女孩拿著氣球，一隻小狗坐在旁邊”。系統自動生成一張賀卡，確保小女孩和狗狗分別位於適當的位置，氣球也在場景中，避免彼此混淆或位置錯誤。", "**電商產品展示圖片生成：** 電商平台可以使用此技術，根據產品描述（例如“一張木桌上放著一個紅色咖啡杯和一本打開的書”）生成高品質的產品展示圖片，確保杯子、書和桌子擺放得宜，吸引顧客。", "**兒童故事書插圖生成：** 作者可以輸入故事場景描述，例如“農場裡，一隻母雞帶著一群小雞在草地上散步，遠處有一座紅色穀倉”。系統自動生成生動的插圖，母雞和小雞清晰可見，穀倉也位於合理的位置，增强故事的吸引力。"], "pitch": "想像一下，一個能夠根據您的文字描述，精準生成包含多個主體、佈局合理、且視覺效果驚豔的圖像的世界。我們正在將這個想像變成現實。我們解決了現有AI圖像生成模型在處理複雜場景時的主體洩漏問題，這是一個嚴重的商業痛點。我們的噪聲誘導佈局技術，能大幅提升圖像生成的精確度和質量，降低後期人工調整的需求，節省大量時間和成本。我們的潛在市場巨大，包括但不限於電商、廣告、遊戲開發、教育以及個性化內容創作等領域。我們尋求投資以加速產品開發，擴大團隊規模，並拓展市場渠道，將我們的技術推向更廣泛的應用，成為生成式AI領域的領導者。", "audio": "audios/2505.21488v1.mp3", "timestamp": "2025-05-29T01:06:03.195567"}
{"query": "AI", "id": "2505.22647v1", "url": "http://arxiv.org/abs/2505.22647v1", "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "summary": "Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach.", "authors": ["Zhe Kong", "Feng Gao", "Yong Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Xunliang Cai", "Guanying Chen", "Wenhan Luo"], "published_date": "2025-05-28", "title_zh": "讓他們說話：音訊驅動的多人對話影片生成", "summary_zh": "現有音訊驅動的人體動畫技術，在生成同步的臉部動作和吸引人的視覺品質影片方面取得了顯著進展。然而，這些方法主要集中於單人動畫，且難以處理多個音訊輸入，面臨音訊與人物之間錯誤綁定的問題。此外，它們在遵循指令的能力方面也存在限制。本文提出一個新的任務：多人對話影片生成，並引入一個名為MultiTalk的新框架來解決多人生成過程中的挑戰。具體而言，對於音訊注入，我們研究了多種方案，並提出了標籤旋轉位置嵌入（L-RoPE）方法來解決音訊和人物綁定問題。此外，在訓練過程中，我們觀察到部分參數訓練和多任務訓練對於保持基礎模型的指令遵循能力至關重要。MultiTalk在多個數據集（包括說話頭部、說話身體和多人數據集）上均優於其他方法，展示了我們方法的強大生成能力。", "applications": ["**多人線上會議/課程增強：** 自動生成與語音同步的虛擬人物影片，讓線上會議或課程更具互動性，提升參與感和理解力，尤其是在網絡環境不佳導致影像延遲或模糊時，提供更清晰的人物表達。", "**虛擬客服/導覽：** 根據客戶的語音提問，自動生成多個虛擬人物的對話場景，提供更生動、擬真的客服或導覽體驗。想像一下，在博物館裡，你可以用語音與多個虛擬歷史人物對話，了解不同角度的歷史故事。", "**遊戲/影視製作：** 快速生成多人對話場景的動畫，大幅降低遊戲或影視製作成本。例如，可以基於劇本和配音，快速生成草稿版本的對話動畫，以便進行預覽和調整。"], "pitch": "我們正在開發 MultiTalk，一個音訊驅動的多人對話影片生成引擎，它解決了現有技術在處理多人互動場景時的瓶頸。我們的 L-RoPE 技術有效解決了音訊與人物的綁定問題，確保影片生成的準確性和真實性。透過部分參數訓練和多任務訓練，MultiTalk 能夠在保證指令遵循的同時，生成高質量、自然的對話影片。市場潛力巨大，涵蓋線上協作、娛樂、教育等多個領域。我們的競爭優勢在於能夠處理複雜的多人互動場景，並提供更高品質、更具沉浸感的用戶體驗。我們尋求投資以加速產品開發、擴大團隊，並推動市場應用，目標是成為元宇宙和虛擬現實領域，多人互動內容生成的領導者。", "audio": "audios/2505.22647v1.mp3", "timestamp": "2025-05-29T03:09:54.038248"}
{"query": "Foundation Model", "id": "2505.22637v1", "url": "http://arxiv.org/abs/2505.22637v1", "title": "Understanding (Un)Reliability of Steering Vectors in Language Models", "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction.", "authors": ["Joschka Braun", "Carsten Eickhoff", "David Krueger", "Seyed Ali Bahrainian", "Dmitrii Krasheninnikov"], "published_date": "2025-05-28", "title_zh": "理解語言模型中方向向量的（不）可靠性", "summary_zh": "方向向量是一種輕量級的方法，透過在推論時將學習到的偏差添加到激活中，來控制語言模型的行為。儘管方向向量展現出有希望的性能，但最近的研究表明，在某些情況下，它可能不可靠，甚至會產生反效果。本文研究了提示類型和激活差異的幾何形狀對方向向量可靠性的影響。首先，我們發現實驗中使用的所有七種提示類型都產生了淨正向的引導效果，但在樣本之間表現出很高的變異性，並且經常產生與期望相反的效果。沒有一種提示類型明顯優於其他類型，但來自不同提示類型的方向向量在方向上通常是不同的（通過餘弦相似度測量）。其次，我們表明，訓練集激活差異之間較高的餘弦相似度預測了更有效的引導。最後，我們觀察到，正向和負向激活更好分離的數據集更易於引導。我們的研究結果表明，當目標行為沒有被連貫的方向表示時，向量引導是不可靠的。", "applications": ["**個性化AI助手：** 根據使用者個性化調整AI助手的行為。例如，使AI助手更具創造力、更正式或更幽默。如果這個技術更可靠，就能確保助手每次互動都符合使用者偏好，避免風格不一致。", "**內容審核與過濾：** 使用方向向量來引導語言模型識別和過濾有害或不當的內容。提升方向向量的可靠性，可以更精確地控制過濾的標準，減少誤判。", "**風格轉換與文本生成：** 將文本從一種風格轉換為另一種風格，例如，將新聞報導轉換為通俗易懂的版本，或者將學術論文轉換為面向大眾的文章。如果方向向量的可靠性提高，就能確保風格轉換的一致性和準確性。"], "pitch": "這項研究揭示了控制語言模型行為的新方法——方向向量的潛在風險與機遇。雖然現階段方向向量的可靠性存在挑戰，但如果能解決這些問題，將會釋放巨大的商業價值。想像一下，我們可以更精確地控制AI模型的輸出，使其更符合用戶的具體需求，應用範圍涵蓋個性化AI助手、精準內容過濾，甚至是風格轉換等領域。我們的投資將致力於提升方向向量的可靠性，打造新一代可控的AI技術，搶佔市場先機，並創造新的應用場景，例如客製化內容生成、精準行銷和風險管理。", "audio": "audios/2505.22637v1.mp3", "timestamp": "2025-05-29T03:10:14.387781"}
{"query": "Diffusion Model", "id": "2505.22643v1", "url": "http://arxiv.org/abs/2505.22643v1", "title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", "summary": "Leveraging recent diffusion models, LiDAR-based large-scale 3D scene\ngeneration has achieved great success. While recent voxel-based approaches can\ngenerate both geometric structures and semantic labels, existing range-view\nmethods are limited to producing unlabeled LiDAR scenes. Relying on pretrained\nsegmentation models to predict the semantic maps often results in suboptimal\ncross-modal consistency. To address this limitation while preserving the\nadvantages of range-view representations, such as computational efficiency and\nsimplified network design, we propose Spiral, a novel range-view LiDAR\ndiffusion model that simultaneously generates depth, reflectance images, and\nsemantic maps. Furthermore, we introduce novel semantic-aware metrics to\nevaluate the quality of the generated labeled range-view data. Experiments on\nthe SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves\nstate-of-the-art performance with the smallest parameter size, outperforming\ntwo-step methods that combine the generative and segmentation models.\nAdditionally, we validate that range images generated by Spiral can be\neffectively used for synthetic data augmentation in the downstream segmentation\ntraining, significantly reducing the labeling effort on LiDAR data.", "authors": ["Dekai Zhu", "Yixuan Hu", "Youquan Liu", "Dongyue Lu", "Lingdong Kong", "Slobodan Ilic"], "published_date": "2025-05-28", "title_zh": "螺旋：語義感知漸進式光達場景生成", "summary_zh": "本研究提出一種名為「螺旋」(Spiral) 的新型光達擴散模型，它能在 range-view 視角下同時生成深度、反射率圖像和語義地圖。相較於既有方法，螺旋在保持計算效率和簡化網路設計優勢的同時，解決了生成未標記光達場景的限制，並避免了依賴預訓練分割模型導致的跨模態一致性問題。實驗證明，螺旋以最小的參數量達到最先進的性能，並且生成的 range image 數據可用於下游分割訓練的合成數據增強，顯著降低光達數據的標記成本。", "applications": ["**自動駕駛模擬數據生成：** 生成更真實、更豐富的自動駕駛訓練場景，加速自動駕駛算法的開發和驗證，同時降低實地測試的成本和風險。", "**機器人導航與地圖構建：** 為機器人提供更精確的環境感知能力，使其能夠在複雜環境中安全導航和進行地圖構建，尤其是在標記數據稀缺的環境下。", "**虛擬現實與遊戲場景創建：**快速生成逼真的3D城市場景，用於VR/AR應用和遊戲開發，大幅提升內容製作效率和沉浸式體驗。"], "pitch": "在自動駕駛、機器人和虛擬現實等領域，高質量的光達數據至關重要，但數據收集和標注成本高昂。「螺旋」是一種革命性的光達場景生成技術，能夠以低成本、高效率的方式產生逼真且帶有語義信息的合成數據，有效解決數據瓶頸。通過與自動駕駛公司、機器人技術公司、遊戲開發商等合作，我們可以將該技術應用於數據增強、算法開發和虛擬場景創建等領域，大幅降低開發成本、加速產品上市時間，並創造巨大的市場價值。螺旋不僅是生成數據的工具，更是開啟光達應用新紀元的鑰匙。", "audio": "audios/2505.22643v1.mp3", "timestamp": "2025-05-29T03:10:30.672688"}
{"query": "AI", "id": "2505.22647v1", "url": "http://arxiv.org/abs/2505.22647v1", "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "summary": "Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach.", "authors": ["Zhe Kong", "Feng Gao", "Yong Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Xunliang Cai", "Guanying Chen", "Wenhan Luo"], "published_date": "2025-05-28", "title_zh": "讓他們說話：音訊驅動的多人對話影片生成", "summary_zh": "現有的音訊驅動人物動畫技術在產生同步的臉部動作和吸引人的視覺品質影片方面取得了顯著進展。然而，這些技術主要集中在單個人物動畫上，並且難以處理多音訊流輸入，導致音訊和人物之間的錯誤連結問題。此外，它們在遵循指令的能力方面也存在限制。本文提出了一項新任務：多人對話影片生成，並介紹了一個名為MultiTalk的新框架來解決多人生成過程中的挑戰。具體來說，對於音訊注入，我們研究了幾種方案，並提出了標籤旋轉位置嵌入（L-RoPE）方法來解決音訊和人物的綁定問題。此外，在訓練過程中，我們觀察到部分參數訓練和多任務訓練對於保留基礎模型的指令遵循能力至關重要。MultiTalk在多個數據集（包括說話頭部、說話身體和多人數據集）上都優於其他方法，證明了我們方法的強大生成能力。", "applications": ["遠程協作會議：自動生成參與者的即時視訊，讓會議更生動、更有臨場感，即使參與者僅提供語音。", "自動生成對話式教學影片：根據課程內容和不同角色的語音，自動生成老師和學生之間互動的教學影片，提高學習效果。", "虛擬助手或遊戲角色：打造更逼真的虛擬助手或遊戲角色，根據使用者的語音指令和對話內容，生成對應的表情和動作，提升互動體驗。"], "pitch": "想像一下，只需輸入多個人的語音，就能立即生成他們自然對話的影片。MultiTalk 技術打破了傳統單人動畫的限制，讓多人互動影片生成變得簡單高效。我們解決了音訊與人物綁定的核心問題，並提升了指令遵循能力，在遠程協作、教育、遊戲等領域有廣闊的應用前景。試想一下：遠程團隊協作不再需要攝影機，虛擬老師可以生動地講解課程，遊戲角色可以根據玩家語音即時互動。MultiTalk不僅能降低影片製作成本，更能創造全新的互動體驗。我們相信MultiTalk將顛覆傳統影片製作方式，成為下一代視訊互動的基石，為創作者和使用者帶來巨大的商業價值。", "audio": "audios/2505.22647v1.mp3", "timestamp": "2025-05-29T04:19:37.779510"}
{"query": "Foundation Model", "id": "2505.22637v1", "url": "http://arxiv.org/abs/2505.22637v1", "title": "Understanding (Un)Reliability of Steering Vectors in Language Models", "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction.", "authors": ["Joschka Braun", "Carsten Eickhoff", "David Krueger", "Seyed Ali Bahrainian", "Dmitrii Krasheninnikov"], "published_date": "2025-05-28", "title_zh": "理解語言模型中轉向向量的（不）可靠性", "summary_zh": "轉向向量是一種輕量級的方法，透過在推論時將學習到的偏差添加到激活值中，來控制語言模型的行為。雖然轉向表現出令人滿意的性能，但最近的研究表明，在某些情況下它可能不可靠甚至適得其反。本文研究了提示類型和激活差異的幾何形狀對轉向可靠性的影響。首先，我們發現我們實驗中使用的所有七種提示類型都會產生淨正向的轉向效果，但在樣本間表現出高度差異，並且常常會產生與預期相反的效果。沒有任何一種提示類型明顯優於其他類型，但不同提示類型產生的轉向向量在方向上常常不同（如餘弦相似度所衡量的那樣）。其次，我們表明訓練集激活差異之間更高的餘弦相似度預測了更有效的轉向。最後，我們觀察到正向和負向激活值分離得更好的數據集更易於轉向。我們的結果表明，當目標行為未由連貫的方向表示時，向量轉向是不可靠的。", "applications": ["**聊天機器人個性化調整:** 針對不同使用者，利用轉向向量微調聊天機器人的回應風格（例如：更幽默、更正式），以提升使用者體驗。但需要注意穩定性問題，避免產生意料之外的負面回應。", "**文本風格轉換:**  在寫作輔助工具中，利用轉向向量快速將文本轉換為不同的風格（例如：將新聞報導轉為社論、將技術文件轉為科普文章）。 需要更精準控制風格，避免產生錯誤解讀。", "**內容審查和安全過濾:** 利用轉向向量引導語言模型避免產生仇恨言論、不實資訊或敏感內容。但可靠性不足可能導致誤判，影響言論自由或訊息傳播。"], "pitch": "語言模型轉向向量技術雖有潛力，但其可靠性問題限制了大規模商業應用。我們將深入研究並解決這些問題，開發更穩健且可控的轉向方法。透過結合提示工程、激活值分析和幾何優化，我們將打造出能精準調整語言模型行為的工具，應用於客戶服務、內容創作、安全過濾等領域。我們將開發一個易於使用的 API 平台，讓企業能夠輕鬆地將這項技術整合到現有的工作流程中，顯著提升效率並降低運營成本。 解決方案若能大幅提升向量轉向的穩定性與可預測性，將能開創巨大的商業價值，特別是在需要精準控制 AI 行為的垂直領域。", "audio": "audios/2505.22637v1.mp3", "timestamp": "2025-05-29T04:19:52.593494"}
{"query": "Diffusion Model", "id": "2505.22643v1", "url": "http://arxiv.org/abs/2505.22643v1", "title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", "summary": "Leveraging recent diffusion models, LiDAR-based large-scale 3D scene\ngeneration has achieved great success. While recent voxel-based approaches can\ngenerate both geometric structures and semantic labels, existing range-view\nmethods are limited to producing unlabeled LiDAR scenes. Relying on pretrained\nsegmentation models to predict the semantic maps often results in suboptimal\ncross-modal consistency. To address this limitation while preserving the\nadvantages of range-view representations, such as computational efficiency and\nsimplified network design, we propose Spiral, a novel range-view LiDAR\ndiffusion model that simultaneously generates depth, reflectance images, and\nsemantic maps. Furthermore, we introduce novel semantic-aware metrics to\nevaluate the quality of the generated labeled range-view data. Experiments on\nthe SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves\nstate-of-the-art performance with the smallest parameter size, outperforming\ntwo-step methods that combine the generative and segmentation models.\nAdditionally, we validate that range images generated by Spiral can be\neffectively used for synthetic data augmentation in the downstream segmentation\ntraining, significantly reducing the labeling effort on LiDAR data.", "authors": ["Dekai Zhu", "Yixuan Hu", "Youquan Liu", "Dongyue Lu", "Lingdong Kong", "Slobodan Ilic"], "published_date": "2025-05-28", "title_zh": "SPIRAL：語義感知的漸進式 LiDAR 場景生成", "summary_zh": "本論文提出一種名為 SPIRAL 的新型基於範圍視圖的 LiDAR 擴散模型，用於同時生成深度、反射圖像和語義地圖。現有方法雖然能生成幾何結構和語義標籤，但基於範圍視圖的方法通常只能產生未標記的 LiDAR 場景，而依賴預訓練分割模型預測語義地圖又會導致次優的跨模態一致性。 SPIRAL 旨在解決這個問題，同時保留範圍視圖表示的優勢，例如計算效率和簡化的網絡設計。論文還引入了新的語義感知指標來評估生成的標記範圍視圖數據的質量。在 SemanticKITTI 和 nuScenes 數據集上的實驗表明，SPIRAL 以最小的參數大小實現了最先進的性能，優於組合生成和分割模型的兩步方法。此外，論文驗證了由 SPIRAL 生成的範圍圖像可以有效地用於下游分割訓練中的合成數據增強，從而顯著減少 LiDAR 數據的標記工作。", "applications": ["**自動駕駛模擬器：** 生成逼真的合成 LiDAR 數據，用於訓練和測試自動駕駛系統，減少對昂貴且耗時的真實世界數據收集的依賴。", "**虛擬實境/擴增實境 (VR/AR)：** 快速創建包含語義信息的 3D 環境模型，用於 VR/AR 應用，例如遊戲、導航和室內設計。", "**智慧城市規劃：** 利用 SPIRAL 生成的 3D 場景數據，模擬城市規劃方案，評估交通流量、行人行為等，為城市規劃提供數據支持。"], "pitch": "SPIRAL 解決了 LiDAR 數據標記成本高昂的痛點，為自動駕駛、VR/AR、智慧城市等領域帶來了革命性的數據生成方案。其獨特的語義感知漸進式生成能力，不僅能高效產生高質量的合成 LiDAR 數據，還能顯著提升下游任務的性能。想像一下，一家自動駕駛公司，只需使用 SPIRAL 就能快速生成海量的訓練數據，加速算法迭代，降低開發成本。這項技術不僅能降低企業的數據獲取成本，還能催生全新的商業模式，例如數據合成服務、自動駕駛模擬平台等。我們相信 SPIRAL 具有巨大的商業潛力，將引領下一代 3D 場景生成技術的發展。", "audio": "audios/2505.22643v1.mp3", "timestamp": "2025-05-29T04:20:06.593672"}
{"query": "AI", "id": "2505.22647v1", "url": "http://arxiv.org/abs/2505.22647v1", "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "summary": "Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach.", "authors": ["Zhe Kong", "Feng Gao", "Yong Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Xunliang Cai", "Guanying Chen", "Wenhan Luo"], "published_date": "2025-05-28", "title_zh": "讓他們說話：音訊驅動的多人對話影片生成", "summary_zh": "音訊驅動的人體動畫技術，例如會說話的頭部和身體生成，在產生同步的面部動作和引人入勝的視覺品質影片方面取得了顯著進展。然而，現有方法主要集中在單人動畫上，難以處理多音訊流輸入，面臨音訊和人物之間的錯誤綁定問題。此外，它們在遵循指令的能力上也存在限制。為了解決這個問題，本文提出了一項新任務：多人對話影片生成，並引入了一個新的框架 MultiTalk，以應對多人生成過程中的挑戰。具體來說，對於音訊注入，我們研究了幾種方案，並提出了標籤旋轉位置嵌入 (L-RoPE) 方法來解決音訊和人物綁定問題。此外，在訓練過程中，我們觀察到部分參數訓練和多任務訓練對於保持基礎模型的指令遵循能力至關重要。MultiTalk 在多個資料集上，包括會說話的頭部、會說話的身體和多人資料集，都比其他方法表現出優越的性能，證明了我們方法的強大生成能力。", "applications": ["**遠距教學與虛擬會議增強：** 將無聊的線上課程或會議變成生動活潑的互動體驗。老師或講者可以使用多個音訊流（學生提問、討論）自動生成參與者的反應動畫，增加互動感和臨場感，提升學習或溝通效果。", "**客製化內容創作：** 讓使用者僅需提供文本或音訊，就能自動生成包含多個角色對話的動畫短片，例如製作個性化的生日祝福影片、遊戲過場動畫，或者用於行銷活動的客製化廣告。", "**虛擬社交互動平台：** 在元宇宙或社交平台上，用戶可以創建自己的虛擬化身，並透過語音進行互動。MultiTalk技術可以讓這些化身根據語音自然地進行對話，提升沉浸感和真實感，改善虛擬社交體驗。"], "pitch": "想像一下，不再需要複雜的動畫製作流程，只需提供音訊，就能自動生成栩栩如生的多人對話影片。MultiTalk 技術解決了現有音訊驅動動畫的關鍵痛點，實現了精準的音訊與人物綁定，並且具有強大的指令遵循能力。這項技術具有顛覆內容創作、教育、娛樂和社交等領域的潛力。我們的團隊正尋求投資，將 MultiTalk 商業化，例如：授權給遊戲公司製作高品質的過場動畫，整合到遠距教學平台提升互動性，或者打造創新的虛擬社交體驗。我們相信，MultiTalk 將重新定義人機互動的方式，創造巨大的商業價值。", "audio": "audios/2505.22647v1.mp3", "timestamp": "2025-05-29T05:14:36.784650"}
{"query": "Diffusion Model", "id": "2505.22643v1", "url": "http://arxiv.org/abs/2505.22643v1", "title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", "summary": "Leveraging recent diffusion models, LiDAR-based large-scale 3D scene\ngeneration has achieved great success. While recent voxel-based approaches can\ngenerate both geometric structures and semantic labels, existing range-view\nmethods are limited to producing unlabeled LiDAR scenes. Relying on pretrained\nsegmentation models to predict the semantic maps often results in suboptimal\ncross-modal consistency. To address this limitation while preserving the\nadvantages of range-view representations, such as computational efficiency and\nsimplified network design, we propose Spiral, a novel range-view LiDAR\ndiffusion model that simultaneously generates depth, reflectance images, and\nsemantic maps. Furthermore, we introduce novel semantic-aware metrics to\nevaluate the quality of the generated labeled range-view data. Experiments on\nthe SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves\nstate-of-the-art performance with the smallest parameter size, outperforming\ntwo-step methods that combine the generative and segmentation models.\nAdditionally, we validate that range images generated by Spiral can be\neffectively used for synthetic data augmentation in the downstream segmentation\ntraining, significantly reducing the labeling effort on LiDAR data.", "authors": ["Dekai Zhu", "Yixuan Hu", "Youquan Liu", "Dongyue Lu", "Lingdong Kong", "Slobodan Ilic"], "published_date": "2025-05-28", "title_zh": "SPIRAL：語義感知漸進式光達場景生成", "summary_zh": "現有的光達場景生成技術雖然進步，但生成帶有語義標籤的大規模3D場景仍有挑戰。SPIRAL 模型採用一種新的範圍視圖光達擴散模型，能夠同時生成深度圖、反射圖和語義地圖。它在保持計算效率和網路設計簡潔的同時，克服了傳統方法語義一致性不佳的問題。實驗證明，SPIRAL模型在參數規模最小的情況下，達到了最先進的性能，且生成的數據可用於下游分割任務的數據增強，有效降低光達數據的標註成本。", "applications": ["**自動駕駛模擬環境生成:** 利用 SPIRAL 生成逼真的合成光達數據，可以大幅降低自動駕駛汽車的測試成本，並涵蓋各種極端或危險的駕駛情境。", "**機器人導航與定位:** 在倉儲、物流等場景中，利用 SPIRAL 生成多樣化的環境數據，訓練機器人進行更精準的導航和定位，提升工作效率。", "**城市規劃與管理:** 通過 SPIRAL 生成帶有語義標籤的城市光達數據，輔助城市規劃者進行建築物識別、道路分析、綠化評估等工作，提升城市管理效率。"], "pitch": "SPIRAL 代表著光達數據生成技術的重大突破，它能以更高效、更精準的方式生成帶有語義標籤的3D場景數據。這項技術具有巨大的商業潛力，尤其是在自動駕駛、機器人、城市規劃等領域。通過提供高質量、低成本的合成數據，SPIRAL能夠加速自動駕駛算法的開發和驗證，降低機器人部署成本，並提升城市管理的智能化水平。投資 SPIRAL，就是投資下一代3D感知技術的未來，搶佔市場先機，獲得豐厚回報。", "audio": "audios/2505.22643v1.mp3", "timestamp": "2025-05-29T05:14:54.297845"}
{"query": "AI", "id": "2505.22647v1", "url": "http://arxiv.org/abs/2505.22647v1", "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "summary": "Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach.", "authors": ["Zhe Kong", "Feng Gao", "Yong Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Xunliang Cai", "Guanying Chen", "Wenhan Luo"], "published_date": "2025-05-28", "title_zh": "讓他們說話：音訊驅動的多人對話影片生成", "summary_zh": "現有的音訊驅動人物動畫技術，例如說話頭像和說話全身生成，在產生同步的臉部動作和吸引人的視覺品質影片方面取得了顯著進展。然而，現有方法主要集中在單人動畫上，並且難以處理多串流音訊輸入，面臨音訊和人物之間不正確綁定的問題。此外，它們在遵循指令的能力方面也存在限制。為了解決這個問題，本文提出了一項新任務：多人對話影片生成，並引入了一個新的框架MultiTalk，以應對多人生成過程中的挑戰。具體來說，對於音訊注入，我們研究了幾種方案，並提出了標籤旋轉位置嵌入（L-RoPE）方法來解決音訊和人物的綁定問題。此外，在訓練過程中，我們觀察到部分參數訓練和多任務訓練對於保持基礎模型的指令遵循能力至關重要。MultiTalk在多個數據集上，包括說話頭像、說話全身和多人數據集，都比其他方法表現出更優越的性能，證明了我們方法的強大生成能力。", "applications": ["**遠距協作與會議:** 將會議錄音轉換為逼真的多人互動影片，提升線上溝通的臨場感與參與度，尤其是在網路環境不佳的情況下，可以優先傳輸音訊，再根據音訊即時生成畫面。", "**語言學習與配音:** 學習者可錄製自己的聲音，生成角色進行對話練習，或將文本轉換為多個角色的對話影片，輔助語言學習和配音練習，提供更生動的學習體驗。", "**遊戲角色互動與故事創作:** 遊戲開發者或創作者可以使用MultiTalk快速生成多個角色的對話場景，提升遊戲或故事的豐富性與真實感，節省大量動畫製作時間和成本。"], "pitch": "我們正在開發一項革命性的技術，名為MultiTalk，它能夠根據音訊即時生成逼真的多人對話影片。這項技術打破了傳統單人動畫的限制，解決了音訊與人物綁定的難題，並具備卓越的指令遵循能力。MultiTalk的應用場景廣闊，從遠距協作到語言學習，再到遊戲開發，都蘊藏著巨大的商業潛力。想像一下，未來人們可以輕鬆地將會議錄音轉化為生動的互動影片，提升線上溝通效率；語言學習者可以透過生成自己的角色進行對話練習，讓學習過程更加有趣；遊戲開發者可以快速創建豐富的角色互動場景，提升遊戲體驗。MultiTalk不僅能顯著降低影片製作成本，更能大幅提升生產效率。我們相信，MultiTalk將徹底改變影音內容的生產與消費方式，成為下一代互動影音的基礎設施。我們正在尋找策略投資者，共同將MultiTalk推向市場，引領一場全新的視覺革命。", "audio": "audios/2505.22647v1.mp3", "timestamp": "2025-05-29T06:20:07.124047"}
{"query": "Foundation Model", "id": "2505.22637v1", "url": "http://arxiv.org/abs/2505.22637v1", "title": "Understanding (Un)Reliability of Steering Vectors in Language Models", "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction.", "authors": ["Joschka Braun", "Carsten Eickhoff", "David Krueger", "Seyed Ali Bahrainian", "Dmitrii Krasheninnikov"], "published_date": "2025-05-28", "title_zh": "理解語言模型中引導向量的(不)可靠性", "summary_zh": "引導向量是一種輕量級方法，透過在推論時將學習到的偏差添加到激活值中，來控制語言模型的行為。雖然引導展現出很有前景的效能，但最近的研究表明，它在某些情況下可能不可靠，甚至會產生反效果。這篇論文研究了提示類型和激活差異的幾何形狀對引導可靠性的影響。研究發現，雖然所有實驗使用的七種提示類型都產生了淨正向的引導效果，但樣本之間的變異性很高，並且常常產生與期望相反的效果。沒有一種提示類型明顯優於其他類型，但不同提示類型產生的引導向量在方向上經常不同（以餘弦相似度衡量）。研究還表明，訓練集激活差異之間較高的餘弦相似度可以預測更有效的引導。最後，研究觀察到，正向和負向激活分離得更好的數據集更易於引導。研究結果表明，當目標行為沒有以連貫的方向表示時，向量引導是不可靠的。", "applications": ["**情緒調整聊天機器人：** 開發能根據使用者情緒，即時調整回應情感傾向的聊天機器人。例如，使用者心情低落時，聊天機器人能透過引導向量產生更積極、鼓勵性的回覆。", "**程式碼自動除錯：** 使用引導向量來影響程式碼生成模型，使其在生成程式碼時更傾向於避免常見錯誤或漏洞。透過引導模型專注於安全和可靠性，降低程式碼出錯的可能性。", "**廣告文案優化：** 應用於自動產生廣告文案，根據不同目標受眾的偏好，使用引導向量調整文案的語氣和內容，以最大化點擊率或轉換率。"], "pitch": "我們正在開發一種更可靠、可控的AI行為調整技術，解決目前引導向量技術的不穩定性問題。我們的研究成果揭示了影響引導效果的關鍵因素，並為提高引導向量的準確性和一致性提供了方向。透過改善AI模型的行為控制，我們可以為客戶提供更精準、更高效的解決方案，應用範圍涵蓋聊天機器人、程式碼生成、廣告文案優化等領域。這項技術將賦予AI模型更強的適應性和可控性，使其更能滿足不同使用場景的需求，具有巨大的商業潛力，例如，在醫療領域協助醫師進行診斷，或者在金融領域提高風險控制能力。我們相信，這項技術將成為未來AI發展的重要基石，並將為各行各業帶來革命性的變革。", "audio": "audios/2505.22637v1.mp3", "timestamp": "2025-05-29T06:20:25.836964"}
{"query": "Diffusion Model", "id": "2505.22643v1", "url": "http://arxiv.org/abs/2505.22643v1", "title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", "summary": "Leveraging recent diffusion models, LiDAR-based large-scale 3D scene\ngeneration has achieved great success. While recent voxel-based approaches can\ngenerate both geometric structures and semantic labels, existing range-view\nmethods are limited to producing unlabeled LiDAR scenes. Relying on pretrained\nsegmentation models to predict the semantic maps often results in suboptimal\ncross-modal consistency. To address this limitation while preserving the\nadvantages of range-view representations, such as computational efficiency and\nsimplified network design, we propose Spiral, a novel range-view LiDAR\ndiffusion model that simultaneously generates depth, reflectance images, and\nsemantic maps. Furthermore, we introduce novel semantic-aware metrics to\nevaluate the quality of the generated labeled range-view data. Experiments on\nthe SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves\nstate-of-the-art performance with the smallest parameter size, outperforming\ntwo-step methods that combine the generative and segmentation models.\nAdditionally, we validate that range images generated by Spiral can be\neffectively used for synthetic data augmentation in the downstream segmentation\ntraining, significantly reducing the labeling effort on LiDAR data.", "authors": ["Dekai Zhu", "Yixuan Hu", "Youquan Liu", "Dongyue Lu", "Lingdong Kong", "Slobodan Ilic"], "published_date": "2025-05-28", "title_zh": "SPIRAL: 語義感知的漸進式 LiDAR 場景生成", "summary_zh": "近年來，基於擴散模型的 LiDAR 大規模 3D 場景生成取得了顯著的成功。現有的基於體素的方法雖然能生成幾何結構和語義標籤，但基於範圍視圖的方法卻僅限於生成未標記的 LiDAR 場景。仰賴預訓練的分割模型來預測語義地圖通常會導致次優的跨模態一致性。為了在保留範圍視圖表示的優勢（例如計算效率和簡化的網路設計）的同時解決這個限制，我們提出了 Spiral，這是一種新穎的範圍視圖 LiDAR 擴散模型，可以同時生成深度、反射圖像和語義地圖。此外，我們引入了新穎的語義感知指標來評估生成的標記範圍視圖資料的品質。在 SemanticKITTI 和 nuScenes 數據集上的實驗表明，Spiral 以最小的參數大小實現了最先進的性能，優於組合生成和分割模型的兩步方法。此外，我們驗證了 Spiral 生成的範圍圖像可以有效地用於下游分割訓練中的合成資料增強，從而顯著減少了 LiDAR 資料的標記工作。", "applications": ["自動駕駛模擬環境：利用生成的逼真 LiDAR 場景，可以更有效地訓練自動駕駛系統，涵蓋各種複雜路況和罕見事件。", "機器人導航與地圖建構：幫助機器人了解其周圍環境並導航，特別是在缺乏真實世界數據的情況下，生成多樣化的訓練數據。", "城市規劃與虛擬實境：為城市規劃者提供更逼真的 3D 環境，並為虛擬實境應用程序創建更豐富的互動體驗。"], "pitch": "Spiral 技術突破了 LiDAR 場景生成的瓶頸，能以更高效、更準確的方式生成帶有語義標籤的 3D 場景。這解決了自動駕駛、機器人等領域訓練數據不足的痛點。其優勢在於計算效率高、參數量小，易於部署。潛在商業價值巨大，一方面可以授權給自動駕駛公司用於模擬訓練，另一方面可以作為雲服務提供商，按需生成各種 LiDAR 場景數據。此外，還可以與現有的遊戲引擎、虛擬實境平台整合，開拓新的應用場景。團隊已證明其技術優於現有方法，並且能顯著減少標記成本，具備強大的市場競爭力。", "audio": "audios/2505.22643v1.mp3", "timestamp": "2025-05-29T06:20:41.421840"}
{"query": "AI", "id": "2505.22647v1", "url": "http://arxiv.org/abs/2505.22647v1", "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "summary": "Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach.", "authors": ["Zhe Kong", "Feng Gao", "Yong Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Xunliang Cai", "Guanying Chen", "Wenhan Luo"], "published_date": "2025-05-28", "title_zh": "讓他們說話：音訊驅動的多人對話影片生成", "summary_zh": "現有的音訊驅動人物動畫技術，像是會說話的頭像和全身動畫，在生成同步的臉部動作和高品質影片方面取得了顯著進展。然而，這些方法主要專注於單個人物動畫，在處理多重音訊輸入時會遇到困難，導致音訊與人物之間的錯誤綁定。此外，它們在遵循指令的能力方面也存在限制。為了應對這些問題，我們提出了一項新任務：多人對話影片生成，並推出了一個新的框架MultiTalk，以解決多人生成過程中的挑戰。具體來說，在音訊注入方面，我們研究了幾種方案，並提出了標籤旋轉位置嵌入（L-RoPE）方法，以解決音訊和人物綁定問題。此外，在訓練過程中，我們發現部分參數訓練和多任務訓練對於保持基礎模型的指令遵循能力至關重要。MultiTalk在多個數據集（包括會說話的頭像、全身動畫和多人數據集）上都取得了優於其他方法的性能，證明了我們方法的強大生成能力。", "applications": ["**多人線上會議/直播強化：** 即時將會議參與者的音訊轉換為自然、同步的頭像或全身動畫，提升互動體驗，甚至可以用預先設定的角色或風格，讓會議更具吸引力。", "**虛擬人物互動劇情生成：** 根據劇本和配音，自動生成包含多個虛擬角色的對話影片，大幅降低動畫製作成本，加速遊戲開發或短影音創作。", "**語言學習輔助工具：** 提供多人情境下的語音練習，根據學習者的發音，即時生成相應角色的口型和表情動畫，提供更沉浸式、更真實的學習體驗。"], "pitch": "MultiTalk解決了音訊驅動人物動畫領域長期存在的痛點，即多人對話場景的真實、自然的影片生成問題。其核心創新L-RoPE有效解決了音訊與人物的綁定問題，同時保持了基礎模型的指令遵循能力。這意味著MultiTalk不僅能生成高质量的動畫，还能根据指令灵活控制人物行为，具有高度的定制化潜力。商业价值体现在多个方面：首先，它可以大幅降低动画制作成本，加速虚拟内容生产，例如游戏、电影、教育等领域。其次，它可以赋能在线会议和直播平台，提供更具吸引力和沉浸感的互动体验，提升用户粘性。最后，它还可以应用于语言学习、社交娱乐等领域，创造新的商业模式。我们认为MultiTalk技术具有广阔的市场前景和巨大的商业价值，有望成为下一代虚拟内容创作的重要引擎。", "audio": "audios/2505.22647v1.mp3", "timestamp": "2025-05-29T07:13:49.377666"}
{"query": "Foundation Model", "id": "2505.22637v1", "url": "http://arxiv.org/abs/2505.22637v1", "title": "Understanding (Un)Reliability of Steering Vectors in Language Models", "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction.", "authors": ["Joschka Braun", "Carsten Eickhoff", "David Krueger", "Seyed Ali Bahrainian", "Dmitrii Krasheninnikov"], "published_date": "2025-05-28", "title_zh": "理解語言模型中導引向量的（不）可靠性", "summary_zh": "導引向量是一種輕量級方法，透過在推論時將學習到的偏差添加到激活值中，來控制語言模型的行為。雖然導引向量表現出很有前景的效能，但最近的研究表明，在某些情況下，它可能不可靠，甚至會適得其反。本研究探討了提示類型和激活差異的幾何形狀對導引可靠性的影響。首先，我們發現實驗中使用的所有七種提示類型都產生了淨正向的導引效果，但在樣本之間表現出很大的變異性，並且常常產生與期望相反的效果。沒有哪種提示類型明顯優於其他類型，然而，來自不同提示類型的導引向量在方向上常常不同（以餘弦相似度衡量）。其次，我們證明了訓練集中激活差異之間更高的餘弦相似度預示著更有效的導引。最後，我們觀察到正向和負向激活分離得更好的數據集更易於導引。我們的結果表明，當目標行為沒有用一個連貫的方向表示時，向量導引是不可靠的。", "applications": ["**客服機器人情緒控制:** 導引向量理論上可以控制客服機器人的情緒，使其更友善或更專業。但目前的技術不夠穩定，可能導致情緒表達不一致或適得其反。", "**內容生成風格調整:** 讓AI寫作工具生成不同風格的文章，例如幽默、嚴肅或科學。當導引向量不可靠時，生成內容的風格可能飄忽不定。", "**程式碼生成偏好設定:** 指導AI程式碼生成工具生成更安全、更高效或更易於維護的程式碼。但如果導引向量不穩定，則無法保證生成的程式碼品質。"], "pitch": "導引向量技術旨在賦予語言模型更細粒度的控制能力，使其能更好地適應特定應用場景。然而，目前該技術的可靠性仍存在問題，導致實際應用效果不佳。我們的研究揭示了影響導引向量可靠性的關鍵因素，為改進該技術提供了方向。想像一下，如果我們能夠克服這些問題，就能夠打造出真正可控、高度客製化的語言模型，在客服、內容生成、程式碼生成等領域具有巨大的應用潛力。我們正在尋求投資，以進一步研究和優化導引向量技術，目標是開發出更可靠、更精確的AI控制方法，從而開創一個全新的AI應用時代。潛在的商業價值包括：提升AI產品的效能和使用者體驗、降低AI部署的風險和成本、以及開創新的AI應用場景。", "audio": "audios/2505.22637v1.mp3", "timestamp": "2025-05-29T07:14:06.787548"}
{"query": "Diffusion Model", "id": "2505.22643v1", "url": "http://arxiv.org/abs/2505.22643v1", "title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", "summary": "Leveraging recent diffusion models, LiDAR-based large-scale 3D scene\ngeneration has achieved great success. While recent voxel-based approaches can\ngenerate both geometric structures and semantic labels, existing range-view\nmethods are limited to producing unlabeled LiDAR scenes. Relying on pretrained\nsegmentation models to predict the semantic maps often results in suboptimal\ncross-modal consistency. To address this limitation while preserving the\nadvantages of range-view representations, such as computational efficiency and\nsimplified network design, we propose Spiral, a novel range-view LiDAR\ndiffusion model that simultaneously generates depth, reflectance images, and\nsemantic maps. Furthermore, we introduce novel semantic-aware metrics to\nevaluate the quality of the generated labeled range-view data. Experiments on\nthe SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves\nstate-of-the-art performance with the smallest parameter size, outperforming\ntwo-step methods that combine the generative and segmentation models.\nAdditionally, we validate that range images generated by Spiral can be\neffectively used for synthetic data augmentation in the downstream segmentation\ntraining, significantly reducing the labeling effort on LiDAR data.", "authors": ["Dekai Zhu", "Yixuan Hu", "Youquan Liu", "Dongyue Lu", "Lingdong Kong", "Slobodan Ilic"], "published_date": "2025-05-28", "title_zh": "SPIRAL：語義感知的漸進式 LiDAR 場景生成", "summary_zh": "這篇論文提出了一個名為 SPIRAL 的新型 LiDAR 場景生成模型，利用擴散模型技術，能夠同時產生深度、反射率影像和語義地圖。不同於以往方法僅能產生無標籤的 LiDAR 場景，SPIRAL 能夠直接生成帶有語義信息的場景，且計算效率高。實驗證明，SPIRAL 在生成品質上優於現有方法，並且可以有效用於下游的語義分割訓練，減少 LiDAR 數據的標註工作量。", "applications": ["**自動駕駛模擬環境生成：** 利用 SPIRAL 產生逼真且帶有語義標籤的 LiDAR 場景，可以創建更真實、多樣化的自動駕駛模擬環境，用於訓練和驗證自動駕駛算法，降低實際路測成本和風險。", "**智慧城市建模與分析：** 基於現有 LiDAR 數據，使用 SPIRAL 擴展和完善城市三維模型，並添加語義信息，例如建築物類型、道路標記等，用於城市規劃、交通管理、災害預防等。", "**虛擬實境/擴增實境 (VR/AR) 應用：** 快速生成真實感 LiDAR 場景，並加入語義信息，可以提升 VR/AR 體驗，例如在虛擬博物館中呈現高度還原的展覽場景，或在 AR 導航中提供更精確的環境感知。"], "pitch": "SPIRAL 是一種突破性的 LiDAR 場景生成技術，它能以更高效、更精確的方式生成帶有語義信息的 3D 環境。這解決了自動駕駛、智慧城市和 VR/AR 領域中缺乏高質量訓練數據的痛點。我們的競爭優勢在於：1) 直接生成語義標籤，無需依賴昂貴的後處理流程；2) 計算效率高，參數少，易於部署；3) 通過數據增強顯著提升下游任務的性能。 我們的商業模式包括：1) 向自動駕駛公司提供授權許可，用于生成训练数据；2) 向智慧城市项目提供定制化的场景建模服务；3) 将技术集成到 VR/AR 开发工具中，收取授权费。 我們預計在未來三年內，SPIRAL 可以佔據 LiDAR 場景生成市場 20% 的份額，創造數千萬美元的收入。", "audio": "audios/2505.22643v1.mp3", "timestamp": "2025-05-29T07:14:23.165786"}
{"query": "AI", "id": "2505.22647v1", "url": "http://arxiv.org/abs/2505.22647v1", "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "summary": "Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach.", "authors": ["Zhe Kong", "Feng Gao", "Yong Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Xunliang Cai", "Guanying Chen", "Wenhan Luo"], "published_date": "2025-05-28", "title_zh": "讓他們說話：音訊驅動的多人對話影片生成", "summary_zh": "現有的音訊驅動人體動畫技術在產生同步的臉部表情和吸引人的視覺品質影片方面取得了顯著進展。然而，它們主要集中在單個人物動畫，並且難以處理多重音訊輸入，容易出現音訊和人物之間的錯誤連結問題。此外，這些技術在遵循指令方面也存在局限性。為了解決這些問題，我們提出了一項新任務：多人對話影片生成，並引入了一個名為MultiTalk的新框架，以應對多人生成過程中的挑戰。具體來說，在音訊注入方面，我們研究了幾種方案，並提出了標籤旋轉位置嵌入（L-RoPE）方法來解決音訊和人物的綁定問題。此外，在訓練過程中，我們觀察到部分參數訓練和多任務訓練對於保持基礎模型遵循指令的能力至關重要。MultiTalk在包括說話頭像、說話身體和多人數據集在內的多個數據集上，與其他方法相比，展現了卓越的性能，證明了我們方法的強大生成能力。", "applications": ["線上會議和協作平台：根據與會者的語音自動生成逼真的頭像動畫，即使沒有開啟鏡頭也能進行自然的溝通，提升線上互動體驗。", "語言學習應用：提供沉浸式口語練習環境，學生可以與由AI驅動的多個虛擬角色進行對話，模擬真實情境，提高口語流利度和自信心。", "客製化遊戲角色互動：遊戲開發者可以使用該技術創建更具表現力的非玩家角色（NPC），讓他們根據玩家的語音輸入做出即時反應，增強遊戲的沉浸感和互動性。"], "pitch": "MultiTalk解決了音訊驅動影片生成領域中一個重要的瓶頸，即多人對話場景的模擬。我們擁有一項突破性的技術，能夠根據多個音訊輸入產生逼真且同步的影片，克服了現有技術在音訊人物綁定和指令遵循方面的局限性。這項技術在線上會議、教育、娛樂等多個垂直領域擁有巨大的潛力。想像一下，未來所有的線上會議都像面對面一樣自然，語言學習不再枯燥，遊戲角色栩栩如生。我們正在尋找投資者共同打造這個未來，將MultiTalk技術推向市場，創造巨大的商業價值。我們的優勢在於獨特的L-RoPE方法和高效的訓練策略，在性能上超越了現有方案，且具備更強大的擴展性，能夠應對日益複雜的應用場景。我們團隊擁有深厚的技術積累和市場洞察力，有信心將MultiTalk打造成下一代音訊驅動影片生成技術的行業標竿。", "audio": "audios/2505.22647v1.mp3", "timestamp": "2025-05-29T08:19:06.620656"}
{"query": "Foundation Model", "id": "2505.22637v1", "url": "http://arxiv.org/abs/2505.22637v1", "title": "Understanding (Un)Reliability of Steering Vectors in Language Models", "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction.", "authors": ["Joschka Braun", "Carsten Eickhoff", "David Krueger", "Seyed Ali Bahrainian", "Dmitrii Krasheninnikov"], "published_date": "2025-05-28", "title_zh": "理解語言模型中轉向向量的（不）可靠性", "summary_zh": "轉向向量是一種輕量級的方法，通過在推理時將學習到的偏差添加到激活中來控制語言模型的行為。 雖然轉向表現出有希望的性能，但最近的研究表明，在某些情況下它可能不可靠甚至適得其反。 本文研究了提示類型和激活差異的幾何形狀對轉向可靠性的影響。 首先，我們發現我們實驗中使用的所有七種提示類型都產生了淨正轉向效果，但在樣本之間表現出很高的方差，並且經常產生與所需效果相反的效果。 沒有一種提示類型明顯優於其他提示類型，但由不同提示類型產生的轉向向量在方向上（通過餘弦相似度測量）通常不同。 其次，我們表明訓練集激活差異之間較高的餘弦相似性預測了更有效的轉向。 最後，我們觀察到正向和負向激活分離得更好的數據集更易於轉向。 我們的研究結果表明，當目標行為沒有以連貫的方向表示時，向量轉向是不可靠的。", "applications": ["**情緒調整聊天機器人：** 開發能根據使用者心情，動態調整回應的情緒聊天機器人。例如，當使用者表達悲傷時，轉向向量能調整模型輸出，使其更具同理心和鼓勵。", "**程式碼風格偏好設定：** 在程式碼生成工具中，使用轉向向量來引導程式碼風格，使其符合特定公司的編碼規範或開發者的個人偏好。這樣可以提高程式碼的可讀性和維護性。", "**內容審查與生成偏好控制：** 用於控制內容生成模型的輸出，避免產生帶有仇恨言論、歧視等不當內容。可以訓練轉向向量，抑制模型生成這些負面內容的傾向，同時鼓勵生成更積極和建設性的內容。"], "pitch": "語言模型轉向向量技術，就像汽車的輕巧方向盤，本應能精準操控AI的輸出。然而，我們的研究揭示了這個方向盤有時會失靈，甚至反向操作。這背後的原因在於訓練資料的品質和一致性。 我們的發現不僅點出了當前技術的局限性，更指明了未來優化的方向：打造更精準、更可靠的AI控制機制。想像一下，如果我們能克服這些挑戰，就能開發出能精準控制內容生成，並避免不當輸出的AI模型。這將在內容審查、客製化體驗、以及安全AI開發等領域，釋放巨大的商業潛力。 我們團隊正在開發更穩健的轉向向量訓練方法，並建立評估其可靠性的標準化流程。我們相信，透過持續的研究和創新，能讓AI的行為更可控、更可預測，為各行業帶來更安全、更有效的應用。", "audio": "audios/2505.22637v1.mp3", "timestamp": "2025-05-29T08:19:23.851748"}
{"query": "Diffusion Model", "id": "2505.22643v1", "url": "http://arxiv.org/abs/2505.22643v1", "title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", "summary": "Leveraging recent diffusion models, LiDAR-based large-scale 3D scene\ngeneration has achieved great success. While recent voxel-based approaches can\ngenerate both geometric structures and semantic labels, existing range-view\nmethods are limited to producing unlabeled LiDAR scenes. Relying on pretrained\nsegmentation models to predict the semantic maps often results in suboptimal\ncross-modal consistency. To address this limitation while preserving the\nadvantages of range-view representations, such as computational efficiency and\nsimplified network design, we propose Spiral, a novel range-view LiDAR\ndiffusion model that simultaneously generates depth, reflectance images, and\nsemantic maps. Furthermore, we introduce novel semantic-aware metrics to\nevaluate the quality of the generated labeled range-view data. Experiments on\nthe SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves\nstate-of-the-art performance with the smallest parameter size, outperforming\ntwo-step methods that combine the generative and segmentation models.\nAdditionally, we validate that range images generated by Spiral can be\neffectively used for synthetic data augmentation in the downstream segmentation\ntraining, significantly reducing the labeling effort on LiDAR data.", "authors": ["Dekai Zhu", "Yixuan Hu", "Youquan Liu", "Dongyue Lu", "Lingdong Kong", "Slobodan Ilic"], "published_date": "2025-05-28", "title_zh": "螺旋：語義感知的漸進式光達場景生成", "summary_zh": "本研究提出一個名為「螺旋」（Spiral）的新型光達擴散模型，它能同時生成深度、反射率圖像和語義地圖。現有方法通常只能生成未標記的光達場景，而依賴預訓練分割模型來預測語義地圖往往會導致次優的跨模態一致性。「螺旋」模型旨在解決這個限制，同時保留範圍視圖表示的優勢，例如計算效率和簡化的網路設計。實驗證明，「螺旋」在SemanticKITTI和nuScenes數據集上達到了最先進的性能，且參數量最小，並且能夠有效用於下游分割訓練中的合成數據增強，顯著減少光達數據的標記工作量。", "applications": ["**自動駕駛模擬器：** 產生逼真的、帶語義標籤的光達場景，用於訓練和驗證自動駕駛系統，無需大量真實世界數據採集。", "**機器人導航：** 創造各種複雜的環境，讓機器人學習在不同情境下進行導航和物件辨識，提升機器人在未知環境中的適應性。", "**智慧城市規劃：** 生成城市環境的光達模型，協助城市規劃者評估不同設計方案對交通流量、行人安全和環境影響的影響，並進行模擬測試。"], "pitch": "各位投資人，想像一下一個能無限量生成帶語義標籤光達數據的引擎。這就是Spiral。我們解決了光達數據標註成本高昂的痛點，提供一個高效、低成本的解決方案，讓自動駕駛、機器人、智慧城市等領域可以加速發展。Spiral的創新擴散模型，不僅性能優異，更降低了運算資源需求，使得大規模場景生成成為可能。我們的早期成果已經證明，利用Spiral生成的數據能顯著提升模型訓練效率。我們相信，Spiral將會成為光達數據生成的業界標準，並在各行業創造巨大的商業價值。現在加入我們，一起掌握光達數據的未來！", "audio": "audios/2505.22643v1.mp3", "timestamp": "2025-05-29T08:19:37.518261"}
{"query": "AI", "id": "2505.22647v1", "url": "http://arxiv.org/abs/2505.22647v1", "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "summary": "Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach.", "authors": ["Zhe Kong", "Feng Gao", "Yong Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Xunliang Cai", "Guanying Chen", "Wenhan Luo"], "published_date": "2025-05-28", "title_zh": "讓他們說話：音訊驅動的多人對話影片生成", "summary_zh": "現有音訊驅動的人體動畫方法，例如說話頭部和說話身體生成，在產生同步的面部運動和引人注目的視覺品質影片方面取得了顯著進展。然而，現有方法主要集中於單人動畫，並且在處理多聲道音訊輸入時遇到困難，面臨音訊和人物之間的不正確綁定問題。此外，它們在指令遵循能力方面表現出局限性。為了解決這個問題，本文提出了一個新的任務：多人對話影片生成，並引入了一個新的框架 MultiTalk，以應對多人生成過程中的挑戰。具體來說，對於音訊注入，我們研究了幾種方案，並提出了標籤旋轉位置嵌入（L-RoPE）方法來解決音訊和人物綁定問題。此外，在訓練過程中，我們觀察到部分參數訓練和多任務訓練對於保持基礎模型的指令遵循能力至關重要。 MultiTalk在包括說話頭部、說話身體和多人數據集在內的多個數據集上，與其他方法相比，取得了優異的性能，展示了我們方法的強大生成能力。", "applications": ["線上教育與培訓：自動生成多位講師之間的對話影片，提升教學互動性。", "社交媒體與娛樂：用戶可輸入多段音訊，讓虛擬角色進行互動對話，創造有趣的短影音內容。", "遠距協作：將多位參與者的語音轉換為虛擬會議畫面，提升溝通效率與真實感。"], "pitch": "MultiTalk技術突破了音訊驅動影片生成領域的瓶頸，實現了多人對話影片的自動生成。這將顛覆傳統影片製作流程，降低製作成本，並開創全新的應用場景。想像一下，只需提供音訊，即可快速生成具備高度真實感的多人互動影片，在教育、娛樂、協作等領域都具有巨大的商業潛力。通過授權、雲端服務、客製化解決方案等方式，我們可打造高營收、高成長的商業模式，搶佔下一代影片生成市場的先機。", "audio": "audios/2505.22647v1.mp3", "timestamp": "2025-05-29T09:14:57.236931"}
{"query": "Foundation Model", "id": "2505.22637v1", "url": "http://arxiv.org/abs/2505.22637v1", "title": "Understanding (Un)Reliability of Steering Vectors in Language Models", "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction.", "authors": ["Joschka Braun", "Carsten Eickhoff", "David Krueger", "Seyed Ali Bahrainian", "Dmitrii Krasheninnikov"], "published_date": "2025-05-28", "title_zh": "理解語言模型中轉向向量的（不）可靠性", "summary_zh": "論文探討一種輕量級控制語言模型行為的方法——轉向向量——在推論時透過加入學習到的偏差來調整模型激活。研究發現，雖然轉向向量展現了潛力，但在某些情況下可能不可靠甚至適得其反。研究分析了提示詞類型和激活差異的幾何形狀對轉向可靠性的影響。實驗表明，不同提示詞類型產生的轉向效果差異很大，且效果可能與預期相反。另外，訓練集激活差異的餘弦相似度越高，轉向效果越好。最後，當正向和負向激活被更好地分離時，資料集更易於轉向。研究結果表明，當目標行為沒有被連貫的方向表示時，向量轉向是不可靠的。", "applications": ["個性化學習輔導：根據學生的學習風格和進度，調整語言模型的輸出，提供更有效的學習建議和內容。", "情緒感知聊天機器人：透過轉向向量控制聊天機器人的情緒表達，使其更能同理使用者並提供更適切的回應。", "內容生成偏好調整：允許使用者微調生成內容的風格、語氣或主題，例如將新聞報導調整為更客觀或更具娛樂性。"], "pitch": "我們解決了語言模型操控性的一個核心問題：可靠性。透過理解並改善轉向向量的可靠性，我們能夠賦予使用者前所未有的對模型行為的控制權。這開啟了個性化AI服務、情緒化人機互動以及更精準內容生成的巨大市場。我們的研究成果為開發更可控、更值得信賴的AI系統奠定了基礎，有望在教育、娛樂、客服等領域創造顛覆性的商業價值。現在的語言模型就像一艘大船，我們要做的是為它裝上更精確的舵，讓它能夠駛向更廣闊的藍海。", "audio": "audios/2505.22637v1.mp3", "timestamp": "2025-05-29T09:15:10.658651"}
{"query": "Diffusion Model", "id": "2505.22643v1", "url": "http://arxiv.org/abs/2505.22643v1", "title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", "summary": "Leveraging recent diffusion models, LiDAR-based large-scale 3D scene\ngeneration has achieved great success. While recent voxel-based approaches can\ngenerate both geometric structures and semantic labels, existing range-view\nmethods are limited to producing unlabeled LiDAR scenes. Relying on pretrained\nsegmentation models to predict the semantic maps often results in suboptimal\ncross-modal consistency. To address this limitation while preserving the\nadvantages of range-view representations, such as computational efficiency and\nsimplified network design, we propose Spiral, a novel range-view LiDAR\ndiffusion model that simultaneously generates depth, reflectance images, and\nsemantic maps. Furthermore, we introduce novel semantic-aware metrics to\nevaluate the quality of the generated labeled range-view data. Experiments on\nthe SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves\nstate-of-the-art performance with the smallest parameter size, outperforming\ntwo-step methods that combine the generative and segmentation models.\nAdditionally, we validate that range images generated by Spiral can be\neffectively used for synthetic data augmentation in the downstream segmentation\ntraining, significantly reducing the labeling effort on LiDAR data.", "authors": ["Dekai Zhu", "Yixuan Hu", "Youquan Liu", "Dongyue Lu", "Lingdong Kong", "Slobodan Ilic"], "published_date": "2025-05-28", "title_zh": "SPIRAL：語義感知的漸進式雷射雷達場景生成", "summary_zh": "利用最新的擴散模型，基於雷射雷達的大規模3D場景生成取得了顯著進展。雖然現有的基於體素的方法可以生成幾何結構和語義標籤，但現有的距離視圖方法僅限於生成未標記的雷射雷達場景。依靠預訓練的分割模型來預測語義地圖通常會導致次優的跨模態一致性。為了在保留距離視圖表示優勢（例如計算效率和簡化的網路設計）的同時解決這個局限性，我們提出了一種新的距離視圖雷射雷達擴散模型Spiral，它可以同時生成深度、反射率圖像和語義地圖。此外，我們引入了新的語義感知指標來評估生成的標記距離視圖數據的質量。在SemanticKITTI和nuScenes數據集上的實驗表明，Spiral以最小的參數尺寸實現了最先進的性能，優於結合生成和分割模型的兩步方法。此外，我們驗證了Spiral生成的距離圖像可以有效地用於下游分割訓練中的合成數據增強，從而顯著減少雷射雷達數據的標註工作。", "applications": ["**自動駕駛模擬環境生成：** 可以用來生成各種逼真的交通場景，包含不同天氣、光照和物件配置，讓自動駕駛系統在虛擬環境中進行充分的測試和訓練，降低實際路測的成本和風險。", "**機器人導航與感知：** 可以生成室內和室外的雷射雷達環境，幫助機器人學習在複雜環境中進行導航和物件識別，例如倉儲機器人、清潔機器人等。", "**城市規劃與建模：** 生成帶有語義信息的城市雷射雷達數據，用於城市規劃、基礎設施維護和災難預測等應用，例如模擬洪水對城市街道的影響。"], "pitch": "SPIRAL是一個利用擴散模型生成語義感知雷射雷達數據的突破性技術。它解決了現有方法在跨模態一致性和標註成本上的痛點。其核心價值在於以更低的成本和更高的效率生成高品質的合成雷射雷達數據，這對自動駕駛、機器人和城市規劃等領域至關重要。相較於競爭對手，SPIRAL模型參數更小，效率更高，同時還提供語義信息，極具商業潛力。我們可以將SPIRAL技術授權給自動駕駛公司、機器人製造商和城市規劃機構，或者基於此開發合成數據服務，為客戶提供客製化的雷射雷達數據解決方案。預期市場規模龐大，回報可期。", "audio": "audios/2505.22643v1.mp3", "timestamp": "2025-05-29T09:15:27.257821"}
{"query": "AI", "id": "2505.22647v1", "url": "http://arxiv.org/abs/2505.22647v1", "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "summary": "Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach.", "authors": ["Zhe Kong", "Feng Gao", "Yong Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Xunliang Cai", "Guanying Chen", "Wenhan Luo"], "published_date": "2025-05-28", "title_zh": "讓他們說話：音訊驅動的多人對話影片生成", "summary_zh": "現有的音訊驅動人物動畫技術在生成同步的臉部動作和高品質影片方面取得了顯著進展，但主要集中於單人動畫，難以處理多音訊流輸入，導致音訊與人物綁定錯誤。此外，這些技術在遵循指令方面也存在局限性。本論文提出一個新的任務：多人對話影片生成，並引入一個新的框架MultiTalk，以解決多人生成過程中的挑戰。具體而言，針對音訊注入，我們研究了幾種方案，並提出了標籤旋轉位置嵌入(L-RoPE)方法來解決音訊和人物的綁定問題。此外，在訓練過程中，我們發現部分參數訓練和多任務訓練對於保留基礎模型遵循指令的能力至關重要。MultiTalk在包括說話頭部、說話身體和多人數據集的多個數據集上取得了優於其他方法的性能，證明了我們方法的強大生成能力。", "applications": ["**線上會議與協作：** 自動生成會議記錄影片，精準呈現每位與會者的發言內容和表情，即使沒有鏡頭也能參與視訊會議。", "**語言學習與角色扮演：** 根據劇本和不同角色的聲音，自動生成多人對話影片，提供更生動的語言學習和角色扮演體驗。", "**歷史人物重現與教育：** 運用歷史錄音或聲音資料，重現歷史人物的對話場景，讓歷史教育更具沉浸感和互動性。"], "pitch": "MultiTalk解決了音訊驅動人物動畫領域的一大痛點：多人對話影片的生成。目前市場上的技術多集中於單人動畫，忽略了多人場景的龐大需求。MultiTalk通過L-RoPE等創新技術，能夠精確地將音訊與對應的人物綁定，並保持良好的指令遵循能力。這項技術的商業價值體現在以下幾個方面：\n\n*   **市場潛力巨大：** 線上會議、語言學習、娛樂、教育等領域對多人對話影片的需求持續增長。\n*   **技術領先：** MultiTalk在生成質量和精度上優於現有方法，具有顯著的競爭優勢。\n*   **可擴展性強：** 該技術可以擴展到更多應用場景，例如：虛擬偶像、遊戲角色動畫等。\n\n我們相信，MultiTalk將會成為音訊驅動人物動畫領域的顛覆者，為相關產業帶來巨大的商業價值。我們的團隊擁有深厚的技術積累和豐富的市場經驗，有能力將MultiTalk成功商業化，成為行業領先者。我們尋求戰略投資者，共同開發市場，實現共贏。", "audio": "audios/2505.22647v1.mp3", "timestamp": "2025-05-29T10:15:16.563059"}
{"query": "Foundation Model", "id": "2505.22637v1", "url": "http://arxiv.org/abs/2505.22637v1", "title": "Understanding (Un)Reliability of Steering Vectors in Language Models", "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction.", "authors": ["Joschka Braun", "Carsten Eickhoff", "David Krueger", "Seyed Ali Bahrainian", "Dmitrii Krasheninnikov"], "published_date": "2025-05-28", "title_zh": "理解語言模型中操縱向量的（不）可靠性", "summary_zh": "操縱向量是一種輕量化的方式，透過在推論時對模型激活值添加學習到的偏差來控制語言模型的行為。雖然這種方法展現了潛力，但近期研究表明，在某些情況下，它可能不可靠甚至適得其反。本研究探討了提示類型和激活值差異幾何結構對操縱可靠性的影響。研究發現，所有實驗使用的七種提示類型總體上都產生了正向的操縱效果，但在樣本之間存在高度方差，並且經常產生與期望相反的效果。沒有哪種提示類型明顯優於其他類型，但不同提示類型產生的操縱向量在方向上通常不同（如餘弦相似度所衡量）。研究還表明，訓練集中激活值差異之間的餘弦相似度越高，預示著更有效的操縱。最後，觀察到正向和負向激活值分離得更好的資料集更易於操縱。研究結果表明，當目標行為沒有以連貫的方向表示時，向量操縱是不可靠的。", "applications": ["**內容審核：** 可以使用操縱向量來引導語言模型更好地檢測和過濾有害內容，例如仇恨言論或煽動暴力，但需要注意其可靠性，避免誤判。", "**客服機器人個性化：** 透過調整操縱向量，客服機器人可以根據客戶的情緒或需求，提供更個性化和同理心的回覆，但要小心可能產生的偏誤或不一致的表現。", "**創意寫作輔助：** 幫助作家在特定風格或主題下產生文本，例如引導模型撰寫恐怖小說或科幻故事，但需要監控其創造性輸出，避免產生重複或缺乏新意的內容。"], "pitch": "我們正在開發一種更可靠、更可控的語言模型行為操縱技術，基於對現有操縱向量方法缺陷的深刻理解。現有技術容易出現偏差、不一致性和不可預測性，導致應用受限。我們的創新之處在於深入研究提示工程和激活空間的幾何特性，以開發更精確和穩健的操縱方法。商業價值在於：1) **更安全的AI：** 顯著降低語言模型產生有害或不當內容的風險，滿足企業對合規性和品牌安全的迫切需求。2) **更優化的AI應用：** 使企業能夠精準地調整AI行為，提升客戶服務、內容生成和數據分析等領域的效率和效果。3) **差異化競爭優勢：** 提供一種獨特且高效的AI控制工具，使我們的客戶能夠在市場上脫穎而出，創造新的商業模式。我們正在尋求投資，以加速技術開發、擴大團隊規模，並與領先企業建立合作夥伴關係，共同開創AI控制的新紀元。", "audio": "audios/2505.22637v1.mp3", "timestamp": "2025-05-29T10:15:35.421828"}
{"query": "AI", "id": "2505.22647v1", "url": "http://arxiv.org/abs/2505.22647v1", "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "summary": "Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach.", "authors": ["Zhe Kong", "Feng Gao", "Yong Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Xunliang Cai", "Guanying Chen", "Wenhan Luo"], "published_date": "2025-05-28", "title_zh": "讓他們說話：音訊驅動的多人對話影片生成", "summary_zh": "現有音訊驅動的人體動畫技術，例如會說話的頭像或全身動畫，在生成同步的臉部動作和吸引人的視覺效果影片方面取得了顯著進展。然而，這些方法主要集中在單人動畫，並且難以處理多重音訊輸入，導致音訊與人物之間的錯誤綁定。此外，它們在遵循指令的能力上也存在限制。為了解決這個問題，我們提出了一個新的任務：多人對話影片生成，並引入一個新的框架MultiTalk，來應對多人生成過程中的挑戰。具體來說，在音訊注入方面，我們研究了多種方案，並提出了標籤旋轉位置嵌入（L-RoPE）方法來解決音訊和人物的綁定問題。此外，在訓練過程中，我們發現部分參數訓練和多任務訓練對於保持基礎模型遵循指令的能力至關重要。MultiTalk在多個數據集上，包括會說話的頭像、會說話的全身和多人數據集，都取得了比其他方法更優越的性能，展現了我們方法的強大生成能力。", "applications": ["遠距協作與會議：讓多方參與者僅透過語音就能產生栩栩如生的會議影片，改善遠端溝通體驗。", "虛擬角色互動：創建具有真實對話能力的多個虛擬角色，應用於遊戲、教育或娛樂領域，提供更沉浸式的體驗。", "自動化內容創作：自動生成對話場景影片，例如新聞播報、故事講述等，降低內容製作成本並提高效率。"], "pitch": "MultiTalk 解決了目前音訊驅動影片生成技術在處理多人對話上的痛點，具備高度可擴展性和優越的生成品質。遠端工作、虛擬實境、教育娛樂等領域對此類技術的需求日益增長。透過授權 MultiTalk 技術，或開發基於 MultiTalk 的 SaaS 平台，可快速搶佔市場先機。想像一下，企業可以利用 MultiTalk 生成逼真的員工培訓影片，遊戲開發者可以創造栩栩如生的 NPC 對話，教育機構可以打造互動式的虛擬教學體驗。市場潛力巨大，投資回報可期。", "audio": "audios/2505.22647v1.mp3", "timestamp": "2025-05-29T11:12:13.125361"}
{"query": "Foundation Model", "id": "2505.22637v1", "url": "http://arxiv.org/abs/2505.22637v1", "title": "Understanding (Un)Reliability of Steering Vectors in Language Models", "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction.", "authors": ["Joschka Braun", "Carsten Eickhoff", "David Krueger", "Seyed Ali Bahrainian", "Dmitrii Krasheninnikov"], "published_date": "2025-05-28", "title_zh": "理解語言模型中轉向向量的（不）可靠性", "summary_zh": "轉向向量是一種輕量級的方法，透過在推論時將學習到的偏差添加到激活值中來控制語言模型的行為。儘管轉向表現出令人期待的性能，但最近的研究表明，在某些情況下它可能不可靠，甚至適得其反。本文研究了提示類型和激活差異的幾何結構對轉向可靠性的影響。首先，我們發現我們實驗中使用的所有七種提示類型都產生了淨正向的轉向效果，但在樣本間表現出高度差異，並且常常產生與預期相反的效果。沒有一種提示類型明顯優於其他提示類型，但不同提示類型產生的轉向向量在方向上通常不同（通過餘弦相似度測量）。其次，我們表明訓練集激活差異之間較高的餘弦相似度預測了更有效的轉向。最後，我們觀察到正向和負向激活更好分離的數據集更易於轉向。我們的結果表明，當目標行為沒有由一個連貫的方向表示時，向量轉向是不可靠的。", "applications": ["**個性化客服機器人：** 想像一個客服機器人，可以根據不同的用戶情緒（開心、沮喪）調整其回應風格。透過轉向向量，可以讓機器人在用戶表達不滿時，提供更具同理心的回應，提升客戶滿意度。", "**安全內容篩選：** 利用轉向向量來控制語言模型生成內容的傾向，例如避免生成仇恨言論或不雅內容。雖然目前可靠性不佳，但如果能優化，可以更精準地引導模型產生更安全的內容。", "**風格化文本生成：** 讓語言模型能夠模仿特定作家的風格，例如莎士比亞或村上春樹。使用者可以透過調整轉向向量來控制文本的語氣、詞彙和句法結構，創造出獨特的文學作品。"], "pitch": "我們正在研究一種讓AI更聽話、更可控的技術——轉向向量。它能像遙控器一樣，微調AI的行為，使其更符合人類的期望。目前，這項技術還存在可靠性問題，但我們正在努力解決。想像一下，如果我們能完美控制AI的生成內容，就能創造出無數商業機會：高度個性化的廣告、自動生成符合特定需求的內容、甚至是完全可控的AI夥伴。我們的研究目標是讓轉向向量變得更穩定、更精準，最終打造一個可信任、可定制的AI世界。這不僅是一項技術突破，更是一次商業模式的革新，具有巨大的投資潛力。", "audio": "audios/2505.22637v1.mp3", "timestamp": "2025-05-29T11:12:31.370221"}
{"query": "Diffusion Model", "id": "2505.22643v1", "url": "http://arxiv.org/abs/2505.22643v1", "title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", "summary": "Leveraging recent diffusion models, LiDAR-based large-scale 3D scene\ngeneration has achieved great success. While recent voxel-based approaches can\ngenerate both geometric structures and semantic labels, existing range-view\nmethods are limited to producing unlabeled LiDAR scenes. Relying on pretrained\nsegmentation models to predict the semantic maps often results in suboptimal\ncross-modal consistency. To address this limitation while preserving the\nadvantages of range-view representations, such as computational efficiency and\nsimplified network design, we propose Spiral, a novel range-view LiDAR\ndiffusion model that simultaneously generates depth, reflectance images, and\nsemantic maps. Furthermore, we introduce novel semantic-aware metrics to\nevaluate the quality of the generated labeled range-view data. Experiments on\nthe SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves\nstate-of-the-art performance with the smallest parameter size, outperforming\ntwo-step methods that combine the generative and segmentation models.\nAdditionally, we validate that range images generated by Spiral can be\neffectively used for synthetic data augmentation in the downstream segmentation\ntraining, significantly reducing the labeling effort on LiDAR data.", "authors": ["Dekai Zhu", "Yixuan Hu", "Youquan Liu", "Dongyue Lu", "Lingdong Kong", "Slobodan Ilic"], "published_date": "2025-05-28", "title_zh": "SPIRAL：語義感知漸進式光達場景生成", "summary_zh": "近年來，基於光達的大規模3D場景生成借助擴散模型取得了顯著進展。雖然基於體素的方法能夠同時生成幾何結構和語義標籤，但現有的range-view方法僅限於生成未標記的光達場景。依賴預訓練的分割模型來預測語義地圖通常會導致次優的跨模態一致性。為了在保持range-view表示優勢（例如計算效率和簡化的網路設計）的同時解決這個限制，我們提出了Spiral，一種新穎的range-view光達擴散模型，可以同時生成深度、反射圖像和語義地圖。此外，我們引入了新的語義感知指標來評估生成的帶標籤range-view數據的質量。在SemanticKITTI和nuScenes數據集上的實驗表明，Spiral以最小的參數尺寸實現了最先進的性能，優於結合生成模型和分割模型的兩步法。此外，我們驗證了Spiral生成的range圖像可以有效地用於下游分割訓練中的合成數據增強，顯著減少了光達數據的標記工作。", "applications": ["**自動駕駛模擬訓練：** SPIRAL能夠生成高度真實且帶有語義標籤的光達場景，可用於創建逼真的自動駕駛模擬環境，降低測試成本並提高安全性。", "**智慧城市規劃：** 通過生成不同情境下的光達場景，幫助城市規劃者更好地理解和預測城市發展的影響，例如，模擬不同建築布局對交通流量的影響。", "**機器人導航：** 為機器人創建各種虛擬環境，訓練機器人進行複雜的導航任務，例如在擁擠的倉庫或雜亂的家庭環境中尋找目標物品。"], "pitch": "SPIRAL 是一項突破性的技術，它使用基於擴散模型的range-view光達生成方法，以更低的計算成本和更小的模型規模，生成帶有語義標籤的3D場景。這項技術解決了自動駕駛、智慧城市和機器人領域對大量高質量訓練數據的需求。我們的核心優勢在於能夠直接生成帶標籤的光達數據，無需依賴外部的分割模型，從而提高了效率和準確性。商業價值體現在以下幾個方面：首先，大幅降低自動駕駛開發商在數據標記上的成本，加速產品上市。其次，提供更靈活和可控的模擬環境，用於測試和驗證自動駕駛系統。第三，為智慧城市和機器人領域提供定制化的數據生成服務，滿足其特定場景的需求。我們相信SPIRAL將成為未來3D數據生成領域的領先技術，具有巨大的市場潛力和投資回報。", "audio": "audios/2505.22643v1.mp3", "timestamp": "2025-05-29T11:12:49.128148"}
{"query": "AI", "id": "2505.22647v1", "url": "http://arxiv.org/abs/2505.22647v1", "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "summary": "Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach.", "authors": ["Zhe Kong", "Feng Gao", "Yong Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Xunliang Cai", "Guanying Chen", "Wenhan Luo"], "published_date": "2025-05-28", "title_zh": "讓他們說話：音訊驅動的多人對話影片生成", "summary_zh": "現有的語音驅動人體動畫技術，像是說話頭像和說話全身生成，在同步面部動作和產生吸引人的視覺效果影片方面取得了顯著進展。然而，這些方法主要集中在單人動畫，並且難以處理多路音訊輸入，面臨音訊和人物之間錯誤綁定的問題。此外，它們在遵循指令的能力上也存在限制。為了解決這個問題，我們提出了一項新任務：多人對話影片生成，並引入一個新的框架MultiTalk，來應對多人生成過程中的挑戰。具體來說，對於音訊注入，我們研究了幾種方案，並提出標籤旋轉位置嵌入（L-RoPE）方法來解決音訊和人物綁定的問題。此外，在訓練過程中，我們觀察到部分參數訓練和多任務訓練對於保持基礎模型的指令遵循能力至關重要。MultiTalk 在包括說話頭像、說話全身和多人資料集在內的幾個資料集上，都比其他方法表現出更優越的性能，證明了我們方法的強大生成能力。", "applications": ["**遠距協作平台增強：** 在視訊會議中，根據與會者的語音自動生成逼真的對話影片，即使在頻寬有限的環境下，也能提升溝通體驗，讓遠距會議更自然。", "**自動化語言學習工具：** 根據學習者設定的語言和情境，生成多人的對話影片，讓學習者身歷其境地學習語言，提升口語和聽力能力。", "**客製化故事生成：** 讓使用者輸入劇情和角色設定，系統根據設定生成包含多人對話的動畫短片，滿足個性化的娛樂需求，例如為孩子們講述獨一無二的故事。"], "pitch": "MultiTalk 解決了現有音訊驅動人體動畫技術無法處理多人對話的痛點，在遠距協作、教育娛樂等領域有巨大的商業潛力。我們的技術不僅能生成高逼真度的對話影片，更具備優異的指令遵循能力，能根據使用者需求客製化內容。想像一下，一個可以根據會議記錄自動生成會議影片的軟體，一個可以根據劇本自動生成動畫短片的平台，一個可以讓語言學習者與 AI 虛擬角色進行沉浸式對話的應用。這些都是 MultiTalk 的潛在應用場景。我們相信，MultiTalk 將成為下一代視訊生成技術的基石，為企業和個人創造巨大的價值。", "audio": "audios/2505.22647v1.mp3", "timestamp": "2025-05-29T12:26:36.936505"}
{"query": "Foundation Model", "id": "2505.22637v1", "url": "http://arxiv.org/abs/2505.22637v1", "title": "Understanding (Un)Reliability of Steering Vectors in Language Models", "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction.", "authors": ["Joschka Braun", "Carsten Eickhoff", "David Krueger", "Seyed Ali Bahrainian", "Dmitrii Krasheninnikov"], "published_date": "2025-05-28", "title_zh": "理解語言模型中導引向量的(不)可靠性", "summary_zh": "導引向量是一種輕量級的方法，透過在推論時對激活值添加學習到的偏差來控制語言模型的行為。雖然導引展現出有希望的性能，但最近的研究表明，在某些情況下，它可能不可靠甚至產生反效果。本研究探討了提示類型和激活差異幾何結構對導引可靠性的影響。我們發現，實驗中使用的所有七種提示類型都產生了淨正向的導引效果，但在樣本之間表現出高度的方差，並且經常產生與期望相反的效果。沒有任何一種提示類型明顯優於其他類型，但不同提示類型產生的導引向量在方向上往往有所不同（以餘弦相似度衡量）。其次，我們發現訓練集中激活差異之間更高的餘弦相似度預測了更有效的導引。最後，我們觀察到正向和負向激活更好地分離的數據集更易於導引。我們的結果表明，當目標行為沒有由一個連貫的方向表示時，向量導引是不可靠的。", "applications": ["**改善客服機器人回應：** 透過導引向量，可以調整客服機器人的語氣，使其更友善、更專業，或更具同理心，提升客戶滿意度。然而，因為研究顯示其不穩定性，必須加入監控機制，確保調整後的語氣始終如一，避免出現語氣不協調，造成反效果。", "**個性化教學內容：** 在教育應用中，可以利用導引向量調整語言模型的輸出，使其適應不同學生的學習風格和需求。例如，對於理解力較弱的學生，可以導引模型生成更簡單、更具體的解釋。同樣，需要監控其一致性，避免隨機產生不準確或誤導性的內容。", "**控制AI生成的藝術作品風格：** 在生成藝術領域，導引向量可以用於調整AI生成的圖像或文字的風格，例如使其更抽象、更寫實，或更具特定流派的特徵。穩定性問題可能導致產出風格不一致，需要持續調整和修正。"], "pitch": "我們正在開發一種更穩定、更可預測的導引向量技術，以解決目前語言模型導引方法中存在的不穩定性問題。現有的導引向量技術雖然能輕量級地控制模型行為，但可靠性不足，在特定情況下甚至會產生反效果。我們的研究深入分析了影響導引向量效果的關鍵因素，例如提示類型和激活差異的幾何結構，並提出了改進方案，旨在建立一套更可靠的導引向量生成和應用流程。這將大幅提升基於大型語言模型的產品和服務的可用性和價值，例如，可以更精確地控制客服機器人的語氣，更有效地調整AI生成內容的風格，以及更安全地進行醫療診斷輔助。我們預計該技術將在客服、教育、內容生成、醫療等領域產生廣泛的應用價值，並為LLM的個性化定制和應用開闢新的商業模式。", "audio": "audios/2505.22637v1.mp3", "timestamp": "2025-05-29T12:26:57.778602"}
{"query": "Diffusion Model", "id": "2505.22643v1", "url": "http://arxiv.org/abs/2505.22643v1", "title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", "summary": "Leveraging recent diffusion models, LiDAR-based large-scale 3D scene\ngeneration has achieved great success. While recent voxel-based approaches can\ngenerate both geometric structures and semantic labels, existing range-view\nmethods are limited to producing unlabeled LiDAR scenes. Relying on pretrained\nsegmentation models to predict the semantic maps often results in suboptimal\ncross-modal consistency. To address this limitation while preserving the\nadvantages of range-view representations, such as computational efficiency and\nsimplified network design, we propose Spiral, a novel range-view LiDAR\ndiffusion model that simultaneously generates depth, reflectance images, and\nsemantic maps. Furthermore, we introduce novel semantic-aware metrics to\nevaluate the quality of the generated labeled range-view data. Experiments on\nthe SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves\nstate-of-the-art performance with the smallest parameter size, outperforming\ntwo-step methods that combine the generative and segmentation models.\nAdditionally, we validate that range images generated by Spiral can be\neffectively used for synthetic data augmentation in the downstream segmentation\ntraining, significantly reducing the labeling effort on LiDAR data.", "authors": ["Dekai Zhu", "Yixuan Hu", "Youquan Liu", "Dongyue Lu", "Lingdong Kong", "Slobodan Ilic"], "published_date": "2025-05-28", "title_zh": "SPIRAL：語義感知漸進式光達場景生成", "summary_zh": "近年來，基於擴散模型的光達大型3D場景生成取得了顯著進展。雖然基於體素的方法能夠同時生成幾何結構和語義標籤，但現有的距離視角方法僅限於生成未標記的光達場景。依賴預訓練分割模型來預測語義地圖通常會導致次優的跨模態一致性。為了克服這個限制，同時保留距離視角表示的優勢，例如計算效率和簡化的網絡設計，我們提出了一種新型距離視角光達擴散模型Spiral，它可以同時生成深度圖、反射圖像和語義地圖。此外，我們引入了新的語義感知指標來評估生成的標記距離視角數據的質量。在SemanticKITTI和nuScenes數據集上的實驗表明，Spiral以最小的參數量實現了最先進的性能，優於結合生成模型和分割模型的兩步方法。此外，我們驗證了Spiral生成的距離圖像可以有效地用於下游分割訓練中的合成數據增強，從而顯著減少光達數據的標註工作量。", "applications": ["自動駕駛模擬器：Spiral可以生成逼真的、帶語義標籤的光達數據，用於訓練和測試自動駕駛系統，無需耗費大量人力和時間進行實際道路測試和數據標註。", "機器人導航與環境理解：Spiral可以生成各種複雜環境的光達場景，幫助機器人更好地理解周圍環境，從而實現更精確的導航和避障。", "城市規劃與虛擬現實：Spiral可以生成帶有語義資訊的城市3D模型，為城市規劃提供數據支持，也可以用於創建更真實的虛擬現實體驗。"], "pitch": "Spiral解決了光達數據生成中語義標註的痛點，能夠高效生成帶有語義資訊的高質量光達數據。這對於自動駕駛、機器人、智慧城市等領域至關重要，因為這些領域都需要大量的、經過精確標註的光達數據來訓練模型。相較於傳統方法，Spiral不僅降低了標註成本，還提高了數據質量。我們計劃將Spiral技術整合到一個易於使用的雲平台，為客戶提供按需生成光達數據的服務。此外，我們還將與自動駕駛公司、機器人公司等建立合作夥伴關係，共同開發基於Spiral的定制化解決方案。市場前景廣闊，具備成為光達數據生成領域領先供應商的潛力。", "audio": "audios/2505.22643v1.mp3", "timestamp": "2025-05-29T12:27:14.323672"}
{"query": "AI", "id": "2505.22647v1", "url": "http://arxiv.org/abs/2505.22647v1", "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "summary": "Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach.", "authors": ["Zhe Kong", "Feng Gao", "Yong Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Xunliang Cai", "Guanying Chen", "Wenhan Luo"], "published_date": "2025-05-28", "title_zh": "讓他們說話：音訊驅動的多人對話影片生成", "summary_zh": "現有的語音驅動人物動畫技術在生成同步的臉部動作和吸引人的視覺效果影片方面取得了顯著進展。然而，這些技術主要集中在單個人物動畫上，且難以處理多個音訊輸入，導致音訊和人物之間的錯誤綁定問題。此外，它們在遵循指令的能力方面也存在局限性。本論文提出一個新的任務：多人對話影片生成，並介紹一個名為MultiTalk的新框架來解決多人生成過程中遇到的挑戰。具體來說，針對音訊注入，我們研究了幾種方案，並提出標籤旋轉位置嵌入（L-RoPE）方法來解決音訊和人物綁定問題。此外，在訓練過程中，我們發現部分參數訓練和多任務訓練對於保持基礎模型遵循指令的能力至關重要。MultiTalk在包括說話頭部、說話身體和多人數據集等多個數據集上取得了優於其他方法的性能，證明了我們方法的強大生成能力。", "applications": ["**線上會議增強：** 將單調的線上會議轉變為具有生動的人物動畫，讓參與者感覺更自然，更具互動性。", "**虛擬角色互動：** 為遊戲或教育應用程式創建逼真的虛擬角色，根據語音輸入進行即時對話和反應，提供更身臨其境的體驗。", "**AI輔助語言學習：** 製作多語種的動畫人物，通過模擬真實對話場景，幫助語言學習者練習口語和聽力，提高學習效率。"], "pitch": "MultiTalk 是一個突破性的技術，能夠根據音訊輸入生成多人對話影片，解決了目前市面上技術在處理多音源和遵循指令上的限制。這項技術的潛在商業價值巨大。設想一下，我們可以將它應用於線上會議平台，大幅提升用戶體驗，吸引更多付費用戶。在遊戲和教育領域，它可以創造更逼真、更具吸引力的虛擬角色，提升產品的競爭力。更進一步，我們可以將 MultiTalk 授權給影視公司，用於快速製作動畫內容，大幅降低製作成本。MultiTalk 擁有龐大的市場潛力，初期可以先鎖定線上會議及遊戲市場，進行產品驗證及市場推廣。後續可擴展至教育、娛樂等其他領域。團隊需要持續優化模型效能，並強化指令遵循能力，以滿足不同應用場景的需求。總而言之，MultiTalk 是一個極具投資價值的項目，有機會顛覆現有的影片生成產業。", "audio": "audios/2505.22647v1.mp3", "timestamp": "2025-05-29T13:26:11.221217"}
{"query": "Foundation Model", "id": "2505.22637v1", "url": "http://arxiv.org/abs/2505.22637v1", "title": "Understanding (Un)Reliability of Steering Vectors in Language Models", "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction.", "authors": ["Joschka Braun", "Carsten Eickhoff", "David Krueger", "Seyed Ali Bahrainian", "Dmitrii Krasheninnikov"], "published_date": "2025-05-28", "title_zh": "理解語言模型中轉向向量的（不）可靠性", "summary_zh": "轉向向量是一種輕量級的方法，透過在推理時將學習到的偏差添加到激活中，來控制語言模型的行為。儘管轉向表現出有希望的性能，但最近的研究表明，在某些情況下，它可能不可靠甚至適得其反。本論文研究了提示類型和激活差異的幾何形狀對轉向可靠性的影響。首先，我們發現我們實驗中使用的所有七種提示類型都產生了淨正向的轉向效果，但在樣本之間表現出很高的方差，並且經常產生與所需效果相反的效果。沒有哪種提示類型明顯優於其他類型，但來自不同提示類型的轉向向量在方向上經常不同（以餘弦相似度衡量）。其次，我們表明訓練集激活差異之間更高的餘弦相似度預測了更有效的轉向。最後，我們觀察到正負激活分離得更好的數據集更易於轉向。我們的結果表明，當目標行為沒有被一個連貫的方向表示時，向量轉向是不可靠的。", "applications": ["**個性化客服機器人：** 針對不同使用者個性或情緒，調整客服機器人的回應風格。例如，對較為嚴肅的客戶，客服機器人使用更正式的語言；對較為輕鬆的客戶，則可使用更幽默的口吻。", "**情境式寫作助手：** 幫助作家或內容創作者根據特定情境或目標受眾調整寫作風格。例如，針對兒童讀者的故事，寫作助手可以調整用詞和語法，使其更易於理解。", "**內容審核與風險控制：** 透過轉向向量，降低語言模型生成有害或不當內容的機率。例如，針對生成式AI應用，可以透過轉向向量避免生成歧視性或仇恨言論。"], "pitch": "我們解決了大型語言模型控制的一個關鍵痛點：一致性和可靠性。目前的轉向向量技術雖然有潛力，但效果不穩定。我們的研究揭示了影響轉向效果的關鍵因素，並為提升其可靠性提供了方向。這項技術可以應用於個性化AI助手、情境式內容生成、以及安全內容審核等領域，大幅提高LLM應用的商業價值。想像一下，一個能夠完美匹配用戶情緒和需求的AI客服，或是一個能夠自動產生符合品牌風格內容的營銷工具。我們的研究是實現這些願景的關鍵一步，具有巨大的市場潛力，尤其是在企業級AI解決方案和AI安全領域。", "audio": "audios/2505.22637v1.mp3", "timestamp": "2025-05-29T13:26:29.675281"}
{"query": "Diffusion Model", "id": "2505.22643v1", "url": "http://arxiv.org/abs/2505.22643v1", "title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", "summary": "Leveraging recent diffusion models, LiDAR-based large-scale 3D scene\ngeneration has achieved great success. While recent voxel-based approaches can\ngenerate both geometric structures and semantic labels, existing range-view\nmethods are limited to producing unlabeled LiDAR scenes. Relying on pretrained\nsegmentation models to predict the semantic maps often results in suboptimal\ncross-modal consistency. To address this limitation while preserving the\nadvantages of range-view representations, such as computational efficiency and\nsimplified network design, we propose Spiral, a novel range-view LiDAR\ndiffusion model that simultaneously generates depth, reflectance images, and\nsemantic maps. Furthermore, we introduce novel semantic-aware metrics to\nevaluate the quality of the generated labeled range-view data. Experiments on\nthe SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves\nstate-of-the-art performance with the smallest parameter size, outperforming\ntwo-step methods that combine the generative and segmentation models.\nAdditionally, we validate that range images generated by Spiral can be\neffectively used for synthetic data augmentation in the downstream segmentation\ntraining, significantly reducing the labeling effort on LiDAR data.", "authors": ["Dekai Zhu", "Yixuan Hu", "Youquan Liu", "Dongyue Lu", "Lingdong Kong", "Slobodan Ilic"], "published_date": "2025-05-28", "title_zh": "SPIRAL：語義感知的漸進式光達場景生成", "summary_zh": "本研究提出了一種名為 SPIRAL 的新型光達擴散模型，用於生成大規模 3D 場景。SPIRAL 的獨特之處在於它能夠同時生成深度、反射率圖像和語義地圖，克服了以往方法只能生成未標記光達場景的限制。SPIRAL 利用 range-view 表示法，在保證計算效率和簡化網路設計的同時，實現了語義感知的場景生成。實驗證明，SPIRAL 在 SemanticKITTI 和 nuScenes 數據集上表現出色，參數量最小，並優於兩階段生成和分割模型。更重要的是，SPIRAL 生成的範圍圖像可以有效地用於下游分割訓練的合成數據擴增，從而顯著減少光達數據的標記工作。", "applications": ["**自動駕駛模擬器增強：** 利用 SPIRAL 生成逼真的、帶有語義標籤的光達數據，用於訓練自動駕駛系統，提高其在各種複雜環境下的感知能力，降低實路測試的成本和風險。", "**智慧城市規劃與仿真：** 生成不同情境下的城市 3D 光達模型，用於模擬交通流量、人群行為等，幫助城市規劃者做出更明智的決策。", "**虛擬實境內容創建：** 生成逼真的光達環境，用於創建沉浸式的虛擬實境體驗，例如遊戲、培訓和旅遊。"], "pitch": "SPIRAL 解決了光達數據生成領域的一個關鍵痛點：缺乏能夠高效生成帶有語義標籤的大規模場景的方案。目前市場上高度依賴人工標注，成本高昂且耗時。SPIRAL 的創新之處在於其語義感知的生成能力，以及利用生成數據進行有效數據增強的潛力。這意味著更低的數據獲取成本、更高的模型準確性和更快的開發週期。我們可以將 SPIRAL 授權給自動駕駛公司、智慧城市建設商以及遊戲開發商，或將其集成到現有的模擬器和開發工具中。此外，我們還可以利用 SPIRAL 生成的數據訓練定制化的AI模型，提供高附加值的服務。初期市場聚焦於自動駕駛和智慧城市，預計在未來幾年內將呈現爆發式增長，SPIRAL 有望成為光達數據生成領域的領頭羊，具有極高的商業價值。", "audio": "audios/2505.22643v1.mp3", "timestamp": "2025-05-29T13:26:46.468513"}
{"query": "AI", "id": "2505.22647v1", "url": "http://arxiv.org/abs/2505.22647v1", "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "summary": "Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach.", "authors": ["Zhe Kong", "Feng Gao", "Yong Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Xunliang Cai", "Guanying Chen", "Wenhan Luo"], "published_date": "2025-05-28", "title_zh": "讓他們說話：音訊驅動的多人對話影片生成", "summary_zh": "現有的音訊驅動的人體動畫技術，例如說話頭像和全身生成，在生成同步的面部表情和吸引人的視覺效果方面取得了顯著進展。然而，這些方法主要集中在單人動畫，並且難以處理多重音訊輸入，面臨音訊和人物之間錯誤綁定的問題。此外，它們在遵循指令的能力方面也存在局限性。為了解決這些問題，本文提出了一項新任務：多人對話影片生成，並引入了一個新框架MultiTalk，以應對多人生成過程中的挑戰。具體來說，針對音訊注入，我們研究了幾種方案，並提出了標籤旋轉位置嵌入（L-RoPE）方法來解決音訊和人物綁定的問題。此外，在訓練過程中，我們觀察到部分參數訓練和多任務訓練對於保持基礎模型的指令遵循能力至關重要。MultiTalk在多個資料集上，包括說話頭像、說話身體和多人資料集，都比其他方法取得了優異的性能，證明了我們方法的強大生成能力。", "applications": ["**遠距會議與教學：** 根據與會者的語音，自動生成逼真的多人對話影片，提升遠距溝通的臨場感和互動性，減少因畫面單調造成的注意力分散。", "**虛擬助理與客服：** 根據使用者的語音指令和問題，創建具有多個虛擬人物的互動影片，提供更生動、個性化的客戶服務和產品介紹。", "**遊戲與娛樂：** 根據遊戲劇本或玩家語音，即時生成多個遊戲角色的對話影片，提升遊戲的沉浸感和故事敘述的豐富性。"], "pitch": "MultiTalk 技術解決了音訊驅動影片生成領域的關鍵痛點，即多人對話場景的真實模擬。透過創新的 L-RoPE 方法和訓練策略，MultiTalk 能準確地將語音與人物進行綁定，並保持模型的指令遵循能力，生成更逼真、更自然的對話影片。其商業價值體現在以下幾個方面：\n\n*   **提升遠距溝通效率：** 透過更生動的視覺呈現，改善遠距會議和教學的體驗，提高參與度和溝通效率。\n*   **賦能虛擬人物：** 讓虛擬助理和客服人員能夠進行更自然的互動，提升客戶滿意度和品牌形象。\n*   **革新遊戲體驗：** 為遊戲角色帶來更豐富的表情和對話，提升遊戲的沉浸感和娛樂性。\n\nMultiTalk 具備廣闊的市場應用前景，從企業協作、教育培訓到娛樂遊戲，都有巨大的商業潛力。早期投資將有機會搶佔市場先機，並在快速發展的 AI 影片生成領域建立領導地位。", "audio": "audios/2505.22647v1.mp3", "timestamp": "2025-05-29T14:13:37.526579"}
{"query": "Foundation Model", "id": "2505.22637v1", "url": "http://arxiv.org/abs/2505.22637v1", "title": "Understanding (Un)Reliability of Steering Vectors in Language Models", "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction.", "authors": ["Joschka Braun", "Carsten Eickhoff", "David Krueger", "Seyed Ali Bahrainian", "Dmitrii Krasheninnikov"], "published_date": "2025-05-28", "title_zh": "理解語言模型中導引向量的（不）可靠性", "summary_zh": "導引向量是一種輕量級的方法，透過在推論時將學習到的偏差加到激活值來控制語言模型的行為。儘管導引顯示出有潛力的性能，但最近的研究表明，在某些情況下它可能不可靠甚至適得其反。本文研究了提示類型和激活差異的幾何形狀對導引可靠性的影響。首先，我們發現我們實驗中使用的所有七種提示類型都產生了淨正面的導引效果，但在樣本之間表現出很高的方差，並且常常產生與期望相反的效果。沒有一種提示類型明顯優於其他類型，但不同提示類型產生的導引向量在方向上通常不同（以餘弦相似度衡量）。其次，我們表明訓練集激活差異之間較高的餘弦相似度可以預測更有效的導引。最後，我們觀察到正激活和負激活分離得更好的數據集更容易被導引。我們的結果表明，當目標行為沒有由連貫的方向表示時，向量導引是不可靠的。", "applications": ["**個性化客服機器人：** 根據不同客戶的情緒（例如積極或消極），利用導引向量調整機器人的回應風格，提升客戶滿意度。", "**內容創作風格調整：** 針對特定寫作風格（例如幽默、正式），使用導引向量控制語言模型的輸出，快速生成符合要求的文案。", "**安全內容過濾：** 使用導引向量避免生成有害或不適當的內容，例如仇恨言論或暴力描述，提升線上平台的安全性。"], "pitch": "我們正在開發一項技術，可以更可靠地控制語言模型的行為。現有的導引向量方法雖然有潛力，但穩定性不足。我們的研究揭示了影響導引效果的關鍵因素，並為改進導引技術提供了方向。想像一下，一個可以精準控制AI反應的平台，不僅能提升使用者體驗，更能降低內容風險。我們的技術將讓企業能更安全、更有效地利用AI力量，在客服、內容創作和安全過濾等領域創造巨大價值。這是一個潛力巨大的市場，我們正在打造下一代AI控制引擎。", "audio": "audios/2505.22637v1.mp3", "timestamp": "2025-05-29T14:13:52.694769"}
{"query": "Diffusion Model", "id": "2505.22643v1", "url": "http://arxiv.org/abs/2505.22643v1", "title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", "summary": "Leveraging recent diffusion models, LiDAR-based large-scale 3D scene\ngeneration has achieved great success. While recent voxel-based approaches can\ngenerate both geometric structures and semantic labels, existing range-view\nmethods are limited to producing unlabeled LiDAR scenes. Relying on pretrained\nsegmentation models to predict the semantic maps often results in suboptimal\ncross-modal consistency. To address this limitation while preserving the\nadvantages of range-view representations, such as computational efficiency and\nsimplified network design, we propose Spiral, a novel range-view LiDAR\ndiffusion model that simultaneously generates depth, reflectance images, and\nsemantic maps. Furthermore, we introduce novel semantic-aware metrics to\nevaluate the quality of the generated labeled range-view data. Experiments on\nthe SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves\nstate-of-the-art performance with the smallest parameter size, outperforming\ntwo-step methods that combine the generative and segmentation models.\nAdditionally, we validate that range images generated by Spiral can be\neffectively used for synthetic data augmentation in the downstream segmentation\ntraining, significantly reducing the labeling effort on LiDAR data.", "authors": ["Dekai Zhu", "Yixuan Hu", "Youquan Liu", "Dongyue Lu", "Lingdong Kong", "Slobodan Ilic"], "published_date": "2025-05-28", "title_zh": "螺旋：語義感知漸進式光達場景生成", "summary_zh": "本研究提出一種名為「螺旋」(Spiral) 的新型光達擴散模型，利用擴散模型技術，能夠同時生成深度、反射圖像和語義地圖，解決了現有基於範圍視圖 (range-view) 方法無法生成帶標籤光達場景的問題。與兩階段方法相比，螺旋在更小的參數規模下，表現出更優異的性能，並能有效用於下游分割任務的合成數據增強，顯著降低光達數據的標註成本。重點是同時生成深度、反射率和語義標籤，而且效率高。", "applications": ["**自動駕駛模擬訓練：** 產生逼真的光達場景，用於訓練自動駕駛系統，降低對真實世界數據的依賴，並加速模型開發。", "**機器人應用：** 為倉庫機器人、巡檢機器人等創建多樣化的虛擬環境，幫助它們學習和適應不同的工作場景。", "**遊戲開發：** 生成具有真實感的光達場景，用於打造更逼真的遊戲世界，提升玩家的沉浸式體驗。"], "pitch": "自動駕駛和機器人領域都面臨數據稀缺和標註成本高昂的挑戰。我們的「螺旋」技術提供了一種高效、經濟的方式來生成帶有語義信息的光達數據，可大幅降低標註成本，加速模型訓練。相比於現有的兩階段方案，螺旋在更小的模型規模下，效果更佳。這意味著更低的計算資源需求和更快的部署速度。我們可以將此技術授權給自動駕駛公司、機器人製造商、遊戲開發商，或者建立一個基於雲的光達數據生成平台，為使用者提供按需生成的服務。潛在市場規模巨大，投資回報可觀。", "audio": "audios/2505.22643v1.mp3", "timestamp": "2025-05-29T14:14:05.933863"}
{"query": "AI", "id": "2505.22647v1", "url": "http://arxiv.org/abs/2505.22647v1", "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "summary": "Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach.", "authors": ["Zhe Kong", "Feng Gao", "Yong Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Xunliang Cai", "Guanying Chen", "Wenhan Luo"], "published_date": "2025-05-28", "title_zh": "讓他們說話：音訊驅動的多人對話影片生成", "summary_zh": "現有的音訊驅動人物動畫技術，雖然在生成同步的臉部動作和高品質影片方面取得了顯著進展，但主要集中在單人動畫，難以處理多音訊輸入，並且會出現音訊和人物綁定錯誤的問題。此外，它們在遵循指令的能力上也有局限性。為了解決這些問題，我們提出了一項新任務：多人對話影片生成，並引入了一個新的框架 MultiTalk，以應對多人生成過程中的挑戰。具體來說，在音訊注入方面，我們研究了幾種方案，並提出了標籤旋轉位置嵌入（L-RoPE）方法來解決音訊和人物的綁定問題。此外，在訓練過程中，我們發現部分參數訓練和多任務訓練對於保持基礎模型遵循指令的能力至關重要。MultiTalk 在包括說話頭部、說話身體和多人數據集等多個數據集上都取得了優於其他方法的性能，證明了我們方法的強大生成能力。", "applications": ["線上會議與活動：將多個參與者的音訊輸入，即時生成逼真的多人對話影片，提升線上會議、遠距教學和虛擬活動的沉浸感和互動性。例如，模擬圓桌會議，讓遠端參與者感覺更像身處現場。", "語言學習與配音：根據不同角色的音訊，自動生成對應人物的說話動畫，輔助語言學習者練習口語，或為動畫片、遊戲等內容快速生成多語言配音影片。", "數位內容創作與社交媒體：創作者可以根據腳本或音訊，輕鬆生成多角色對話影片，用於短影音平台、社群媒體內容創作，節省製作成本和時間。"], "pitch": "MultiTalk 解決了現有音訊驅動人物動畫技術在多人對話影片生成方面的痛點，具有廣泛的應用前景。它降低了數位內容創作的門檻，提升了線上互動體驗，並且可以應用於教育、娛樂等多個領域。市場規模巨大，潛在商業價值包括：1. 授權技術給線上會議、遠距教學平台，收取授權費。2. 提供雲端服務，讓使用者可以根據音訊生成高品質的多人對話影片，收取訂閱費。3. 與內容創作平台合作，提供自動影片生成功能，提升平台競爭力。我們預計 MultiTalk 將成為未來互動式數位內容生成的關鍵技術，具有極高的投資回報。", "audio": "audios/2505.22647v1.mp3", "timestamp": "2025-05-29T15:14:37.048472"}
{"query": "Foundation Model", "id": "2505.22637v1", "url": "http://arxiv.org/abs/2505.22637v1", "title": "Understanding (Un)Reliability of Steering Vectors in Language Models", "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction.", "authors": ["Joschka Braun", "Carsten Eickhoff", "David Krueger", "Seyed Ali Bahrainian", "Dmitrii Krasheninnikov"], "published_date": "2025-05-28", "title_zh": "理解語言模型中導引向量的（不）可靠性", "summary_zh": "導引向量是一種輕量級方法，透過在推論時對激活值添加學習到的偏差來控制語言模型的行為。雖然導引展現了有前景的性能，但最近的研究表明，在某些情況下它可能不可靠甚至產生反效果。本研究探討了提示類型和激活差異的幾何形狀對導引可靠性的影響。 首先，我們發現我們實驗中使用的所有七種提示類型都產生了淨正面的導引效果，但在樣本之間表現出高度的方差，並且常常產生與期望相反的效果。 沒有哪種提示類型明顯優於其他類型，但不同提示類型產生的導引向量在方向上往往不同（透過餘弦相似度測量）。 其次，我們表明訓練集激活差異之間更高的餘弦相似度預示著更有效的導引。 最後，我們觀察到正激活和負激活分離得更好的資料集更易於導引。 我們的結果表明，當目標行為沒有用一個連貫的方向表示時，向量導引是不可靠的。", "applications": ["**客製化AI助理回應：** 針對不同使用者或情境，調整AI助理的回應風格（例如：專業、幽默、簡潔），但需要注意控制效果，避免產生意料之外或不適當的回應。", "**風險控制：** 避免AI生成仇恨言論或不當內容。 透過導引向量來抑制特定類型輸出，但需要小心評估，避免過度抑制或影響生成品質。", "**程式碼生成輔助：** 導引語言模型生成特定風格或功能的程式碼，例如：生成更易於理解的程式碼或專門用於特定框架的程式碼。 但需注意導引向量的可靠性，確保生成的程式碼符合預期，並進行充分測試。"], "pitch": "導引向量技術旨在輕量級地控制語言模型的行為，但其可靠性問題限制了大規模應用。我們的研究揭示了導引向量可靠性的關鍵因素，並提供了改善策略的方向。商業價值體現在：\n\n*   **降低AI產品風險：** 透過更精確地控制AI的輸出，降低產生有害內容的風險，提升企業聲譽與合規性。\n*   **提升客製化能力：** 實現更個性化的AI服務，提高使用者滿意度與產品黏性。\n*   **加速AI應用開發：** 降低fine-tuning成本，簡化AI模型的客製化流程，加速產品上市時間。\n\n我們正在開發相關的工具和技術，以提高導引向量的可靠性，並將其應用於各個領域。我們尋求投資者合作，共同打造安全、可靠、可控的AI產品。", "audio": "audios/2505.22637v1.mp3", "timestamp": "2025-05-29T15:15:02.656675"}
{"query": "Diffusion Model", "id": "2505.22643v1", "url": "http://arxiv.org/abs/2505.22643v1", "title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", "summary": "Leveraging recent diffusion models, LiDAR-based large-scale 3D scene\ngeneration has achieved great success. While recent voxel-based approaches can\ngenerate both geometric structures and semantic labels, existing range-view\nmethods are limited to producing unlabeled LiDAR scenes. Relying on pretrained\nsegmentation models to predict the semantic maps often results in suboptimal\ncross-modal consistency. To address this limitation while preserving the\nadvantages of range-view representations, such as computational efficiency and\nsimplified network design, we propose Spiral, a novel range-view LiDAR\ndiffusion model that simultaneously generates depth, reflectance images, and\nsemantic maps. Furthermore, we introduce novel semantic-aware metrics to\nevaluate the quality of the generated labeled range-view data. Experiments on\nthe SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves\nstate-of-the-art performance with the smallest parameter size, outperforming\ntwo-step methods that combine the generative and segmentation models.\nAdditionally, we validate that range images generated by Spiral can be\neffectively used for synthetic data augmentation in the downstream segmentation\ntraining, significantly reducing the labeling effort on LiDAR data.", "authors": ["Dekai Zhu", "Yixuan Hu", "Youquan Liu", "Dongyue Lu", "Lingdong Kong", "Slobodan Ilic"], "published_date": "2025-05-28", "title_zh": "SPIRAL: 語義感知漸進式光達場景生成", "summary_zh": "SPIRAL是一種新的光達場景生成模型，利用擴散模型，能同時生成深度、反射圖像和語義地圖，解決了現有方法只能生成無標籤光達場景的問題。它基於range-view表示，具有計算效率高和網路設計簡化的優點。通過語義感知指標評估生成資料的品質，實驗證明SPIRAL在SemanticKITTI和nuScenes資料集上達到最先進的性能，並且可以有效地用於下游分割訓練的合成資料增強，從而顯著減少光達資料的標籤工作。", "applications": ["自動駕駛模擬環境：創建更真實、多樣的模擬環境，用於訓練和測試自動駕駛系統，尤其是在極端天氣或罕見交通狀況下。", "智慧城市規劃：快速生成不同城市佈局的光達點雲資料，用於分析交通流量、建築物可視性、以及資源分配等問題，協助城市規劃者進行決策。", "機器人導航與地圖構建：協助機器人在未知的環境中快速建立地圖，並進行自主導航，特別是在室內或複雜的工業環境中。"], "pitch": "SPIRAL解決了光達場景生成中的一個關鍵痛點：生成帶有語義標籤的高品質光達資料。我們的模型不僅計算效率高，而且性能領先，能顯著降低資料標註成本，加速自動駕駛、智慧城市和機器人等領域的發展。透過提供逼真的合成光達資料，我們能幫助企業降低開發成本、提高產品可靠性。想像一下，自動駕駛公司不必耗費大量資金和時間去收集和標註真實世界的光達資料，而是可以使用SPIRAL生成無限量的訓練資料，更快地實現自動駕駛技術的商業化。這將是一個數十億美元的市場，而SPIRAL將成為這個市場的關鍵推動者。", "audio": "audios/2505.22643v1.mp3", "timestamp": "2025-05-29T15:15:19.445648"}
{"query": "AI", "id": "2505.22647v1", "url": "http://arxiv.org/abs/2505.22647v1", "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "summary": "Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach.", "authors": ["Zhe Kong", "Feng Gao", "Yong Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Xunliang Cai", "Guanying Chen", "Wenhan Luo"], "published_date": "2025-05-28", "title_zh": "讓他們說話：音訊驅動的多人對話影片生成", "summary_zh": "現有的音訊驅動人體動畫技術，例如口說人頭和口說全身生成，在產生同步的臉部動作和吸引人的視覺品質影片方面取得了顯著進展。然而，現有方法主要集中於單人動畫，並且難以處理多串流音訊輸入，面臨音訊與人物之間錯誤綁定的問題。此外，它們在遵循指令的能力方面也表現出局限性。為了解決這個問題，本文提出了一個新的任務：多人對話影片生成，並引入一個新的框架 MultiTalk，以應對多人生成過程中的挑戰。具體來說，對於音訊注入，我們研究了多種方案，並提出了標籤旋轉位置嵌入（L-RoPE）方法來解決音訊和人物綁定的問題。此外，在訓練過程中，我們觀察到部分參數訓練和多任務訓練對於保持基礎模型的指令遵循能力至關重要。MultiTalk 在多個數據集上（包括口說人頭、口說全身和多人數據集）都取得了優於其他方法的性能，證明了我們方法的強大生成能力。", "applications": ["**線上教育與協作：** 自動生成多人互動教學影片，老師和學生可以異地進行對話，提高線上學習的沉浸感和互動性，甚至能將過往的音訊課程轉為生動的影片。", "**遊戲與虛擬實境：** 創造更逼真的人物互動，遊戲角色可以根據玩家的語音即時反應，讓NPC之間的對話更加自然，提升遊戲體驗。虛擬實境中的多人對話也能更加流暢真實。", "**電影製作與內容創作：** 降低動畫製作成本，快速生成多人對話場景的動畫影片，例如為新聞報導配上虛擬人物的對話，或者快速製作短劇。", "**數位客服：** 根據用戶語音生成客服人員的對話影片，提供更人性化、更生動的客戶服務體驗。可支援多種語言和口音，大幅降低客服成本。"], "pitch": "MultiTalk解決了音訊驅動影片生成領域中長期存在的痛點，即多人對話場景的逼真生成。我們提出的L-RoPE方法有效解決了音訊和人物綁定的問題，確保生成影片中的人物能正確對應到語音。這項技術的應用前景廣泛，從教育、遊戲到影視製作，都能帶來革命性的變革。更重要的是，它能極大地降低成本，例如在動畫製作和線上教育領域。我們的商業模式可以是技術授權、API服務或垂直領域的解決方案，例如為教育機構定制互動式學習平台。MultiTalk的潛在市場規模巨大，預計未來幾年將呈現指數級增長，我們相信這是一項極具投資價值的技術。", "audio": "audios/2505.22647v1.mp3", "timestamp": "2025-05-29T16:16:42.743281"}
{"query": "Foundation Model", "id": "2505.22637v1", "url": "http://arxiv.org/abs/2505.22637v1", "title": "Understanding (Un)Reliability of Steering Vectors in Language Models", "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction.", "authors": ["Joschka Braun", "Carsten Eickhoff", "David Krueger", "Seyed Ali Bahrainian", "Dmitrii Krasheninnikov"], "published_date": "2025-05-28", "title_zh": "理解語言模型中轉向向量的（不）可靠性", "summary_zh": "這篇論文研究了如何透過「轉向向量」這個輕量級方法來控制語言模型的行為。轉向向量透過在推理時將學習到的偏差添加到激活中來實現控制。雖然轉向向量展現出良好的效能，但近期的研究表明，在某些情況下它可能不可靠，甚至會產生反效果。本研究探討了提示類型和激活差異的幾何形狀對轉向可靠性的影響。研究發現，不同提示類型雖都有正面效果，但樣本間差異大，且常產生反效果。不同提示類型產生的轉向向量方向也常不同。激活差異的餘弦相似度越高，轉向效果越好。正向和負向激活分離越好，模型越容易被控制。結果表明，當目標行為無法以一致的方向表示時，向量轉向就不可靠。", "applications": ["**內容審核：** 利用轉向向量引導語言模型更準確地識別和過濾有害或不適當的內容，例如仇恨言論或暴力內容。 即使模型在原始訓練中對此類內容存在偏差，也能有效校正。", "**情緒控制：** 調整語言模型生成文本的情緒傾向。例如，將負面情緒轉換為更積極或中性的情緒，可用於客戶服務機器人，提升互動體驗。", "**創意寫作輔助：** 協助作家產生特定風格或主題的文本。例如，引導模型產生特定作者的風格、特定時代的語氣，或是特定情節走向的文本。"], "pitch": "我們發現控制大型語言模型行為的轉向向量技術，目前存在可靠性問題。我們的研究深入分析了影響轉向向量效能的關鍵因素，並提出了提升可靠性的策略。這項技術的商業潛力巨大，可以應用於內容審核、情緒控制、創意寫作輔助等多個領域。想像一下，一個能自動過濾有害內容、根據客戶情緒調整回應、甚至能模仿不同作家風格的AI。我們的研究成果是實現這些願景的基石。透過我們的技術，企業能夠更有效地利用大型語言模型，提升產品價值，創造競爭優勢。我們正在尋找投資者，共同將這項技術商業化，引領AI應用的新時代。", "audio": "audios/2505.22637v1.mp3", "timestamp": "2025-05-29T16:16:58.551998"}
{"query": "Diffusion Model", "id": "2505.22643v1", "url": "http://arxiv.org/abs/2505.22643v1", "title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", "summary": "Leveraging recent diffusion models, LiDAR-based large-scale 3D scene\ngeneration has achieved great success. While recent voxel-based approaches can\ngenerate both geometric structures and semantic labels, existing range-view\nmethods are limited to producing unlabeled LiDAR scenes. Relying on pretrained\nsegmentation models to predict the semantic maps often results in suboptimal\ncross-modal consistency. To address this limitation while preserving the\nadvantages of range-view representations, such as computational efficiency and\nsimplified network design, we propose Spiral, a novel range-view LiDAR\ndiffusion model that simultaneously generates depth, reflectance images, and\nsemantic maps. Furthermore, we introduce novel semantic-aware metrics to\nevaluate the quality of the generated labeled range-view data. Experiments on\nthe SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves\nstate-of-the-art performance with the smallest parameter size, outperforming\ntwo-step methods that combine the generative and segmentation models.\nAdditionally, we validate that range images generated by Spiral can be\neffectively used for synthetic data augmentation in the downstream segmentation\ntraining, significantly reducing the labeling effort on LiDAR data.", "authors": ["Dekai Zhu", "Yixuan Hu", "Youquan Liu", "Dongyue Lu", "Lingdong Kong", "Slobodan Ilic"], "published_date": "2025-05-28", "title_zh": "SPIRAL：語義感知漸進式光達場景生成", "summary_zh": "現有的光達場景生成技術，雖然能生成幾何結構，但缺乏語義標籤。SPIRAL 是一個創新的 range-view 光達擴散模型，它能同時生成深度、反射率圖像和語義地圖。它在保持計算效率的同時，解決了跨模態一致性的問題。實驗證明，SPIRAL 以最小的參數量達到最先進的性能，且能有效用於下游分割訓練中的合成數據增強，顯著降低光達數據的標籤成本。", "applications": ["自動駕駛模擬訓練：生成逼真的城市環境和道路場景，用於測試和訓練自動駕駛算法，無需大量真實世界數據採集。", "機器人導航與規劃：創建各種室內或戶外環境的模擬場景，幫助機器人學習如何導航和規劃路徑，例如倉庫機器人或家庭服務機器人。", "虛擬現實（VR）與擴增實境（AR）：快速生成逼真的3D環境，用於VR/AR應用程序，例如建築設計預覽或遊戲場景創建。"], "pitch": "我們正在開發 SPIRAL，一種革命性的光達場景生成技術，能夠以更低的成本、更高的效率生成帶有語義標籤的高質量 3D 場景。 傳統光達數據標註成本高昂，嚴重阻礙了自動駕駛、機器人等領域的發展。 SPIRAL 通過 AI 生成合成數據，顯著降低了數據採集和標註的成本，加速 AI 模型的訓練和部署。 我們正在尋求種子輪投資，用於擴大研發團隊，建立雲端生成平台，並與自動駕駛、機器人等行業的領先企業建立戰略合作，共同推動 AI 技術的落地應用，搶佔市場先機。", "audio": "audios/2505.22643v1.mp3", "timestamp": "2025-05-29T16:17:10.954853"}
{"query": "AI", "id": "2505.22647v1", "url": "http://arxiv.org/abs/2505.22647v1", "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "summary": "Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach.", "authors": ["Zhe Kong", "Feng Gao", "Yong Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Xunliang Cai", "Guanying Chen", "Wenhan Luo"], "published_date": "2025-05-28", "title_zh": "讓他們說話：音訊驅動的多人對話影片生成", "summary_zh": "現有的音訊驅動人物動畫技術在生成同步臉部動作和視覺品質良好的影片方面取得了顯著進展，但它們主要集中於單個人物動畫，並且難以處理多音源輸入，面臨音訊和人物之間錯誤綁定的問題。此外，它們在遵循指令的能力方面也存在局限性。為了解決這個問題，我們提出了一個新的任務：多人對話影片生成，並引入了一個新的框架MultiTalk，以應對多人生成過程中的挑戰。具體來說，對於音訊注入，我們研究了幾種方案，並提出了標籤旋轉位置嵌入（L-RoPE）方法來解決音訊和人物綁定問題。此外，在訓練過程中，我們發現部分參數訓練和多任務訓練對於保持基礎模型遵循指令的能力至關重要。MultiTalk在多個數據集（包括talking head、talking body和多人數據集）上實現了優於其他方法的性能，證明了我們方法的強大生成能力。", "applications": ["**遠程協作與溝通：** 允許使用者根據多方語音輸入生成多人對話影片，模擬真實會議場景，提升遠程溝通的沉浸感和效率。例如，可以輸入多人會議的錄音，自動生成參與者頭像或全身像的對話動畫，讓觀看者彷彿身臨其境。", "**客製化內容創作：** 用於創建多人對話的動畫、遊戲或教育影片。例如，使用者可以輸入不同角色的配音，自動生成這些角色對話的影片，大幅簡化動畫製作流程。", "**虛擬人際互動：** 為社交平台、虛擬世界或元宇宙平台提供更自然的虛擬人物互動體驗。使用者可以輸入一段對話文字或語音，系統自動生成逼真的虛擬人物對話影片，增强互動的真实感和趣味性。"], "pitch": "MultiTalk框架解决了音訊驱动多人对话影片生成的关键难题，其潜力巨大。现有的talking head技术主要集中在单人应用，而MultiTalk突破了这一瓶颈，解锁了更广阔的市场空间。核心创新L-RoPE技术解决了音訊与人物绑定问题，保证了生成影片的准确性和可信度。想象一下，我们可以将多人会议的音频直接转化为逼真的视频会议记录，或者快速生成多人对话的动画内容。这不仅提高了效率，也降低了内容创作的门槛。尤其是在远程办公、在线教育和虚拟社交领域，MultiTalk拥有巨大的商业价值。我们预计，通过与现有视频会议平台、内容创作工具和社交媒体平台的整合，MultiTalk将迅速占领市场，成为下一代音讯驱动视频生成技术的领导者。团队拥有深厚的技术积累，加上清晰的市场定位，我们有信心将MultiTalk打造成为一个具有颠覆性创新的产品。", "audio": "audios/2505.22647v1.mp3", "timestamp": "2025-05-29T17:12:44.931809"}
{"query": "Foundation Model", "id": "2505.22637v1", "url": "http://arxiv.org/abs/2505.22637v1", "title": "Understanding (Un)Reliability of Steering Vectors in Language Models", "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction.", "authors": ["Joschka Braun", "Carsten Eickhoff", "David Krueger", "Seyed Ali Bahrainian", "Dmitrii Krasheninnikov"], "published_date": "2025-05-28", "title_zh": "理解語言模型中轉向向量的（不可）靠性", "summary_zh": "轉向向量是一種輕量級的方法，透過在推理時對激活值添加學習到的偏差來控制語言模型的行為。儘管轉向向量展現了有前景的性能，但最近的研究表明，在某些情況下它可能不可靠，甚至適得其反。本文研究了提示類型和激活差異的幾何形狀對轉向可靠性的影響。 研究發現，所有實驗中使用的七種提示類型都產生了淨正向的轉向效果，但在樣本之間表現出高度差異，並且通常會產生與預期相反的效果。 沒有一種提示類型明顯優於其他類型，然而，不同提示類型產生的轉向向量在方向上經常不同（如餘弦相似度所衡量）。 此外，研究表明，訓練集激活差異之間較高的餘弦相似度預示著更有效的轉向。 最後，觀察到正向和負向激活分離得更好的數據集更易於轉向。 這些結果表明，當目標行為沒有以連貫的方向表示時，向量轉向是不可靠的。", "applications": ["情緒語氣調控：根據用戶需求，控制AI生成內容的情緒，例如將客服機器人從中立語氣轉為更具同情心或鼓勵性的語氣。", "客製化內容生成：針對不同受眾，客製化生成特定風格或領域的內容，例如針對兒童生成更易懂的故事，針對專家生成更專業的報告。", "避免不當言論：透過轉向向量引導AI避免生成歧視、仇恨或不雅內容，提升AI內容的安全性和可靠性。"], "pitch": "我們正在解決生成式AI領域中一個關鍵問題：如何更可靠地控制AI模型的行為。目前廣泛使用的「轉向向量」技術雖然前景看好，但其穩定性與可預測性仍有待提升。我們的研究深入剖析了影響轉向向量可靠性的因素，例如提示類型和數據集的特性。這項研究不僅能顯著提升現有AI應用的效能，例如情緒調控、客製化內容生成與避免不當言論，也能為AI安全領域帶來突破。我們相信，透過我們的技術，能降低AI生成內容的風險，提高AI的可用性與商業價值，為企業創造更安全、更高效的AI解決方案。", "audio": "audios/2505.22637v1.mp3", "timestamp": "2025-05-29T17:12:59.246462"}
{"query": "Diffusion Model", "id": "2505.22643v1", "url": "http://arxiv.org/abs/2505.22643v1", "title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", "summary": "Leveraging recent diffusion models, LiDAR-based large-scale 3D scene\ngeneration has achieved great success. While recent voxel-based approaches can\ngenerate both geometric structures and semantic labels, existing range-view\nmethods are limited to producing unlabeled LiDAR scenes. Relying on pretrained\nsegmentation models to predict the semantic maps often results in suboptimal\ncross-modal consistency. To address this limitation while preserving the\nadvantages of range-view representations, such as computational efficiency and\nsimplified network design, we propose Spiral, a novel range-view LiDAR\ndiffusion model that simultaneously generates depth, reflectance images, and\nsemantic maps. Furthermore, we introduce novel semantic-aware metrics to\nevaluate the quality of the generated labeled range-view data. Experiments on\nthe SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves\nstate-of-the-art performance with the smallest parameter size, outperforming\ntwo-step methods that combine the generative and segmentation models.\nAdditionally, we validate that range images generated by Spiral can be\neffectively used for synthetic data augmentation in the downstream segmentation\ntraining, significantly reducing the labeling effort on LiDAR data.", "authors": ["Dekai Zhu", "Yixuan Hu", "Youquan Liu", "Dongyue Lu", "Lingdong Kong", "Slobodan Ilic"], "published_date": "2025-05-28", "title_zh": "SPIRAL：語義感知漸進式 LiDAR 場景生成", "summary_zh": "本文提出了一個名為 SPIRAL 的新型 LiDAR 場景生成模型，它使用 diffusion 模型，並專注於 range-view 視角。相較於現有方法，SPIRAL 能同時生成深度、反射率圖像和語義地圖，克服了過去 range-view 方法無法生成帶標籤 LiDAR 場景的限制。SPIRAL 在保持計算效率和簡化網路設計的優勢下，透過整合語義信息，提升了生成結果的品質，並在 SemanticKITTI 和 nuScenes 數據集上取得了最佳效能。更重要的是，SPIRAL 生成的圖像能有效用於下游分割訓練的合成數據擴增，大幅降低 LiDAR 數據的標註成本。", "applications": ["**自動駕駛模擬器強化：** 使用 SPIRAL 生成更多樣化、逼真的 LiDAR 場景，用於訓練和驗證自動駕駛系統，提升其在各種環境下的感知能力和安全性。", "**機器人導航與地圖構建：** 生成不同環境下的 LiDAR 數據，用於訓練機器人導航系統，使其能適應各種室內和室外環境，並更有效率地構建地圖。", "**智慧城市建模與視覺化：** 生成帶語義標籤的 LiDAR 數據，用於建立更精確的城市 3D 模型，方便城市規劃、交通管理和災害應對等應用。"], "pitch": "SPIRAL 是一個突破性的 LiDAR 場景生成模型，透過其卓越的語義感知能力和高效的 range-view 架構，我們能以極低的成本生成大量高品質、帶語義標籤的 LiDAR 數據。這為自動駕駛、機器人、智慧城市等領域帶來了巨大的商業潛力。想像一下，透過 SPIRAL，我們可以大幅降低自動駕駛公司的數據標註成本，加速其產品開發進程；我們也可以為機器人公司提供無窮無盡的訓練數據，使其機器人能在任何環境下安全有效地運作。更重要的是，SPIRAL 的技術還可以應用於國防、安保等領域，生成逼真的模擬環境，提升相關系統的性能。我們相信，SPIRAL 不僅能帶來顯著的經濟效益，更能推動整個行業的發展。", "audio": "audios/2505.22643v1.mp3", "timestamp": "2025-05-29T17:13:16.324922"}
{"query": "AI", "id": "2505.22647v1", "url": "http://arxiv.org/abs/2505.22647v1", "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "summary": "Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach.", "authors": ["Zhe Kong", "Feng Gao", "Yong Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Xunliang Cai", "Guanying Chen", "Wenhan Luo"], "published_date": "2025-05-28", "title_zh": "讓他們說話：音訊驅動的多人對話影片生成", "summary_zh": "現有的音訊驅動人物動畫技術，雖然在生成同步的臉部表情和高品質影片方面取得了顯著進展，但大多集中在單人動畫，難以處理多音訊流輸入，經常發生音訊和人物綁定錯誤的問題。此外，這些方法在指令遵循能力上也存在局限性。為了解決這些問題，我們提出了一項新的任務：多人對話影片生成，並推出一個名為MultiTalk的新框架，以應對多人生成中的挑戰。具體來說，在音訊注入方面，我們研究了幾種方案，並提出了標籤旋轉位置嵌入（L-RoPE）方法來解決音訊和人物的綁定問題。此外，在訓練過程中，我們發現部分參數訓練和多任務訓練對於保持基礎模型的指令遵循能力至關重要。MultiTalk在多個數據集上，包括說話頭像、說話身體和多人數據集上，都比其他方法表現出更優越的性能，展示了我們方法的強大生成能力。", "applications": ["線上會議與教學：將會議錄音轉化為生動的虛擬人物對話影片，提升參與感和互動性。", "遊戲角色互動：根據遊戲玩家的語音指令，生成不同角色間自然流暢的對話動畫，增加遊戲沉浸感。", "影視製作輔助：快速生成草稿級別的對話場景動畫，輔助編劇和導演進行視覺化預覽和故事板製作。"], "pitch": "MultiTalk解決了音訊驅動影片生成領域的關鍵痛點：多人對話的真實性和互動性。目前市場上的相關技術要么只能處理單人，要么多人互動效果不佳。MultiTalk的L-RoPE綁定技術和訓練策略，能顯著提升多人對話影片的品質和真實感。其潛在商業價值巨大，可以應用於線上教育、遊戲開發、影視製作、社交媒體等多個領域。例如，可以為線上教育平台提供更生動的課程內容，幫助遊戲開發者降低角色動畫製作成本，或者讓社交媒體用戶創造更具吸引力的內容。團隊已在多個數據集上驗證了MultiTalk的優越性能，並擁有清晰的技術路線圖和可行的商業化策略，是一個極具投資價值的項目。", "audio": "audios/2505.22647v1.mp3", "timestamp": "2025-05-29T18:18:49.255173"}
{"query": "Foundation Model", "id": "2505.22637v1", "url": "http://arxiv.org/abs/2505.22637v1", "title": "Understanding (Un)Reliability of Steering Vectors in Language Models", "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction.", "authors": ["Joschka Braun", "Carsten Eickhoff", "David Krueger", "Seyed Ali Bahrainian", "Dmitrii Krasheninnikov"], "published_date": "2025-05-28", "title_zh": "理解語言模型中轉向向量的(不)可靠性", "summary_zh": "轉向向量是一種輕量級的方法，通過在推論時將學習到的偏差添加到激活值中，來控制語言模型的行為。雖然轉向表現出令人鼓舞的性能，但最近的研究表明，在某些情況下它可能不可靠，甚至會產生反作用。本文研究了提示類型和激活值差異的幾何形狀對轉向可靠性的影響。首先，我們發現實驗中使用的所有七種提示類型都產生了淨正面的轉向效果，但在樣本間表現出高度差異，並且常常產生與期望相反的效果。沒有任何一種提示類型明顯優於其他提示類型，但不同提示類型產生的轉向向量在方向上（通過餘弦相似度衡量）通常不同。其次，我們表明，訓練集激活值差異之間更高的餘弦相似度預測了更有效的轉向。最後，我們觀察到，正負激活值分離更好的數據集更易於轉向。我們的結果表明，當目標行為沒有由一個連貫的方向表示時，向量轉向是不可靠的。", "applications": ["**客製化客服機器人：** 針對不同客戶情緒（正面/負面/中立）進行轉向，讓機器人更有效地回應不同情境，例如安撫生氣的客戶。", "**內容過濾與偏見移除：** 使用轉向向量降低語言模型產生有害或帶有偏見內容的機率，例如減少種族歧視或性別歧視的言論。", "**程式碼生成導向：** 透過轉向向量，引導程式碼生成模型產生特定風格或功能的程式碼，例如生成更易讀或更高效的程式碼。"], "pitch": "我們正在解決人工智慧模型行為控制的關鍵問題。傳統方法複雜且計算成本高昂，而我們的研究聚焦於轉向向量，一種輕量級且高效的控制技術。雖然轉向向量有潛力，但目前的可靠性是個瓶頸。我們的研究揭示了影響轉向向量可靠性的關鍵因素，並為提高其穩定性和可預測性奠定了基礎。想像一下，您可以精準控制AI模型的輸出，無論是調整客服機器人的語氣，還是確保生成無偏見的內容。我們提供的解決方案不僅節省了計算資源，更開創了AI客製化的新紀元。我們正在尋找資金以進一步開發更穩健的轉向技術，並將其商業化，目標是為企業和開發者提供一個強大的AI行為控制平台，最終打造更安全、更可靠、更具價值的人工智慧應用。", "audio": "audios/2505.22637v1.mp3", "timestamp": "2025-05-29T18:19:04.949942"}
{"query": "Diffusion Model", "id": "2505.22643v1", "url": "http://arxiv.org/abs/2505.22643v1", "title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", "summary": "Leveraging recent diffusion models, LiDAR-based large-scale 3D scene\ngeneration has achieved great success. While recent voxel-based approaches can\ngenerate both geometric structures and semantic labels, existing range-view\nmethods are limited to producing unlabeled LiDAR scenes. Relying on pretrained\nsegmentation models to predict the semantic maps often results in suboptimal\ncross-modal consistency. To address this limitation while preserving the\nadvantages of range-view representations, such as computational efficiency and\nsimplified network design, we propose Spiral, a novel range-view LiDAR\ndiffusion model that simultaneously generates depth, reflectance images, and\nsemantic maps. Furthermore, we introduce novel semantic-aware metrics to\nevaluate the quality of the generated labeled range-view data. Experiments on\nthe SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves\nstate-of-the-art performance with the smallest parameter size, outperforming\ntwo-step methods that combine the generative and segmentation models.\nAdditionally, we validate that range images generated by Spiral can be\neffectively used for synthetic data augmentation in the downstream segmentation\ntraining, significantly reducing the labeling effort on LiDAR data.", "authors": ["Dekai Zhu", "Yixuan Hu", "Youquan Liu", "Dongyue Lu", "Lingdong Kong", "Slobodan Ilic"], "published_date": "2025-05-28", "title_zh": "SPIRAL：語義感知的漸進式 LiDAR 場景生成", "summary_zh": "本論文提出一種名為 SPIRAL 的新型 LiDAR 擴散模型，它能夠同時生成深度、反射率圖像和語義地圖。SPIRAL 採用 range-view 表示法，兼顧了計算效率和簡化網絡設計的優點，並通過引入語義感知的指標來評估生成數據的質量。實驗證明，SPIRAL 在 SemanticKITTI 和 nuScenes 數據集上取得了最先進的性能，並且參數量最小，同時也能有效地用於下游分割訓練中的合成數據增強，從而顯著減少 LiDAR 數據的標註工作。", "applications": ["**自動駕駛模擬器：** 使用 SPIRAL 生成逼真的 LiDAR 場景，可以有效訓練自動駕駛系統，降低對真實世界數據的依賴，並加速開發和測試流程。", "**機器人導航與避障：** 在倉庫、工廠等複雜環境中，利用 SPIRAL 生成的場景數據訓練機器人，提升其導航和避障能力，尤其是在缺乏足夠真實數據的情況下。", "**城市規劃與智慧城市：** 通過 SPIRAL 生成不同情境下的城市 LiDAR 數據，可以輔助城市規劃人員進行模擬和評估，例如道路改造、建築設計對 LiDAR 感知的影響。"], "pitch": "我們正在開發 SPIRAL，一款基於擴散模型的 LiDAR 場景生成技術，它能以低成本、高效率的方式創造逼真的、帶語義標籤的 LiDAR 數據。相較於現有方案，SPIRAL 速度更快，參數更少，生成質量更高。這項技術在自動駕駛、機器人和智慧城市等領域具有廣闊的應用前景。其商業價值體現在：1) 顯著降低數據標註成本，加速 AI 模型的開發週期；2) 提供安全可靠的模擬環境，助力自動駕駛等高風險領域的產品驗證；3) 為缺乏數據的應用場景提供解決方案。我們正在尋求投資，以加速 SPIRAL 的商業化進程，將其打造成 LiDAR 感知領域的關鍵基礎設施，搶占市場先機。", "audio": "audios/2505.22643v1.mp3", "timestamp": "2025-05-29T18:19:18.892469"}
{"query": "AI", "id": "2505.22647v1", "url": "http://arxiv.org/abs/2505.22647v1", "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "summary": "Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach.", "authors": ["Zhe Kong", "Feng Gao", "Yong Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Xunliang Cai", "Guanying Chen", "Wenhan Luo"], "published_date": "2025-05-28", "title_zh": "讓他們說話：音訊驅動的多人對話影片生成", "summary_zh": "現有的音訊驅動人體動畫技術，在生成同步的臉部動作和高品質視覺影片方面取得了顯著進展。然而，這些方法主要集中在單一人體動畫上，並且難以處理多音源輸入，導致音訊和人物之間的關聯錯誤。此外，它們在遵循指令的能力上也存在局限性。為了應對這些問題，我們提出了一項新任務：多人對話影片生成，並引入了一個名為 MultiTalk 的新框架，以解決多人生成過程中的挑戰。具體來說，在音訊注入方面，我們研究了幾種方案，並提出了標籤旋轉位置嵌入（L-RoPE）方法，以解決音訊和人物綁定問題。此外，在訓練過程中，我們發現部分參數訓練和多任務訓練對於保持基礎模型的指令遵循能力至關重要。MultiTalk 在多個數據集上（包括說話頭部、說話身體和多人數據集）都取得了優於其他方法的性能，證明了我們方法的強大生成能力。", "applications": ["線上教育：可以將一段文字或音訊腳本轉化為多位虛擬講師進行互動教學，提升學習體驗。", "遠距協作：模擬真實會議場景，根據語音即時生成參與者的頭像和肢體動作，增強溝通的臨場感。", "影視娛樂：自動生成多角色對話場景，大幅降低動畫製作成本，甚至可以讓歷史人物『復活』對談。"], "pitch": "MultiTalk 解決了現有音訊驅動人體動畫技術在多人對話場景下的瓶頸，能根據多個音源生成逼真的多人對話影片。這項技術具有巨大的商業潛力，尤其是在教育、娛樂和協作領域。試想一下，能夠低成本地創建互動式教學影片、生動的線上會議體驗，以及逼真的虛擬角色對話場景，其市場需求將是巨大的。MultiTalk 的關鍵創新點 L-RoPE 方法有效解決了音訊與人物的關聯問題，使其在競爭中脫穎而出。我們相信，MultiTalk 有潛力成為音訊驅動人體動畫領域的領先技術，並在多個行業產生顛覆性影響。因此，我們正在尋求投資，以加速產品開發和市場推廣，將 MultiTalk 打造成一個廣泛應用的平台。", "audio": "audios/2505.22647v1.mp3", "timestamp": "2025-05-29T19:10:26.654097"}
{"query": "Foundation Model", "id": "2505.22637v1", "url": "http://arxiv.org/abs/2505.22637v1", "title": "Understanding (Un)Reliability of Steering Vectors in Language Models", "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction.", "authors": ["Joschka Braun", "Carsten Eickhoff", "David Krueger", "Seyed Ali Bahrainian", "Dmitrii Krasheninnikov"], "published_date": "2025-05-28", "title_zh": "理解語言模型中轉向向量的(不)可靠性", "summary_zh": "轉向向量是一種輕量級方法，透過在推論時將學習到的偏差添加到激活層來控制語言模型的行為。雖然轉向展現了有前景的性能，但近期的研究表明，在某些情況下它可能不可靠，甚至會產生反作用。本論文研究了提示類型和激活差異的幾何形狀對轉向可靠性的影響。首先，我們發現實驗中使用的所有七種提示類型都產生了淨正向的轉向效果，但在樣本之間表現出高度差異，並且經常產生與所需效果相反的效果。沒有任何一種提示類型明顯優於其他類型，但不同提示類型產生的轉向向量通常在方向上有所不同（以餘弦相似度衡量）。其次，我們證明了訓練集激活差異之間較高的餘弦相似性可以預測更有效的轉向。最後，我們觀察到正向和負向激活分離得更好的數據集更易於轉向。我們的結果表明，當目標行為沒有由連貫的方向表示時，向量轉向是不可靠的。", "applications": ["**個性化學習體驗：** 透過轉向向量調整語言模型的回應風格，以適應不同學生的學習風格和知識水平，例如將模型調整為更簡潔、更詳細或更具鼓勵性的風格。", "**內容審核與安全：** 使用轉向向量避免語言模型產生有害、偏見或不當的內容。例如，可以轉向模型，使其更傾向於生成中性、客觀的資訊，並避免生成涉及歧視或仇恨言論的內容。", "**創意寫作輔助：** 透過轉向向量引導語言模型產生特定風格或主題的文章、故事或詩歌。 例如，可以引導模型生成更浪漫、懸疑或科幻風格的文本。"], "pitch": "我們正在開發一種革命性的方法，透過微調語言模型的『轉向向量』，實現對其行為的精準控制。雖然現有技術存在不穩定性，但我們的研究揭示了關鍵影響因素，使我們能大幅提升轉向的可靠性和效果。這將帶來巨大的商業價值，從個性化教育產品，到內容審核服務，再到創意寫作工具，都將因為我們的技術而變得更加強大和可控。 試想一下，一個AI輔導系統，能根據學生的學習風格自動調整教學方式；或者一個內容審核工具，能以極高的精準度過濾有害信息。這不僅能提高效率，更能創造前所未有的用戶體驗。 我們的團隊擁有深厚的AI背景和對自然語言理解的深刻理解，我們相信透過我們的技術，我們能夠unlock語言模型的真正潛力，並在這個快速發展的市場中取得領先地位。", "audio": "audios/2505.22637v1.mp3", "timestamp": "2025-05-29T19:10:46.926035"}
{"query": "Diffusion Model", "id": "2505.22643v1", "url": "http://arxiv.org/abs/2505.22643v1", "title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", "summary": "Leveraging recent diffusion models, LiDAR-based large-scale 3D scene\ngeneration has achieved great success. While recent voxel-based approaches can\ngenerate both geometric structures and semantic labels, existing range-view\nmethods are limited to producing unlabeled LiDAR scenes. Relying on pretrained\nsegmentation models to predict the semantic maps often results in suboptimal\ncross-modal consistency. To address this limitation while preserving the\nadvantages of range-view representations, such as computational efficiency and\nsimplified network design, we propose Spiral, a novel range-view LiDAR\ndiffusion model that simultaneously generates depth, reflectance images, and\nsemantic maps. Furthermore, we introduce novel semantic-aware metrics to\nevaluate the quality of the generated labeled range-view data. Experiments on\nthe SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves\nstate-of-the-art performance with the smallest parameter size, outperforming\ntwo-step methods that combine the generative and segmentation models.\nAdditionally, we validate that range images generated by Spiral can be\neffectively used for synthetic data augmentation in the downstream segmentation\ntraining, significantly reducing the labeling effort on LiDAR data.", "authors": ["Dekai Zhu", "Yixuan Hu", "Youquan Liu", "Dongyue Lu", "Lingdong Kong", "Slobodan Ilic"], "published_date": "2025-05-28", "title_zh": "螺旋：語義感知的漸進式光達場景生成", "summary_zh": "本研究提出一種名為「螺旋」(Spiral) 的新型光達場景生成模型，利用擴散模型技術，能夠同時生成深度、反射率影像，以及語義地圖。相較於現有方法，螺旋模型在保持計算效率和簡化網路設計優勢的同時，克服了無法直接生成帶有語義標籤光達場景的限制。實驗證明，螺旋模型在SemanticKITTI和nuScenes數據集上表現出色，參數量最小，並且生成的資料可用於下游分割任務的合成資料增強，有效減少光達數據的標籤工作。", "applications": ["自動駕駛模擬環境：生成逼真的交通場景，用於訓練和測試自動駕駛演算法。", "機器人導航：創建多樣化的室內/室外環境，幫助機器人學習導航和物體識別。", "虛擬實境(VR)遊戲開發：快速生成具有語義資訊的3D地圖，加速VR遊戲場景的搭建。"], "pitch": "螺旋模型解決了光達場景生成的關鍵痛點：效率與語義資訊的平衡。它利用range-view表示，保持了計算效率，同時直接生成帶有語義標籤的數據，克服了傳統方法需要依賴外部分割模型導致的一致性問題。其商業價值體現在幾個方面：一，大幅降低自動駕駛、機器人等領域的數據採集和標註成本，尤其在長尾場景數據方面；二，加速AI模型的訓練和開發，提高模型在複雜環境下的魯棒性；三，提供更真實、更豐富的模擬環境，助力自動駕駛等技術的商業化落地。基於螺旋模型技術，我們能夠為自動駕駛、機器人、VR等行業提供更高效、更低成本的3D場景生成解決方案，具有巨大的市場潛力。", "audio": "audios/2505.22643v1.mp3", "timestamp": "2025-05-29T19:11:02.242474"}
{"query": "AI", "id": "2505.22647v1", "url": "http://arxiv.org/abs/2505.22647v1", "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "summary": "Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach.", "authors": ["Zhe Kong", "Feng Gao", "Yong Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Xunliang Cai", "Guanying Chen", "Wenhan Luo"], "published_date": "2025-05-28", "title_zh": "讓他們說話：音訊驅動的多人對話影片生成", "summary_zh": "現有的音訊驅動人物動畫技術在生成同步臉部動作和高品質影片方面取得了顯著進展，但主要集中在單個人物動畫，難以處理多音訊輸入，導致音訊與人物之間的錯誤綁定。此外，它們在遵循指令的能力上也存在限制。本研究提出了一個新的任務：多人對話影片生成，並引入一個名為MultiTalk的新框架來應對多人生成中的挑戰。具體來說，針對音訊注入，我們研究了多種方案，並提出Label Rotary Position Embedding (L-RoPE)方法來解決音訊和人物綁定問題。此外，在訓練過程中，我們發現部分參數訓練和多任務訓練對於保持基礎模型遵循指令的能力至關重要。 MultiTalk在多個數據集上，包括說話頭像、說話身體和多人數據集上，都比其他方法表現出更優越的性能，證明了我們方法的強大生成能力。", "applications": ["**遠距教學平台:** 根據多名學生的問題音訊，自動生成包含老師和學生之間互動的影片，提升線上學習的沉浸感和互動性。", "**多人視訊會議增強:** 在多人視訊會議中，根據每個人的聲音，生成更自然的臉部表情和身體動作，減少視訊延遲和畫面僵硬感，提升溝通效率。", "**虛擬偶像/遊戲角色對話生成:**  為虛擬偶像或遊戲角色創建更生動逼真的對話動畫，只需提供配音，就能自動生成角色之間的互動和情緒表達。"], "pitch": "MultiTalk 正在革新音訊驅動的動畫市場。 現有技術僅限於單人動畫，而 MultiTalk 能夠根據多個音訊輸入生成逼真的多人對話影片，解決了音訊與人物綁定、指令遵循等關鍵問題。 想像一下：遠距教學平台可以自動生成逼真的師生互動，大幅提升學生參與度； 遊戲開發者可以快速創建更具沉浸感的角色對話，節省大量動畫製作成本； 企業可以利用 AI 生成會議記錄，將枯燥的錄音轉化為引人入勝的視訊摘要。  MultiTalk 的應用潛力巨大，涵蓋教育、娛樂、企業協作等多個領域。 我們的團隊具備深厚的技術積累，並已在多個基準測試中證明了技術領先性。 我們正在尋找投資者，共同將 MultiTalk 打造成市場領導者，引領下一代互動式視訊體驗。", "audio": "audios/2505.22647v1.mp3", "timestamp": "2025-05-29T20:15:38.426044"}
{"query": "Foundation Model", "id": "2505.22637v1", "url": "http://arxiv.org/abs/2505.22637v1", "title": "Understanding (Un)Reliability of Steering Vectors in Language Models", "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction.", "authors": ["Joschka Braun", "Carsten Eickhoff", "David Krueger", "Seyed Ali Bahrainian", "Dmitrii Krasheninnikov"], "published_date": "2025-05-28", "title_zh": "理解語言模型中轉向向量的（不）可靠性", "summary_zh": "轉向向量是一種輕量化的方法，通過在推理時向激活值添加學習到的偏差來控制語言模型的行為。雖然轉向顯示出有希望的性能，但最近的研究表明，在某些情況下它可能不可靠甚至會產生反作用。本研究探討了提示類型和激活差異幾何結構對轉向可靠性的影響。首先，我們發現實驗中使用的所有七種提示類型都產生了淨正面的轉向效果，但在樣本之間表現出高度的差異，並且常常產生與期望相反的效果。沒有一種提示類型明顯優於其他類型，但來自不同提示類型的轉向向量在方向上通常存在差異（通過餘弦相似度測量）。其次，我們表明訓練集激活差異之間較高的餘弦相似度預測了更有效的轉向。最後，我們觀察到正負激活被更好分離的數據集更易於轉向。我們的結果表明，當目標行為沒有由連貫的方向表示時，向量轉向是不可靠的。", "applications": ["**個性化學習輔導：** 根據學生的學習風格和理解程度，利用轉向向量調整語言模型的回答方式，使其更易於學生理解和吸收。例如，對於理解能力較弱的學生，可以調整模型使其提供更簡潔、更直觀的解釋。", "**情感客服機器人：** 通過轉向向量控制客服機器人的情感表達，例如，在處理投訴時，可以調整模型使其更具同理心和耐心，以提高客戶滿意度。", "**內容審核：** 利用轉向向量抑制語言模型生成有害內容的傾向。例如，可以訓練轉向向量來減少模型生成仇恨言論或偏見性內容的可能性。"], "pitch": "各位投資人，我們正在解決大型語言模型的可控性問題。目前的語言模型雖然強大，但行為難以預測和精確控制，這限制了它們在許多敏感領域的應用。我們的研究揭示了現有轉向向量技術的局限性，並為開發更可靠、更可控的語言模型奠定了基礎。我們計劃基於這些發現，開發出新的算法和工具，使企業能夠更加安全、高效地部署語言模型，例如在金融、醫療和教育等關鍵領域。想象一下，一個可以完全避免生成錯誤資訊的醫療診斷模型，或者一個能夠完美客服、降低投訴的智能客戶服務系統。這個市場潛力巨大，我們相信我們的技術將成為下一代語言模型的核心組件。我們需要您的投資，加速我們的研發，搶佔市場先機！", "audio": "audios/2505.22637v1.mp3", "timestamp": "2025-05-29T20:15:56.517235"}
{"query": "Diffusion Model", "id": "2505.22643v1", "url": "http://arxiv.org/abs/2505.22643v1", "title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", "summary": "Leveraging recent diffusion models, LiDAR-based large-scale 3D scene\ngeneration has achieved great success. While recent voxel-based approaches can\ngenerate both geometric structures and semantic labels, existing range-view\nmethods are limited to producing unlabeled LiDAR scenes. Relying on pretrained\nsegmentation models to predict the semantic maps often results in suboptimal\ncross-modal consistency. To address this limitation while preserving the\nadvantages of range-view representations, such as computational efficiency and\nsimplified network design, we propose Spiral, a novel range-view LiDAR\ndiffusion model that simultaneously generates depth, reflectance images, and\nsemantic maps. Furthermore, we introduce novel semantic-aware metrics to\nevaluate the quality of the generated labeled range-view data. Experiments on\nthe SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves\nstate-of-the-art performance with the smallest parameter size, outperforming\ntwo-step methods that combine the generative and segmentation models.\nAdditionally, we validate that range images generated by Spiral can be\neffectively used for synthetic data augmentation in the downstream segmentation\ntraining, significantly reducing the labeling effort on LiDAR data.", "authors": ["Dekai Zhu", "Yixuan Hu", "Youquan Liu", "Dongyue Lu", "Lingdong Kong", "Slobodan Ilic"], "published_date": "2025-05-28", "title_zh": "SPIRAL：語義感知漸進式光達場景生成", "summary_zh": "本研究提出一個名為SPIRAL的新型光達擴散模型，旨在同時生成深度圖、反射圖像和語義地圖。現有的方法往往只能生成未標記的光達場景，或者需要依賴預訓練的分割模型來預測語義地圖，導致跨模態一致性較差。 SPIRAL 利用範圍視圖表示的優勢，例如計算效率和簡化的網路設計，並通過引入新的語義感知指標來評估生成帶標籤的範圍視圖資料的品質。在 SemanticKITTI 和 nuScenes 資料集上的實驗表明，SPIRAL 以最小的參數尺寸實現了最先進的性能，優於組合生成模型和分割模型的兩步方法。此外，SPIRAL 生成的範圍圖像可以有效地用於下游分割訓練中的合成資料增強，從而顯著減少光達資料的標記工作。", "applications": ["**自動駕駛模擬與測試：** 為自動駕駛系統生成逼真的模擬環境，無需大量真實世界資料收集和手動標註，降低開發成本並加速迭代。", "**機器人導航與地圖構建：** 幫助機器人學習在複雜環境中導航，並構建更加精確和豐富的環境地圖，提升機器人的自主性和可靠性。", "**城市規劃與智慧交通：** 生成不同城市場景的 LiDAR 資料，用於城市規劃、交通流量分析和基礎設施維護，提供更全面的決策依據。"], "pitch": "SPIRAL 是一個突破性的 LiDAR 場景生成技術，解決了現有方法的局限性，在自動駕駛、機器人和智慧城市等領域具有巨大的商業潛力。 其關鍵優勢在於能夠高效地生成帶語義標籤的逼真 LiDAR 資料，大幅降低訓練成本，加速產品開發週期。 我們相信 SPIRAL 可以成為自動駕駛公司、機器人製造商以及城市規劃機構的關鍵技術合作夥伴，為他們提供獨特的競爭優勢，並推動相關產業的快速發展。 我們尋求資金支持，以加速 SPIRAL 的商業化，並將其推廣到更廣闊的市場。", "audio": "audios/2505.22643v1.mp3", "timestamp": "2025-05-29T20:16:10.981168"}
{"query": "AI", "id": "2505.22647v1", "url": "http://arxiv.org/abs/2505.22647v1", "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "summary": "Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach.", "authors": ["Zhe Kong", "Feng Gao", "Yong Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Xunliang Cai", "Guanying Chen", "Wenhan Luo"], "published_date": "2025-05-28", "title_zh": "讓他們說話：音訊驅動的多人對話影片生成", "summary_zh": "現有的音訊驅動人像動畫技術，如說話頭像和說話身體生成，在生成同步的臉部動作和吸引人的視覺效果影片方面取得了顯著進展。 然而，現有方法主要集中在單個人物動畫上，並且難以處理多個音訊輸入，面臨音訊和人物之間的錯誤綁定問題。 此外，它們在指令遵循能力方面也存在局限性。 為了解決這個問題，本文提出了一個新任務：多人對話影片生成，並引入一個新的框架 MultiTalk，以應對多人生成過程中的挑戰。 具體來說，對於音訊注入，我們研究了幾種方案，並提出了標籤旋轉位置嵌入 (L-RoPE) 方法來解決音訊和人物綁定問題。 此外，在訓練過程中，我們觀察到部分參數訓練和多任務訓練對於保持基礎模型的指令遵循能力至關重要。 MultiTalk 在多個數據集（包括說話頭像、說話身體和多人數據集）上實現了優於其他方法的性能，證明了我們方法的強大生成能力。", "applications": ["遠距協作會議：根據每個參與者的聲音生成逼真的對話影片，提升線上會議的沉浸感和溝通效率。", "虛擬偶像互動：讓多個虛擬偶像根據語音指令進行自然對話，打造更豐富的直播和遊戲體驗。", "多語音配音製作：自動將劇本內容配上多個角色的聲音，並生成對應的影片，大幅降低影音製作成本。"], "pitch": "MultiTalk 解決了音訊驅動影片生成領域長期存在的痛點：多人對話影片的生成。 現有技術難以處理多人音訊和人物的正確關聯，且缺乏指令遵循能力。 MultiTalk 的 L-RoPE 技術和創新的訓練方法，使其在生成逼真、同步的多人對話影片方面具備顯著優勢。 潛在商業價值巨大：它可以應用於遠距協作、虛擬偶像、影音製作等多個領域，大幅提升效率並降低成本。 我們相信 MultiTalk 將引領下一代音訊驅動影片生成技術，並在市場上取得領先地位。我們正在尋找種子輪資金，以加速產品開發和市場推廣，將 MultiTalk 打造為業界標準。", "audio": "audios/2505.22647v1.mp3", "timestamp": "2025-05-29T21:13:14.285943"}
{"query": "Foundation Model", "id": "2505.22637v1", "url": "http://arxiv.org/abs/2505.22637v1", "title": "Understanding (Un)Reliability of Steering Vectors in Language Models", "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction.", "authors": ["Joschka Braun", "Carsten Eickhoff", "David Krueger", "Seyed Ali Bahrainian", "Dmitrii Krasheninnikov"], "published_date": "2025-05-28", "title_zh": "理解語言模型中引導向量的(不)可靠性", "summary_zh": "引導向量是一種輕量級的方法，通過在推理時向激活添加學習到的偏差來控制語言模型的行為。雖然引導向量展示了有前景的性能，但最近的研究表明，在某些情況下，它可能不可靠，甚至產生反效果。本文研究了提示類型和激活差異的幾何形狀對引導可靠性的影響。我們發現，所有七種提示類型都能產生淨正向的引導效果，但在樣本之間表現出很高的差異，並且經常產生與所需效果相反的效果。沒有一種提示類型明顯優於其他類型，但不同提示類型產生的引導向量在方向上經常不同（通過餘弦相似度測量）。其次，我們證明了訓練集激活差異之間較高的餘弦相似度可以預測更有效的引導。最後，我們觀察到，正負激活分離得更好的數據集更易於引導。我們的結果表明，當目標行為沒有由一個連貫的方向表示時，向量引導是不可靠的。", "applications": ["**個性化客戶服務機器人：** 根據客戶的情緒或需求，動態調整機器人的回應風格，例如將原本正式的回答變得更親切，或者在客戶感到沮喪時更積極主動地提供幫助。", "**客製化教育內容生成：** 針對不同學習風格的學生，引導模型生成更適合他們的教材。例如，對於視覺型學習者，生成更多圖表和圖片輔助的解釋；對於聽覺型學習者，生成更多口語化的描述。", "**安全內容過濾：**  快速且有效地阻止語言模型生成有害或不當內容。通過引導向量，強化模型對負面內容的回避能力，並降低誤判的可能性。"], "pitch": "我們正在開發一種利用引導向量技術，提升語言模型應用穩定性和可控性的解決方案。目前的語言模型在生成內容時，往往存在方向不穩定、結果不可預期的問題，這限制了它們在許多關鍵領域的應用。我們的研究發現揭示了影响引導向量效果的关键因素，并提供了提升模型穩定性的方法。例如，針對企業級客戶服務應用，我們可以提供定制化的引導向量，確保機器人以一致且符合品牌形象的方式與客戶互動。這不僅能提升客戶滿意度，還能降低因模型輸出錯誤而產生的品牌風險。我們認為，隨著語言模型在各行業的滲透，對模型穩定性和可控性的需求將持續增長，這將為我們的解決方案帶來巨大的市場潛力。", "audio": "audios/2505.22637v1.mp3", "timestamp": "2025-05-29T21:13:32.818422"}
{"query": "Diffusion Model", "id": "2505.22643v1", "url": "http://arxiv.org/abs/2505.22643v1", "title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", "summary": "Leveraging recent diffusion models, LiDAR-based large-scale 3D scene\ngeneration has achieved great success. While recent voxel-based approaches can\ngenerate both geometric structures and semantic labels, existing range-view\nmethods are limited to producing unlabeled LiDAR scenes. Relying on pretrained\nsegmentation models to predict the semantic maps often results in suboptimal\ncross-modal consistency. To address this limitation while preserving the\nadvantages of range-view representations, such as computational efficiency and\nsimplified network design, we propose Spiral, a novel range-view LiDAR\ndiffusion model that simultaneously generates depth, reflectance images, and\nsemantic maps. Furthermore, we introduce novel semantic-aware metrics to\nevaluate the quality of the generated labeled range-view data. Experiments on\nthe SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves\nstate-of-the-art performance with the smallest parameter size, outperforming\ntwo-step methods that combine the generative and segmentation models.\nAdditionally, we validate that range images generated by Spiral can be\neffectively used for synthetic data augmentation in the downstream segmentation\ntraining, significantly reducing the labeling effort on LiDAR data.", "authors": ["Dekai Zhu", "Yixuan Hu", "Youquan Liu", "Dongyue Lu", "Lingdong Kong", "Slobodan Ilic"], "published_date": "2025-05-28", "title_zh": "SPIRAL：語義感知的漸進式 LiDAR 場景生成", "summary_zh": "本文提出一個名為 SPIRAL 的新型 LiDAR 擴散模型，能在 Range View 視角下同時生成深度、反射率圖像和語義地圖。以往的 Range View 方法只能生成未標記的 LiDAR 場景，而依賴預訓練分割模型進行語義預測又會導致跨模態一致性不佳。SPIRAL 解決了這個問題，同時保留了 Range View 表示的計算效率和簡化網路設計的優勢。實驗結果顯示，SPIRAL 在 SemanticKITTI 和 nuScenes 數據集上表現出色，參數量最小，且能有效用於下游分割訓練中的合成數據增強，從而減少 LiDAR 數據的標記工作。", "applications": ["**自動駕駛模擬器:** 利用 SPIRAL 生成逼真的、帶有語義標籤的 LiDAR 數據，可以訓練自動駕駛系統，提高其在各種環境下的感知能力。", "**機器人導航與地圖構建:** 生成的 LiDAR 場景可用於訓練機器人，使其能夠在複雜環境中導航，並建立詳細的地圖，特別是在缺乏真實數據的環境中。", "**虛擬現實與增強現實:** 生成具有語義信息的 3D 環境，用於創建更真實和互動性更強的 VR/AR 體驗，例如城市規劃的可視化或遊戲場景的創建。"], "pitch": "SPIRAL 是一款基於擴散模型的 LiDAR 場景生成技術，能在 Range View 視角下高效且精準地生成帶有語義標籤的 3D 環境。相比現有方案，SPIRAL 大幅提升了生成數據的質量和一致性，並顯著降低了計算成本。我們正在建立一個平台，讓用戶能輕鬆生成客製化的 LiDAR 數據，加速自動駕駛、機器人、VR/AR 等領域的開發。透過提供高品質的合成數據，我們能有效降低客戶的數據獲取和標記成本，加速產品上市時間，並在這些快速增長的市場中佔據領導地位。我們的商業模式基於 SaaS，提供按需訂閱的數據生成服務，並且我們已經與多家自動駕駛公司展開了初步合作。", "audio": "audios/2505.22643v1.mp3", "timestamp": "2025-05-29T21:13:47.997557"}
{"query": "AI", "id": "2505.22647v1", "url": "http://arxiv.org/abs/2505.22647v1", "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "summary": "Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach.", "authors": ["Zhe Kong", "Feng Gao", "Yong Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Xunliang Cai", "Guanying Chen", "Wenhan Luo"], "published_date": "2025-05-28", "title_zh": "讓他們說話：音訊驅動的多人對話影片生成", "summary_zh": "現有的音訊驅動人物動畫技術，雖然在生成同步的臉部動作和具有吸引力的視覺品質影片方面取得了顯著進展，但主要集中在單人動畫上，難以處理多音訊串流輸入，容易出現音訊與人物綁定錯誤的問題，且指令遵循能力也有限。本文提出了一項新任務：多人對話影片生成，並引入了一個名為MultiTalk的新框架來解決多人生成過程中的挑戰。特別是，針對音訊注入，我們研究了幾種方案，並提出了標籤旋轉位置嵌入 (L-RoPE) 方法來解決音訊和人物綁定問題。此外，在訓練過程中，我們發現部分參數訓練和多任務訓練對於保持基礎模型的指令遵循能力至關重要。MultiTalk在多個數據集上（包括說話頭像、說話全身和多人數據集）都優於其他方法，證明了我們方法的強大生成能力。", "applications": ["**遠距協作與會議：** 將多方語音輸入轉換為逼真的多人對話影片，模擬面對面會議的體驗，即使參與者身處不同地點也能有效溝通。", "**語言學習與口語練習：** 用戶可以選擇不同的角色和情境，透過與系統生成的對話影片互動，練習口語表達和聽力理解能力。", "**虛擬主播與內容創作：** 根據提供的音訊腳本和角色設定，自動生成多人對話影片，用於新聞播報、娛樂節目、教育內容等，大幅降低製作成本。"], "pitch": "MultiTalk 解決了音訊驅動影片生成領域長期存在的痛點：多人對話。透過獨創的 L-RoPE 技術和優化的訓練策略，我們能生成逼真且同步的多人對話影片，打破了現有技術在應用場景上的限制。試想一下，它可以應用於下一代遠距協作平台，提供沉浸式會議體驗；可以打造客製化的語言學習工具，提升學習效率；更可以賦能內容創作者，以低成本、高效率的方式製作高品質影片。MultiTalk 具有廣闊的市場前景和巨大的商業價值，將顛覆現有的影片製作和溝通方式，成為元宇宙和人工智能時代的殺手級應用。", "audio": "audios/2505.22647v1.mp3", "timestamp": "2025-05-29T22:13:00.393158"}
{"query": "Foundation Model", "id": "2505.22637v1", "url": "http://arxiv.org/abs/2505.22637v1", "title": "Understanding (Un)Reliability of Steering Vectors in Language Models", "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction.", "authors": ["Joschka Braun", "Carsten Eickhoff", "David Krueger", "Seyed Ali Bahrainian", "Dmitrii Krasheninnikov"], "published_date": "2025-05-28", "title_zh": "理解語言模型中轉向向量的（不）可靠性", "summary_zh": "轉向向量是一種輕量級的方法，透過在推論時將學習到的偏差添加到激活值，來控制語言模型的行為。雖然轉向展現出令人期待的效能，但最近的研究表明，在某些情況下它可能不可靠，甚至會產生反效果。本文研究了提示詞類型和激活差異的幾何形狀對轉向可靠性的影響。研究發現，所有實驗中使用的七種提示詞類型都產生了淨正向的轉向效果，但在樣本之間表現出高度的變異性，並且經常產生與期望相反的效果。沒有哪種提示詞類型明顯優於其他類型，但不同提示詞類型產生的轉向向量在方向上經常不同（如餘弦相似度所衡量）。此外，訓練集激活差異之間的餘弦相似度越高，預測的轉向效果越好。最後，研究觀察到，正向和負向激活值分離程度較高的數據集更易於轉向。研究結果表明，當目標行為沒有以連貫的方向表示時，向量轉向是不可靠的。", "applications": ["個性化聊天機器人：針對不同用戶或場景，調整聊天機器人的語氣、風格或專業知識，例如將客服機器人調整為更具同情心，或者將法律諮詢機器人調整為更淺顯易懂。", "內容審核微調：在不重新訓練整個模型的情況下，使用轉向向量來抑制不當內容，例如仇恨言論或暴力描述，提高內容審核的準確性和效率。", "生成創意寫作：控制故事的情感基調或主題走向，例如讓模型生成更幽默或更恐怖的故事，為作家提供更靈活的創作工具。"], "pitch": "我們研究揭示了現有語言模型控制技術（轉向向量）的局限性，並指出了提高其可靠性的關鍵因素。這為開發更精準、更可控的AI應用開啟了新的機會。我們的技術不僅可以應用於個性化聊天機器人、內容審核等領域，還能賦能各行各業，例如在醫療領域用於輔助診斷，在金融領域用於風險評估，在教育領域用於個性化學習。透過優化轉向向量的穩定性，我們能打造更可信賴的AI解決方案，潛在商業價值巨大。 我們團隊計劃基於研究成果，開發一套穩健的轉向向量生成與評估框架，並將其整合到現有AI開發平台中，降低使用門檻，加速AI商業化進程。我們相信，這將帶來顛覆性的創新，引領AI技術進入一個更加可控、可靠的新時代。", "audio": "audios/2505.22637v1.mp3", "timestamp": "2025-05-29T22:13:17.639594"}
{"query": "Diffusion Model", "id": "2505.22643v1", "url": "http://arxiv.org/abs/2505.22643v1", "title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", "summary": "Leveraging recent diffusion models, LiDAR-based large-scale 3D scene\ngeneration has achieved great success. While recent voxel-based approaches can\ngenerate both geometric structures and semantic labels, existing range-view\nmethods are limited to producing unlabeled LiDAR scenes. Relying on pretrained\nsegmentation models to predict the semantic maps often results in suboptimal\ncross-modal consistency. To address this limitation while preserving the\nadvantages of range-view representations, such as computational efficiency and\nsimplified network design, we propose Spiral, a novel range-view LiDAR\ndiffusion model that simultaneously generates depth, reflectance images, and\nsemantic maps. Furthermore, we introduce novel semantic-aware metrics to\nevaluate the quality of the generated labeled range-view data. Experiments on\nthe SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves\nstate-of-the-art performance with the smallest parameter size, outperforming\ntwo-step methods that combine the generative and segmentation models.\nAdditionally, we validate that range images generated by Spiral can be\neffectively used for synthetic data augmentation in the downstream segmentation\ntraining, significantly reducing the labeling effort on LiDAR data.", "authors": ["Dekai Zhu", "Yixuan Hu", "Youquan Liu", "Dongyue Lu", "Lingdong Kong", "Slobodan Ilic"], "published_date": "2025-05-28", "title_zh": "SPIRAL：語義感知漸進式光達場景生成", "summary_zh": "近年來，基於擴散模型的光達大型3D場景生成技術取得了顯著進展。雖然現有的體素方法能夠同時生成幾何結構和語義標籤，但現有的範圍視角方法僅限於生成未標記的光達場景。依靠預訓練的分割模型來預測語義地圖通常會導致次優的跨模態一致性。為了在保留範圍視角表示優勢（例如計算效率和簡化的網路設計）的同時解決此限制，我們提出了一種新的範圍視角光達擴散模型 Spiral，它可以同時生成深度、反射圖像和語義地圖。此外，我們引入了新的語義感知指標來評估生成的標記範圍視角資料的品質。在 SemanticKITTI 和 nuScenes 數據集上的實驗表明，Spiral 以最小的參數尺寸實現了最先進的性能，優於結合生成模型和分割模型的兩步方法。此外，我們驗證了 Spiral 生成的範圍圖像可以有效地用於下游分割訓練中的合成資料擴充，從而顯著減少光達資料的標註工作。", "applications": ["**自動駕駛模擬器增強：** 針對特定道路環境或罕見事件，快速生成逼真的合成光達數據，提升自動駕駛系統在各種情況下的感知能力。", "**機器人環境感知訓練：** 生成不同複雜度的室內/外環境光達數據，幫助機器人學習在各種環境中進行導航、物件識別和避障。", "**智慧城市規劃：** 生成不同城市區域的光達模型，用於模擬建築物的陰影影響、交通流量以及最佳化基礎設施部署，而無需實際掃描整個城市。"], "pitch": "SPIRAL 是一個基於擴散模型的創新光達場景生成技術，能夠以更高的效率和更小的參數量生成帶有語義標籤的光達數據。 透過降低對大量真實世界標註數據的依賴，SPIRAL 有潛力大幅降低自動駕駛、機器人以及智慧城市等領域的開發成本和時間。 其生成的合成數據可以用於訓練更強大的 AI 模型，並加速相關產品的上市。 我們相信 SPIRAL 技術在數據驅動的感知領域具有巨大的商業價值，能為各行業帶來突破性的進展。", "audio": "audios/2505.22643v1.mp3", "timestamp": "2025-05-29T22:13:30.086553"}
{"query": "AI", "id": "2505.22647v1", "url": "http://arxiv.org/abs/2505.22647v1", "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "summary": "Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach.", "authors": ["Zhe Kong", "Feng Gao", "Yong Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Xunliang Cai", "Guanying Chen", "Wenhan Luo"], "published_date": "2025-05-28", "title_zh": "讓他們說話：音訊驅動的多人對話影片生成", "summary_zh": "現有的音訊驅動人物動畫技術，像是會說話的頭像和會說話的身體，在產生同步的面部動作和吸引人的視覺品質影片方面取得了顯著的進展。然而，現有的方法主要集中在單個人物動畫上，並且難以處理多音訊流的輸入，面臨音訊和人物之間綁定錯誤的問題。此外，它們在遵循指令的能力方面也存在局限性。為了解決這個問題，本文提出了一個新的任務：多人對話影片生成，並引入一個新的框架 MultiTalk，來應對多人生成過程中的挑戰。具體來說，對於音訊注入，我們研究了幾種方案，並提出了標籤旋轉位置嵌入（L-RoPE）方法來解決音訊和人物綁定的問題。此外，在訓練過程中，我們觀察到部分參數訓練和多任務訓練對於保持基礎模型的指令遵循能力至關重要。MultiTalk 在包括會說話的頭像、會說話的身體和多人資料集在內的幾個資料集上，相比其他方法，取得了卓越的性能，證明了我們方法的強大生成能力。", "applications": ["線上會議與教學：將多人對話影片生成技術應用於線上會議或教學平台，可以根據與會者的語音自動生成生動的人物動畫，讓線上互動更加自然真實，提升參與感。", "遊戲角色互動：在遊戲中，可以根據玩家的語音輸入，即時生成遊戲角色的面部表情和動作，讓玩家與遊戲角色之間的互動更加豐富且身臨其境。", "個人化內容創作：使用者可以透過輸入多個語音片段，自動生成一段包含多個虛擬人物對話的影片，用於創作短劇、動畫或教育內容，降低內容創作的門檻。"], "pitch": "MultiTalk 解決了音訊驅動人物動畫領域的重大瓶頸：多人對話影片的生成。現有的技術主要集中在單人動畫，無法應對複雜的多人互動場景。MultiTalk 的 L-RoPE 方法有效地解決了音訊和人物綁定的問題，並透過優化訓練策略提升了指令遵循能力，在多個數據集上取得了領先的性能。這項技術的商業價值巨大，可以應用於線上會議、遊戲、內容創作等多個領域。例如，將 MultiTalk 整合到線上會議平台，可以顯著提升用戶體驗，吸引更多付費用戶。從創投角度來看，MultiTalk 具有高度的可擴展性和商業化潛力，值得投資。", "audio": "audios/2505.22647v1.mp3", "timestamp": "2025-05-29T23:13:01.168417"}
{"query": "Foundation Model", "id": "2505.22637v1", "url": "http://arxiv.org/abs/2505.22637v1", "title": "Understanding (Un)Reliability of Steering Vectors in Language Models", "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction.", "authors": ["Joschka Braun", "Carsten Eickhoff", "David Krueger", "Seyed Ali Bahrainian", "Dmitrii Krasheninnikov"], "published_date": "2025-05-28", "title_zh": "理解語言模型中引導向量的（不）可靠性", "summary_zh": "這篇論文研究了引導向量在控制語言模型行為上的可靠性。引導向量是一種輕量級方法，通過在推理時向激活添加學習到的偏差來影響模型輸出。雖然引導展現出潛力，但研究表明它在某些情況下可能不可靠甚至適得其反。這篇論文探討了提示類型和激活差異的幾何結構對引導可靠性的影響。 研究發現，不同的提示類型雖然整體上產生積極影響，但在樣本之間存在很大的差異，並且經常產生與期望相反的效果。 此外，訓練集中激活差異之間更高的餘弦相似度預示著更有效的引導。最後，正負激活分離良好的數據集更易於引導。 總之，研究表明，當目標行為不能用一個連貫的方向表示時，向量引導是不可靠的。", "applications": ["**個性化客服機器人：** 通過引導向量，可以讓客服機器人更容易學習特定客戶的偏好和溝通風格，實現更個性化和高效的服務。但由於引導向量的不可靠性，需要針對不同客戶群體進行更精細的調整，避免產生相反的效果，例如把原本禮貌的機器人變得粗魯。", "**生成式藝術作品風格控制：** 藝術家可以使用引導向量來控制AI生成的藝術作品的風格，例如讓它更偏向印象派或抽象主義。然而，如果風格的定義不夠明確，引導向量可能導致作品風格混亂，需要藝術家不斷實驗和微調。", "**AI輔助寫作：** 幫助作者產生特定情緒或語氣的文章。例如，引導向量可以幫助AI生成更幽默或更嚴肅的文章。但如果作者的需求不明確，引導向量可能導致文章風格不一致或與作者的意圖相悖。"], "pitch": "我們正在開發一種更可靠且可預測的AI模型控制技術，基於對引導向量的深入理解。目前引導向量雖然輕量，但其效果不穩定，限制了其在各領域的廣泛應用。我們的研究揭示了提示工程和激活差異的幾何結構對引導向量可靠性的影響，為改進引導向量方法提供了方向。 我們計劃開發一種自動化的方法，能夠基於數據集的特性和用戶的需求，選擇最有效的提示類型和優化引導向量的設計，從而提升AI模型的行為控制精確度和穩定性。 這項技術的商業價值體現在：1. 提升AI模型的可用性和用戶體驗； 2. 降低AI模型部署和維護的成本，因為減少了試錯的次數； 3. 開啟了更多AI應用場景，例如個性化AI助手、自動化內容創作和智能決策支持。 我們的目標是成為下一代AI模型控制技術的領導者，賦能各行各業，釋放AI的真正潛力。", "audio": "audios/2505.22637v1.mp3", "timestamp": "2025-05-29T23:13:18.676591"}
{"query": "Diffusion Model", "id": "2505.22643v1", "url": "http://arxiv.org/abs/2505.22643v1", "title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", "summary": "Leveraging recent diffusion models, LiDAR-based large-scale 3D scene\ngeneration has achieved great success. While recent voxel-based approaches can\ngenerate both geometric structures and semantic labels, existing range-view\nmethods are limited to producing unlabeled LiDAR scenes. Relying on pretrained\nsegmentation models to predict the semantic maps often results in suboptimal\ncross-modal consistency. To address this limitation while preserving the\nadvantages of range-view representations, such as computational efficiency and\nsimplified network design, we propose Spiral, a novel range-view LiDAR\ndiffusion model that simultaneously generates depth, reflectance images, and\nsemantic maps. Furthermore, we introduce novel semantic-aware metrics to\nevaluate the quality of the generated labeled range-view data. Experiments on\nthe SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves\nstate-of-the-art performance with the smallest parameter size, outperforming\ntwo-step methods that combine the generative and segmentation models.\nAdditionally, we validate that range images generated by Spiral can be\neffectively used for synthetic data augmentation in the downstream segmentation\ntraining, significantly reducing the labeling effort on LiDAR data.", "authors": ["Dekai Zhu", "Yixuan Hu", "Youquan Liu", "Dongyue Lu", "Lingdong Kong", "Slobodan Ilic"], "published_date": "2025-05-28", "title_zh": "SPIRAL：語義感知的漸進式光達場景生成", "summary_zh": "SPIRAL是一種新型的光達場景生成模型，它利用擴散模型，能夠同時生成深度、反射圖像和語義地圖。相較於以往的方案，SPIRAL在計算效率和網路設計上更為簡潔，並且在生成帶有語義標籤的光達數據方面表現出色。實驗證明，SPIRAL的性能優於需要先生成再分割的兩階段方法，並且生成的數據可以用於下游的語義分割訓練，有效減少光達數據的標註成本。", "applications": ["自動駕駛模擬：利用SPIRAL生成大量逼真的、帶有語義信息的LiDAR場景，用於訓練和驗證自動駕駛算法，提高其在各種環境下的魯棒性。", "智慧城市規劃：通過SPIRAL生成不同城市規劃方案下的LiDAR數據，可以模擬環境變化對感測器的影響，輔助城市規劃設計，例如評估不同建築布局對自動駕駛車輛的影響。", "虛擬現實與遊戲開發：SPIRAL可以快速生成逼真的3D LiDAR場景，用於創建沉浸式的虛擬現實體驗和遊戲環境，減少手動建模的工作量。"], "pitch": "SPIRAL利用擴散模型技術，開創性地解決了光達場景生成中的語義信息缺失問題，實現了高效且高質量的帶標籤光達數據生成。這解決了自動駕駛、智慧城市等領域中數據標註成本高昂的痛點。其潛在商業價值體現在：\n\n*   **數據增強服務：** 提供基於SPIRAL的數據增強服務，幫助自動駕駛公司大幅降低數據標註成本，加速模型迭代。\n*   **模擬環境平台：** 构建基於SPIRAL生成的虛擬環境平台，用於自动驾驶算法的测试和验证，提供更安全、高效的解决方案。\n*   **智慧城市解決方案：** 將SPIRAL應用於智慧城市規劃，提供更全面的數據支持，輔助城市管理者進行決策，提升城市智能化水平。\n\nSPIRAL技術具有顯著的性能優勢和廣闊的應用前景，我們有信心將其打造為下一代3D感知領域的核心技術。", "audio": "audios/2505.22643v1.mp3", "timestamp": "2025-05-29T23:13:32.596994"}
{"query": "AI", "id": "2505.22647v1", "url": "http://arxiv.org/abs/2505.22647v1", "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation", "summary": "Audio-driven human animation methods, such as talking head and talking body\ngeneration, have made remarkable progress in generating synchronized facial\nmovements and appealing visual quality videos. However, existing methods\nprimarily focus on single human animation and struggle with multi-stream audio\ninputs, facing incorrect binding problems between audio and persons.\nAdditionally, they exhibit limitations in instruction-following capabilities.\nTo solve this problem, in this paper, we propose a novel task: Multi-Person\nConversational Video Generation, and introduce a new framework, MultiTalk, to\naddress the challenges during multi-person generation. Specifically, for audio\ninjection, we investigate several schemes and propose the Label Rotary Position\nEmbedding (L-RoPE) method to resolve the audio and person binding problem.\nFurthermore, during training, we observe that partial parameter training and\nmulti-task training are crucial for preserving the instruction-following\nability of the base model. MultiTalk achieves superior performance compared to\nother methods on several datasets, including talking head, talking body, and\nmulti-person datasets, demonstrating the powerful generation capabilities of\nour approach.", "authors": ["Zhe Kong", "Feng Gao", "Yong Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Xunliang Cai", "Guanying Chen", "Wenhan Luo"], "published_date": "2025-05-28", "title_zh": "讓他們說話：音訊驅動的多人對話影片生成", "summary_zh": "現有的語音驅動人物動畫技術，如口說頭像和口說全身生成，在產生同步的臉部動作和吸引人的視覺效果方面取得了顯著進展。然而，這些方法主要集中在單個人物動畫上，並且難以處理多流音訊輸入，面臨音訊和人物之間錯誤綁定的問題。此外，它們在遵循指令的能力上也存在限制。為了解決這個問題，我們提出了一個新的任務：多人對話影片生成，並引入了一個新的框架 MultiTalk 來解決多人生成過程中的挑戰。具體來說，對於音訊注入，我們研究了幾種方案，並提出了標籤旋轉位置嵌入（L-RoPE）方法來解決音訊和人物綁定的問題。此外，在訓練過程中，我們觀察到部分參數訓練和多任務訓練對於保持基礎模型的指令遵循能力至關重要。 MultiTalk 在多個數據集上，包括口說頭像、口說全身和多人數據集，都取得了比其他方法更優越的性能，證明了我們方法的強大生成能力。", "applications": ["**遠距協作與會議：** 透過麥克風陣列捕捉多人語音，自動生成逼真的對話影片，提升遠端會議的沉浸感和互動性，讓與會者彷彿身臨其境。", "**語言學習：** 根據學習者的語音輸入，生成與虛擬人物的自然對話影片，提供更生動有趣的語言練習體驗，並可模擬不同情境，提高口語能力。", "**遊戲與娛樂：** 讓遊戲中的 NPC (非玩家角色) 能夠根據玩家的語音指令做出更自然和逼真的反應，提升遊戲的互動性和沉浸感，創造更豐富的遊戲體驗。"], "pitch": "MultiTalk 解決了現有語音驅動動畫技術在多人對話場景中的瓶頸，創造了更真實自然的數位互動體驗。想像一下，我們能夠讓任何多人對話以逼真的視覺效果呈現，從遠端會議到沉浸式遊戲，MultiTalk 打開了全新的應用領域。我們的 L-RoPE 方法解決了音訊與人物的綁定難題，Multi-task learning 確保模型能有效遵循指令。這項技術在遠距協作、教育和娛樂領域具有巨大的潛力，有望徹底改變人機互動的方式，我們相信 MultiTalk 能夠成為下一代數位內容生成的關鍵技術。", "audio": "audios/2505.22647v1.mp3", "timestamp": "2025-05-30T01:04:02.036452"}
{"query": "Foundation Model", "id": "2505.22637v1", "url": "http://arxiv.org/abs/2505.22637v1", "title": "Understanding (Un)Reliability of Steering Vectors in Language Models", "summary": "Steering vectors are a lightweight method to control language model behavior\nby adding a learned bias to the activations at inference time. Although\nsteering demonstrates promising performance, recent work shows that it can be\nunreliable or even counterproductive in some cases. This paper studies the\ninfluence of prompt types and the geometry of activation differences on\nsteering reliability. First, we find that all seven prompt types used in our\nexperiments produce a net positive steering effect, but exhibit high variance\nacross samples, and often give an effect opposite of the desired one. No prompt\ntype clearly outperforms the others, and yet the steering vectors resulting\nfrom the different prompt types often differ directionally (as measured by\ncosine similarity). Second, we show that higher cosine similarity between\ntraining set activation differences predicts more effective steering. Finally,\nwe observe that datasets where positive and negative activations are better\nseparated are more steerable. Our results suggest that vector steering is\nunreliable when the target behavior is not represented by a coherent direction.", "authors": ["Joschka Braun", "Carsten Eickhoff", "David Krueger", "Seyed Ali Bahrainian", "Dmitrii Krasheninnikov"], "published_date": "2025-05-28", "title_zh": "理解語言模型中導引向量的（不）可靠性", "summary_zh": "導引向量是一種輕量級方法，透過在推論時將學習到的偏差添加到激活值，來控制語言模型的行為。雖然導引向量展現了有前景的效能，但最近的研究表明，在某些情況下，它可能不可靠甚至適得其反。本論文研究了提示類型和激活差異的幾何形狀對導引可靠性的影響。研究發現，雖然實驗中使用的所有七種提示類型都產生了淨正向的導引效果，但在樣本之間表現出很高的變異性，並且經常產生與期望效果相反的效果。沒有一種提示類型明顯優於其他類型，但不同提示類型產生的導引向量在方向上通常不同（如餘弦相似度所測量）。研究也表明，訓練集激活差異之間的餘弦相似度越高，預測的導引效果就越好。最後，研究觀察到，正向和負向激活分離得更好的數據集更易於導引。研究結果表明，當目標行為沒有由一個連貫的方向表示時，向量導引是不可靠的。", "applications": ["**個性化客服聊天機器人：** 利用導引向量，可以針對不同客戶情境調整聊天機器人的回應風格，例如更積極或更保守，但需要注意可能產生的不一致性，透過本研究的發現，可以調整訓練方式，提高可靠性。", "**AI寫作助手：** 導引向量可以控制AI寫作的風格，例如更正式或更幽默。但如果風格定義不明確，可能導致寫作風格不連貫。本研究可以幫助改善導引向量的訓練，確保寫作風格的一致性。", "**內容審核：** 導引向量可以被應用於自動識別並過濾不當內容，例如仇恨言論或暴力內容。然而，如果訓練數據不足或標籤不夠清晰，可能會導致誤判或漏判。本研究的成果能幫助提升導引向量的可靠性，進而提高內容審核的準確性。"], "pitch": "這項研究揭示了語言模型導引向量技術的潛在風險與限制，指出其可靠性受提示類型和激活數據的影響。儘管導引向量有輕量級控制語言模型行為的優勢，但在沒有充分理解其底層機制的情況下貿然應用，可能導致不可預測甚至反效果的結果。本研究提供的洞見，對於開發更可靠、更穩健的導引向量技術至關重要。透過優化訓練數據和提示策略，我們可以顯著提升導引向量的精確度和一致性，使其在客戶服務、內容創作和審核等領域發揮更大的價值。從創投角度來看，投資於專注於提升導引向量可靠性的技術團隊，具有巨大的商業潛力，可以為企業提供更精準、更可控的AI解決方案，從而在競爭激烈的市場中脫穎而出。", "audio": "audios/2505.22637v1.mp3", "timestamp": "2025-05-30T01:04:40.475200"}
{"query": "Diffusion Model", "id": "2505.22643v1", "url": "http://arxiv.org/abs/2505.22643v1", "title": "SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation", "summary": "Leveraging recent diffusion models, LiDAR-based large-scale 3D scene\ngeneration has achieved great success. While recent voxel-based approaches can\ngenerate both geometric structures and semantic labels, existing range-view\nmethods are limited to producing unlabeled LiDAR scenes. Relying on pretrained\nsegmentation models to predict the semantic maps often results in suboptimal\ncross-modal consistency. To address this limitation while preserving the\nadvantages of range-view representations, such as computational efficiency and\nsimplified network design, we propose Spiral, a novel range-view LiDAR\ndiffusion model that simultaneously generates depth, reflectance images, and\nsemantic maps. Furthermore, we introduce novel semantic-aware metrics to\nevaluate the quality of the generated labeled range-view data. Experiments on\nthe SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves\nstate-of-the-art performance with the smallest parameter size, outperforming\ntwo-step methods that combine the generative and segmentation models.\nAdditionally, we validate that range images generated by Spiral can be\neffectively used for synthetic data augmentation in the downstream segmentation\ntraining, significantly reducing the labeling effort on LiDAR data.", "authors": ["Dekai Zhu", "Yixuan Hu", "Youquan Liu", "Dongyue Lu", "Lingdong Kong", "Slobodan Ilic"], "published_date": "2025-05-28", "title_zh": "SPIRAL：語義感知的漸進式雷達場景生成", "summary_zh": "近年來，基於擴散模型的雷達3D場景生成取得了顯著進展。然而，現有的範圍視角方法在生成帶語義標籤的雷達場景方面受到限制。本研究提出Spiral，一種新穎的範圍視角雷達擴散模型，可以同時生成深度圖、反射率圖像和語義地圖。Spiral在保持範圍視角表示優勢（如計算效率和簡化網路設計）的同時，解決了現有方法的局限性。實驗證明，Spiral以最小的參數量實現了最先進的性能，並且生成的範圍圖像可以有效地用於下游分割訓練中的合成數據增強，從而顯著減少雷達數據的標註工作。", "applications": ["**自動駕駛模擬環境生成：** Spiral可以快速生成逼真的、帶有語義標籤的雷達數據，用於自動駕駛汽車的訓練和測試，降低真實世界測試的成本和風險。", "**機器人SLAM（同步定位與地圖構建）數據增強：** 機器人可以在Spiral生成的合成雷達環境中訓練SLAM算法，提高機器人在複雜和未知環境中的定位和地圖構建能力。", "**智慧城市規劃與可視化：** Spiral可以基於現有地理數據生成城市環境的3D雷達模型，幫助城市規劃者進行交通流量分析、設施佈局優化等，並為市民提供更直觀的可視化體驗。"], "pitch": "Spiral技術解決了雷達數據生成的瓶頸，特別是在帶有語義標籤的數據方面。相較於現有方法，Spiral效率更高、成本更低，生成的數據質量也更好。這使得它在自動駕駛、機器人、智慧城市等領域具有巨大的商業潛力。我們提供的產品不僅可以降低數據獲取和標註的成本，還可以加速相關算法的開發和部署。通過提供API或SDK，我們可以將Spiral集成到現有平台上，為客戶提供定制化的雷達數據生成服務。此外，我們還可以將Spiral生成的數據用於創建高精度的3D地圖，為自動駕駛和機器人導航提供基礎設施。總之，Spiral技術具有顛覆性，有望成為未來雷達應用領域的關鍵技術。", "audio": "audios/2505.22643v1.mp3", "timestamp": "2025-05-30T01:05:10.093097"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在基於視覺的空間智能方面的能力", "summary_zh": "現有的多模態大型語言模型(MLLM)在處理2D視覺任務上表現出色，但在空間智能方面仍有進步空間。Spatial-MLLM提出了一種新的框架，僅基於2D視覺資訊進行空間推理。與傳統依賴額外3D或2.5D數據的模型不同，Spatial-MLLM採用雙編碼器架構：一個提取語義特徵的預訓練2D視覺編碼器，以及一個從視覺幾何模型骨幹初始化的空間編碼器，用於提取3D結構特徵。這兩個編碼器的輸出會被整合，以增強模型的空間理解能力。此外，還提出了一種空間感知的幀採樣策略，在推理時選擇視頻序列中空間資訊豐富的幀，即使在token長度有限的情況下，也能確保模型關注於對空間推理至關重要的幀。論文構建了Spatial-MLLM-120k數據集，並利用有監督微調和GRPO方法進行訓練。實驗結果表明，Spatial-MLLM在各種基於視覺的空間理解和推理任務中都取得了最先進的性能。", "applications": ["**自動駕駛輔助：** 通過分析車載攝像頭拍攝的2D影像，即時判斷車輛與行人、障礙物的空間關係，預測潛在碰撞風險，提供更精確的駕駛輔助。", "**室內機器人導航：** 機器人僅憑藉攝像頭捕捉的2D畫面，就能理解室內空間結構，進行精確的定位和路徑規劃，實現更智能的自主導航。", "**建築設計與巡檢：** 從2D建築圖紙或現場拍攝的照片中，推斷建築結構的3D信息，輔助設計師進行方案優化，或幫助巡檢人員發現隱藏的結構性問題。"], "pitch": "Spatial-MLLM解決了多模態大型語言模型在僅有2D視覺輸入時，空間推理能力不足的痛點。它的雙編碼器架構和空間感知採樣策略，讓模型能從2D圖像中提取和理解3D空間信息，大幅提升了視覺空間智能。這種技術在自動駕駛、機器人、建築、AR/VR等領域具有廣泛的應用前景。團隊提出的框架，並透過建立資料集及實驗驗證其有效性，證明了團隊的技術領先性。這將創造一個龐大的市場，讓投資者有機會在早期進入並分享AI空間智能爆發的紅利。Spatial-MLLM不僅是一項技術突破，更是一個變革性的機會，值得投資者重點關注。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-30T03:08:01.249144"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：基於校正流變換器的免訓練多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一個全新的框架，它利用 LoRA 模型實現多概念圖像編輯，完全不需要重新訓練模型。它基於一個關鍵發現：在 Flux 風格的擴散變換器中，針對不同概念的 Transformer 特徵會在去噪過程的早期激活空間上連貫的區域。LoRAShop 可以為每個概念導出一個解耦的潛在遮罩，並僅在限定概念的區域內混合相應的 LoRA 權重。因此，最終編輯效果能夠無縫地將多個主體或風格融入原始場景，同時保留全局上下文、光照和細節。實驗表明，LoRAShop 在身份保持方面優於其他基線方法。由於無需重新訓練和外部約束，LoRAShop 將個性化的擴散模型變成了一個實用的 'LoRA 版 Photoshop' 工具，並為組合視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**個性化產品設計：** 用戶可以上傳自己的產品圖片（例如：鞋子、包包），並指定不同的設計風格（例如：賽博龐克、復古、簡約），LoRAShop 可以快速生成多種不同風格的產品設計圖，幫助用戶找到最喜歡的樣式。", "**虛擬試衣間增強：** 用戶可以上傳自己的照片，然後選擇不同的服裝款式和材質（例如：羊毛、絲綢、皮革），LoRAShop 可以生成逼真的虛擬試穿效果，讓用戶在購買前就能看到穿上不同服裝的樣子。", "**快速生成內容變現：**自媒體創作者或設計師能迅速生成各種風格的圖像，例如在風景照中加入特定人物或動物，或者為產品圖套用不同主題，從而提升內容產出效率，增加創作變現管道。"], "pitch": "LoRAShop 解決了圖像編輯領域的痛點，即在不重新訓練模型的情況下，高效且精確地編輯包含多個概念的圖像。其核心優勢在於免訓練、高效率、高精度，以及易於整合現有 LoRA 模型。這使得 LoRAShop 在個性化圖像生成、產品設計、虛擬試穿、以及創意內容生產等領域具有廣闊的應用前景。我們可以將其定位為一個 B2B 或 B2C 的解決方案，提供 API 接口或獨立應用程序，賦能設計師、電商平台、社交媒體平台等，快速生成個性化、高品質的圖像內容，創造巨大的商業價值。尤其在 AI 生成內容 (AIGC) 市場快速增長的背景下，LoRAShop 的高效性和易用性將使其成為一個極具競爭力的產品。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-30T03:08:29.273282"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在視覺空間智能中的能力", "summary_zh": "現有的多模態大型語言模型（MLLM）在處理2D視覺任務上表現出色，但空間智能仍然是個挑戰。Spatial-MLLM提出了一個新的框架，僅使用2D圖像進行視覺空間推理。它採用雙編碼器架構，一個用於提取語義特徵，另一個基於視覺幾何模型提取3D結構特徵。通過連接器將兩者整合，強化空間理解。此外，它還使用空間感知的幀採樣策略，選擇視頻序列中對空間推理至關重要的幀。研究團隊也建立Spatial-MLLM-120k數據集，並透過監督式微調和GRPO進行模型訓練。實驗結果顯示，Spatial-MLLM在多個真實世界數據集上，於視覺空間理解和推理任務中，達到了最先進的性能。", "applications": ["**自動駕駛/機器人導航：** 透過車載鏡頭或機器人視角，即時理解周圍環境的空間結構，進行更精確的定位、路徑規劃和避障。", "**室內設計/建築規劃：** 從2D照片或影片中理解房間或建築物的空間佈局，輔助設計師進行虛擬擺設、重新規劃，或模擬不同光照下的效果。", "**運動分析/體感遊戲：** 從2D影片中分析運動員的動作軌跡和空間關係，提供運動建議或開發更真實的體感互動遊戲體驗。"], "pitch": "Spatial-MLLM解決了多模態大語言模型在空間智能上的瓶頸，使其僅需2D視覺輸入即可進行高效的3D空間理解，這開啟了廣闊的商業應用前景。想象一下，自動駕駛汽車能够更準確地感知和避讓障礙物，無需昂貴的LiDAR傳感器也能實現高精度導航；建築設計師可以根據客戶提供的房屋照片，快速進行虛擬裝修和空間優化；運動員可以通過手機錄製的訓練影片，得到個性化的指導建議。Spatial-MLLM不僅降低了硬體成本，更提升了智能系統的適應性和易用性。基於此技術，我們可以開發出針對特定行業的解決方案，如智能安防、物流管理、零售分析等，快速搶佔市場先機，創造巨大的商業價值。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-30T04:18:41.357125"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：基於修正流轉換器的免訓練多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一個利用 LoRA 模型進行多概念圖像編輯的新框架。它發現 Flux 風格的擴散轉換器中，概念特定的特徵在去噪過程早期就激活空間上連貫的區域。因此，LoRAShop 可以從先前的前向傳遞中導出每個概念的解耦潛在遮罩，並僅在概念周圍的區域內混合相應的 LoRA 權重。這種方法可以在保留全局背景、光照和細節的同時，無縫地將多個主體或風格融入到原始場景中。LoRAShop 無需重新訓練和外部約束，將個性化擴散模型轉變為實用的 'LoRA 版 Photoshop' 工具，為組合式視覺敘事和快速創意迭代開闢了新途徑。", "applications": ["**個性化電商商品展示：** 用戶上傳家居照片，LoRAShop 能夠無縫地將用戶選中的沙發、燈具等電商商品融入照片中，讓用戶直觀地看到商品擺放在家中的效果，提升購買意願。", "**虛擬服裝試穿：** 用戶上傳自拍照，LoRAShop 能夠將各種服裝款式無縫地疊加到照片上，讓用戶在家就能體驗線上試穿，減少退貨率。", "**快速生成個人化藝術作品：** 用戶提供幾張個人風格照片（例如，穿衣風格、喜歡的顏色等），LoRAShop 能夠基於這些照片生成具有用戶個人風格的藝術作品，例如頭像、壁紙等，滿足個性化定制需求。"], "pitch": "LoRAShop 是一種革命性的圖像編輯技術，無需重新訓練模型即可實現多概念圖像生成和編輯。它解決了當前生成式 AI 在圖像編輯方面，難以精確控制編輯區域和保持細節一致性的痛點。想像一下，一個 '即插即用' 的圖像編輯引擎，可以無縫地將各種風格、對象和效果整合到現有圖片中，就像使用 Photoshop 的濾鏡一樣簡單。電商、時尚、遊戲、設計等行業都將因此受益，能夠大幅降低內容創作的成本，並實現更高效的個性化定制。LoRAShop 的商業價值在於其技術的易用性、高效性和廣泛的應用前景，有望成為 AI 圖像編輯領域的領導者。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-30T04:18:57.218055"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI對齊的扭曲：偏好最佳化真的能最佳化偏好嗎？", "summary_zh": "大型語言模型在預訓練後，會基於成對比較與人類偏好對齊。現有的對齊方法，如基於PPO的RLHF和DPO，都假設模型會與單一偏好模型對齊，但實際上使用者偏好卻是多樣的。因此，這些方法是否能產生平均而言滿足使用者的模型，都是個問題。本研究借鑒社會選擇理論，使用Bradley-Terry模型建模使用者的比較，並引入了「扭曲」的概念，來衡量對齊方法的表現。扭曲定義為最佳平均效用與學習策略的平均效用之間的最差比例。研究發現，Nash Learning from Human Feedback能達到最佳的minimax扭曲，而RLHF和DPO則表現不佳，可能導致顯著的效用損失。", "applications": ["**個人化推薦系統：** 根據用戶的獨特偏好，而非基於大眾平均喜好，提供更精準的商品或內容推薦。例如，一個電影推薦系統能真正了解用戶喜歡的冷門電影類型，而非僅僅推薦熱門電影。", "**客戶服務聊天機器人：** 針對不同客戶的個性化需求，提供更有效率且令人滿意的協助。例如，一個性格急躁的客戶可能需要快速簡潔的答案，而一個需要詳細解釋的客戶則希望得到耐心且全面的回覆。", "**醫療診斷輔助：** 在考慮多種診斷方案時，能根據患者的個人病史、生活習慣和風險偏好，提供更符合其需求的治療建議。例如，在藥物選擇方面，可以根據患者對副作用的容忍度進行調整。"], "pitch": "想像一個AI不再只是迎合大眾平均喜好，而是真正理解每個人的獨特需求的世界。我們的研究揭示了現有AI對齊方法的缺陷，並提出了解決方案，使AI更能適應使用者多樣的偏好。這項技術將顛覆個人化服務，從電商到醫療，都能提供前所未有的精準度和滿意度。我們將授權企業打造真正以人為本的AI，抓住個性化需求的巨大市場，獲得競爭優勢，並在AI時代建立更牢固的客戶關係。這不僅僅是一個產品，而是一個賦予AI真正智能的基礎設施，引領我們走向一個以人為本的AI新時代。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-05-30T05:14:11.751228"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升多模態大型語言模型在基於視覺的空間智能中的能力", "summary_zh": "現有的多模態大型語言模型 (MLLM) 在 2D 視覺任務上表現出色，但在空間智能方面仍有提升空間。Spatial-MLLM 是一個新框架，僅使用 2D 視覺資料進行空間推理。它採用雙編碼器架構，一個用於提取語義特徵，另一個基於視覺幾何模型提取 3D 結構特徵。還設計了空間感知幀採樣策略，確保模型專注於對空間推理至關重要的幀。通過在 Spatial-MLLM-120k 資料集上進行訓練，該模型在各種真實世界資料集上取得了最先進的性能。", "applications": ["**自動駕駛輔助系統：** 利用安裝在車輛上的2D攝影機，即時理解周圍環境的3D空間結構，例如精準判斷障礙物的距離和相對位置，提升自動駕駛的安全性和可靠性。", "**室內導航與機器人應用：** 在無GPS信號的室內環境中，機器人透過視覺資料建立空間地圖，實現自主導航和物件識別，例如在倉庫中進行貨物分揀或在醫院中進行醫療用品配送。", "**增强現實（AR）與虛擬現實（VR）：** 透過手機或頭戴裝置的攝影機，Spatial-MLLM 可以理解使用者的空間位置和環境，將虛擬物件精準地疊加在真實世界中，提供更沉浸式的 AR/VR 體驗，例如 AR 遊戲或虛擬裝修應用。"], "pitch": "Spatial-MLLM 解決了多模態大型語言模型在理解基於視覺的空間資訊方面的瓶頸。它不需要額外的 3D 或 2.5D 資料，僅透過 2D 視覺資料就能實現強大的空間推理能力。這項技術的商業價值巨大，在自動駕駛、機器人、AR/VR等領域都有廣泛的應用前景。它能显著提升现有视觉应用的智能化水平，例如在自动驾驶中提升安全性和可靠性，在机器人领域实现更精准的室内导航，在AR/VR领域提供更逼真的体验。我们计划进一步优化模型性能，拓展应用场景，并寻求与相关行业的领导者合作，共同打造基于 Spatial-MLLM 的创新解决方案。投資 Spatial-MLLM，就是投资未来空间智能的新时代。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-30T05:14:27.255232"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用校正流轉換器進行免訓練多概念圖像生成與編輯", "summary_zh": "LoRAShop是首個使用LoRA模型進行多概念圖像編輯的框架。它基於對Flux風格擴散轉換器內部特徵交互模式的關鍵觀察：特定概念的轉換器特徵在去噪過程的早期就會激活空間上連貫的區域。LoRAShop利用這一觀察結果，在前向傳遞中為每個概念導出一個解耦的潛在遮罩，並僅在圍繞個性化概念的區域內混合相應的LoRA權重。最終的編輯能將多個主體或風格無縫地融入原始場景，同時保留全局上下文、光照和細節。實驗證明LoRAShop在身份保持方面優於基準模型。LoRAShop無需重新訓練和外部約束，將個性化擴散模型轉變為實用的「帶有LoRA的Photoshop」工具，並為組合式視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**電商產品展示增強：** 想像一下，可以快速將不同風格的家具融入到使用者家中拍攝的照片中，讓他們更直觀地看到產品的效果。無需專業攝影師，只需上傳照片，選擇家具風格，LoRAShop就能生成逼真的模擬圖。", "**個人化動漫角色創作：** 用戶可以上傳自己的照片或描述，然後指定幾個想融合的動漫風格（例如：宮崎駿、新海誠），LoRAShop就能生成一個具有個人特色的動漫人物，用於頭像、社交媒體分享或遊戲角色設計。", "**快速生成概念設計圖：** 建築師或室內設計師可以快速將不同的設計概念（例如：現代風格、復古風格、北歐風格）融入到現有的建築照片中，快速生成多種方案供客戶選擇，縮短設計週期。"], "pitch": "LoRAShop解決了個性化圖像編輯領域的一個關鍵痛點：耗時的重新訓練。 它利用校正流轉換器的特性，提供無需重新訓練即可將多個概念無縫融合到圖像中的能力，極大降低了使用門檻，並提高了效率。 其潛在商業價值巨大，體現在以下幾個方面：\n\n*   **SaaS模式圖像編輯平台：** 可以將LoRAShop技術整合到線上圖像編輯平台，提供個性化、高效的圖像編輯服務，面向電商、設計、娛樂等行業。\n*   **API服務集成：** 將LoRAShop作為API提供給其他應用程序，例如電商平台、遊戲開發引擎、社交媒體應用，讓它們具備個性化圖像生成和編輯的能力。\n*   **授權和IP合作：** 可以與現有的設計軟體公司（例如：Adobe）合作，將LoRAShop技術授權給他們，提升其產品的競爭力。 此外，可以與知名IP合作，推出獨特的個性化圖像生成服務，例如生成特定動漫風格的個人形象。\n\nLoRAShop具備技術領先性、易用性和廣泛的應用場景，有望在個性化圖像編輯市場中取得領先地位，創造可觀的商業價值。 其免訓練的特性將會顛覆現有的圖像編輯流程，大幅降低成本，提高效率，並為用戶提供前所未有的創意空間。 在快速發展的AI圖像生成領域，LoRAShop無疑是一項極具潛力的投資。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-30T05:14:49.510760"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI對齊的扭曲：偏好優化是否真的優化了偏好？", "summary_zh": "大型語言模型在預訓練後會根據成對比較與人類偏好對齊。現有對齊方法（如基於PPO的RLHF和DPO）建立在與單一偏好模型對齊的假設上，但實際應用場景中用戶偏好多樣。因此，這些對齊方法是否能讓用戶平均滿意都尚不清楚，這對於多元對齊來說是最基本的要求。本文借鑒社會選擇理論，並透過個人Bradley-Terry (BT) 模型來模擬用戶比較，引入了對齊方法的扭曲概念：最佳可實現平均效用與學習到的策略的平均效用之間的最壞情況比率。這種扭曲概念有助於區分不同的對齊方法：基於人類回饋的納什學習(Nash Learning from Human Feedback) 在效用分布、比較對分布以及與參考策略的可容許KL散度方面都實現了minimax最佳扭曲，而RLHF和DPO則遭受顯著更大的扭曲，甚至可能是無界的。", "applications": ["**個人化推薦系統：** 優化購物、影音或新聞推薦，不再僅僅基於單一偏好模型，而是考量用戶群體的多元偏好，提供更精準且更符合個別需求的推薦。", "**產品設計與市場調研：** 透過了解不同用戶群體的偏好分布，協助產品設計團隊在功能、外觀或定價方面做出更周全的決策，提升產品的市場接受度。", "**政治議題與政策制定：** 在政策制定過程中，藉由分析不同群體的偏好，可以更有效地評估政策的影響，並制定更具包容性和公平性的政策方案，降低群體間的對立和不滿。"], "pitch": "我們的研究揭示了現行AI對齊方法在處理多元用戶偏好時的根本缺陷，導致AI的行為與用戶的期望存在嚴重偏差。我們提出的扭曲概念能夠量化這種偏差，並幫助開發更可靠、更公平的AI系統。基於納什學習的對齊方法展現了卓越的魯棒性和效能，能夠在各種複雜情境下有效處理多元偏好。這項技術的商業潛力巨大，可以應用於各行各業，包括個人化推薦、產品設計、政治決策等。通過投資我們的技術，您可以開發出真正以人為本的AI產品，提高用戶滿意度，建立更強大的品牌忠誠度，並在競爭激烈的市場中脫穎而出。我們提供的不僅僅是AI對齊技術，更是一個能夠顯著提升您的商業價值和社會影響力的戰略投資機會。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-05-30T06:20:07.590441"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在基於視覺的空間智能方面的能力", "summary_zh": "這篇論文提出Spatial-MLLM，一種新穎的框架，專注於純粹從2D視覺觀察進行空間推理。不同於傳統依賴3D或2.5D數據的3D MLLM，Spatial-MLLM 利用視覺幾何基礎模型的結構先驗，透過雙編碼器架構（一個提取語義特徵，另一個提取3D結構特徵）以及空間感知的幀採樣策略，有效提升了模型在僅有2D輸入（如圖像或視頻）下的空間理解能力。研究團隊還構建了一個名為 Spatial-MLLM-120k 的數據集，並使用監督微調和 GRPO 進行訓練。實驗證明，Spatial-MLLM 在各種基於視覺的空間理解和推理任務中都取得了最先進的性能。", "applications": ["**自動駕駛導航：** 基於車載攝像頭的2D圖像和視頻，無需額外LiDAR等3D傳感器，即可更精準地理解周圍環境的空間佈局，提升導航準確性。", "**建築工地安全監控：** 分析工地攝像頭拍攝的視頻，自動識別潛在的安全隱患，例如工人是否處於危險區域，預防事故發生。", "**室內機器人：** 使機器人僅憑藉攝像頭拍攝的2D影像，就能夠理解房間的佈局和物體的位置，從而更好地完成清潔、搬運等任務。"], "pitch": "Spatial-MLLM 解決了現有MLLM在僅有2D視覺數據下進行空間推理的瓶頸。其核心創新在於利用視覺幾何基礎模型的結構先驗，大幅提升了空間理解能力，無需額外3D或2.5D數據。這代表著更低的成本、更廣泛的應用場景。我們預計在自動駕駛、安全監控、機器人等領域擁有巨大的商業潛力。Spatial-MLLM 可以幫助企業降低對昂貴3D傳感器的依賴，同時提供更智能、更可靠的空間感知能力。透過授權技術、提供API服務，以及針對特定行業提供客製化解決方案，我們可以快速佔領市場，並創造高額回報。我們相信 Spatial-MLLM 將引領下一代視覺智能的發展，成為該領域的領先者。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-30T06:20:21.409114"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：基於校正流變換器的免訓練多概念圖像生成與編輯", "summary_zh": "LoRAShop是一種新的圖像編輯框架，使用LoRA模型進行多概念編輯。它利用了Flux風格擴散變換器中的特徵交互模式：概念特定的變換器特徵會在去噪過程的早期激活空間上連貫的區域。LoRAShop據此推導出每個概念的解耦潛在遮罩，並僅在包含要個性化概念的區域內混合相應的LoRA權重。這樣可以將多個主體或風格無縫集成到原始場景中，同時保留全局上下文、光照和細節。LoRAShop無需重新訓練和外部約束，將個性化擴散模型變成一個實用的'LoRA版Photoshop'工具，為組合視覺敘事和快速創意迭代開闢了新途徑。", "applications": ["**個人化商品設計：**使用者可以輕鬆將自己的寵物、家人或愛好融入手機殼、T恤、馬克杯等商品的設計中，無需專業設計技能。", "**虛擬試穿與場景模擬：**在電商平台中，使用者可以將服裝、家具等物品快速融入自己的照片或模擬場景中，預覽效果，提升購物體驗。", "**遊戲角色與場景客製化：**遊戲開發者或玩家可以利用LoRAShop快速生成具有個人特色的遊戲角色、服裝和場景，增加遊戲的沉浸感和可玩性。"], "pitch": "LoRAShop是下一代圖像編輯工具，它重新定義了創意內容的生成方式。想像一下，一個Photoshop等級的工具，但使用者無需任何專業訓練即可輕鬆將多個概念融入圖像，創造出獨一無二的作品。LoRAShop的免訓練特性大幅降低了使用門檻，使個人化內容創作成為可能。我們相信，LoRAShop將顛覆圖像編輯、電商、遊戲等領域。其商業價值體現在：\n\n*   **高度使用者黏性：**易用性和個人化功能吸引大量使用者，提高平台活躍度。\n*   **新型廣告模式：**品牌可以提供個人化廣告體驗，提高廣告效益。\n*   **授權模式：**將技術授權給電商平台、遊戲公司等，獲取授權費用。\n\n我們正在尋找投資者，共同將LoRAShop打造成視覺內容創作領域的領導者。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-30T06:20:38.205033"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流變換器實現免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop是一個免訓練的多概念圖像編輯框架，它基於對Flux風格擴散變換器內部特徵交互模式的關鍵觀察：概念特定的變換器特徵在去噪過程的早期階段會激活空間上連貫的區域。LoRAShop利用此觀察結果，為每個概念導出一個解耦的潛在掩碼，並僅在限定概念的區域內混合相應的LoRA權重。這使得多個主體或風格可以無縫地整合到原始場景中，同時保留全局上下文、光照和精細細節。實驗證明LoRAShop在身份保留方面優於基準方法。透過消除重新訓練和外部約束，LoRAShop將個性化的擴散模型轉變為一個實用的「基於LoRA的Photoshop」工具，並為組合視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**快速客製化廣告素材：** 行銷團隊可以快速將產品融入不同的情境和風格，產生大量個性化的廣告素材，例如將汽車放置在不同的風景中，或將服裝設計應用於不同的人物模型上，而無需重新訓練模型。", "**個性化藝術創作工具：**藝術家可以利用LoRAShop輕鬆將不同的藝術風格和元素融合到自己的作品中，例如將特定畫家的風格融入照片中，或將不同的角色形象合併到一幅畫中，促進藝術創作的實驗和探索。", "**虛擬試穿與造型設計：** 電商平台可以利用LoRAShop讓顧客上傳自己的照片，並試穿不同的服裝和配件，或是改變髮型和妝容，提供更直觀和個性化的購物體驗，提升購買意願。"], "pitch": "LoRAShop解決了現有圖像編輯工具需要大量訓練和缺乏靈活性的問題，提供了一個免訓練、高效且易於使用的多概念圖像編輯方案。其核心優勢在於能夠精確控制不同概念的融合，同時保留原始圖像的細節和上下文。這使其在廣告、藝術、電商等領域具有巨大的商業潛力。例如，可以大幅降低廣告素材的製作成本，提高藝術創作的效率，並為電商平台提供更具吸引力的用戶體驗。LoRAShop有潛力成為下一代圖像編輯工具的領跑者，並重新定義圖像創作和應用的方式。我們相信，通過持續的研發和商業化，LoRAShop將創造可觀的市場價值。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-30T07:13:36.832350"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在基於視覺的空間智能方面的能力", "summary_zh": "本論文提出Spatial-MLLM，一個全新的框架，專注於僅從2D視覺觀察進行空間推理。不同於依賴額外3D或2.5D數據的現有3D MLLM，Spatial-MLLM利用視覺幾何基礎模型中的強結構先驗知識，採用雙編碼器架構：一個提取語義特徵的預訓練2D視覺編碼器，以及一個從視覺幾何模型主幹初始化的空間編碼器，用以提取3D結構特徵。透過連接器整合兩種特徵，形成統一的視覺令牌，以增強空間理解。此外，論文還提出空間感知幀採樣策略，在推理時選擇視頻序列中空間信息量最大的幀，確保即使在有限的令牌長度下，模型也能專注於對空間推理至關重要的幀。論文並建構Spatial-MLLM-120k數據集，並使用監督式微調和GRPO進行訓練。在各種真實世界數據集上的廣泛實驗表明，Spatial-MLLM在各種基於視覺的空間理解和推理任務中，均達到最先進的性能。", "applications": ["自動駕駛導航：利用車載攝像頭的2D影像，即時構建周圍環境的3D空間結構，輔助車輛做出更精確的導航決策，尤其是在缺乏高精度地圖的區域。", "室內設計輔助：用戶通過手機拍攝房間照片，Spatial-MLLM就能理解房間的空間佈局，並能根據用戶需求，虛擬地擺放傢俱或調整裝修風格，提供直觀的設計方案。", "運動訓練分析：分析運動員的2D視頻，重構其運動軌跡和姿勢的3D空間信息，為教練提供更精準的訓練建議，提升運動表現。"], "pitch": "Spatial-MLLM解决了多模態大語言模型(MLLM)在仅有2D视觉输入时，空间智能不足的问题。它利用创新的双编码器架构和空间感知帧采样策略，大幅提升了模型对2D图像或视频中空间信息的理解和推理能力。我们构建了大规模数据集并进行了充分的验证，证明了Spatial-MLLM的优越性能。这技术在自动驾驶、机器人、室内设计等领域有广泛的应用前景。我们的竞争优势在于不依赖额外的3D或2.5D数据，只需2D视觉输入，就能提供强大的空间理解能力，降低了部署成本和难度。我们正在寻求融资，以进一步扩大数据集规模，提升模型性能，并将其应用于更多行业，建立市场领先地位。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-30T08:18:37.577645"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：基於修正流轉換器的免訓練多概念圖像生成與編輯", "summary_zh": "LoRAShop是一個利用LoRA模型進行多概念圖像編輯的新框架。它觀察到擴散轉換器中，特定概念的特徵會在去噪過程的早期激活空間連貫的區域。LoRAShop利用這個發現，在先前的前向傳播中為每個概念導出解耦的潛在遮罩，並且只在概念的邊界區域內混合相應的LoRA權重。這樣產生的編輯能夠將多個主體或風格無縫地整合到原始場景中，同時保留全局上下文、光照和細節。實驗表明，LoRAShop比基線方法更好地保持了身份一致性。通過消除重新訓練和外部約束，LoRAShop將個性化的擴散模型變成一個實用的「LoRA版Photoshop」工具，並為組合視覺故事講述和快速創意迭代開闢了新的途徑。", "applications": ["**個性化商品定製：** 讓消費者可以上傳多張不同風格或人物的照片，快速生成融合這些元素的新商品圖片，例如將寵物和特定風格的背景融合製作個性化手機殼或T恤。", "**電影/遊戲場景設計：** 設計師可以快速將多個角色或物品整合到一個背景中，創造多樣化的場景概念圖，加速視覺化設計流程。", "**教育應用：** 讓學生可以透過簡單的操作，將不同歷史人物、事件或科學概念融合到同一張圖片中，創造視覺化的學習材料，提升學習的趣味性和理解度。"], "pitch": "LoRAShop解決了傳統圖像編輯中複雜且耗時的問題，讓使用者能以極低的成本，快速生成高品質、高度客製化的圖像。這不僅能大幅提升設計效率，更能創造全新的商業模式。想像一下，一個電商平台可以讓消費者直接在線上編輯產品圖片，個性化設計獨一無二的商品，大幅提升購買意願。此外，LoRAShop簡化的流程和無需重新訓練的特性，降低了AI圖像生成的門檻，讓更多人能夠輕鬆運用這項技術，開拓無限的可能性。投資LoRAShop，就是投資一個易於使用、應用廣泛且具備巨大潛力的AI圖像編輯工具，預計能在電商、遊戲、設計等領域掀起一場革命。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-30T08:18:53.155932"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI對齊的失真：偏好優化真的優化了偏好嗎？", "summary_zh": "現今大型語言模型在經過預訓練後，會基於成對比較來與人類偏好對齊。然而，主流對齊方法（如基於PPO的RLHF和DPO）假設與單一偏好模型對齊，忽略了用戶偏好的多樣性。這篇論文指出，這些方法甚至無法保證產生的模型能滿足用戶的平均偏好。透過社會選擇理論，作者定義了對齊方法的「失真」：最佳平均效用與學習策略平均效用之間的最差情況比率。研究表明，Nash Learning from Human Feedback 在多種情況下都能達到最佳的最小最大失真，而 RLHF 和 DPO 則可能遭受嚴重的失真。", "applications": ["**個人化推薦系統：** 根據用戶過去的選擇和回饋，調整推薦的演算法，使其更符合用戶的個人口味，避免一刀切的推薦結果。例如，一個音樂 App 可以根據用戶對不同歌曲的評價，調整歌曲推薦引擎，讓用戶更常聽到喜歡的音樂。", "**醫療診斷輔助：** 不同醫生可能對同一病症有不同的診斷方法和治療方案。AI 可以學習多位醫生的經驗，並根據患者的具體情況和醫生的偏好，提供更全面的診斷建議。", "**自動駕駛決策：** 在複雜的交通狀況下，不同的駕駛者可能對安全性和效率有不同的權衡。AI 可以學習不同駕駛者的偏好，並根據駕駛者的設定和當前路況，做出更符合駕駛者意願的決策。"], "pitch": "這篇論文揭示了現有AI對齊方法在用戶偏好多樣性下的局限性，並提出了一種更穩健的解決方案。這為開發真正能滿足用戶需求的AI系統提供了理論基礎。商業價值體現在：\n\n*   **提高用戶滿意度：** 通過更精準地對齊用戶偏好，可以顯著提高產品和服務的用戶滿意度和留存率。\n*   **降低產品開發成本：** 減少因錯誤對齊導致的迭代和修正，從而降低產品開發成本。\n*   **拓展應用場景：** 更穩健的對齊方法可以將AI應用到更廣泛的領域，例如個人化醫療、自動駕駛等。\n\n我們的投資方向是那些致力於開發更具彈性和適應性的對齊方法，並且能夠在真實世界中大規模部署的團隊。我們相信，這種技術將在AI領域產生深遠的影響，並帶來巨大的商業價值。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-05-30T09:14:31.801971"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在基於視覺的空間智能方面的能力", "summary_zh": "本論文提出Spatial-MLLM，一個從純2D視覺資料進行空間推理的新框架。不同於以往需要額外3D資訊的3D MLLM，Spatial-MLLM利用預訓練的視覺幾何模型，結合雙編碼器架構，分別提取語義特徵和3D結構特徵，並使用空間感知幀抽樣策略，在token長度有限的情況下，選取對於空間推理至關重要的幀。透過在Spatial-MLLM-120k數據集上的訓練，模型在各種現實世界的數據集上，展現了基於視覺的空間理解和推理任務中的最先進性能。", "applications": ["**自動駕駛/機器人導航：** 讓自動駕駛汽車或機器人能夠僅憑攝影機影像，更準確地理解周遭環境的空間結構，提高導航的安全性與效率。", "**室內設計/建築規劃：** 根據2D照片或影片，快速生成3D模型，協助設計師或建築師進行空間規劃、虛擬導覽，或是評估現有空間的改裝方案。", "**虛擬實境/擴增實境：** 創建更逼真的VR/AR體驗，使用者可以透過手機或頭戴裝置，與數位世界中的空間進行更自然的互動，例如在虛擬家中擺設家具。"], "pitch": "Spatial-MLLM 解決了多模態大型語言模型（MLLM）在僅有2D視覺輸入時，空間理解能力不足的問題，開創了基於視覺的空間智能新紀元。我們的雙編碼器架構和空間感知幀抽樣策略，讓模型能夠從圖像和影片中提取豐富的3D結構信息，實現更精確的空間推理。市場潛力巨大，應用範圍廣泛，涵蓋自動駕駛、機器人、室內設計、VR/AR等領域。相較於依賴額外3D數據的競品，Spatial-MLLM 更具成本效益和通用性。透過授權、SDK、或提供SaaS服務，我們可以快速搶佔市場，成為基於視覺空間智能領域的領導者，預計在未來三年內達到數億美元的市場規模，為投資者帶來豐厚回報。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-30T09:14:47.291710"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：基於修正流變換器的免訓練多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一個利用 LoRA 模型進行多概念圖像編輯的全新框架。它基於對 Flux 風格擴散變換器內部特徵交互模式的觀察：特定概念的變換器特徵在去噪過程早期激活空間相干區域。LoRAShop 利用這一點，在先前的正向傳遞中為每個概念導出一個解耦的潛在遮罩，並僅在概念周圍的區域內混合相應的 LoRA 權重，從而將多個主體或風格無縫整合到原始場景中，同時保留全局上下文、光照和細節。LoRAShop無需重新訓練和外部約束，將個性化擴散模型轉變為實用的「基於 LoRA 的 Photoshop」工具，並為組合式視覺敘事和快速創意迭代開闢了新途徑。", "applications": ["**個性化頭像生成：** 使用者可以上傳自己的照片，LoRAShop 可以輕鬆將其與不同的藝術風格或服裝搭配，生成獨一無二的頭像，無需複雜的操作。", "**產品情境化展示：** 電商商家可以將商品照片與不同的背景或環境融合，快速生成多樣化的商品展示圖，例如將沙發放置在不同風格的客廳中。", "**快速概念設計迭代：** 設計師可以使用 LoRAShop 快速生成不同風格或元素的設計草圖，例如在建築設計中快速嘗試不同的外觀風格，加速設計流程。"], "pitch": "LoRAShop 解決了傳統圖像編輯方法在多概念融合方面的挑戰，提供了一個無需重新訓練的、高效率的解決方案。其商業價值體現在多個方面：首先，大幅降低了個性化圖像生成和編輯的門檻，面向更廣泛的使用者群體。其次，其快速迭代和靈活組合的能力，賦能創意工作者和企業更高效地進行內容創作和產品設計。第三，LoRAShop 可以作為圖像處理引擎整合到現有的設計軟件、電商平台或社交媒體應用中，創造巨大的商業潛力。我們相信 LoRAShop 將引領圖像編輯領域的變革，成為未來內容創作的重要工具。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-30T09:15:02.293477"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升基於視覺的空間智能中多模態大型語言模型的能力", "summary_zh": "Spatial-MLLM 是一種新的框架，旨在提升多模態大型語言模型（MLLM）在僅有 2D 圖像或影片輸入情況下的空間推理能力。它使用雙編碼器架構，一個提取語義特徵，另一個基於視覺幾何模型提取 3D 結構特徵，並結合空間感知的幀採樣策略，專注於對空間推理至關重要的幀。透過新的數據集和訓練方法，Spatial-MLLM 在各種真實世界的數據集上，於基於視覺的空間理解和推理任務中，展現了最先進的性能。", "applications": ["**自動駕駛導航：** 透過車載攝影機的影像，Spatial-MLLM 可以理解道路的幾何結構和車輛周圍的空間關係，提升自動駕駛的導航能力，例如預測潛在的碰撞風險或優化行駛路線。", "**AR/VR 環境互動：** 在擴增實境或虛擬實境應用中，Spatial-MLLM 可以分析使用者透過頭戴裝置看到的場景，讓虛擬物件更真實地與真實環境互動，例如正確地將虛擬家具放置在房間中，或是在遊戲中實現更精確的空間定位。", "**建築設計與工程：** 透過現有建築的 2D 照片或影片，Spatial-MLLM 可以推斷建築物的 3D 結構，協助建築師和工程師進行設計修改、結構分析或進行虛擬實境模擬，大幅提升工作效率。"], "pitch": "Spatial-MLLM 解決了多模態大型語言模型在 2D 視覺資料中缺乏空間推理能力的痛點，開啟了廣闊的商業機會。想像一下，僅需現有照片和影片，就能重建複雜的 3D 環境，應用範圍涵蓋自動駕駛、AR/VR、建築設計，甚至機器人導航。相較於依賴昂貴的 3D 掃描或深度感測器，Spatial-MLLM 提供了更具成本效益且可擴展的解決方案。我們的技術能大幅降低各行業的開發成本，加速創新，並創造全新的商業模式，例如基於視覺的空間數據分析服務、虛擬導覽解決方案，以及更智能的自動化系統。 我們尋求投資，以擴大數據集規模，優化模型性能，並將 Spatial-MLLM 整合到各行業的應用中，成為基於視覺的空間智能領域的領導者。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-30T10:14:35.714009"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：利用修正流變換器實現免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop是一個基於LoRA模型的圖像編輯框架，無需重新訓練即可編輯多個概念。它利用了Flux風格擴散變換器中的特徵互動模式，發現概念特定的特徵會在降噪過程早期激活空間上連貫的區域。因此，LoRAShop可以在前向傳遞中為每個概念提取解耦的潛在遮罩，並僅在概念邊界區域內混合相應的LoRA權重。這樣可以在保留全局上下文、光照和細節的同時，將多個主題或風格無縫集成到原始場景中。LoRAShop相較於其他方法能更好地保留身份特徵，並將個性化的擴散模型變成一個實用的『LoRA版本的Photoshop』工具，為組合視覺敘事和快速創意迭代開闢了新途徑。", "applications": ["**虛擬試穿：** 用戶可以將不同款式的衣服“穿”在自己的照片上，快速查看搭配效果，無需實際試穿。", "**產品設計原型：** 設計師可以快速將不同的設計元素（例如顏色、材質、形狀）組合到產品模型上，生成多個原型方案進行比較和選擇。", "**個性化藝術創作：** 用戶可以將自己喜歡的藝術風格（例如梵谷、莫內）應用到自己的照片上，創作獨一無二的藝術作品。"], "pitch": "LoRAShop正在徹底改變圖像編輯和生成。它免去了傳統方法所需的耗時的重新訓練，讓用戶能夠前所未有地輕鬆地將多個概念無縫集成到圖像中。想像一下：一個平台，讓任何人都能夠像專業設計師一樣創建高度個性化的視覺內容。我們的商業價值來自於幾個關鍵領域：首先，簡化了設計流程，降低了設計門檻，使個人和企業都能夠快速迭代並創建引人注目的視覺內容。其次，基於LoRA技術的圖像定制能力打開了電子商務、廣告和社交媒體的新機會。最後，通過提供一種低成本、易於使用的解決方案，LoRAShop有潛力佔領個性化圖像生成市場的領導地位。我們相信LoRAShop將成為創意表達的強大工具，並為用戶帶來巨大的價值。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-30T10:14:51.178439"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI對齊的扭曲：偏好優化真的能優化偏好嗎？", "summary_zh": "大型語言模型在預訓練後，會基於成對比較與人類偏好對齊。然而，現有的對齊方法（如基於PPO的RLHF和DPO）假設模型要與單一偏好模型對齊，但實際上使用者偏好是多樣的。因此，這些對齊方法是否能產出平均而言滿足使用者的模型，仍是個問題。本論文借鑒社會選擇理論，透過Bradley-Terry模型模擬使用者比較，引入了對齊方法的「扭曲」概念，即最佳可實現的平均效用與學習策略平均效用之間的最差情況比率。研究表明，Nash Learning from Human Feedback在各種情況下都表現出最佳的扭曲，而RLHF和DPO則表現出較差的扭曲，甚至可能無限大。", "applications": ["**個性化推薦系統：** 根據每個使用者的獨特偏好調整內容推薦，而非僅僅基於群體平均。例如，針對音樂、電影或新聞的推薦。", "**客戶服務機器人：** 開發能根據不同客戶的情緒、溝通風格和問題複雜性調整互動方式的客戶服務機器人。避免因單一標準回答方式造成客戶不滿。", "**醫療決策輔助系統：** 協助醫生根據患者的個人病史、生活方式和價值觀，制定更個性化的治療方案。避免過度簡化的通用治療方案忽略了患者的獨特性。"], "pitch": "現有的AI對齊方法在處理多樣化偏好時存在嚴重問題，導致模型效果不佳甚至產生負面影響。我們提出的「扭曲」概念及其分析方法，揭示了這些方法的根本缺陷。通過Nash Learning from Human Feedback，我們提供了一種更健壯且能夠適應使用者多樣化偏好的解決方案。想像一下，一個能真正理解並滿足每個使用者獨特需求的AI助手，而非千篇一律的產品。這將在個性化推薦、客戶服務和醫療決策等領域帶來顛覆性的變革，創造巨大的商業價值。我們正在尋找投資者，共同打造下一代真正以人為本的AI系統。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-05-30T11:11:27.680648"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在基於視覺的空間智能方面的能力", "summary_zh": "現有的多模態大型語言模型（MLLM）在2D視覺任務上表現出色，但在空間智能方面仍有提升空間。這篇論文提出了Spatial-MLLM，一個僅基於2D視覺資訊進行空間推理的新框架。與傳統依賴CLIP的MLLM不同，Spatial-MLLM利用視覺幾何基礎模型提取3D結構特徵，並結合2D語義特徵，形成雙編碼器架構。同時，論文還提出了一種空間感知的幀採樣策略，確保模型在有限的token長度下，能專注於對空間推理至關重要的幀。論文構建了Spatial-MLLM-120k數據集，並通過監督式微調和GRPO進行訓練。實驗結果表明，Spatial-MLLM在各種基於視覺的空間理解和推理任務中，達到了最先進的性能。", "applications": ["導航輔助系統：基於手機攝像頭的影像，即時推斷使用者與周圍環境的空間關係，提供更精準的步行或駕駛導航，例如：告訴你「前方5公尺左轉」或「注意前方行人正在過馬路」。", "智能家居控制：根據掃地機器人或監控攝影機提供的影像，理解房間的空間佈局，實現更智能化的控制，例如：自動識別並避開障礙物，或根據人員位置調整燈光和溫度。", "AR遊戲/應用：增強現實應用能更精準地將虛擬物件放置在真實世界中，並模擬更真實的物理互動，例如：讓虛擬寵物能自然地繞過桌腳，或在AR遊戲中建立更複雜的空間解謎。", "無人機視覺巡檢：讓無人機能夠僅僅依靠視覺資訊就能夠更智能的識別複雜的空間環境，比如巡檢橋樑，電力線等等，不再需要額外的定位或者雷射雷達等設備就能完成任務"], "pitch": "Spatial-MLLM解決了多模態大型語言模型在空間理解方面的瓶頸，僅需2D視覺輸入即可實現強大的空間推理能力。這項技術具有廣泛的應用前景，例如：導航、智能家居、AR/VR、機器人等。我們預計市場對無需額外感測器的視覺空間智能解決方案的需求將持續增長。透過授權Spatial-MLLM技術或將其整合到現有產品中，我們能夠快速進入這些市場，並建立起領導地位。我們的模型在性能上優於現有方案，且降低了硬體成本，具有顯著的商業優勢，將為投資者帶來豐厚的回報。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-30T11:11:44.903731"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：利用修正流變換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop是第一個使用LoRA模型進行多概念圖像編輯的框架。它基於一個關鍵發現：在Flux風格的擴散變換器中，概念特定的變換器特徵在去噪過程的早期就激活了空間上連貫的區域。LoRAShop利用這一發現，在先前的正向傳遞中為每個概念導出解耦的潛在遮罩，並僅在限定概念的區域內混合相應的LoRA權重。這樣生成的編輯可以將多個主題或風格無縫地整合到原始場景中，同時保留全局上下文、光照和精細細節。實驗表明，LoRAShop比基線方法更能保持身份一致性。通過消除重新訓練和外部約束，LoRAShop將個性化擴散模型轉變為實用的“LoRA版Photoshop”工具，並為組合視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**客製化頭像快速生成：** 用戶上傳幾張照片後，即可快速生成具有特定風格（例如卡通、油畫）或融入特定元素（例如戴著帽子、穿著特定服裝）的個人頭像。", "**產品情境化廣告生成：** 廣告商可以輕鬆地將產品融入各種生活場景中，無需專業攝影師或複雜的建模流程，快速生成不同風格的廣告素材。", "**室內設計快速預覽：** 用戶可以將家具或裝飾品的圖片添加到現有的房間照片中，快速預覽擺放效果，並嘗試不同的風格搭配。"], "pitch": "LoRAShop 解決了個性化圖像生成和編輯領域的關鍵痛點：無需耗時耗力的重新訓練模型。透過獨特的空間解耦方法，LoRAShop 能夠在現有圖像上快速、精準地融入多個概念，極大提升了效率和靈活性。想像一下，設計師、行銷人員、甚至普通用戶，都能以極低的成本和極高的效率，創造出個性化、定制化的視覺內容。這將徹底改變圖像生成領域的遊戲規則，擁有巨大的商業潛力。我們的初步驗證已經證明了LoRAShop優於現有方法的性能，以及它在各種應用場景中的可行性。 我們相信 LoRAShop 將成為圖像創作領域的革命性工具，為我們帶來可觀的回報。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-30T11:12:01.514376"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI對齊的失真：偏好優化是否真的優化了偏好？", "summary_zh": "大型語言模型在預訓練後會根據成對比較與人類偏好對齊。現有對齊方法（如基於PPO的RLHF和DPO）假設與單一偏好模型對齊，但實際應用場景中，用戶偏好各異。研究表明，這些方法未必能讓模型平均而言滿足用戶需求，這對於多元對齊來說是最低要求。本研究借鑒社會選擇理論，通過個別的Bradley-Terry模型模擬用戶比較，引入了對齊方法的「失真」概念，即最佳可實現平均效用與學習到的策略平均效用之間的最差情況比率。研究發現，Nash Learning from Human Feedback 實現了 minimax 最優失真，而 RLHF 和 DPO 則可能遭受顯著甚至無限的失真。", "applications": ["**個性化教育平台：** 根據學生不同的學習風格和偏好，動態調整教學內容和方式，避免使用對某些學生不友好的單一教學模型，提升學習效率和滿意度。", "**智能客服系統：** 能夠針對不同用戶的需求和情緒，提供更貼心、更有效的問題解決方案，避免使用通用的客服腳本導致用戶體驗下降。", "**內容推薦引擎：** 更精準地推薦用戶可能感興趣的內容，避免過度推送同質化內容，提升用戶黏著度和平台價值。"], "pitch": "我們開發了一種全新的AI對齊方法，能夠有效解決現有方法在多元偏好下的「失真」問題。這意味著我們的技術可以幫助企業構建更加個性化、更加有效的AI產品，例如：更智能的教育平台、更貼心的客服系統、更精準的內容推薦引擎。與競爭對手相比，我們的技術能夠顯著提升用戶滿意度、用戶黏著度和產品商業價值。我們正在尋求種子輪融資，用於進一步完善技術並擴大市場規模，我們相信這將是一個巨大的潛在市場，並為投資者帶來豐厚的回報。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-05-30T12:26:22.661059"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在視覺空間智能方面的能力", "summary_zh": "本研究提出Spatial-MLLM，一個僅從2D視覺資訊中進行空間推理的新框架。與傳統依賴額外3D或2.5D資料的模型不同，Spatial-MLLM利用預訓練的視覺幾何基礎模型來提取3D結構特徵，並結合2D視覺編碼器提取的語義特徵，強化模型對空間的理解。此外，提出空間感知的幀採樣策略，確保模型在有限的token長度下，專注於空間推理關鍵幀。透過在Spatial-MLLM-120k資料集上進行訓練，實驗證明Spatial-MLLM在各種視覺空間理解和推理任務中表現出色。", "applications": ["**自動駕駛：** 分析車載攝像頭拍攝的2D影像，理解車輛周圍的3D空間結構，例如預測行人路徑、識別交通標誌，提高行車安全。", "**室內導航：** 透過手機相機掃描室內環境，建立空間模型，為視障人士或方向感不佳的人提供更精準的導航服務。", "**AR/VR應用：** 在AR/VR環境中，利用Spatial-MLLM增強虛擬物件與現實環境的互動，例如將虛擬家具擺放在真實房間中，並根據房間結構進行調整，提供更沉浸式的體驗。"], "pitch": "Spatial-MLLM解決了多模態大型語言模型(MLLM)在僅有2D視覺輸入下進行空間推理的瓶頸。其潛在商業價值巨大，尤其是在自動駕駛、智慧城市、AR/VR等領域。相較於傳統需要額外3D感測器的解決方案，Spatial-MLLM成本更低、部署更簡便，且能更有效率地利用現有的2D影像資料。我們預計Spatial-MLLM將引領一波基於視覺的空間智能應用浪潮，在智慧生活、工業自動化等領域開創新的商業模式。透過我們的技術，企業可以更輕鬆地開發出更智能、更具空間感知能力的產品和服務，在市場上取得領先優勢。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-30T12:26:38.980554"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：基於修正流轉換器的免訓練多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一個使用 LoRA 模型進行多概念圖像編輯的新框架。它基於對 Flux 風格擴散轉換器內部特徵交互模式的觀察，發現概念特定的轉換器特徵在去噪過程的早期階段會激活空間連貫的區域。LoRAShop 利用這個觀察，為每個概念導出一個解耦的潛在遮罩，並僅在限定概念的區域內混合相應的 LoRA 權重。這種方法能將多個主體或風格無縫整合到原始場景中，同時保留全局上下文、光照和細節。實驗表明，LoRAShop 在身份保留方面優於基線方法。透過消除重新訓練和外部約束，LoRAShop 將個人化的擴散模型變成了一個實用的“使用 LoRA 的 Photoshop”工具，並為組合視覺敘事和快速創意迭代開闢了新途徑。", "applications": ["**個性化商品設計：** 用戶上傳產品照片，然後添加想要的圖案、文字、顏色等LoRA風格，快速生成客製化的商品預覽圖，例如印有特定風格寵物頭像的T恤或特定設計的杯子。", "**虛擬妝容試用：** 在照片上添加不同的妝容風格（例如，性感風、可愛風、哥特風），用戶可以快速查看不同妝容風格的效果，無需實際化妝，方便選購適合自己的化妝品。", "**創意圖像合成：** 將不同的 LoRA 風格的角色或物品輕鬆添加到現有圖片中，例如將動畫角色融入真實場景，或者將不同藝術風格的建築物拼接到一起，創造獨特且富有個性的圖像。"], "pitch": "LoRAShop 解決了圖像編輯領域中個性化與效率之間的痛點。現有的圖像編輯工具複雜且需要專業技能，而 LoRAShop 透過免訓練的方式，讓用戶能夠以極低的門檻輕鬆實現多概念圖像編輯。它的主要商業價值在於：\n\n1.  **大幅降低圖像生成和編輯的成本：** 無需耗時的重新訓練過程，節省了大量計算資源和人力成本。\n2.  **賦能內容創作者和普通用戶：** 簡化了圖像編輯流程，讓更多人參與到內容創作中來，擴大了市場潛力。\n3.  **開啟了個性化和定制化應用的新篇章：** 在電商、社交媒體、廣告營銷等領域具有廣闊的應用前景。例如，可以快速生成個性化的廣告素材，提升廣告的点击率和转化率。\n\n我們相信 LoRAShop 具有顛覆圖像編輯行業的潛力，有望成為下一代圖像編輯工具的標竿。我們尋求投資，以加速產品開發和市場推廣，將 LoRAShop 打造為一個全球領先的圖像編輯平台。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-30T12:27:01.165244"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在基於視覺的空間智能中的能力", "summary_zh": "這篇論文介紹了一個名為 Spatial-MLLM 的新框架，旨在提升多模態大型語言模型（MLLMs）在僅基於2D視覺輸入的空間推理能力。它使用雙編碼器架構，一個提取語義特徵，另一個從視覺幾何基礎模型的骨幹初始化，提取3D結構特徵。此外，還提出了一種空間感知的幀採樣策略，以確保模型專注於對空間推理至關重要的幀。透過在Spatial-MLLM-120k數據集上進行訓練，該模型在各種真實世界數據集上展現了最先進的視覺空間理解和推理性能。", "applications": ["**自動駕駛輔助：** 根據車載攝像頭的2D圖像，更準確地判斷周圍環境的3D結構和空間關係，提高避障和導航的可靠性。", "**智慧家庭：** 透過家庭監控鏡頭的2D畫面，理解房間的空間佈局和物體之間的關係，實現更智能的家居控制和安全監控。", "**醫療影像分析：** 從醫學圖像（例如 X 光片或 CT 掃描）中推斷出 3D 結構，輔助醫生進行更精確的診斷和手術規劃，特別是在只有 2D 影像可用的情況下。"], "pitch": "Spatial-MLLM 解決了多模態大型語言模型在僅基於 2D 視覺輸入的空間推理方面的瓶頸。它提供了一種更有效、更輕量級的方法，無需額外的 3D 或 2.5D 數據即可獲得優秀的空間理解能力。想像一下，我們能將現有的監控系統、自動駕駛汽車或醫療影像分析工具升級，賦予它們更強大的空間感知能力，而無需進行昂貴的硬體升級。Spatial-MLLM 的商業價值體現在其廣泛的應用潛力，以及它在降低成本和提高效率方面的優勢。我們相信，Spatial-MLLM 將成為下一代智慧視覺系統的關鍵技術，為投資者帶來豐厚的回報。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-30T13:26:15.579631"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流轉換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一個免訓練的圖像編輯框架，它利用 LoRA 模型實現多概念編輯。 它的核心洞見是：在 Flux 樣式的 diffusion transformers 中，特定概念的特徵會在去噪過程的早期激活空間上連貫的區域。 因此，LoRAShop 可以提取出每個概念的解耦潛在遮罩，然後僅在包含這些概念的區域內混合相應的 LoRA 權重。 這樣，就能無縫地將多個主體或風格融入原始場景，同時保留全局上下文、光照和細節。 LoRAShop 在保持身份方面優於其他方法，並且無需重新訓練或外部約束，將個性化的 diffusion 模型轉變為實用的 \"LoRA 版 Photoshop\"，並為組合視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**個性化商品設計：** 用戶可以輕鬆地將寵物的照片或喜愛的風格元素融入到T恤、手機殼、馬克杯等商品設計中，無需專業設計技能。", "**虛擬試穿/試妝：** 用戶可以上傳自己的照片，並將不同的服裝或妝容風格「疊加」到自己的身上，快速預覽效果，決定是否購買。", "**遊戲角色客製化：** 玩家可以將自己的照片或理想中的角色形象融入到遊戲角色的外觀設計中，打造獨一無二的遊戲體驗。"], "pitch": "LoRAShop 打破了個性化圖像編輯的技術壁壘，讓普通用戶也能輕鬆實現專業級的創意。 其免訓練、高效率的特性，大幅降低了使用門檻和成本，在電商、遊戲、設計等領域具有廣闊的應用前景。 我們相信，LoRAShop 將顛覆傳統圖像編輯模式，引領一場基於個性化和創造力的視覺革命，具有巨大的市場潛力。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-30T13:26:27.797208"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在視覺空間智能方面的能力", "summary_zh": "本論文提出 Spatial-MLLM，一個全新的框架，旨在純粹基於2D視覺觀察提升多模態大型語言模型（MLLM）的空間推理能力。不同於以往依賴額外3D或2.5D數據的3D MLLM，Spatial-MLLM利用預訓練的視覺幾何基礎模型的結構先驗知識，採用雙編碼器架構：一個用於提取語義特徵的2D視覺編碼器，以及一個從視覺幾何模型主幹初始化的空間編碼器，用於提取3D結構特徵。通過連接器整合兩種特徵，提升空間理解能力。此外，還提出了一種空間感知幀採樣策略，在推理時選擇視頻序列中空間信息豐富的幀，即使在有限的token長度下，也能確保模型專注於對空間推理至關重要的幀。通過Spatial-MLLM-120k數據集的訓練，模型在各種真實世界數據集上的實驗結果表明，在基於視覺的空間理解和推理任務中，Spatial-MLLM達到了最先進的性能。", "applications": ["**自動駕駛/機器人導航：** 根據車載攝像頭或機器人攝像頭拍攝的2D圖像和視頻，理解周圍環境的空間結構，進行精確的路線規劃和障礙物避讓。", "**室內設計/建築規劃：** 根據室內照片或視頻，自動生成房間的3D模型，輔助設計師進行佈局規劃和裝飾設計，甚至能根據客戶描述的空間需求，自動生成設計方案。", "**醫療影像分析：** 從CT或MRI掃描的2D切片中，重建3D器官模型，協助醫生進行疾病診斷和手術規劃，例如精確定位腫瘤位置和計算體積。"], "pitch": "Spatial-MLLM突破了傳統MLLM對3D或2.5D數據的依賴，僅憑藉2D視覺輸入即可實現強大的空間推理能力。這意味著巨大的商業潛力，特別是在缺乏3D數據或獲取3D數據成本高昂的應用場景。我們利用預訓練的視覺幾何模型，有效降低了模型訓練成本，提高了性能，並通過空間感知幀採樣策略提升了資源利用率。我們的技術可以賦能自動駕駛、機器人、室內設計、醫療影像等各個領域，提供更智能、更高效的解決方案。我們相信，Spatial-MLLM將成為下一代空間智能應用的關鍵技術，並帶來顛覆性的創新。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-30T14:14:27.190084"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流轉換器，無需訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一個創新的框架，利用預訓練的 LoRA 模型實現多概念圖像編輯，無需額外訓練。 它的關鍵洞察在於，在 Flux 風格的擴散轉換器中，概念特定的特徵會在去噪過程的早期，激活空間上一致的區域。LoRAShop 藉此推導出每個概念的解耦潛在遮罩，並僅在概念的邊界區域內混合相應的 LoRA 權重。 這樣產生的編輯能將多個主題或風格無縫整合到原始場景中，同時保留全域上下文、光照和細節。 實驗證明，LoRAShop 在身份保持方面優於其他方法。 藉由消除重新訓練和外部約束，LoRAShop 將個性化的擴散模型轉變為實用的「基於 LoRA 的 Photoshop」工具，為組合視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**個性化頭像生成器：** 用戶可以輕鬆將多個個人化元素（例如，髮型、服裝、配飾）整合到頭像中，無需專業圖像編輯技能，快速生成風格多樣且獨特的個人形象。", "**虛擬試穿應用：** 用戶可以將不同款式和品牌的服裝、眼鏡等虛擬地疊加到自己的照片上，預覽效果，而無需實際試穿，提升購物體驗。", "**創意內容生成平台：** 內容創作者可以快速將不同的圖像元素（例如，背景、人物、物體）組合在一起，創造出獨特的視覺內容，用於社交媒體、廣告等用途，大幅提升創作效率。"], "pitch": "LoRAShop 打破了圖像編輯的門檻，讓使用者無需專業技能或耗時的訓練，就能輕鬆實現多概念的圖像生成和編輯。 想像一下一個基於 LoRA 的 Photoshop，使用者可以像堆積木一樣，將不同的風格、人物、物件融入圖片中，同時保留圖片本身的細節和光影。 這解決了現有圖像編輯工具的複雜性和高成本問題。 LoRAShop 的商業價值在於，它可以被整合到各種應用程式中，例如個性化頭像生成器、虛擬試穿平台、創意內容生成工具等，觸及廣大的用戶群體，並創造新的商業模式。 我們預計 LoRAShop 將引領圖像編輯領域的變革，為用戶帶來更便捷、更具創意的體驗。 潛在收益來源包括授權費用、API 使用費、以及整合到合作夥伴產品中的分成。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-30T14:14:48.400174"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升多模態大型語言模型在基於視覺的空間智能中的能力", "summary_zh": "Spatial-MLLM是一個全新的框架，旨在提升多模態大型語言模型(MLLM)在僅基於2D視覺輸入下的空間推理能力。不同於以往依賴額外3D或2.5D數據的模型，Spatial-MLLM透過結合預訓練的2D視覺編碼器（提取語義特徵）和從視覺幾何模型骨幹初始化的空間編碼器（提取3D結構特徵），實現更強大的空間理解能力。此外，該模型還採用空間感知的幀採樣策略，在推理時選擇資訊量豐富的幀，優化資源利用率。透過Spatial-MLLM-120k數據集的訓練，模型在多個真實世界數據集上展現了卓越的性能。", "applications": ["自動駕駛：分析行車記錄儀影片，識別道路上的障礙物、理解交通標誌的位置，並預測其他車輛的行為，從而提升自動駕駛系統的安全性。", "機器人導航：讓機器人僅透過攝像頭即可理解複雜的室內環境，規劃最佳路徑，避免碰撞，並執行精確的任務，例如搬運物品。", "建築和房地產：基於房屋圖片或影片生成空間佈局圖，方便虛擬看房、室內設計或改造規劃，提升用戶體驗並降低溝通成本。"], "pitch": "Spatial-MLLM解決了現有MLLM在僅基於2D視覺信息進行空間推理的瓶頸，開闢了多個高潛力市場。它通過獨特的雙編碼器架構和空間感知的幀採樣策略，顯著提升了模型在理解和推理空間信息方面的能力。這意味著，我們可以基於現有的視覺數據（例如行車記錄儀影片、監控錄像、房地產圖片）開發出更智能、更具實用性的應用。考慮到自動駕駛、機器人、建築設計等領域對空間理解的迫切需求，Spatial-MLLM擁有巨大的商業價值，可以通過SaaS平台、API服務、或者嵌入式系統等方式進行商業化，快速佔領市場，並構建一個基於空間智能的生態系統。初期投資將用於模型訓練、數據集擴充和團隊建設，預計在12-18個月內可以推出首個商業化產品。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-30T15:13:58.535824"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop: 基於校正流轉換器，免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一個能用 LoRA 模型進行多概念圖像編輯的框架，無需重新訓練。它基於一個關鍵發現：在 Flux 風格的擴散轉換器中，特定概念的轉換器特徵會在去噪過程的早期激活空間上連貫的區域。LoRAShop利用這個特性，在先前的正向傳播中為每個概念導出一個分離的潛在遮罩，並僅在圍繞個性化概念的區域內混合相應的 LoRA 權重。這樣，编辑能无缝地将多个主体或风格整合到原始场景中，同时保留全局上下文、光照和精细细节。實驗表明，LoRAShop 比其他方法更好地保留了身份。通过消除重新训练和外部约束，LoRAShop 将个性化扩散模型转变为一个实用的“LoRA 版 Photoshop”工具，为组合式视觉叙事和快速创意迭代开辟了新途径。", "applications": ["**虛擬試穿/搭配:** 用戶可以將不同風格的衣服（LoRA 模型代表不同風格）“穿”在自己的照片上，預覽效果，無需真正購買或試穿。", "**產品設計原型快速生成:** 設計師可以快速將不同的設計元素（例如，不同的椅子腿，LoRA 模型代表不同設計）組合到一個現有的椅子模型上，快速生成多個設計原型。", "**創意圖像合成:** 用戶可以將自己與喜歡的卡通人物（LoRA模型代表卡通人物）合併到同一張圖片中，創作獨特的個人化圖像。"], "pitch": "LoRAShop 是一個革命性的圖像編輯工具，它將個性化擴散模型轉化為易於使用的『LoRA 版 Photoshop』。我們無需重新訓練模型，就能快速將多個概念融入圖像，保留細節和上下文。想像一下，一個產品設計師可以立即將不同風格的組件應用於他們的模型，或者一個服裝零售商提供一個虛擬試穿的體驗，讓顧客在購買前就能看到搭配效果。這個技術不僅能提升設計效率，還能創造全新的用戶體驗。更重要的是，LoRAShop 解鎖了圖像編輯的無限可能性，降低了創作門檻，讓每個人都能輕鬆創作獨一無二的視覺內容。這在電商、娛樂、廣告和教育等領域都有巨大的商業潛力。我們相信 LoRAShop 將顛覆圖像編輯行業，成为创意产业的新引擎，值得早期投資。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-30T15:14:20.831429"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升多模態大型語言模型在視覺空間智能方面的能力", "summary_zh": "這篇論文介紹了一種新的框架 Spatial-MLLM，旨在僅從 2D 圖像或影片中提升多模態大型語言模型（MLLM）的空間智能。與依賴額外3D數據的現有方法不同，Spatial-MLLM 利用視覺幾何基礎模型，透過雙編碼器架構提取 2D 語義特徵和 3D 結構特徵，並結合空間感知的幀採樣策略，在僅有 2D 輸入的情況下，也能實現卓越的空間理解和推理能力。他們建立了一個Spatial-MLLM-120k數據集並訓練模型。實驗證明，Spatial-MLLM 在各種真實世界數據集上都取得了最先進的性能。", "applications": ["**自動駕駛輔助：** 利用車載攝像頭的 2D 影像，即時判斷周遭環境的 3D 空間結構，例如偵測行人位置、預測車輛軌跡，提升自動駕駛的安全性。", "**室內機器人導航：** 機器人僅需透過 2D 攝影機掃描室內環境，就能建立 3D 地圖，進行精確導航、避開障礙物，甚至理解房間的用途，例如區分客廳和臥室。", "**醫療影像分析：** 從 CT 或 MRI 的 2D 切片影像中重建 3D 模型，輔助醫生進行疾病診斷和手術規劃，提高醫療精準度。"], "pitch": "Spatial-MLLM 解決了多模態大型語言模型在僅有 2D 視覺輸入時空間推理能力不足的痛點。這項技術可以廣泛應用於自動駕駛、機器人導航、醫療影像分析等領域，具有巨大的市場潛力。相較於需要額外 3D 數據的傳統方法，Spatial-MLLM 更具成本效益和部署靈活性。我們的雙編碼器架構和空間感知幀採樣策略在多個真實世界的數據集上都展現了卓越的性能，證明了其技術領先性。 我們相信，Spatial-MLLM 將成為提升視覺空間智能的關鍵技術，為各行各業帶來顛覆性的創新應用，具備高度的投資價值。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-30T16:16:54.084320"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流變換器實現免訓練的多概念圖像生成和編輯", "summary_zh": "LoRAShop是一個創新的框架，讓使用者能用LoRA模型，無需重新訓練，直接編輯圖像中的多個概念。它基於對Flux風格擴散變換器內部特徵交互模式的關鍵觀察：概念特定的變換器特徵在去噪過程早期，就會激活空間上連貫的區域。LoRAShop利用這一點，在預先進行的前向傳遞中，為每個概念導出一個解耦的潛在遮罩，並僅在限定這些概念的區域內混合相應的LoRA權重。這樣產生的編輯能夠無縫地將多個主體或風格整合到原始場景中，同時保留全局上下文、光照和細節。實驗表明，LoRAShop在身份保留方面優於其他方法。透過消除重新訓練和外部約束，LoRAShop將個性化的擴散模型變成一個實用的“LoRA版Photoshop”工具，為構圖視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**快速製作個人化行銷素材：** 假設一家小型服裝店想為其最新系列製作宣傳圖片，可以使用LoRAShop快速將模特兒穿上不同款式的衣服，並調整場景風格，生成多樣化的行銷素材，無需耗時費力的傳統拍攝流程。", "**客製化數位藝術創作：** 藝術家可以使用LoRAShop將不同藝術風格（例如梵谷的星夜和莫內的睡蓮）融合到自己的作品中，或者將特定的物體（例如寵物或汽車）嵌入到現有的畫作中，創造出獨一無二的數位藝術品。", "**遊戲角色和場景的快速迭代設計：** 遊戲開發者可以利用LoRAShop快速測試不同的角色外觀、服裝和場景風格，並在不需要重新訓練模型的情況下，即時預覽和修改，大大縮短遊戲開發週期。"], "pitch": "LoRAShop 是一個顛覆性的圖像編輯技術，它讓使用者無需耗時的重新訓練，就能輕鬆地將多個概念融合到圖像中。想像一下，一個讓每個人都能成為設計師的工具，一個能夠將創意快速轉化為視覺內容的平台。 LoRAShop的免訓練特性大幅降低了使用門檻，使其應用範圍廣泛，從個人化的行銷內容生成到遊戲資產的快速開發，都具有巨大的潛力。 我們相信 LoRAShop 有機會成為視覺內容創作領域的領導者，為內容創作者、企業和遊戲開發者提供強大的工具，並為我們帶來豐厚的回報。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-30T16:17:14.162864"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升多模態大型語言模型在視覺空間智能方面的能力", "summary_zh": "現有的多模態大型語言模型在處理2D視覺任務上表現優異，但空間智能仍是挑戰。Spatial-MLLM提出了一個新的框架，僅從2D圖像進行視覺空間推理。它採用雙編碼器架構：一個2D視覺編碼器提取語義特徵，一個從視覺幾何模型骨幹初始化的空間編碼器提取3D結構特徵。此外，它還使用空間感知的幀採樣策略，選擇信息量大的幀，以優化空間推理性能。在Spatial-MLLM-120k數據集上訓練後，該模型在各種真實數據集上都表現出卓越的空間理解和推理能力。", "applications": ["自動駕駛：利用行車記錄器的影像，即時判斷車輛周圍的空間關係，例如障礙物距離、道路坡度等，提升駕駛安全。", "智慧家居：透過監視器影像，理解房間的3D結構，讓機器人能更精準地導航和操作，例如避開障礙物、正確地放置物品。", "醫療影像分析：從X光或MRI影像中提取3D空間信息，協助醫生判斷腫瘤大小、位置，以及與周邊組織的關係，提升診斷準確性。"], "pitch": "Spatial-MLLM解決了傳統多模態模型在2D輸入條件下空間推理能力不足的問題，僅需2D圖像即可進行3D空間理解。這開啟了許多商業機會，包括但不限於：增強自動駕駛的感知能力，提升機器人導航的精度，以及改進醫療影像的診斷效率。其核心價值在於，降低了對額外3D數據的依賴，降低了成本，並拓寬了應用範圍。我們相信，Spatial-MLLM將成為下一代智能系統的重要組成部分，為各行各業帶來顛覆性的改變。投資Spatial-MLLM，就是投資未來。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-30T17:12:13.708884"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流變換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop是第一個使用LoRA模型進行多概念圖像編輯的框架。它基於對Flux風格擴散變換器內部特徵交互模式的關鍵觀察：概念特定的變換器特徵在去噪過程的早期階段激活空間上連貫的區域。LoRAShop利用此觀察結果，在先前的正向傳遞中為每個概念導出一個解耦的潛在遮罩，並且僅在限制待個性化概念的區域內混合相應的LoRA權重。由此產生的編輯將多個主題或風格無縫整合到原始場景中，同時保留全局上下文、光照和精細細節。實驗表明，LoRAShop提供比基準方法更好的身份保留。通過消除重新訓練和外部約束，LoRAShop將個性化的擴散模型轉變為實用的“帶有LoRA的Photoshop”工具，並為組合視覺敘事和快速創意迭代開闢了新途徑。", "applications": ["**客製化商品設計：** 用戶上傳寵物、家人或喜歡的物品照片，快速生成帶有這些元素的T恤、馬克杯、手機殼等商品設計圖，無需專業設計技能。", "**AI風格化肖像：** 用戶提供個人照片，選擇多種藝術風格（例如：梵谷、水墨畫、賽博龐克），LoRAShop快速生成個人風格的頭像或海報，用於社交媒體或個人收藏。", "**場景融合與物件替換：** 在旅行照片中，快速替換天空、增加特效或將人物移至不同背景，讓照片更具創意和個性，無需複雜的後期處理技巧。"], "pitch": "LoRAShop解決了AI圖像編輯領域中，將多個概念融合到一張圖像的痛點。它免訓練的特性大幅降低了使用門檻，使其更具吸引力。想像一下，一個用戶友好的平台，讓每個人都能輕鬆地將自己或朋友融入到電影場景、創造獨一無二的禮物或快速生成各種廣告素材。其商業價值在於：\n\n*   **潛在用戶規模龐大：** 從個人用戶到小型企業，任何需要快速生成、編輯圖像的人都是潛在客戶。\n*   **訂閱模式：** 可以通過提供不同級別的編輯功能、概念數量或API調用次數來建立訂閱模式。\n*   **合作夥伴關係：** 可以與電商平台、印刷公司、廣告公司等建立合作夥伴關係，提供更全面的服務。\n*   **技術壁壘：** LoRAShop的技術優勢在於免訓練和高度個性化，具有較高的技術壁壘，能夠保持競爭力。\n\nLoRAShop將成為AI圖像編輯領域的Game Changer，有望顛覆傳統的圖像編輯方式，具有巨大的商業潛力。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-30T17:12:33.993552"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升基於視覺的空間智能中多模態大型語言模型的能力", "summary_zh": "本研究提出 Spatial-MLLM，一個新穎的框架，旨在僅從2D視覺資訊中提升多模態大型語言模型(MLLM)的空間推理能力。Spatial-MLLM 使用雙編碼器架構：一個提取語義特徵的2D視覺編碼器，以及一個從視覺幾何模型初始化的空間編碼器，提取3D結構特徵。透過連接器將兩者整合，並採用空間感知幀採樣策略，專注於影片序列中對於空間推理至關重要的幀。透過大量實驗證明，Spatial-MLLM 在各種基於視覺的空間理解和推理任務中表現出色，達到最先進的水平。", "applications": ["**自動駕駛輔助：** 透過車載攝影機的2D影像，即時分析道路的3D結構，預測潛在的危險（例如：前方道路的傾斜程度、路面的坑洞）。", "**居家照護機器人：** 機器人利用2D影像判斷室內環境的空間結構，輔助老年人或行動不便者安全移動，例如：判斷地板高低差、避開障礙物。", "**虛擬實境(VR)互動：** 根據使用者在VR環境中看到的2D畫面，更精準地理解其空間方位和深度感知，提升互動的真實感，例如：使用者丟擲物體時，模型能更精準判斷物體與其他物件的碰撞。", "**(額外)無人機巡檢:** 無人機在2D影像的基礎上建立目標物體(如橋梁、電塔)的3D模型，以便更精確地進行缺陷檢測和分析，並生成巡檢報告。"], "pitch": "Spatial-MLLM 解決了多模態大型語言模型在僅有2D視覺資訊下進行空間推理的難題，打破了對額外3D或2.5D數據的依賴。 其雙編碼器架構和空間感知幀採樣策略，顯著提升了模型在各項視覺空間理解任務中的性能。 我們預期 Spatial-MLLM 將在自動駕駛、機器人、虛擬實境、無人機巡檢等領域釋放巨大的商業潛力。 其核心技術可授權給相關企業，或進一步開發垂直領域的AI解決方案，例如：為自動駕駛系統提供更精準的環境感知能力，為VR遊戲提供更真實的互動體驗，為無人機巡檢提供更智能的缺陷檢測。 我們相信 Spatial-MLLM 具備高度的市場競爭力，有望成為視覺AI領域的下一個獨角獸。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-30T18:18:50.617563"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流轉換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop是一個創新的框架，讓使用者無需重新訓練模型，就能用LoRA模型進行多概念圖像編輯。它的核心原理是透過觀察擴散轉換器內部特徵交互模式，發現概念特定的轉換器特徵會在降噪過程初期激活空間連貫的區域。LoRAShop利用這個特性，為每個概念導出分離的潛在遮罩，並僅在概念周圍區域混合相應的LoRA權重。這種方法能將多個主體或風格無縫整合到原始場景中，同時保留全局上下文、光照和細節。簡而言之，LoRAShop讓圖像編輯更快速、更精準，效果也更好。", "applications": ["**個性化頭像生成器：** 用戶可以上傳幾張自己的照片（或參考圖），LoRAShop就能生成多種風格（例如：卡通、油畫、未來主義）的個性化頭像，無需每次都重新訓練模型。", "**產品宣傳海報自動生成：** 電商商家可以提供產品圖片和幾個風格標籤（例如：復古、簡約、奢華），LoRAShop就能自動生成不同風格的宣傳海報，大大降低設計成本和時間。", "**室內設計風格轉換：** 用戶可以上傳房間照片，然後指定幾種不同的設計風格（例如：北歐、日式、地中海），LoRAShop就能快速模擬房間在不同風格下的樣貌，方便用戶選擇和調整。"], "pitch": "LoRAShop 解決了個性化圖像生成和編輯領域的一個關鍵痛點：重新訓練模型的成本過高。透過我們的免訓練技術，LoRAShop 開啟了『LoRA 版本的 Photoshop』的應用，讓用戶可以輕鬆地將多個概念整合到圖像中，同時保留原始場景的細節和上下文。這種技術具備巨大的商業潛力，可以應用於個性化內容生成、廣告創意設計、虛擬試穿等領域。我們的核心優勢是速度、效率和靈活性，相比於需要耗時訓練的傳統方法，LoRAShop 能夠在幾秒鐘內完成圖像編輯，大大降低了用戶的使用門檻。我們相信 LoRAShop 將會顛覆圖像編輯領域，成為內容創作者和企業的必備工具。我們的商業模式可以基於訂閱、API 調用或軟體授權，預計將在未來幾年內實現爆發式增長。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-30T18:19:10.689165"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI對齊的扭曲：偏好優化真的能優化偏好嗎？", "summary_zh": "現今大型語言模型仰賴成對比較來對齊人類偏好。然而，目前的對齊方法（如基於PPO的RLHF和DPO）假設用戶偏好一致，但在實際應用中，用戶偏好往往是多樣的。因此，這些方法是否能讓模型平均滿足用戶，都還是個問題。本研究基於社會選擇理論，並透過個人布萊德利-泰瑞（BT）模型來模擬用戶比較，引入了對齊方法的「扭曲」概念：最佳平均效用和學習策略的平均效用之間的最差情況比例。 研究發現，相較於RLHF和DPO，Nash Learning from Human Feedback在不同情境下，能達到最小最大化的最佳扭曲。", "applications": ["**個性化推薦系統：** 根據每個用戶獨特的偏好，而非平均偏好，提供更精準的商品、電影或音樂推薦，提升使用者滿意度和轉換率。", "**醫療診斷輔助：** 考慮不同醫生或患者對診斷結果的不同權重和偏好，避免單一偏好模型造成的誤判，提供更客觀全面的輔助診斷。", "**自動駕駛系統：** 針對不同駕駛者（例如追求速度、安全、省油）的偏好，調整駕駛風格，提供更符合個人需求的駕駛體驗，提升安全性與舒適度。"], "pitch": "我們的研究揭示了現有AI對齊方法在處理多樣化用戶偏好時的缺陷，即現行的優化方法可能並未真正優化用戶的真實偏好。我們提出了一種新的衡量指標「扭曲」，並證明了Nash Learning from Human Feedback在處理多樣化偏好方面更具優勢。這意味著，我們可以構建更公平、更個性化，且更符合實際需求的AI系統。想像一下，一個真正了解你需求的AI助手，而不是一個基於平均偏好而設計的通用工具。這不僅能大幅提升用戶體驗，還能開闢多個商業機會，例如個性化推薦、醫療診斷輔助、自動駕駛等領域，帶來巨大的經濟效益。我們相信，基於我們研究的技術，能夠在AI對齊領域取得突破，為社會帶來更具價值和可信賴的AI解決方案。 我們正在尋求投資，將此研究成果商業化，建立下一代更智慧、更懂你的AI系統。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-05-30T19:11:18.626973"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升多模態大型語言模型在基於視覺的空間智能上的能力", "summary_zh": "這篇論文介紹了Spatial-MLLM，一個新的框架，旨在提升多模態大型語言模型(MLLM)在僅有2D視覺輸入下進行空間推理的能力。與依賴額外3D/2.5D數據的現有方法不同，Spatial-MLLM利用預訓練的視覺幾何模型提取3D結構特徵，並與2D視覺編碼器提取的語義特徵融合。此外，還提出了空間感知幀採樣策略，在有限的token長度下，選擇對空間推理至關重要的幀。透過大規模數據集Spatial-MLLM-120k的訓練，Spatial-MLLM在各種真實世界的數據集上，展現了在基於視覺的空間理解和推理任務中的最先進性能。", "applications": ["**智慧駕駛輔助：** 透過車載攝影機拍攝的2D影像，即時理解周圍環境的3D結構（例如其他車輛、行人、路標的位置和距離），提升自動駕駛的安全性和可靠性。", "**擴增實境(AR)導航：** 使用手機鏡頭捕捉到的2D影像，精準理解使用者當前環境的3D空間，提供更準確和自然的AR導航體驗，例如在室內空間導航或提供建築物資訊。", "**建築設計與室內裝潢：** 基於2D平面圖或照片，自動生成3D模型，並進行虛擬的室內設計和裝潢，讓設計師和使用者能夠更直觀地預覽設計效果，並進行修改。"], "pitch": "Spatial-MLLM解決了多模態大型語言模型在僅有2D視覺輸入下進行空間推理的瓶頸，具備巨大的商業潛力。傳統3D MLLM需要額外的3D/2.5D數據，限制了應用場景。Spatial-MLLM憑藉其獨特的架構和空間感知幀採樣策略，可在各種基於2D影像的應用中，實現精準的空間理解和推理。智慧駕駛、AR導航、建築設計等領域都存在對該技術的強烈需求。我們預計Spatial-MLLM將成為下一代空間智能應用的核心技術，具有顯著的先發優勢和巨大的市場成長空間。初期可專注於智慧駕駛和AR應用，後續拓展至建築設計、機器人等領域。尋求資金用於擴大模型規模、開發應用API，以及拓展市場合作夥伴。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-30T19:11:38.727103"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：利用修正流轉換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop是一個創新的框架，能利用LoRA模型進行多概念圖像編輯，且無需重新訓練。它基於一個重要發現：在Flux-style擴散轉換器中，概念特定的轉換器特徵會在去噪過程的早期階段激活空間上連貫的區域。LoRAShop藉此為每個概念導出一個解耦的潛在遮罩，並僅在包含個性化概念的區域內混合相應的LoRA權重。這使得編輯能將多個主體或風格無縫整合到原始場景中，同時保留全局上下文、光照和精細細節。實驗證明，LoRAShop在身份保留方面優於其他方法，並且無需重新訓練和外部約束，將個性化擴散模型轉變為實用的“LoRA版Photoshop”，開闢了組合視覺敘事和快速創意迭代的新途徑。", "applications": ["**個性化商品設計：** 用戶可以上傳一張產品照片，然後添加各種風格或元素（如藝術風格、品牌標誌、不同的材質），即時生成多種設計方案，無需專業設計師。", "**快速視覺原型設計：** 設計師可以快速將多個元素組合，創造出新的視覺概念，例如將建築設計草圖與環境圖像融合，或是將不同角色形象融合到遊戲場景中，快速生成視覺原型。", "**內容創作工具：** 影片製作者或社群媒體用戶可以輕鬆地將不同概念融入現有圖像或影片中，例如將寵物的照片融入熱門電影場景，或將朋友的頭像換成不同的卡通人物，創造有趣的內容。"], "pitch": "LoRAShop解決了AI圖像編輯領域中模型重新訓練的痛點，提供了一種無需額外訓練即可快速融合多個概念的解決方案。其核心價值在於顯著降低了個性化圖像編輯的門檻，擴展了其應用範圍。想像一下，一個免學編程，免重新訓練，就能輕鬆創造出你想要的圖像工具。LoRAShop可以授權給大型圖像編輯軟體公司（如Adobe）進行整合，成為Photoshop等工具的核心功能，或打造獨立的AI圖像編輯平台，以訂閱模式向用戶提供服務。此外，與電商平台合作，提供個性化商品設計服務，將成為新的盈利增長點。我們正在打造的是一個圖像編輯的『樂高積木』，讓想像力無限延伸，而無需耗費大量的時間與資源在模型訓練上。 初期著重於B端客戶（如設計公司、電商平台），中期擴展至C端用戶，搶佔AI圖像編輯市場的領先地位。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-30T19:12:00.537080"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI對齊的扭曲：偏好優化是否真的在優化偏好？", "summary_zh": "大型語言模型在預訓練後會基於成對比較與人類偏好對齊。然而，現有的對齊方法（如基於PPO的RLHF和DPO）假設它們與單一偏好模型對齊，但實際上用戶的偏好是多樣化的。這導致一個問題：這些方法是否真的能讓用戶平均而言感到滿意？為了探討這個問題，我們引入了「扭曲」的概念，衡量對齊方法的表現與理想狀況的差距。結果顯示，Nash Learning from Human Feedback在不同情況下表現穩健，而RLHF和DPO則容易產生顯著甚至無限大的扭曲。", "applications": ["**個人化推薦系統：** 理解使用者對電影、音樂或商品的偏好各不相同，設計能適應並提供更精準推薦的系統，避免只推送大眾喜愛但使用者可能不感興趣的內容。", "**多人協作AI助手：** 在多人參與的項目中，AI助手需理解每個成員的意見，並權衡利弊，做出能讓團隊成員整體滿意的決策，而非只依照某個成員的偏好。", "**醫療診斷輔助：** 考慮不同患者對治療方式的偏好（例如，手術或藥物治療），輔助醫生做出既符合醫學標準，又尊重患者個人意願的治療方案。"], "pitch": "現今AI對齊方法忽略了使用者偏好的多樣性，導致AI可能並未真正滿足使用者。我們開發了一套評估對齊方法「扭曲」程度的指標，並發現現有方法存在顯著缺陷。我們的研究揭示了AI對齊領域的潛在危機，並為開發更穩健、更能適應多樣化偏好的AI系統指明了方向。這是一個數十億美元級別的市場，包括個人化推薦、多人協作工具、醫療診斷輔助等。我們正在尋找投資，以開發新一代的AI對齊算法，並將其應用於各個領域，確保AI能真正服務於人類，而不是僅僅依照單一標準執行任務。我們不僅在優化算法，更是在優化AI的未來。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-05-30T20:14:59.717730"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升多模態大型語言模型在基於視覺的空間智能中的能力", "summary_zh": "本研究提出了Spatial-MLLM，一個新的框架，旨在提升多模態大型語言模型（MLLM）在僅使用2D視覺數據時的空間推理能力。與以往依賴額外3D或2.5D數據的3D MLLM不同，Spatial-MLLM利用視覺幾何基礎模型中的強結構先驗知識，通過一個雙編碼器架構，分別提取語義特徵和3D結構特徵，並整合到統一的視覺tokens中。此外，還提出了一種空間感知幀採樣策略，在推理時選擇包含更多空間信息的幀，從而提升模型在空間理解和推理任務中的表現。實驗結果表明，Spatial-MLLM在多個真實世界的數據集上取得了最先進的性能。", "applications": ["**自動駕駛：** 基於車載攝像頭的2D圖像，更準確地理解周圍環境的空間結構，預測車輛的行駛軌跡，提升行駛安全性。", "**智慧家居：** 分析2D圖像或視頻，理解室內空間佈局，輔助機器人進行導航、物品定位和家庭環境管理，例如：準確找到丟失的遙控器。", "**醫療影像分析：** 從2D醫學影像（如X光片）中推斷出3D器官結構，輔助醫生進行診斷，例如：更精準判斷骨折位置和程度。"], "pitch": "Spatial-MLLM突破了現有多模態模型對3D數據的依賴，僅需2D視覺輸入即可實現強大的空間推理能力，開闢了更廣闊的應用場景。在自動駕駛、智慧家居和醫療影像等領域，對降低數據採集成本、提升效率和安全性具有重要意義。作為早期投資，Spatial-MLLM有望成為下一代視覺智能的核心組件，在千億美元級市場中佔據領先地位，並可通過授權許可、API服務和行業解決方案等多種方式實現盈利，具備極高的投資回報潛力。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-30T20:15:12.602216"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流變換器實現免訓練的多概念圖像生成和編輯", "summary_zh": "LoRAShop是首個利用LoRA模型進行多概念圖像編輯的框架。它基於對Flux風格擴散變換器內部特徵交互模式的觀察：概念特定的變換器特徵在去噪過程的早期階段激活空間連貫的區域。LoRAShop利用這一點，在先前的正向傳遞中為每個概念導出一個解耦的潛在遮罩，並僅在限定概念的區域內混合相應的LoRA權重。由此產生的編輯能將多個主體或風格無縫整合到原始場景中，同時保留全局上下文、光照和精細細節。實驗表明，LoRAShop比基線方法更能保持身份一致性。通過消除重新訓練和外部約束，LoRAShop將個性化擴散模型轉變為實用的“基於LoRA的Photoshop”工具，並為組合視覺敘事和快速創意迭代開闢了新途徑。", "applications": ["**客製化商品設計：** 用戶可以快速將自己的照片或設計元素融入現有商品圖片中，例如在T恤、馬克杯或手機殼上添加個人頭像或寵物圖案，免去複雜的設計流程。", "**虛擬試穿/試用：**  模擬用戶在家中或身上試穿衣服、擺放家具、更換髮型等，無需親身前往店面或下載大量數據，提升購物體驗。", "**廣告素材快速生成：** 行銷人員可快速生成多種版本的廣告圖像，針對不同受眾添加不同的商品、人物或風格，大幅提升廣告投放效率和精準度。"], "pitch": "LoRAShop提供免訓練、高效的多概念圖像編輯能力，解決了個性化圖像生成領域的痛點。現有方案往往需要耗時的重新訓練或繁瑣的外部約束，而LoRAShop僅需預訓練的LoRA模型即可實現。這極大地降低了使用門檻，將AI圖像編輯能力賦能給更廣泛的用戶群體，包括電商平台、廣告公司、設計師等。其商業價值體現在：1. 降低設計成本，提升生產效率；2. 賦能個性化行銷，提高用戶參與度；3. 開創全新的圖像創作和編輯模式。我們正在打造的不僅僅是一個圖像編輯工具，而是一個AI驅動的創意生態系統，擁有巨大的市場潛力。下一步將著重優化用戶體驗，擴展支持的概念種類，並探索更高級的圖像編輯功能，例如基於文本的精細化編輯。我們的目標是成為AI圖像編輯領域的領頭羊。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-30T20:15:28.213628"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升多模態大型語言模型在視覺空間智能中的能力", "summary_zh": "本論文提出Spatial-MLLM，一種從純2D視覺觀察進行空間推理的新框架。它採用雙編碼器架構，利用預訓練的2D視覺編碼器提取語義特徵，並用視覺幾何模型的骨幹初始化空間編碼器，提取3D結構特徵。此外，還提出了一種空間感知的幀採樣策略，在推理時選擇具有空間信息的幀，即使在有限的token長度下，模型也能專注於對空間推理至關重要的幀。通過在Spatial-MLLM-120k數據集上的訓練，該模型在各種真實世界數據集上展現了最先進的視覺空間理解和推理性能。", "applications": ["**智慧駕駛輔助：** 利用Spatial-MLLM分析行車記錄器影片，即時判斷周遭車輛的距離、行進方向與速度，預測潛在碰撞風險，並給予駕駛者更精確的輔助資訊，提升行車安全。", "**智慧監控與安防：** 將Spatial-MLLM應用於監視器畫面分析，自動識別異常行為模式，例如人群聚集、摔倒、或物品遺失等，並發出警報，協助安保人員快速反應，提升安防效率。", "**室內導航與機器人控制：** Spatial-MLLM可以分析環境影像，讓機器人理解空間結構和物體位置，實現更精確的室內導航和物品抓取等任務。例如，在倉儲環境中，機器人可以透過影像識別貨架和貨物，並規劃最佳路徑進行揀貨。"], "pitch": "Spatial-MLLM解決了現有3D MLLM依賴額外3D數據的限制，開創了從純2D視覺輸入進行空間推理的新方向。其在智慧駕駛、智慧監控和機器人控制等領域具有廣泛應用前景。我們團隊利用獨特的雙編碼器架構和空間感知採樣策略，在無需額外3D數據的情況下，實現了最先進的空間理解和推理能力。Spatial-MLLM的核心優勢在於降低了部署成本，簡化了數據收集流程，使其更容易應用於各種實際場景。想像一下，讓現有的監視器、行車記錄器，甚至手機相機，都能擁有專業的空間智能，這將是一個巨大的市場。我們正在尋求種子輪/A輪投資，以擴大我們的數據集，進一步優化模型性能，並將Spatial-MLLM整合到更多商業應用中，成為視覺空間智能領域的領導者。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-30T21:12:34.818722"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：利用修正流變換器實現免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop是第一個基於LoRA模型的多概念圖像編輯框架。它利用了Flux風格擴散變換器中一個關鍵觀察：概念特定的變換器特徵在去噪過程早期就激活了空間上連貫的區域。LoRAShop藉此推導出每個概念的解耦潛在遮罩，並僅在概念的邊界區域內混合相應的LoRA權重，從而實現無縫地將多個主體或風格融入原始場景，同時保留全局上下文、光照和細節。LoRAShop無需重新訓練或外部約束，將個人化擴散模型轉變為實用的“LoRA版Photoshop”，為組合視覺敘事和快速創意迭代開闢了新途徑。", "applications": ["**客製化產品設計：** 用戶可以輕鬆將自己的寵物、風格或圖像元素融入到產品設計中，例如將寵物的照片添加到手機殼、T恤或馬克杯上，並立即預覽效果，無需專業設計師協助。", "**虛擬試穿/試戴：** 在電商平台上，用戶可以上傳自己的照片，然後使用LoRAShop將虛擬的衣服、眼鏡或髮型添加到照片中，模擬真實的穿戴效果，提高購買意願。", "**創意內容生成：** 內容創作者可以快速生成包含特定人物、風格或場景的圖像，用於社交媒體貼文、廣告素材或故事板，大幅縮短內容製作時間。"], "pitch": "LoRAShop為圖像編輯帶來了革命性的突破，透過免訓練的多概念編輯功能，讓個人化圖像生成與編輯變得前所未有的簡單高效。其核心優勢在於大幅降低了圖像客製化的門檻，讓使用者無需專業技能即可輕鬆創造獨一無二的視覺內容。我們認為LoRAShop在客製化產品、電商平台、創意內容生成等領域具有巨大的商業潛力。透過與電商平台、設計工作室、內容創作平台合作，LoRAShop可以提供強大的圖像客製化引擎，賦能他們為用戶提供更豐富、更個性化的產品與服務。LoRAShop將顛覆現有的圖像編輯流程，成為視覺內容創作的新標準，擁有巨大的市場價值與成長空間。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-30T21:12:53.526378"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升多模態大型語言模型在視覺空間智能中的能力", "summary_zh": "這篇論文介紹了一種名為 Spatial-MLLM 的新框架，旨在提升多模態大型語言模型（MLLMs）的視覺空間推理能力。現有的3D MLLMs通常需要額外的3D或2.5D數據來引入空間感知，限制了它們在只有2D輸入（例如圖像或視頻）場景中的應用。Spatial-MLLM 採用雙編碼器架構，一個用於提取語義特徵，另一個（基於視覺幾何模型）用於提取3D結構特徵。此外，還提出了一種空間感知幀採樣策略，以選擇視頻序列中對空間推理至關重要的幀。該模型在 Spatial-MLLM-120k 數據集上進行了訓練，並在各種真實世界數據集上取得了最先進的性能。", "applications": ["**自動駕駛輔助：** 分析行車記錄器影像，理解周圍環境的空間關係，輔助駕駛決策，例如預測車輛與行人、障礙物的距離和相對速度。", "**室內機器人導航：** 在只有2D影像輸入的情況下，幫助機器人理解室內空間結構，進行更精確的導航和物體識別，例如掃地機器人規劃清掃路線。", "**醫學影像分析：** 分析X光或超音波影像，輔助醫生判斷病灶位置、大小和形狀，並理解器官之間的空間關係，提高診斷準確性。"], "pitch": "Spatial-MLLM解決了現有MLLM在純2D視覺輸入下缺乏空間智能的痛點，開創了更廣泛的應用可能性。其雙編碼器架構和空間感知幀採樣策略使其在視覺空間理解和推理方面達到最先進水平。想像一下，一個能夠僅憑藉普通鏡頭拍攝的影片就能準確判斷周圍環境的3D結構的AI。這項技術不僅能提升自動駕駛的安全性，簡化機器人導航，更能在醫療影像分析等領域帶來革命性的變革。我們有信心，Spatial-MLLM將成為下世代視覺AI的關鍵技術，擁有巨大的商業潛力。我們正在尋找投資，以加速模型訓練、擴展數據集，並將其應用於更多垂直領域，實現商業化落地。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-30T22:13:06.826402"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用校正流變換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一個基於 LoRA 模型的多概念圖像編輯框架。它利用 Flux 風格擴散變換器的特性，發現概念特定的變換器特徵在降噪過程早期會激活空間上連貫的區域。透過在正向傳遞中為每個概念導出解耦的潛在遮罩，並僅在概念邊界區域內混合相應的 LoRA 權重，LoRAShop 能夠將多個主題或風格無縫地整合到原始場景中，同時保留全局上下文、光照和細節。無需重新訓練和外部約束，LoRAShop 將個人化的擴散模型轉變為一個實用的「基於 LoRA 的 Photoshop」工具，為合成視覺故事講述和快速創意迭代開闢了新的途徑。", "applications": ["**客製化產品設計：** 使用者可以上傳自己的照片，並將產品(例如：鞋子、手機殼)的風格融入照片中，預覽客製化產品的效果。", "**虛擬試穿/試戴：** 使用者可以將衣服或配件的風格融入自己的照片中，模擬穿戴效果，提升線上購物體驗。", "**AI輔助創作：** 藝術家或設計師可以快速地將不同的風格或主題融合到現有圖像中，產生新的創意作品，大幅提升創作效率。"], "pitch": "LoRAShop 提供免訓練的多概念圖像編輯能力，核心優勢在於無需耗時的重新訓練即可實現精確、細緻的圖像操作。想像一下，一個無需專業技能的使用者，也能輕鬆將不同的風格、物件融入照片，創造出獨一無二的視覺內容。這將顛覆現有的圖像編輯模式，催生全新的應用場景。例如，我們能將此技術應用於電商平台，提供更具沉浸感的虛擬試穿體驗，大幅提升轉換率；或者與內容創作平台合作，為使用者提供更強大的 AI 輔助創作工具。透過授權、API 介面等商業模式，LoRAShop 有潛力成為圖像處理領域的關鍵技術，創造巨大的市場價值。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-30T22:13:36.202790"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI對齊的扭曲：偏好優化真的能優化偏好嗎？", "summary_zh": "大型語言模型在預訓練後，會基於成對比較與人類偏好對齊。然而，現有對齊方法，如基於PPO的RLHF和DPO，都假設與單一偏好模型對齊，但實際應用中用戶偏好卻是多樣的。這篇論文指出，這些對齊方法甚至不一定能產生平均而言滿足用戶的模型，這對於多元化的對齊來說是最基本的要求。論文引入了「扭曲」的概念，衡量對齊方法在最壞情況下，實際獲得的平均效用與最佳可實現的平均效用之間的比例。研究發現，Nash Learning from Human Feedback 具有最佳的最小最大扭曲，而 RLHF 和 DPO 在沒有 KL 約束的情況下就存在較高的扭曲，甚至在完整的設置中可能出現無界扭曲。", "applications": ["**個性化推薦系統優化：** 針對不同用戶群體的偏好，設計更有效的演算法，減少推薦結果的扭曲，提升用戶滿意度，避免陷入同溫層。", "**醫療診斷輔助系統：** 考量不同醫生或患者對診斷結果的偏好和權重，避免系統過度依賴單一判斷標準，提供更全面的診斷建議。", "**自動駕駛決策系統：** 考慮不同乘客對駕駛風格的偏好（例如，安全優先或效率優先），在保證安全的前提下，盡可能滿足乘客的個性化需求。"], "pitch": "各位投資人，我們發現現有AI對齊方法在面對多元用戶偏好時存在嚴重缺陷，導致AI系統無法有效滿足用戶需求，產生高昂的機會成本和潛在風險。我們的研究引入了「扭曲」的概念，並提出了一種更穩健的對齊方法。這項技術可以應用於個性化推薦、醫療診斷、自動駕駛等廣泛領域，解決目前AI系統無法有效捕捉多樣化人類偏好的痛點。透過降低對齊扭曲，我們可以打造更智能、更人性化的AI產品，大幅提升用戶滿意度，創造巨大的商業價值。想像一下，一個能真正理解您獨特需求的AI助手，這不僅僅是技術突破，更是商業模式的革新。我們正在尋找能夠一起引領AI對齊新時代的合作夥伴，共同塑造更美好的未來。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-05-30T23:13:07.321269"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在基於視覺的空間智能方面的能力", "summary_zh": "Spatial-MLLM 是一個新穎的框架，旨在提升多模態大型語言模型 (MLLM) 在僅使用 2D 圖像或影片進行空間推理方面的能力。它透過雙編碼器架構，利用預訓練的 2D 視覺編碼器提取語義特徵，並使用從視覺幾何模型骨幹初始化的空間編碼器提取 3D 結構特徵。此外，還提出了一種空間感知幀採樣策略，在推理時選擇影片序列中空間資訊最豐富的幀。實驗證明，Spatial-MLLM 在各種基於視覺的空間理解和推理任務中，表現優於現有技術。", "applications": ["**自動駕駛：** 理解交通場景中的物體位置、距離和移動方向，提升自動駕駛的安全性和可靠性。", "**建築設計：** 從平面圖或照片中推斷建築物的 3D 結構，協助建築師進行設計和視覺化。", "**醫療影像分析：** 從 CT 或 MRI 掃描中分析器官和組織的空間關係，輔助醫生進行診斷和手術規劃。"], "pitch": "Spatial-MLLM 解鎖了多模態大型語言模型在純 2D 視覺資料上的空間智能潛力，無需額外的 3D 或 2.5D 數據。這為許多應用場景開闢了新的可能性，例如自動駕駛、建築設計和醫療影像分析。我們的模型在各種真實世界數據集中都表現出卓越的性能，證明了其商業可行性。我們計劃將該技術授權給相關行業，並開發定制化的解決方案，以滿足特定客戶的需求。Spatial-MLLM 的潛在市場規模巨大，我們相信它將在未來幾年內成為視覺智能領域的重要驅動力。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-30T23:13:25.412936"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流轉換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一種全新的圖像編輯框架，它利用 LoRA 模型，能夠在不需要重新訓練的情況下，將多個概念融合到同一張圖片中。其核心發現是，Flux 類型的擴散轉換器中，概念特定的特徵會在去噪過程的早期階段激活空間上連貫的區域。LoRAShop 利用這個特性，在正向傳播中為每個概念導出一個解耦的潛在遮罩，並僅在限定概念的區域內混合相應的 LoRA 權重。這樣產生的編輯可以無縫地將多個主體或風格整合到原始場景中，同時保留整體上下文、光照和細節。簡而言之，LoRAShop 讓個性化的擴散模型變成了一種實用的「帶有 LoRA 的 Photoshop」，為組合式視覺故事講述和快速創意迭代開闢了新的途徑。", "applications": ["**個性化商品設計：** 用戶可以上傳自己的照片，並選擇不同的風格或元素（例如：添加卡通風格、更換背景為海灘），LoRAShop 可以快速生成客製化的商品圖樣，例如手機殼、T恤等。", "**藝術創作輔助：** 藝術家可以利用 LoRAShop 將多個靈感來源（例如：不同畫家的風格、不同物體的紋理）融合到自己的作品中，加速創作流程並探索新的藝術風格。", "**廣告素材快速生成：** 行銷團隊可以快速生成具有不同主題和風格的廣告圖片，例如：將模特兒放置在不同的場景中，或者使用不同的產品風格，測試哪種風格最能吸引目標受眾。"], "pitch": "LoRAShop 解決了圖像編輯領域中需要大量訓練和高度客製化的痛點，提供了一種免訓練、高效且高度可控的多概念圖像生成與編輯方案。它的核心優勢在於簡化了個性化擴散模型的應用，使其能夠像 Photoshop 一樣易於使用。商業價值體現在以下幾個方面：降低圖像生成成本，加速內容創作流程，提升廣告素材的轉化率，以及為個性化商品設計和藝術創作提供新的可能性。通過授權技術、提供雲服務或開發應用程式等方式，LoRAShop 具有巨大的市場潛力，能夠在廣告、電商、設計和藝術等領域產生顛覆性影響。我們相信 LoRAShop 有望成為下一代圖像編輯的基礎設施。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-30T23:14:00.403461"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI對齊的扭曲：偏好優化是否真的優化了偏好？", "summary_zh": "現今大型語言模型透過成對比較與人類偏好對齊。然而，主流對齊方法，如基於PPO的RLHF和DPO，假設所有使用者擁有單一偏好模型，這在使用者偏好多元的環境下並不成立。因此，這些方法是否能平均滿足使用者需求仍是未知數。本研究借鑒社會選擇理論，並透過Bradley-Terry模型模擬使用者比較，引入了「扭曲」的概念，即最佳平均效用與學習策略平均效用之間的最差情況比率。 研究發現，Nash Learning from Human Feedback在多種情況下表現穩健，能達到最小最大最佳扭曲值。 相反，RLHF和DPO的扭曲程度較高，甚至在沒有KL約束下就已表現不佳，完整設置下更可能出現指數級甚至無限大的扭曲。", "applications": ["**個人化推薦系統改進：** 根據不同使用者族群的偏好，調整推薦演算法，避免只針對「平均」使用者，提升各族群的滿意度。", "**醫療診斷輔助系統優化：** 考量不同醫生和病人的主觀判斷標準，避免系統過度依賴單一診斷標準，提高診斷的準確性和安全性。", "**自動駕駛行為決策客製化：** 根據不同駕駛者的駕駛習慣和風險偏好，調整自動駕駛系統的行為模式，提升駕駛體驗和安全性。"], "pitch": "我們研究揭示了現有AI對齊方法在處理多元使用者偏好時的缺陷，並提出了一個量化指標「扭曲」來衡量對齊效果。這指出了當前技術的瓶頸，為下一代更精準、更能適應使用者偏好的AI系統提供了明確的改進方向。 我們發現的Nash Learning方法展示了極大的潛力，能夠更穩健地處理不同偏好的使用者，進而提升使用者體驗。 我們相信，透過將這些研究成果應用到例如個人化推薦、醫療診斷和自動駕駛等領域，能夠創造顯著的商業價值。 想像一下，一個能真正理解並滿足您個人偏好的AI助手，它會改變各個行業的面貌。 現在正是投資這個未來趨勢的絕佳時機。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-05-31T01:04:11.142826"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在基於視覺的空間智能上的能力", "summary_zh": "Spatial-MLLM 是一個新型框架，旨在提升多模態大型語言模型 (MLLM) 在僅使用 2D 視覺輸入（例如圖片或影片）的情況下進行空間推理的能力。它透過雙編碼器架構，結合預訓練的 2D 視覺編碼器提取語義特徵，以及從視覺幾何模型主幹初始化的空間編碼器提取 3D 結構特徵。同時提出一種空間感知幀採樣策略，在有限的 token 長度下，模型能夠專注於對空間推理至關重要的幀。通過 Spatial-MLLM-120k 數據集進行訓練，Spatial-MLLM 在各種現實世界數據集上展示了最先進的視覺空間理解和推理性能。", "applications": ["**自動駕駛/機器人導航：** 從車載攝像頭或機器人視角影片中，理解周圍環境的空間結構，實現更精準的導航和避障。", "**建築/室內設計：** 僅從室內照片或影片中，生成空間佈局、物件位置等資訊，輔助設計師進行方案構思或自動生成設計草圖。", "**監控/安全應用：** 分析監控影片，自動識別異常行為（例如跌倒、擅闖禁區）發生的空間位置，提升安全預警效率。"], "pitch": "Spatial-MLLM 為視覺 AI 開闢了全新的可能性，它克服了傳統 MLLM 在空間理解上的限制，尤其是在缺乏 3D 數據的情況下。這項技術的獨特之處在於它能從純 2D 數據中提取 3D 空間資訊，使其在多個行業具有巨大的潛力。想像一下，自動駕駛汽車能夠更好地理解複雜的城市環境，建築師可以快速生成基於照片的設計方案，監控系統能夠更精準地識別異常行為。Spatial-MLLM 構建了一個強大的空間推理引擎，為基於視覺的 AI 應用帶來了質的飛躍。我們相信，通過進一步的研發和商業化，Spatial-MLLM 將成為智慧視覺領域的關鍵基礎設施，並帶來可觀的回報。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-31T01:04:28.569651"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：利用修正流變換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop是一個創新的框架，它能使用LoRA模型對圖像進行多概念編輯，且無需重新訓練模型。 它基於一個重要的觀察：在Flux風格的擴散變換器中，概念特定的變換器特徵會在去噪過程的早期階段激活空間上連貫的區域。 因此，LoRAShop能夠在先前的正向傳遞中為每個概念導出一個分離的潛在遮罩，並僅在限定概念的區域內混合相應的LoRA權重。 這種方法可以在保留全局上下文、光照和細節的同時，將多個主體或風格無縫地整合到原始場景中。 LoRAShop無需重新訓練和額外的約束，將個性化的擴散模型轉變為一個實用的「基於LoRA的Photoshop」工具，為構圖視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**快速產品原型設計：** 設計師可以快速將不同風格的零件（例如：不同品牌的輪胎、不同顏色的車身）組合到汽車圖像中，生成多種產品設計方案，無需耗時的3D建模或手繪。", "**虛擬服裝試穿：** 用戶可以將自己的照片上傳，並將不同風格的衣服或配飾“穿”在身上，模擬試穿效果，方便購物決策。", "**个性化头像定制：** 用户可以使用不同的艺术风格（例如：水彩、油画、卡通）和特征（例如：发型、服装）来定制自己的专属头像，满足社交媒体和游戏的需求。"], "pitch": "LoRAShop 颠覆了传统图像编辑模式，它无需重新训练模型即可实现多概念图像编辑，极大地降低了用户门槛和开发成本。想象一下，设计师可以轻松地将各种元素融合到图像中，电商平台可以提供更逼真的虚拟试穿体验，用户可以自由定制个性化的数字资产。LoRAShop 的核心优势在于其强大的概念分离和融合能力，以及无需训练的便利性。这种技术拥有巨大的商业潜力，可以应用于电商、游戏、广告、设计等多个领域。通过授权 LoRAShop 技术，或将其集成到现有图像编辑工具中，可以快速抢占市场，并构建一个围绕个性化图像创作的生态系统。其无需重新训练的特性，也意味着更低的服务器成本和更快的迭代速度，这对于追求效率和创新性的企业来说极具吸引力。因此，LoRAShop 不仅仅是一个图像编辑工具，更是一个通往 AI 驱动的个性化视觉内容创作的未来钥匙。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-31T01:04:51.697647"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI對齊的失真：偏好優化真的優化了偏好嗎？", "summary_zh": "現今的大型語言模型在預訓練後，會基於成對比較來與人類偏好對齊。主流的對齊方法，例如基於PPO的RLHF和DPO，都假設對齊的是單一偏好模型，但實際上用戶的偏好是多樣化的。因此，這些對齊方法是否能產生平均而言滿足用戶的模型仍然不明確，而這應是多元化對齊的最低要求。我們引入了「失真」的概念，用來衡量對齊方法的表現，它代表最佳可實現的平均效用與學習到的策略的平均效用之間的最差情況比率。研究結果顯示，Nash Learning from Human Feedback 達到了minimax最佳失真，而RLHF和DPO在沒有KL約束下就已經遭受較高的失真，在完整設定下甚至可能產生無界失真。簡單來說，這篇論文研究了在用戶偏好多樣性的情況下，現有的AI對齊方法是否真的能很好地滿足用戶偏好，並提出了一種新的評估指標和方法。", "applications": ["**個性化推薦系統：** 應用於電商平台或影音平台，根據使用者真實偏好，克服演算法假設的單一偏好模型限制，提供更精準、更符合使用者需求的商品或內容推薦。", "**醫療診斷輔助系統：** 應用於醫療領域，考量不同醫生或患者對診斷結果或治療方案的不同偏好，避免系統受到單一偏好影響，提供更全面、客觀的診斷輔助。", "**教育資源匹配系統：** 應用於教育領域，根據學生和老師的不同學習和教學偏好，智能匹配學習資源和教學方式，提升學習效率和教學效果。"], "pitch": "我們發現現有的AI對齊方法在處理多樣化用戶偏好時存在嚴重問題，導致模型無法有效滿足用戶需求，帶來了巨大的商業機會。我們的研究提出了一種新的評估指標和方法，可以更準確地衡量對齊方法的表現，並為開發更有效的對齊算法提供指導。我們相信，基於我們的研究成果，可以開發出更個性化、更精準的AI應用，例如更高效的推薦系統、更客觀的醫療輔助診斷工具，以及更智能化的教育資源匹配系統，在相關領域產生巨大的商業價值，並帶來可觀的投資回報。我們的團隊擁有深厚的AI背景和豐富的產品開發經驗，我們正在尋求種子輪投資，以加速研究成果的商業化進程。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-05-31T03:07:43.060692"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在視覺空間智能中的能力", "summary_zh": "Spatial-MLLM是一種新的框架，旨在純粹從2D視覺資訊（例如圖片或影片）中提升多模態大型語言模型(MLLM)的空間推理能力。不同於以往需要額外3D或2.5D數據才能進行空間理解的模型，Spatial-MLLM利用一個雙編碼器架構，分別提取語義特徵和3D結構特徵，再將這些特徵融合。此外，還提出了一種空間感知的幀採樣策略，在有限的資源下選擇對空間推理至關重要的幀。透過在Spatial-MLLM-120k數據集上進行訓練，Spatial-MLLM在各種現實世界的數據集上展現了最先進的視覺空間理解和推理性能。", "applications": ["**自動駕駛系統：** 理解複雜的交通場景，例如預測車輛、行人和其他物體的移動軌跡，並做出更安全的駕駛決策，特別是在只有2D攝影機輸入的情況下。", "**機器人導航：** 讓機器人能夠僅憑藉視覺資訊（例如攝影機畫面）在未知的環境中導航，避免碰撞，並完成指定的任務，例如倉庫揀貨或建築物巡檢。", "**虛擬實境/擴增實境：** 提升VR/AR體驗的真實感，例如，讓使用者在觀看2D影片時，能夠更準確地理解場景的3D空間關係，並與虛擬物件進行更自然的互動。"], "pitch": "各位投資人，我們正在重新定義機器視覺的未來。Spatial-MLLM 解決了 MLLM 在空間智能領域的重大瓶頸，使其能僅憑 2D 影像資料就能實現精準的空間推理。這開創了廣闊的應用前景，從提升自動駕駛安全性、優化機器人導航到打造更沉浸式的 VR/AR 體驗。我們的技術比現有方案更具效率和泛用性，無需額外的3D感測器或數據。 我們已經證明了Spatial-MLLM在現實世界數據集上的卓越性能，並構建了專用的訓練數據集。我們的商業模式將包括軟體授權、客製化解決方案，以及在自動駕駛、機器人和遊戲等高增長市場的應用。我們正在尋找投資夥伴，共同將Spatial-MLLM推向市場，引領下一代視覺智能革命，預計未來幾年將產生數十億美元的市場價值。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-31T03:07:59.672710"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流變換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一個基於 LoRA 模型的多概念圖像編輯框架。它利用了 Flux 式擴散變換器內部特徵交互模式的一個關鍵觀察：概念特定的變換器特徵在去噪過程早期就會激活空間上連貫的區域。因此，LoRAShop 可以為每個概念導出解耦的潛在遮罩，並僅在包含要個性化的概念的區域內混合相應的 LoRA 權重。這樣產生的編輯能夠無縫地將多個主體或風格整合到原始場景中，同時保留全局上下文、光照和精細細節。LoRAShop 相較於基線方法，能夠更好地保留身份信息。它無需重新訓練和外部約束，將個性化的擴散模型轉變為實用的“使用 LoRA 的 Photoshop”工具，並為組合式視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**個性化產品設計預覽：** 用戶可以上傳自己的照片或選擇不同風格，即時預覽個性化產品（如手機殼、T恤）的效果，無需設計師協助。", "**快速生成兒童故事插圖：** 輸入角色描述和場景，LoRAShop 可以快速生成風格統一、角色一致的插圖，方便父母或老師創作個性化的兒童故事。", "**電影場景快速迭代：** 電影製作人可以使用 LoRAShop 快速替換演員、修改場景風格，進行前期視覺化和場景規劃，大大縮短迭代時間和降低成本。"], "pitch": "LoRAShop 代表了圖像編輯領域的重大突破，它讓非專業用戶也能輕鬆實現專業級的圖像編輯效果。無需重新訓練和複雜的參數調整，用户就能快速融合多個概念，打造獨一無二的視覺内容。想像一下，一個擁有海量 LoRA 模型市場的未來，用戶可以像選擇濾鏡一樣，選擇不同風格和主題的 LoRA 模型，應用到他們的照片和圖像中。這不僅降低了圖像編輯的門檻，也為內容創作者提供了無限的創作空間。LoRAShop 的潛在商業價值體現在以下幾個方面：1）大幅提升圖像編輯的效率和易用性，吸引更廣泛的用戶群體。2）開創了全新的 LoRA 模型市場，激勵更多創作者分享和交易他們的模型。3）為個性化內容生成提供了強大的技術支持，可以應用於電商、廣告、娛樂等眾多領域。我們相信 LoRAShop 有潜力顛覆現有的圖像編輯市場，成为新一代视觉内容创作的核心引擎。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-31T03:08:17.207454"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI對齊的扭曲：偏好優化真的能優化偏好嗎？", "summary_zh": "大型語言模型在預訓練後，會基於成對比較與人類偏好對齊。然而，現有的對齊方法（如基於PPO的RLHF和DPO）假設所有使用者都只有單一的偏好模型，但實際上使用者偏好非常多元。因此，這些方法是否能讓使用者平均滿意仍是個疑問，這也是多元對齊的基本要求。本研究借鑒社會選擇理論，透過個別的Bradley-Terry (BT) 模型來模擬使用者的比較，提出了一種衡量對齊方法*扭曲*的指標：即最佳可實現的平均效用，與學習策略的平均效用之間的最壞情況比率。研究發現，Nash Learning from Human Feedback 能夠達到極小極大最佳扭曲，但RLHF和DPO則存在較大的扭曲，甚至在某些情況下會是無界的。", "applications": ["**個人化推薦系統：** 理解使用者間的偏好差異，避免推薦系統只推薦多數人喜歡的，導致少數人的特殊偏好被忽略。例如，一個線上音樂平台可以根據使用者的歷史聆聽紀錄和明確的偏好設定，推薦更符合個人口味的音樂，而不是只推薦熱門歌曲。", "**醫療決策輔助：** 在治療方案選擇上，考慮不同患者對治療效果和副作用的權衡。醫生可以利用系統分析不同治療方案的偏好分布，提供更客製化的建議，而不是只依照標準流程治療。", "**政治民意調查：** 更精準地了解不同群體對政策的偏好，避免單一民意調查結果掩蓋少數族群的聲音。可以設計更精細的調查問卷，並結合AI模型分析不同人口結構的偏好，更客觀地反映社會真實情況。"], "pitch": "我們解決了AI對齊領域的一個核心問題：如何處理多元化的使用者偏好，避免現有方法造成的『偏好扭曲』。我們的研究揭示了現有RLHF和DPO方法的缺陷，並提出了一種更穩健的解決方案。商業價值體現在：(1)提升AI產品的客戶滿意度和留存率，特別是在個人化推薦、醫療輔助決策等需要高度客製化的領域；(2) 為企業提供更精準的市場分析和使用者行為洞察，協助制定更有效的產品策略；(3)降低因AI演算法偏見帶來的法律和聲譽風險。我們的技術具有高度可擴展性和商業化潛力，可以整合到現有的AI開發流程中，成為下一代AI對齊技術的關鍵組件。我們正在尋找投資者，一起打造更公平、更符合人類多元需求的AI產品。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-05-31T04:17:17.338979"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升多模態大型語言模型在基於視覺的空間智能中的能力", "summary_zh": "Spatial-MLLM是一種新穎的框架，旨在僅通過2D視覺觀察進行空間推理，從而提升多模態大型語言模型（MLLM）的空間智能。它採用雙編碼器架構，一個提取語義特徵，另一個從視覺幾何模型中提取3D結構特徵。此外，還設計了一種空間感知的幀採樣策略，專注於視頻序列中對空間推理至關重要的幀。模型基於Spatial-MLLM-120k數據集進行訓練，並在各種實際數據集上實現了最先進的性能。", "applications": ["**自動駕駛：**幫助車輛理解周圍環境的3D結構，例如預測其他車輛的運動軌跡或識別道路上的障礙物，即使只有2D攝像頭輸入。", "**室內導航：**協助機器人在室內環境中導航，例如在倉庫中找到特定商品或在家庭環境中幫助老年人或殘疾人士移動，僅需基於攝像頭的2D影像。", "**AR/VR 內容創建：**讓開發者更容易創建與現實世界精確對齊的AR/VR體驗，例如自動生成3D模型，並根據2D圖像快速定位虛擬對象在真實場景中的位置。"], "pitch": "Spatial-MLLM解決了多模態大型語言模型在僅使用2D視覺輸入進行空間推理方面的瓶頸。相較於依賴額外3D數據的解決方案，我們的模型更具靈活性和可擴展性，適用於更廣泛的應用場景。自動駕駛、機器人導航、AR/VR等領域對空間智能的需求日益增長，Spatial-MLLM在這些領域具有巨大的商業潛力。我們計劃將Spatial-MLLM整合到更廣泛的AI解決方案中，並通過API服務和授權許可等方式實現商業化。團隊擁有深厚的AI技術積累和產品開發經驗，有信心將Spatial-MLLM打造成為行業領先的空間智能解決方案。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-31T04:17:30.351909"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：利用修正流轉換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop是一個創新的框架，它利用LoRA模型實現多概念圖像編輯，無需重新訓練。核心洞察在於Flux風格擴散轉換器中，概念特定的轉換器特徵在降噪過程早期就會激活空間上一致的區域。LoRAShop藉此為每個概念導出分離的潛在遮罩，並僅在概念周圍區域混合相應的LoRA權重。因此，產生的編輯可以將多個主體或風格無縫地融入原始場景，同時保留全局背景、光照和細節。實驗表明，LoRAShop在保持身份一致性方面優於其他方法，透過消除重新訓練和外部約束，LoRAShop將個人化的擴散模型轉變為實用的「LoRA版Photoshop」，為組合視覺故事和快速創意迭代開闢了新途徑。", "applications": ["**產品設計快速迭代：** 設計師可以快速將不同的材質、顏色或風格應用於產品圖像，並即時預覽效果，加速產品設計流程。", "**虛擬試穿與搭配：** 用戶可以將自己的照片上傳，並將不同的服飾、髮型或配飾應用於照片，實現虛擬試穿和搭配，提升購物體驗。", "**個性化內容創作：** 用戶可以輕鬆地將自己或朋友的照片融入到不同的場景或藝術風格中，創作獨特的個性化內容，用於社交媒體分享或紀念品製作。"], "pitch": "LoRAShop解決了圖像編輯領域中耗時且成本高昂的痛點——無需重新訓練即可快速編輯圖像中的多個概念。這意味著更快的開發週期、更低的運營成本和更廣泛的應用場景。其『LoRA版Photoshop』的願景，使其在圖像編輯、內容創作和個性化定制等市場具有巨大的商業潛力。想象一下，一個可以讓你立即將你的愛貓咪放到星際飛船駕駛艙的應用程序，或者一個允許你設計完美婚禮請柬的平台，而無需任何專業知識。LoRAShop正在賦能創意，並準備好在視覺內容生成領域掀起一場革命。我們的早期測試顯示用戶參與度很高，並有強烈的意願為這種便捷、易於使用的工具付費。我們正在尋求投資，以擴大我們的研發團隊，構建一個商業化的平台，並將LoRAShop推向全球市場。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-31T04:17:46.661076"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI對齊的扭曲：偏好優化真的能優化偏好嗎？", "summary_zh": "目前的大型語言模型會基於人類的成對比較進行對齊。然而，現有的對齊方法，例如基於PPO的RLHF和DPO，假設模型能與單一偏好模型對齊，但實際上使用者擁有不同的偏好。因此，這些對齊方法產生的模型是否能平均滿足使用者尚不明確，這是多元對齊的最低要求。本研究借鑒社會選擇理論，並透過個別的Bradley-Terry模型來模擬使用者的比較，引入了對齊方法的扭曲概念，即最佳可實現的平均效用與學習策略的平均效用之間的最差比例。研究發現，Nash Learning from Human Feedback在各種效用分佈、比較對的分佈和允許的KL散度下，能穩健地實現最小最大化的最佳扭曲，而RLHF和DPO則在沒有KL約束的情況下就遭受較高的扭曲，甚至在完整設置中遭受無界扭曲。簡單來說，就是現在的AI對齊方法可能無法有效地考慮使用者偏好的多樣性，導致結果不如預期。", "applications": ["**個性化教育系統：** 根據每個學生的學習風格和偏好，調整課程內容和教學方式，提高學習效率和參與度。目前的教育系統往往一刀切，無法針對每個學生的獨特性進行優化。", "**產品推薦引擎：** 根據使用者的不同需求和偏好，提供更精準的產品推薦，提升購買轉換率和使用者滿意度。解決目前推薦引擎同質化嚴重，無法真正理解使用者深層次需求的痛點。", "**社群媒體內容過濾：** 根據使用者的興趣和價值觀，過濾掉令人不快的內容，創造更友善和健康的網路環境。避免使用者因為演算法偏誤而陷入資訊繭房。"], "pitch": "目前的AI對齊方法在處理使用者偏好差異方面存在缺陷，導致效用降低，使用者體驗不佳。我們的研究揭示了這些缺陷，並提供了一種更穩健的解決方案，即Nash Learning from Human Feedback。這項技術的潛在商業價值巨大：想像一下，一個可以真正理解並滿足每個使用者獨特需求的人工智慧系統，無論是個性化教育、產品推薦，還是內容過濾，都能帶來顯著的效率提升和使用者滿意度提升。這不僅能提高企業的營收和利潤，更能建立更強大的使用者忠誠度和品牌聲譽。我們相信，這項技術將是未來人工智慧發展的關鍵趨勢，我們尋求投資者共同開創一個更個性化、更高效、更友善的人工智慧時代。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-05-31T05:13:15.030585"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在基於視覺的空間智能方面的能力", "summary_zh": "這篇論文提出 Spatial-MLLM，一個新的框架，專門用於從純2D視覺資訊中進行空間推理。它結合了兩種編碼器：一個提取語義特徵的2D視覺編碼器，以及一個從視覺幾何模型骨幹初始化的空間編碼器，用於提取3D結構特徵。此外，他們還提出了一種空間感知幀採樣策略，在推論時選擇資訊量最大的幀。透過在 Spatial-MLLM-120k 資料集上進行訓練，Spatial-MLLM 在各種現實世界數據集上，於基於視覺的空間理解和推理任務中，都達到了最先進的性能。", "applications": ["輔助導航：幫助視障人士或在複雜環境中導航，例如大型商場或倉庫，通過分析手機攝像頭拍攝的影像，提供精確的方位和方向指示。", "自動駕駛：提升自動駕駛系統對周遭環境的空間理解能力，例如更準確地判斷車輛與行人、建築物之間的距離和位置關係，從而提高安全性。", "虛擬實境(VR/AR)：讓VR/AR應用程序更精確地感知用戶所處的空間，實現更逼真的互動體驗，例如在虛擬空間中放置物件，並使其與真實世界的家具產生互動。"], "pitch": "Spatial-MLLM 解決了現有MLLM在空間推理方面的局限性，特別是在只有2D視覺輸入的情況下。 我們獨特的雙編碼器架構和空間感知幀採樣策略，使其在基於視覺的空間理解和推理任務中表現卓越。想像一下，有了Spatial-MLLM，我們可以打造更智能、更安全的自動駕駛系統、為視障人士提供革命性的導航工具、以及創造更身臨其境的VR/AR體驗。我們的技術在智能交通、輔助技術、娛樂等多個領域都具有巨大的潛力。我們正在尋找投資者，一起將這項突破性的技術推向市場，建立一個以更精確、更直觀的空間理解能力為基礎的未來。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-31T05:13:27.805365"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop: 無需訓練，基於修正流變換器的多概念圖像生成與編輯", "summary_zh": "LoRAShop是首個基於LoRA模型的多概念圖像編輯框架。它觀察到在Flux風格的擴散變換器中，特定概念的變換器特徵會在去噪過程的早期階段激活空間上連貫的區域。LoRAShop利用這一觀察，在先前的正向傳播中為每個概念導出一個解耦的潛在遮罩，並僅在包含待個性化概念的區域內混合相應的LoRA權重。由此產生的編輯能夠將多個主體或風格無縫地集成到原始場景中，同時保留全局上下文、光照和細節。實驗表明，LoRAShop比基準模型更能保持身份一致性。通過消除重新訓練和外部約束，LoRAShop將個性化的擴散模型轉變為一個實用的『基於LoRA的Photoshop』工具，並為組合視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**個性化圖像編輯App：** 使用者可以輕鬆地將自己或其他人的風格、服飾或物品添加到現有照片中，例如更換背景、添加新的配飾或改變服裝風格。", "**電商產品展示增強：** 商家可以快速生成不同風格或場景下的產品圖片，例如將同一款沙發展示在不同風格的客廳中，或者讓模特穿上不同款式的服裝。", "**遊戲角色定制：** 遊戲開發者可以利用LoRAShop快速生成不同風格、裝扮或外觀的遊戲角色，方便玩家進行個性化定制，提升遊戲體驗。"], "pitch": "LoRAShop打破了傳統圖像編輯的壁壘，無需耗時的重新訓練，就能實現多概念的精準編輯。想像一下，一個人人都能使用的『AI Photoshop』，讓視覺創作變得簡單、高效、且充滿個性。這不僅能顛覆現有圖像編輯市場，更能在電商、遊戲等領域創造巨大價值。其商業潛力體現在：\n\n*   **低成本高效益：** 省去了訓練成本，大大降低了AI圖像編輯的門檻。\n*   **廣泛應用場景：** 從個人用戶到企業，都能找到應用LoRAShop的場景。\n*   **競爭優勢明顯：** 在保持身份一致性的同時，實現精細的多概念編輯，超越現有技術水平。\n\n我們相信，LoRAShop將引領下一代AI圖像編輯革命，並為投資者帶來豐厚的回報。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-31T05:13:42.176369"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI對齊的扭曲：偏好優化真的能優化偏好嗎？", "summary_zh": "現今大型語言模型透過成對比較與人類偏好對齊。主流對齊方法，如基於PPO的RLHF和DPO，假設模型會對齊單一偏好模型。然而，實際應用場景中使用者偏好各異。本研究引入「扭曲」概念，衡量對齊方法在最差情況下，實際模型效用與最佳效用的差距。研究發現，Nash Learning from Human Feedback能達到最佳扭曲，而RLHF和DPO的扭曲程度則嚴重許多，尤其在完整設定下。", "applications": ["個人化推薦系統：改善購物、影音等推薦的準確性，確保能照顧到不同使用者的獨特品味，避免推薦過於集中在熱門選項。", "醫療診斷輔助：在診斷決策中，權衡不同醫生的專業意見和病患的個人偏好，避免過度依賴單一診斷模型，減少誤診風險。", "法律判決支援：幫助法官在量刑時，考慮不同社會群體的觀點和受害者的情感，避免判決過於單一化，提升判決的公平性。"], "pitch": "我們解決了AI對齊領域的核心問題：如何讓AI真正理解並滿足多樣化的使用者偏好，而非只迎合單一標準。我們的研究揭示了現有方法的缺陷，並提供了一種更穩健的解決方案。想像一下，一個推薦系統，它不僅僅給你推薦大眾喜歡的，而是真正了解你的獨特品味。一個醫療診斷系統，它會考慮不同醫生的意見和病患的偏好，做出更全面的判斷。這就是我們的技術能帶來的價值。我們提供了一種能夠量化和優化AI對齊的「扭曲」指標，並找到了一種有效降低扭曲的方法。這項技術具有巨大的商業潛力，尤其是在需要高度個人化和公平性的應用場景，例如推薦系統、醫療診斷、法律判決等等。投資我們的技術，您將能擁有一個更可靠、更公正、更能滿足使用者需求的AI解決方案，從而在市場上取得領先地位。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-05-31T06:17:56.846783"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在視覺空間智能方面的能力", "summary_zh": "這篇論文提出了一個新的框架 Spatial-MLLM，它只基於 2D 圖像就能進行空間推理。過去的多模態大型語言模型（MLLM）在 2D 視覺任務上表現出色，但在提升空間智能方面仍有挑戰，而且需要額外的 3D 或 2.5D 資料。 Spatial-MLLM 使用雙編碼器架構，一個提取語義特徵，另一個從視覺幾何模型提取 3D 結構特徵，並整合這些特徵以增強空間理解。此外，還提出了一種空間感知幀採樣策略，在有限的token長度下，選擇對空間推理至關重要的幀。論文還建構了 Spatial-MLLM-120k 資料集進行訓練，並在多個真實世界資料集上驗證了 Spatial-MLLM 在視覺空間理解和推理任務中的卓越表現。", "applications": ["**自動駕駛與導航：** 分析行車記錄器影片，即時理解道路環境和車輛周圍的空間關係，提升自動駕駛的安全性和準確性。", "**機器人導航與操作：** 讓機器人僅透過攝影機影像就能理解周圍環境，執行複雜的空間任務，例如在倉庫中揀貨、在狹小空間中操作。", "**智慧家居與監控：** 從監視器影像中理解房屋內的空間布局和物體位置，實現更精準的行為監控和智慧控制，例如自動調整燈光、辨識跌倒事件。"], "pitch": "Spatial-MLLM 解決了多模態大語言模型在純2D輸入下進行空間推理的關鍵瓶頸。相較於需要額外3D數據的傳統方案，Spatial-MLLM 更具實用性，應用場景廣泛。我們的技術能顯著提升自動駕駛、機器人、智慧家居等領域的空間理解能力，降低成本並提高效率。我們已證明在多個真實世界數據集上的領先性能。我們正在尋求投資以加速產品開發，擴大資料集規模，並將 Spatial-MLLM 整合到更多應用場景中，搶占市場先機，成為視覺空間智能領域的領導者。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-31T06:18:11.847663"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流變換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一個開創性的框架，它利用預先訓練好的 LoRA 模型，無需重新訓練，即可對圖像進行多概念編輯。 其核心思想在於，它發現 Flux 式擴散變換器內部，特定概念的 Transformer 特徵在去噪過程的早期階段，會激活空間上連貫的區域。 LoRAShop 藉此提取出每個概念的解耦潛在遮罩，並僅在概念周圍區域混合相應的 LoRA 權重，從而將多個主體或風格無縫整合到原始場景中，同時保留全局上下文、光照和細節。 實驗證明，LoRAShop 在身份保留方面優於其他方法，並將個性化的擴散模型轉變為實用的 'LoRA 版 Photoshop' 工具，開闢了構圖視覺敘事和快速創意迭代的新途徑。", "applications": ["**快速風格轉移與人物融合：** 用戶可以輕鬆將自己或朋友的照片融入特定藝術風格的畫作中，例如將自己的照片與梵谷的星夜風格融合。", "**產品設計原型快速迭代：** 設計師可以快速修改產品圖片，例如替換汽車顏色、輪胎樣式，甚至更換背景，無需耗時的 3D 建模或渲染。", "**電影特效與場景合成：** 電影製作人可以快速將不同演員的臉部特徵融合到同一個角色中，或者將多個場景元素無縫地整合到一個場景中，節省大量後期製作時間和成本。"], "pitch": "LoRAShop 解決了現有圖像編輯方案中，多概念融合困難、訓練成本高昂的問題，為用戶提供了一個無需訓練、快速且易於使用的解決方案。 憑藉其卓越的身份保留能力和對全局上下文的理解，LoRAShop 在圖像編輯領域具有顛覆性潛力。 商業價值體現在以下幾個方面：1) 授權給現有的圖像編輯軟體，提升其功能和用戶體驗，開闢新的營收來源。 2) 打造一個獨立的雲端圖像編輯平台，提供基於訂閱的服務，吸引設計師、藝術家和普通用戶。 3) 與電商平台合作，為用戶提供個性化的產品圖片生成和編輯服務，提升產品銷售額。 LoRAShop 的核心技術具有高度可擴展性，未來可應用於視頻編輯、遊戲開發等領域，市場前景廣闊，投資回報可觀。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-31T06:18:28.042520"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升多模態大型語言模型在視覺空間智能方面的能力", "summary_zh": "這篇論文提出Spatial-MLLM，一個全新的框架，旨在提升多模態大型語言模型(MLLMs)在純2D視覺觀察下的空間推理能力。不同於以往仰賴額外3D或2.5D數據的模型，Spatial-MLLM透過結合預訓練的2D視覺編碼器和空間編碼器，提取語義和3D結構特徵。此外，他們提出了一種空間感知幀採樣策略，選擇影片序列中對空間推理至關重要的幀。透過在Spatial-MLLM-120k數據集上的訓練和微調，Spatial-MLLM在各種真實世界的數據集上，於視覺空間理解和推理任務中達到了最先進的性能。", "applications": ["**自動駕駛：** 透過車載攝影機提供的2D影像，Spatial-MLLM可以更準確地理解周圍環境的3D結構，例如判斷道路坡度、行人位置和障礙物距離，提升自動駕駛的安全性。", "**室內導航：** 利用手機或平板電腦的攝像頭，Spatial-MLLM可以幫助使用者在室內空間中導航，例如在大型商場或博物館中找到特定店鋪或展覽。", "**機器人操作：** 在倉儲物流或製造業中，Spatial-MLLM可以幫助機器人理解環境的3D結構，並執行複雜的任務，例如抓取放置在特定位置的物體。"], "pitch": "Spatial-MLLM解決了多模態大型語言模型在2D視覺數據下的空間推理痛點，為各行各業開啟了全新的可能性。透過專利的雙編碼器架構和空間感知幀採樣策略，我們能更有效地利用現有2D視覺資料，大幅降低導入3D感測器的成本。想像一下，自動駕駛、室內導航、機器人操作等領域，都能因為Spatial-MLLM的加入，以更低的成本、更強大的性能，實現智慧化的飛躍。我們相信Spatial-MLLM具有巨大的商業潛力，能夠革新基於視覺理解的AI應用，並成為下一代空間智能的核心引擎。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-31T07:12:38.288717"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：基於校正流轉換器的免訓練多概念圖像生成與編輯", "summary_zh": "LoRAShop是一個全新的框架，能夠使用LoRA模型進行多概念圖像編輯，無需重新訓練。它發現了Flux風格擴散轉換器內部特徵交互的關鍵模式：特定概念的轉換器特徵在去噪過程的早期就激活空間連貫的區域。LoRAShop利用這個發現，為每個概念導出一個解耦的潛在遮罩，並且只在包含待個性化概念的區域內混合相應的LoRA權重。編輯後的圖像能夠無縫地將多個主體或風格融入原始場景，同時保留全局上下文、光照和精細細節。實驗表明，LoRAShop在身份保留方面優於現有方法。透過消除重新訓練和外部約束，LoRAShop將個性化的擴散模型變成一個實用的“LoRA版Photoshop”工具，並為組合視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**個人化頭像生成：** 允許用戶上傳少量個人照片（例如5張），然後使用LoRAShop將自己添加到各種風格的圖像中，例如科幻電影場景、古典油畫風格等，快速生成個性化社交媒體頭像。", "**虛擬試穿/試戴：** 在電商平台上，用戶可以上傳自己的照片，然後使用LoRAShop將不同的服裝或配飾“穿戴”在自己身上，模擬真實的試穿/試戴效果，提高購物體驗和購買意願。", "**廣告素材快速生成：** 廣告公司可以快速生成針對特定受眾的廣告素材。例如，針對年輕族群，可以使用LoRAShop將產品融入到年輕人喜歡的流行文化場景中，生成更具吸引力的廣告。"], "pitch": "LoRAShop解決了圖像編輯領域的一個痛點：多概念編輯的效率和質量問題。現有方法通常需要大量重新訓練或複雜的外部約束，LoRAShop則提供了一種免訓練、高質量的解決方案。其核心價值在於：1. **加速圖像生成流程：** 大幅降低了圖像生成和編輯的時間成本，提高了生產力。2. **賦能非專業用戶：** 降低了圖像編輯的門檻，使普通用戶也能輕鬆創造出高品質的個性化圖像。3. **商業化潛力巨大：** 在電商、社交媒體、廣告營銷等領域具有廣泛的应用前景。我們相信LoRAShop能夠顛覆現有的圖像編輯工作流程，成為下一代圖像生成和編輯的基石。我們的團隊擁有深厚的AI技術積累和豐富的產品開發經驗，希望能夠獲得貴機構的投資，共同將LoRAShop打造成為圖像編輯领域的独角兽企业。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-31T07:12:55.814852"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI對齊的失真：偏好優化是否真的優化了偏好？", "summary_zh": "大型語言模型在預訓練後，會基於成對比較與人類偏好對齊。但目前最先進的對齊方法（例如基於PPO的RLHF和DPO）都假設與單一偏好模型對齊，忽略了使用者偏好的多樣性。因此，這些方法是否能讓使用者平均滿意，都是個問題。本論文借鑒社會選擇理論，透過個人Bradley-Terry模型模擬使用者比較，定義了對齊方法的「失真」：最佳可實現平均效用與學習策略的平均效用之間的最壞情況比例。研究發現，Nash Learning from Human Feedback 在效用分布、比較對的分布和KL散度限制下，表現出最佳的最小最大失真。相反，RLHF和DPO 即使沒有KL約束，也存在較高的失真，在完整設置中，根據比較對的抽樣方式，失真可能呈指數級增長甚至無限大。", "applications": ["**個性化推薦系統：** 根據使用者過去的互動行為和明確偏好，更精準地推薦商品或服務，避免因模型預設的單一偏好而錯失潛在使用者。", "**自動駕駛系統：** 針對不同駕駛者的駕駛習慣（例如：激進型、保守型），提供更符合其偏好的駕駛策略，而非統一的駕駛模式，提升駕駛舒適性和安全性。", "**客戶服務機器人：** 根據客戶的歷史互動和即時情緒，調整機器人的回應方式和提供的解決方案，提供更個性化和高效的客戶服務體驗。"], "pitch": "我們發現現有AI對齊方法在處理多樣化使用者偏好時存在嚴重缺陷，導致模型提供的服務並不能有效滿足使用者需求。我們的研究提供了一種新的評估指標「失真」，並提出了一種更魯棒的對齊方法。這代表著巨大的商業潛力，尤其是在需要高度個性化和使用者滿意度的領域。例如，個性化推薦、自動駕駛、客戶服務等。透過降低AI對齊的「失真」，我們能打造更精準、更人性化的AI系統，提升使用者體驗，增加使用者黏性，最終為企業創造更大的商業價值。我們的技術能幫助企業在AI領域取得競爭優勢，搶佔市場先機。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-05-31T08:16:25.490890"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升基於視覺的空間智能中多模態大型語言模型的能力", "summary_zh": "Spatial-MLLM 是一個新框架，旨在讓多模態大型語言模型 (MLLM) 僅透過 2D 視覺輸入，就能進行更強大的空間推理。與依賴額外 3D 數據的模型不同，Spatial-MLLM 使用雙編碼器架構：一個用於提取語義特徵的 2D 視覺編碼器，和一個基於視覺幾何模型的骨幹，用於提取 3D 結構特徵的空間編碼器。這種方法配合空間感知的幀採樣策略，讓模型能有效利用有限的資訊進行空間推理。透過 Spatial-MLLM-120k 數據集進行訓練後，模型在多個真實世界數據集上，展現了最先進的空間理解和推理性能。", "applications": ["**自動駕駛輔助：** 分析車載攝影機提供的 2D 影像，即時判斷道路標線、障礙物和交通號誌的空間位置關係，提升駕駛安全性。", "**室內導航與地圖構建：** 利用手機或機器人的攝影鏡頭，在室內環境中建立 3D 地圖，並提供精準的導航服務，尤其是在 GPS 訊號較弱的環境。", "**建築設計與施工監控：** 從 2D 設計圖或施工現場的影像中，推斷建築物的 3D 結構，幫助設計師或工程師進行空間規劃和施工進度監控。"], "pitch": "Spatial-MLLM 解決了現有 MLLM 在僅有 2D 視覺輸入下進行空間推理的痛點。我們開發的框架能更精準地理解和推斷 2D 影像中的 3D 空間資訊，這在自動駕駛、室內導航和建築設計等領域具有廣泛的應用前景。現有的 3D MLLM 需要額外的 3D 或 2.5D 數據，限制了其應用範圍。Spatial-MLLM 的突破性技術使其能夠僅透過 2D 影像實現高性能的空間理解，大幅降低了數據採集成本和模型部署的複雜性。透過授權核心算法、提供定制化解決方案或開發垂直行業應用，Spatial-MLLM 具有巨大的商業潛力，有望成為下一代空間智能應用的基石。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-31T08:16:39.227846"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：基於修正流轉換器，無需訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop是首個使用LoRA模型進行多概念圖像編輯的框架。它基於一個關鍵發現：在Flux風格的擴散轉換器中，概念特定的轉換器特徵在去噪過程的早期階段，會在空間上一致的區域激活。因此，LoRAShop可以從先前的正向傳遞中導出每個概念的解耦潛在遮罩，並僅在概念邊界的區域內混合相應的LoRA權重。這樣的編輯可以將多個主體或風格無縫地整合到原始場景中，同時保留全局上下文、光照和精細細節。實驗表明，LoRAShop比基準方法提供更好的身份保留。通過消除重新訓練和外部約束，LoRAShop將個性化擴散模型轉變為實用的“LoRA版Photoshop”工具，並為組合視覺故事敘述和快速創意迭代開闢了新的途徑。", "applications": ["**個性化頭像生成與編輯：** 用戶可以上傳少量照片，然後使用LoRAShop快速生成具有用戶特定風格或加入特定元素的頭像，例如在原本沒有戴帽子的照片中加上一頂帽子。", "**產品設計原型快速迭代：** 設計師可以將現有產品圖像與不同材質、顏色或風格的LoRA模型結合，快速生成多種設計原型，無需重新建模或渲染。", "**創意內容生成：** 廣告商或藝術家可以將不同風格的LoRA模型與現有圖像結合，創造出獨特且吸引人的廣告素材或藝術作品，例如將一輛普通汽車改造成蒸汽朋克風格的汽車。"], "pitch": "LoRAShop解決了個性化圖像編輯和生成領域的一個關鍵痛點：訓練成本和時間。它通過創新的方法，利用現有的LoRA模型，實現了無需訓練的多概念編輯，大幅降低了使用門檻，使其成為一個真正的'LoRA版Photoshop'。 其潛在商業價值巨大，包括：1）簡化了個性化內容創作流程，降低了營銷成本，提升了營銷效率；2）加速了產品設計的迭代速度，缩短了產品上市时间；3）开辟了新的创意内容生成模式，赋能艺术家和设计师。 LoRAShop拥有成为下一代图像编辑和生成平台的基础，并可以基于此开发一系列增值服务，例如LoRA模型市场、定制化编辑工具等， 预计市场规模将达到数十亿美元。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-31T08:16:54.927513"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI對齊的扭曲：偏好優化真的優化了偏好嗎？", "summary_zh": "目前大型語言模型利用成對比較來對齊人類偏好。但現有對齊方法假設存在單一偏好模型，忽略了用戶偏好的多樣性。本研究引入「扭曲」的概念，衡量對齊方法在不同用戶偏好下的表現。結果顯示，某些方法（如Nash Learning from Human Feedback）表現較佳，而RLHF和DPO等方法在處理多樣性偏好時可能存在顯著的扭曲，導致模型無法有效滿足平均用戶的需求。", "applications": ["**個性化推薦系統：** 根據用戶對不同商品或內容的成對比較，更準確地推斷用戶真實偏好，避免推薦結果過於單一或不相關。", "**醫療診斷輔助：** 醫生在評估多種治療方案時，可以利用AI根據過往病患的偏好數據，考量不同患者對風險和療效的權衡，提供更個性化的治療建議。", "**產品設計決策：** 通過分析用戶對產品不同功能的成對比較，識別大多數用戶的核心需求和偏好，避免產品設計過於追求花俏功能而忽略用戶基本需求。"], "pitch": "我們的研究揭示了現有AI對齊方法在處理多樣性偏好時的缺陷，並提出了改進方向。對於創投而言，這意味著巨大的商業機會。想像一下，一個能真正理解並適應用戶個性化偏好的AI系統，將在各行各業引發革命。從更精準的廣告投放、更個性化的教育內容到更安全的自動駕駛系統，我們的技術能顯著提升用戶體驗和滿意度，為投資者帶來豐厚的回報。我們正在打造下一代AI，它不僅僅是智能，更是真正的以人為本。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-05-31T09:13:07.999433"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升多模態大型語言模型在視覺空間智能中的能力", "summary_zh": "最新多模態大型語言模型（MLLMs）在二維視覺任務上表現出色，但提升其空間智能仍具挑戰。現有三維 MLLMs 需額外三維或二點五維數據以加入空間感知，限制了它們在僅有二維輸入（如圖像或影片）情境下的應用。本研究提出 Spatial-MLLM，一種基於純二維觀察進行視覺空間推理的新框架。與依賴為語義理解優化的CLIP視覺編碼器的傳統影片 MLLMs 不同，我們的關鍵在於釋放前饋視覺幾何基礎模型的強大結構先驗知識。具體來說，我們提出一種雙編碼器架構：預訓練的二維視覺編碼器用於提取語義特徵，而空間編碼器（從視覺幾何模型的主幹初始化）用於提取三維結構特徵。連接器將這兩種特徵整合為統一的視覺令牌，以增強空間理解。此外，我們在推理時提出了一種空間感知幀採樣策略，該策略選擇影片序列中空間信息量大的幀，確保即使在有限的令牌長度下，模型也能專注於對空間推理至關重要的幀。除了架構改進外，我們還構建了 Spatial-MLLM-120k 數據集，並使用監督式微調和 GRPO 在其上訓練模型。在各種真實世界數據集上的大量實驗表明，我們的 spatial-MLLM 在廣泛的基於視覺的空間理解和推理任務中實現了最先進的性能。", "applications": ["**自動駕駛感知系統:** 透過車載攝像頭的2D影像，即時建構周遭環境的3D空間結構，提升對行人、車輛和路況的判斷準確性。", "**室內導航與定位:** 在沒有GPS訊號的室內環境，例如商場或博物館，透過手機鏡頭分析2D影像，提供精準的導航路線和定位服務。", "**AR/VR應用開發:** 簡化開發流程，允許開發者僅使用2D圖像或影片創建更具沉浸感和互動性的擴增實境和虛擬實境體驗。例如，將2D照片轉換為3D模型，或在真實場景中放置虛擬物體。"], "pitch": "Spatial-MLLM 解決了現有MLLM在空間理解上的瓶頸，尤其是在僅有2D視覺輸入的場景中。它利用視覺幾何模型的結構先驗知識，在不依賴額外3D數據的情況下，實現了更強大的空間推理能力。這項技術的潛在商業價值巨大，體現在降低自動駕駛感知系統的成本、提升室內定位精度、加速AR/VR內容創作等方面。我們相信Spatial-MLLM能夠改變視覺空間智能的格局，並在多個行業中創造新的應用機會。我們正在尋求投資，以擴大數據集規模、優化模型性能，並將其應用於實際場景中，以實現大規模商業化。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-31T09:13:24.479058"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流變換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop是一個利用LoRA模型進行多概念圖像編輯的框架，無需重新訓練。它發現Flux風格的擴散變換器在去噪過程早期，概念特定的特徵會在空間上相干的區域激活。LoRAShop藉此為每個概念提取出解耦的潛在遮罩，並僅在概念邊界區域內混合相應的LoRA權重。這使得編輯能無縫地將多個主體或風格融入原始場景，同時保留全局背景、光線和細節。實驗表明LoRAShop在身份保留方面優於其他方法。它消除了重新訓練和外部限制，將個性化的擴散模型變成一個實用的“基於LoRA的Photoshop”工具，為組合視覺故事和快速創意迭代開闢了新途徑。", "applications": ["**產品設計修改：**設計師可以快速地將不同材質、顏色或風格應用到產品圖片上，例如為家具添加不同的面料紋理，或將汽車改變成不同的顏色和配置，而無需重新建模和渲染。", "**個人化照片生成：**用戶可以輕鬆地將自己或朋友的照片風格融入到特定的場景中，例如將自己“放入”歷史畫作中，或者根據個人喜好調整照片的風格，例如將風景照片變成水彩畫風格。", "**遊戲資產快速生成：**遊戲開發者可以快速生成具有不同風格和特徵的角色或場景，例如為角色添加不同的服裝和髮型，或將場景改變成不同的天氣和光照條件，大幅縮短開發時間。"], "pitch": "LoRAShop解決了圖像編輯領域長期存在的痛點：如何在保留圖像細節和全局一致性的前提下，快速、靈活地融合多個概念和風格。通過無需重新訓練的LoRA模型，LoRAShop將圖像編輯的門檻大幅降低，讓更多人能夠參與到創意設計中。其潛在的商業價值體現在：1. 授權圖像編輯軟體公司，將其整合到現有產品中，提升產品競爭力；2. 為電商平台提供AI輔助的產品圖片生成與編輯服務，提升產品展示效果和用戶體驗；3. 打造一個基於LoRAShop的創意內容平台，鼓勵用戶分享和交易個性化的圖像編輯方案。LoRAShop有望成為圖像編輯領域的顛覆性技術，並在消費級應用和專業級設計領域都擁有廣闊的市場前景。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-31T09:13:38.175354"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI 對齊的扭曲：偏好優化真的能優化偏好嗎？", "summary_zh": "大型語言模型在預訓練後，會透過成對比較來對齊人類偏好。現行的對齊方法，如基於PPO的RLHF和DPO，假設模型能與單一偏好模型對齊，但實際上使用者擁有不同的偏好。因此，這些方法是否能讓模型平均而言滿足使用者，都還不清楚。本研究借鑒社會選擇理論，透過Bradley-Terry模型建模使用者的比較，定義了對齊方法的扭曲：最佳可實現的平均效用與模型學習到的策略的平均效用之間的最差比例。研究發現，Nash Learning from Human Feedback能在各種情況下達到最佳的最小最大扭曲，而RLHF和DPO則會遭受更大的扭曲，甚至在某些情況下扭曲是無界的。", "applications": ["**個性化教育輔導系統：** 系統根據每個學生的學習風格和偏好進行客製化調整，避免使用統一的“最佳”教學策略，導致部分學生學習成效不佳。", "**多樣化內容推薦平台：** 平台根據不同使用者的興趣和價值觀，推薦新聞、音樂、電影等內容，避免演算法過度強調單一“流行”趨勢，忽略小眾但重要的偏好。", "**多人協作決策工具：** 工具幫助團隊在決策過程中考慮到每個成員的不同意見和優先級，避免決策結果只反映少數人的偏好，造成團隊不滿和協作效率下降。"], "pitch": "現今的AI對齊方法存在嚴重的偏好扭曲問題，導致AI模型無法有效滿足多元使用者的需求。我們的研究揭示了RLHF和DPO等主流方法的缺陷，並證明了Nash Learning from Human Feedback 的優越性。我們提出的扭曲概念和分析框架，為評估和改進AI對齊方法提供了關鍵指標。這將賦能企業構建更公平、更個性化的AI產品，例如在推薦系統、教育平台和協作工具等領域。通過減少偏好扭曲，我們可以提高用戶滿意度，提升AI應用的採用率和商業價值，從而建立更值得信賴的AI生態系統。我們的技術有助於實現真正以人為本的AI，打造更智能、更人性化的未來。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-05-31T10:13:15.832009"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升多模態大型語言模型在視覺空間智能方面的能力", "summary_zh": "本研究提出Spatial-MLLM，一個基於純2D視覺資訊進行空間推理的新框架。不同於依賴額外3D或2.5D數據的傳統方法，Spatial-MLLM利用預訓練的視覺幾何模型骨幹來提取3D結構特徵，並結合2D視覺編碼器提取的語義特徵，透過雙編碼器架構實現更強的空間理解能力。此外，還提出了一種空間感知幀採樣策略，能在有限的tokens限制下，選擇對空間推理至關重要的幀。透過Spatial-MLLM-120k數據集的監督微調和GRPO訓練，Spatial-MLLM在多個真實世界數據集上取得了視覺空間理解和推理任務的最新最佳性能。", "applications": ["**自動駕駛/無人機導航：** 基於行車記錄儀或無人機攝影機提供的2D影像，推斷出周遭環境的3D結構，輔助自動駕駛系統進行更精準的路線規劃和避障。", "**室內導航/機器人導航：** 在室內環境中，僅憑2D影像即可建立空間模型，幫助機器人或使用者進行導航和定位，例如在大型商場或倉庫中找到特定商品或區域。", "**建築設計/城市規劃：** 基於建築物或城市街景的2D照片，快速生成3D模型，用於輔助建築設計師或城市規劃師進行方案設計和評估。"], "pitch": "Spatial-MLLM解決了多模態大型語言模型（MLLM）在僅有2D視覺輸入下的空間推理能力瓶頸。與需要額外3D或2.5D數據的傳統方法不同，我們利用預訓練視覺幾何模型，大幅降低了對數據的需求，擴大了應用範圍。自動駕駛、無人機、機器人導航、以及建築設計等領域都存在對此技術的強烈需求。Spatial-MLLM的優勢在於更低的數據依賴、更強的空間推理能力以及更廣泛的應用前景。我們計劃透過API形式提供解決方案，並針對不同行業進行客製化應用開發，搶佔AI視覺空間智能領域的市場先機，創造巨大的商業價值。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-31T10:13:29.302291"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：基於修正流變換器的免訓練多概念圖像生成與編輯", "summary_zh": "LoRAShop是一個創新的圖像編輯框架，利用LoRA模型實現多概念的圖像操控。它觀察到在Flux風格的擴散變換器中，概念特定的特徵會在去噪過程早期激活空間上連貫的區域。LoRAShop利用這個特性，在正向傳遞中為每個概念導出解耦的潛在遮罩，並僅在限定概念的區域內混合相應的LoRA權重。如此一來，編輯後的圖像能無縫地將多個主體或風格融入原始場景，同時保留全局上下文、光照和精細細節。LoRAShop無需重新訓練或外部約束，將個人化的擴散模型變成一個實用的'LoRA版的Photoshop'工具，為組合式視覺故事敘述和快速創意迭代開闢了新的途徑。", "applications": ["**虛擬試穿應用：** 用戶可以上傳自己的照片，然後使用LoRAShop將各種衣服、髮型、妝容等'概念'添加到照片中，查看不同的搭配效果，而無需實際試穿或化妝。", "**個性化設計工具：** 設計師可以利用LoRAShop快速生成多個設計方案，例如，改變沙發的顏色、材質或添加新的裝飾元素，而無需重新建模或渲染整個場景。", "**快速製作廣告素材：** 廣告商可以通過LoRAShop將不同的產品'概念'無縫地融入現有的廣告圖片或影片中，快速生成多個版本的廣告素材，以測試不同的目標受眾和營銷策略。"], "pitch": "LoRAShop解決了圖像編輯領域的關鍵痛點：傳統方法需要大量訓練和手動調整，耗時且成本高昂。LoRAShop提供免訓練的多概念編輯能力，將個人化的擴散模型轉化為即時的、易於使用的創意工具。這為廣大的用戶群體——從普通消費者到專業設計師和廣告商——開啟了無限的創作可能性。其商業價值體現在以下幾個方面：\n\n*   **降低圖像編輯成本：** 無需昂貴的專業軟件和熟練的設計師，即可快速生成高品質的圖像。\n*   **加速創意迭代：** 快速生成多個版本的設計方案，提高創作效率。\n*   **個性化內容生成：** 滿足用戶對個性化內容的需求，提升用戶體驗。\n*   **潛在的商業模式：** 可作為獨立應用程式、插件或API提供，亦可嵌入到現有的圖像編輯軟件中。License授權和按使用量計費都是可行的商業模式。LoRAShop有望顛覆現有的圖像編輯市場，成為下一個獨角獸。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-31T10:13:46.617923"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI對齊的扭曲：偏好優化真的在優化偏好嗎？", "summary_zh": "現今大型語言模型在預訓練後，會透過成對比較的方式與人類偏好對齊。然而，主流的對齊方法(例如基於PPO的RLHF和DPO)假設使用者偏好一致，但實際上使用者偏好多樣。因此，這些方法是否能讓使用者平均滿意都無法確定。本研究引入「扭曲」的概念，衡量對齊方法在最糟情況下，模型平均效用與最佳平均效用之間的比例。研究發現，Nash Learning from Human Feedback能達到最佳的最小最大扭曲，而RLHF和DPO則在不同情境下會產生更大的扭曲，甚至是不受限制的扭曲。", "applications": ["**個人化推薦系統微調：** 針對不同使用者族群微調推薦系統，減少因偏好差異造成的推薦結果偏差，提升使用者滿意度。例如，針對注重價格的使用者，優化推薦演算法，使其更偏向低價商品。", "**客服機器人情境應答優化：** 客服機器人面對不同使用者提出的問題，避免使用單一標準答案，而是根據使用者過去的互動紀錄和可能的偏好，提供更符合使用者期望的個性化回覆。例如，針對對技術細節感興趣的使用者，提供更詳細的技術說明。", "**教育平台教材個性化生成：** 為不同學習風格的學生生成個性化的教材。例如，針對視覺學習者，生成包含大量圖片和圖表的教材；針對聽覺學習者，生成包含音頻和影片的教材。"], "pitch": "我們發現現有AI對齊方法在面對使用者偏好差異時存在嚴重缺陷，導致模型產生的結果無法滿足大多數使用者。我們的研究提出了一種新的衡量指標「扭曲」，可以有效評估不同對齊方法的優劣。我們的方法(Nash Learning from Human Feedback)能顯著降低扭曲，提高使用者滿意度。在商業上，這意味著更精準的推薦系統、更有效的客服機器人以及更個性化的教育平台，能夠大幅提升使用者黏著度，降低客戶流失率，並最終帶來更高的營收。 我們正在尋找種子輪投資，以將我們的研究成果商業化，開發出下一代更具彈性和更安全的AI對齊技術，引領AI真正服務於人類多樣化的需求。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-05-31T11:10:35.898414"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在基於視覺的空間智能方面的能力", "summary_zh": "現有的多模態大型語言模型(MLLM)在2D視覺任務上表現出色，但提升其空間智能仍具挑戰。過去的方法依賴額外的3D或2.5D數據，限制了其在僅有2D輸入（如圖像或影片）場景中的應用。本論文提出Spatial-MLLM，一種僅基於2D觀察進行視覺空間推理的全新框架。它採用雙編碼器架構，分別從預訓練的2D視覺編碼器提取語義特徵，以及從視覺幾何模型提取3D結構特徵。一個連接器將兩者整合為統一的視覺標記，以增強空間理解。此外，還提出了一種空間感知幀採樣策略，在推理時選擇影片序列中空間信息豐富的幀，確保模型在有限的標記長度下，專注於對空間推理至關重要的幀。研究團隊構建了Spatial-MLLM-120k數據集，並使用監督式微調和GRPO進行訓練。實驗證明，Spatial-MLLM在各種現實世界的數據集上，於基於視覺的空間理解和推理任務中，均取得了最先進的性能。", "applications": ["**自動駕駛導航：** 根據車載攝像頭提供的2D圖像或影片，理解周圍環境的3D空間結構，輔助車輛進行精準的導航和避障。", "**室內機器人：** 讓掃地機器人或服務型機器人能僅通過2D相機或影片，理解房間的空間佈局，更有效地規劃清潔路線或執行任務，例如避開障礙物、找到特定物品。", "**AR/VR 遊戲開發：** 基於2D圖像或影片，快速構建逼真的3D虛擬環境，提升AR/VR遊戲的沉浸感和互動性，讓玩家在虛擬世界中進行更自然的空間互動。"], "pitch": "Spatial-MLLM解決了多模態大型語言模型在僅有2D視覺輸入下進行空間推理的關鍵痛點。它不僅提升了模型在自動駕駛、機器人等領域的空間理解能力，更為AR/VR等沉浸式體驗的開發開啟了新的可能性。傳統的3D重建成本高昂且複雜，而Spatial-MLLM僅需2D數據就能實現高效的空間推理，大幅降低了開發成本。我們預期Spatial-MLLM將成為下一代視覺智能的核心技術，在空間理解、智慧感知等領域擁有廣闊的商業應用前景，市場潛力巨大。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-31T11:10:51.404523"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：利用修正流轉換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一種全新的圖像編輯框架，它使用 LoRA 模型，無需重新訓練就能對圖像進行多概念編輯。它的核心是觀察到 Flux 風格擴散轉換器內部的特徵交互模式：特定概念的轉換器特徵在去噪過程早期就會激活空間上連貫的區域。LoRAShop 利用這個特性，在先前的正向傳遞中，為每個概念導出一個解耦的潛在遮罩，並僅在限定概念的區域內混合相應的 LoRA 權重。這樣做的結果是，編輯可以將多個主體或風格無縫整合到原始場景中，同時保留全局上下文、光照和精細細節。實驗證明，LoRAShop 在身份保留方面優於基準模型。通過消除重新訓練和外部約束，LoRAShop 將個人化擴散模型變成了一個實用的 '帶有 LoRA 的 Photoshop' 工具，並為組合式視覺故事講述和快速創意迭代開闢了新途徑。", "applications": ["**個性化頭像快速生成：** 用戶可以快速生成融合了自己和寵物/愛好等多個概念的個性化頭像，用於社交媒體或遊戲平台。", "**產品設計快速迭代：** 設計師可以快速將不同風格或材質應用到產品圖片上，例如將一張椅子圖片快速轉換成工業風或北歐風，加速設計驗證過程。", "**歷史人物角色扮演：** 用戶可以輕鬆將自己的照片與歷史人物的服裝和背景融合，生成逼真的角色扮演圖像，用於娛樂或教育目的。"], "pitch": "LoRAShop 解決了圖像編輯領域中長期存在的痛點：需要大量訓練才能實現複雜的多概念編輯。它憑藉免訓練的特性，大幅降低了使用門檻和時間成本，讓個人化圖像編輯變得觸手可及。想像一下，設計師再也不需要耗費數小時重新訓練模型，就可以快速探索各種設計方案；內容創作者可以輕鬆創造出獨一無二的視覺內容，吸引更多粉絲；電商平台可以利用LoRAShop，讓用戶在購買前就能預覽產品的個性化定制效果。LoRAShop 具備極高的商業潛力，可應用於圖像編輯軟體、電商平台、社交媒體、遊戲開發等領域，有望顛覆傳統圖像編輯模式，為市場帶來全新價值。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-31T11:11:05.997676"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升多模態大型語言模型在基於視覺的空間智能中的能力", "summary_zh": "Spatial-MLLM是一種新的框架，旨在提升多模態大型語言模型 (MLLM) 在僅基於 2D 圖像或影片的空間推理能力。它透過使用雙編碼器架構，結合預訓練的 2D 視覺編碼器提取語義特徵，以及從視覺幾何模型骨幹初始化的空間編碼器提取 3D 結構特徵。此外，它還引入了一種空間感知幀採樣策略，以選擇影片中對於空間推理至關重要的幀。研究團隊構建了 Spatial-MLLM-120k 資料集，並使用監督式微調和 GRPO 訓練模型。實驗結果表明，Spatial-MLLM 在各種基於視覺的空間理解和推理任務中都取得了最先進的性能。", "applications": ["**自動駕駛/機器人導航：** 僅使用車載相機的 2D 影像，讓自動駕駛汽車或機器人更好地理解周圍環境的空間結構，從而實現更安全、更精確的導航。", "**室內設計/虛擬實境：** 根據平面圖或房間照片，快速生成 3D 模型，協助室內設計師進行規劃，或讓使用者在 VR 環境中體驗空間佈局。", "**醫療影像分析：** 從 CT 或 MRI 掃描影像中，自動識別器官的空間位置和關係，輔助醫生進行診斷和手術規劃。"], "pitch": "Spatial-MLLM解決了MLLM在理解2D圖像或影片中的空間信息的痛點，為機器視覺領域帶來了突破。現有的MLLM在處理空間信息時需要額外的3D數據，而Spatial-MLLM僅需2D輸入即可實現高精度的空間理解。這意味著更廣泛的應用場景和更低的數據獲取成本。我們的技術可以應用於自動駕駛、機器人導航、室內設計、醫療影像分析等多個高潛力市場。透過授權、API服務或嵌入式解決方案，我們可以快速將Spatial-MLLM整合到現有的產品和服務中。Spatial-MLLM不僅是技術的革新，更是商業模式的創新，將為投資者帶來豐厚的回報。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-31T12:23:37.854633"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：基於修正流轉換器的免訓練多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一個創新的圖像編輯框架，它能用 LoRA 模型進行多概念圖像編輯，無需重新訓練。它基於對 Flux 風格擴散轉換器內部特徵交互模式的關鍵觀察：特定概念的轉換器特徵在去噪過程的早期階段就會激活空間上連貫的區域。因此，LoRAShop 能為每個概念導出一個解耦的潛在遮罩，並僅在概念周圍的區域內混合相應的 LoRA 權重。這樣一來，就能將多個主題或風格無縫整合到原始場景中，同時保留全局上下文、光照和細節。LoRAShop 優於現有方法，能更好地保留原始圖像的身份訊息，且無需重新訓練或外部約束，使其成為一個實用的「LoRA 版 Photoshop」，為組合視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**虛擬服裝試穿：** 用戶可以將不同款式的衣服（LoRA模型代表不同款式）無縫整合到自己的照片中，查看試穿效果，而無需實際更換衣服。", "**室內裝潢設計：** 改變房間的風格，例如將客廳從現代風格變成波西米亞風格，或者在房間裡添加新的家具（LoRA模型代表家具類型），快速呈現裝修效果。", "**個性化藝術創作：** 藝術家可以結合多種不同的藝術風格或元素（每個風格/元素由一個 LoRA 模型代表），創作獨一無二的圖像，例如將印象派的筆觸與賽博龐克的元素融合在一起。"], "pitch": "LoRAShop 提供了一個無需重新訓練即可進行多概念圖像編輯的突破性解決方案，賦予使用者前所未有的創作自由。想像一下，一個不需要專業技能就能輕鬆將多種風格和主題融合到圖像中的工具，這將徹底改變圖像生成和編輯的產業。我們的商業價值體現在幾個方面：1. 龐大的用戶群體：從普通用戶到專業設計師，都有潛在的需求。2. 多樣化的應用場景：電商、遊戲、廣告、藝術創作等行業都可應用。3. 顯著的技術優勢：免訓練、高保真度、快速迭代，這些都是競爭對手難以匹敵的。因此，LoRAShop 具有巨大的商業潛力，能夠創造一個全新的圖像編輯和創作生態系統，並帶來豐厚的回報。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-31T12:23:55.328702"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI對齊的失真：偏好優化是否真的優化了偏好？", "summary_zh": "大型語言模型在預訓練後會根據成對比較與人類偏好對齊。然而，目前主流的對齊方法（如基於PPO的RLHF和DPO）假設基於單一偏好模型進行對齊，但實際應用場景中用戶的偏好卻是多樣的。因此，這些對齊方法是否能產生平均而言滿足用戶需求的模型，仍是未知數，而這應是多元對齊的最低要求。本研究借鑒社會選擇理論，透過個別的Bradley-Terry模型對用戶的比較進行建模，引入了對齊方法的「失真」概念：最佳可達成的平均效用與學習到的策略的平均效用之間的最差情況比率。研究發現，Nash Learning from Human Feedback具有minimax最佳失真，而RLHF和DPO則存在較大的失真。", "applications": ["**客製化AI助理：** 為不同用戶群體（例如，兒童、青少年、老年人）提供針對其特定偏好量身定制的回應風格和內容。避免所有用戶都接受同一種『標準』回答，造成刻板印象或不適用性。", "**醫療診斷輔助：** 不同醫生在診斷和治療方案上可能存在偏好差異。AI系統可以根據醫生的偏好，提供更符合其習慣和經驗的輔助資訊，提高診斷效率和準確性。", "**法律判決輔助：** 在法律领域，对于相似案件，不同法官可能在量刑和判决标准上存在差异。AI系统可以学习不同法官的偏好，并为他们提供更有针对性的建议，从而提高法律判决的公平性和效率。"], "pitch": "我們解決了現有AI對齊方法中的核心問題：忽略了用戶偏好的多樣性，導致對齊效果不佳甚至失真。我們的研究提供了一種新的度量標準（失真）來評估不同對齊方法的優劣，並證明了某些方法（如Nash Learning from Human Feedback）具有更好的魯棒性和可擴展性。這項技術的商業價值在於，它可以打造真正個性化且更能滿足用戶需求的AI產品，從而提高用戶滿意度、增強產品競爭力，並開拓新的應用場景。例如，客製化AI助理可以大幅提升使用率和用戶黏著度，醫療診斷輔助系統可以減少誤診率，而法律判決輔助系統則能提升判決的公平性。透過授權我們的技術，我們可以幫助企業構建更可靠、更人性化的AI解決方案，佔領市場先機。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-05-31T13:23:30.602371"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升多模態大型語言模型在視覺空間智能上的能力", "summary_zh": "這篇論文介紹了一種新的框架 Spatial-MLLM，它能讓多模態大型語言模型（MLLM）僅憑2D圖像或影片，就能更好地理解和推理空間關係，而不需要額外的3D或2.5D數據。Spatial-MLLM 使用雙編碼器結構，一個提取語義特徵，另一個提取3D結構特徵，並結合空間感知的幀採樣策略，來提升模型對空間信息的理解和推理能力。實驗結果顯示，Spatial-MLLM在多種真實世界的數據集上，都取得了最先進的性能。", "applications": ["**無人機導航與避障：** 無人機可以僅憑攝像頭拍攝的2D畫面，利用 Spatial-MLLM 理解周圍環境的空間結構，實現更精準的導航和避障，避免碰撞。", "**自動駕駛感知系統：** 自動駕駛車輛可以利用 Spatial-MLLM 分析攝像頭捕捉到的影像，更準確地判斷車道線、障礙物的位置和距離，提升駕駛安全性。", "**虛擬實境(VR)室內設計：** 藉由用戶提供的房間照片或影片，Spatial-MLLM 可以理解房間的空間結構，協助使用者在 VR 環境中進行室內設計，模擬家具擺放效果。"], "pitch": "Spatial-MLLM 打破了傳統多模態模型對3D數據的依賴，僅需2D圖像或影片即可實現卓越的空間理解能力。這代表我們能夠為現有的影像分析服務增加強大的空間推理能力，而無需大幅修改現有硬體設施。想像一下，你可以用現有的監控攝影機了解工廠動線、危險區域；或是用手機鏡頭就能快速完成室內空間建模，提供更精準的AR/VR體驗。這將顛覆智能安防、自動駕駛輔助、甚至家居設計等多個領域。Spatial-MLLM 不僅僅是技術突破，更代表著降低成本、加速部署，以及創造全新商業模式的機會。我們正在尋找合作夥伴，共同將這項技術推向市場，引領新一代的空間智能革命。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-31T13:23:45.952082"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：基於校正流轉換器的免訓練多概念圖像生成與編輯", "summary_zh": "LoRAShop是一個創新的框架，它利用LoRA模型實現多概念圖像編輯，無需重新訓練。透過觀察Flux風格擴散轉換器內部特徵交互模式，發現概念特定的轉換器特徵在去噪過程的早期階段即會激活空間相干區域。LoRAShop利用這一點，在預先的正向傳遞中為每個概念導出一個解耦的潛在遮罩，並僅在限制個性化概念的區域內混合相應的LoRA權重。這使得編輯後的圖像能夠將多個主體或風格無縫整合到原始場景中，同時保留全局上下文、光照和細節。實驗表明，LoRAShop比其他方法更能保留身份資訊。透過消除重新訓練和外部約束，LoRAShop將個性化擴散模型轉變為實用的`LoRA版本的Photoshop`工具，並為組合式視覺敘事和快速創意迭代開闢了新途徑。", "applications": ["**線上服裝搭配：** 用戶上傳自己的照片，可以輕鬆將不同款式的衣服（LoRA模型代表不同服裝風格）融合到照片中，預覽穿搭效果，無需實際試穿。", "**室內設計模擬：** 房屋仲介或屋主可以快速將不同風格的家具（LoRA模型代表不同家具風格）添加到房間照片中，展示裝修後的視覺效果，方便客戶選擇。", "**藝術風格轉換與融合：** 藝術家或設計師可以將多種藝術風格（LoRA模型代表不同藝術風格）混合到圖像中，創造獨特的藝術作品，或者快速迭代探索不同的風格組合。"], "pitch": "LoRAShop解決了個性化圖像編輯領域的一個關鍵痛點：複雜且耗時的重新訓練。它基於免訓練的架構，讓用戶可以像使用Photoshop一樣，輕鬆將多個概念（例如不同的風格、物體、人物）融合到同一張圖片中，且保留原圖的細節和上下文。這為創意工作者、設計師、電商平台等帶來巨大的效率提升。其商業價值體現在：\n\n*   **降低創作門檻：** 降低了圖像編輯的技術門檻，讓更多人可以輕鬆創作個性化的內容。\n*   **加速內容生成：** 大幅縮短圖像編輯和創作時間，提高生產力。\n*   **提升用戶體驗：** 提供更直觀、更便捷的圖像編輯工具，提升用戶滿意度。\n\nLoRAShop的潛在盈利模式包括：軟體授權、API服務、雲端平台等。尤其在電商、廣告、遊戲等行業，對個性化圖像生成和編輯有著巨大需求，LoRAShop具有極高的商業潛力。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-31T13:24:04.569651"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升多模態大型語言模型在視覺空間智能方面的能力", "summary_zh": "論文提出一個名為Spatial-MLLM的新框架，旨在純粹從2D視覺觀察中提升多模態大型語言模型(MLLM)的空間推理能力。 不同於依賴額外3D或2.5D數據的傳統3D MLLM，Spatial-MLLM的關鍵在於釋放前饋視覺幾何基礎模型的強大結構先驗知識。 透過雙編碼器架構，分別提取語義特徵和3D結構特徵，並結合空間感知幀採樣策略，模型能專注於對空間推理至關重要的幀。Spatial-MLLM在各種真實世界數據集上進行了廣泛實驗，證明其在視覺空間理解和推理任務中達到了最先進的性能。", "applications": ["**自動駕駛/機器人導航：** 利用2D攝像頭輸入（例如道路影像）即時理解周圍環境的3D結構，更準確地進行路徑規劃和障礙物避免，尤其是在GPS信號微弱或完全喪失的環境下。", "**醫療影像分析：** 從CT或MRI掃描的2D切片中重建3D器官結構，協助醫生進行腫瘤定位、手術規劃和疾病診斷，提高診斷準確率和效率。", "**建築物室內設計/裝修：** 僅憑藉手機拍攝的幾張房間照片，快速生成3D模型，方便用戶進行虛擬擺設、家具配置和裝修方案模擬，無需專業的3D建模技能。"], "pitch": "Spatial-MLLM解決了多模態大型語言模型在2D視覺輸入下的空間智能瓶頸，利用創新架構和數據集，實現了在視覺空間理解和推理任務上的突破性性能。 其無需額外3D數據的特性，大大降低了部署成本和複雜性，使其具有廣泛的應用前景，涵蓋自動駕駛、醫療影像、室內設計等高潛力市場。投資Spatial-MLLM，意味著搶先佈局下一代基於視覺的AI技術，有望在多個垂直領域帶來顛覆性變革和可觀的商業回報。 我們可以授權技術，也可以創建專用應用，例如為建築公司提供易於使用的3D建模軟件。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-31T14:11:50.966216"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流轉換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一種新的框架，能夠使用 LoRA 模型進行多概念圖像編輯，無需重新訓練。它利用了 Flux 風格擴散轉換器中，概念特定的特徵會在去噪過程早期激活空間上連貫的區域這一關鍵觀察。透過這種方式，LoRAShop 可以為每個概念導出解耦的潛在遮罩，並僅在概念的邊界區域內混合相應的 LoRA 權重。 如此一來，就能將多個主體或風格無縫地整合到原始場景中，同時保留全局上下文、光照和細節。實驗證明，LoRAShop 在身份保留方面優於基準方法。 LoRAShop 透過消除重新訓練和外部約束，將個性化擴散模型轉變為實用的“LoRA 版 Photoshop”工具，並為組合式視覺故事講述和快速創意迭代開闢了新途徑。", "applications": ["**個性化頭像與情境合成：** 用戶上傳多張不同風格或物件的照片，LoRAShop 就能將這些風格或物件無縫整合到頭像或特定情境中，例如將貓咪和卡通風格融入個人頭像。", "**快速生成多樣化的產品設計方案：** 設計師可以快速迭代產品設計，例如將不同材質（木紋、金屬）、顏色、圖案融合到同一產品原型上，生成多種設計方案供選擇，無需重新建模或渲染。", "**電影特效與場景編輯：** 電影製作團隊可以使用 LoRAShop 將不同的視覺元素（例如爆炸、火焰、奇幻生物）融入到現有場景中，快速創建複雜的特效，而無需大量的後期處理。"], "pitch": "LoRAShop 是一個顛覆性的圖像編輯工具，它利用 LoRA 模型和修正流轉換器，在免訓練的情況下實現多概念圖像的生成和編輯。與傳統方法相比，LoRAShop 速度更快、成本更低，並能更好地保留原始圖像的細節和全局上下文。 想像一下，一個設計師可以輕鬆地將不同的風格和元素融合到一個產品原型中，或者一個電影製作人可以快速地創建複雜的視覺效果。 這是一個巨大的市場機會，涵蓋了電商、遊戲、娛樂、設計等多個領域。 我們的商業模式可以包括：授權 LoRAShop 技術給大型企業，提供基於雲端的圖像編輯服務，以及建立一個由 LoRA 模型組成的市場，讓用戶可以購買和分享自己的模型。 LoRAShop 將成為圖像編輯領域的 Game Changer，我們相信它具有巨大的商業潛力。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-31T14:12:09.111017"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在基於視覺的空間智能中的能力", "summary_zh": "這篇論文提出了一個名為Spatial-MLLM的新框架，旨在提升多模態大型語言模型(MLLM)在僅有2D圖像或影片輸入情況下的空間推理能力。Spatial-MLLM透過一個雙編碼器架構，分別利用預訓練的2D視覺編碼器提取語義特徵，並利用從視覺幾何模型主幹初始化的空間編碼器提取3D結構特徵。此外，論文還提出了一種空間感知的幀採樣策略，以確保模型在有限的token長度下，專注於對空間推理至關重要的幀。Spatial-MLLM在各種現實世界的數據集上進行了廣泛的實驗，證明其在基於視覺的空間理解和推理任務中取得了最先進的性能。", "applications": ["**自動駕駛/機器人導航：** 基於路口攝影機或車載攝影機的2D影像，讓自動駕駛系統或機器人能夠更準確地理解周遭環境的空間關係，例如判斷車輛、行人、障礙物的位置和距離，進而做出更安全的行車決策。", "**室內設計/虛擬看房：** 從現有的房屋照片或影片中，自動生成房屋的3D模型，並理解房間的空間佈局，方便室內設計師進行設計規劃，或讓消費者在線上進行沉浸式的虛擬看房體驗。", "**運動分析/遊戲AI：** 分析運動員的比賽影片，提取運動員的動作和位置信息，用於評估運動表現或訓練指導。在遊戲中，可以讓AI角色更好地理解遊戲場景的空間結構，做出更智能的決策，例如戰術佈局、路線規劃等。"], "pitch": "Spatial-MLLM解決了多模態大型語言模型在僅有2D視覺輸入時，空間推理能力不足的問題。透過創新性的雙編碼器架構和空間感知的幀採樣策略，Spatial-MLLM在基於視覺的空間理解和推理任務中取得了顯著的性能提升。這項技術的潛在商業價值巨大，它可以應用於自動駕駛、機器人導航、室內設計、虛擬看房、運動分析、遊戲AI等眾多領域。團隊擁有深厚的技術積累，並建立了專門的數據集進行模型訓練。我們相信，Spatial-MLLM將引領基於視覺的空間智能領域的發展，並為投資者帶來豐厚的回報。我們正在尋找戰略投資者，共同將這項技術推向市場，實現商業化。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-31T15:12:32.069486"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：利用校正流變換器進行免訓練多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一個創新框架，利用 LoRA 模型實現多概念圖像編輯，無需重新訓練。其核心發現是 Flux 風格擴散變換器中，特定概念的特徵會在去噪過程的早期階段激活空間上連貫的區域。LoRAShop 運用這個特性，為每個概念導出解耦的潛在遮罩，並僅在限定概念的區域內混合相應的 LoRA 權重。這種方法能將多個主體或風格無縫整合到原始場景中，同時保留全局上下文、光照和精細細節。實驗證明，LoRAShop 在身份保留方面優於現有方法，將個人化擴散模型轉變為實用的「LoRA 版 Photoshop」，為組合視覺敘事和快速創意迭代開闢了新途徑。", "applications": ["**個性化商品設計：** 用戶可以上傳寵物、家人或自己的照片，LoRAShop 能快速生成帶有這些個性化元素的產品圖像，例如 T 恤、馬克杯或手機殼，無需專業設計師的協助。", "**室內裝修模擬：** 用戶可以拍攝客廳照片，然後用 LoRAShop 快速嘗試不同風格的家具、牆面顏色或裝飾品，提前預覽裝修效果，避免不必要的開銷和後悔。", "**電影場景合成：** 電影製作人員可以快速將演員的面孔和造型融入到不同的背景和時代中，創造出逼真的歷史人物或幻想場景，大幅縮短製作時間和降低成本。"], "pitch": "LoRAShop 解決了圖像編輯領域的一個核心痛點：個性化與效率的平衡。它無需重新訓練模型，就能讓使用者輕鬆地將多個概念融入圖像，實現真正的「LoRA 版 Photoshop」。這種免訓練、易於使用的特性，使其具備廣闊的商業前景。我們可以將 LoRAShop 技術授權給電商平台、圖像編輯軟體公司、廣告公司和遊戲開發商，讓他們能快速生成個性化內容，提高用戶參與度和銷售額。想像一下，一個電商平台可以讓用戶輕鬆定制自己喜歡的產品圖片，從而提高點擊率和轉化率。這不僅能帶來直接的收入增長，更能鞏固平台的品牌影響力。LoRAShop 的核心價值在於它 democratizes 圖像創作，讓更多人能夠輕鬆表達自己的創意，並將創意轉化為商業價值。 我們正在尋找合作夥伴，共同將 LoRAShop 推向市場，引領圖像編輯領域的下一次革命。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-31T15:12:54.800319"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升多模態大型語言模型在視覺空間智能方面的能力", "summary_zh": "Spatial-MLLM 是一個新的框架，旨在提升多模態大型語言模型(MLLM)在僅基於2D視覺輸入（如圖像或影片）時的空間推理能力。它使用雙編碼器架構，一個提取語義特徵，另一個基於視覺幾何模型提取3D結構特徵。Spatial-MLLM 還提出了一種空間感知幀採樣策略，只選擇對空間推理至關重要的幀。作者構建了一個名為 Spatial-MLLM-120k 的數據集，並用監督微調和 GRPO 在其上訓練模型。實驗表明，Spatial-MLLM 在各種真實世界的數據集上，於視覺空間理解和推理任務中，都達到了最先進的性能。", "applications": ["**自動駕駛感知:** 根據車載攝影機的2D影像，更精準地理解周圍環境的3D空間結構，例如預測道路坡度、車道線位置變化，以及障礙物的體積和距離，提高行車安全。", "**虛擬實境/擴增實境互動:** 在僅有2D影像輸入的情況下，讓使用者在虛擬空間中與物件進行更自然的互動，例如在AR遊戲中，根據使用者拍攝的照片，讓虛擬物件正確地放置在真實世界的平面上，並與光影互動。", "**智慧監控:** 分析監控攝影機拍攝的2D影片，更有效地理解場景中的行為和事件，例如偵測跌倒事件、識別異常人群聚集，以及預測潛在的衝突熱點，提升安全監控效率。"], "pitch": "Spatial-MLLM 解決了多模態大型語言模型(MLLM)在僅基於2D視覺輸入時，缺乏精準空間推理能力的痛點。我們提出的框架透過雙編碼器架構和空間感知幀採樣策略，顯著提升了 MLLM 對場景空間的理解能力。這項技術在自動駕駛、AR/VR、智慧監控等領域具有巨大的商業潛力。我們的模型不僅可以提高現有系統的準確性和效率，更可以賦能新的應用場景，例如更具沉浸感的 AR/VR 體驗和更智能的監控系統。我們已經建立了一個大型訓練數據集，並透過實驗證明了我們模型的卓越性能。我們相信 Spatial-MLLM 將成為下一代視覺智能應用的關鍵技術，並為投資者帶來豐厚的回報。投資 Spatial-MLLM，就是投資視覺空間智能的未來！", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-31T16:15:17.172303"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：利用修正流轉換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一個創新的框架，能使用 LoRA 模型進行多概念圖像編輯，無需重新訓練。 它利用 Flux 風格擴散轉換器中，概念特定的特徵會在去噪過程早期激活空間上連貫的區域的特性，在初始前向傳遞中為每個概念導出分離的潛在遮罩，並僅在包圍概念的區域內混合相應的 LoRA 權重。 這樣產生的編輯能無縫地將多個主體或風格整合到原始場景中，同時保留全局上下文、光照和精細細節。 實驗證明 LoRAShop 在身分保留方面優於基線方法。 透過消除重新訓練和外部約束，LoRAShop 將個人化的擴散模型轉變為實用的「帶有 LoRA 的 Photoshop」工具，並為組合視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**客製化商品設計：** 讓消費者上傳幾張圖片（例如寵物、家人、喜愛的景色）後，就能快速生成帶有這些元素，且風格一致的客製化商品設計圖稿，例如手機殼、T恤、馬克杯等。", "**虛擬試穿與造型搭配：** 用戶可以上傳自己的照片，然後將不同的 LoRA 模型代表的服飾、髮型、妝容等「融合」到照片中，模擬試穿和造型搭配的效果，無需實際換裝即可看到效果。", "**快速生成故事板：** 漫畫家或動畫師可以利用 LoRAShop 快速生成帶有特定角色和場景的多格故事板，加速創作流程，並快速迭代不同視覺風格。"], "pitch": "想像一下，一個「AI 設計工坊」，用戶無需具備專業知識，就能輕鬆將多個概念融合到一張圖片中，生成獨一無二的視覺內容。LoRAShop 的核心價值在於其『免訓練』特性，大幅降低了使用門檻和成本。 透過將 LoRA 模型與修正流轉換器巧妙結合，我們打造了一個極具商業潛力的平台，可以廣泛應用於電商、廣告、遊戲、娛樂等領域。我們預計透過 SaaS 模式，提供圖像生成和編輯 API，以及客製化 LoRA 模型訓練服務，創造多元的收入來源。此外，LoRAShop 的技術也能應用於元宇宙內容創建，為用戶提供更個性化、更沉浸式的體驗。 我們相信 LoRAShop 將會是下一代視覺內容創作的遊戲規則改變者，具有巨大的市場價值。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-31T16:15:47.538685"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在基於視覺的空間智能方面的能力", "summary_zh": "本論文提出Spatial-MLLM，一個新的框架，專注於僅使用2D圖像進行基於視覺的空間推理。與過去需要額外3D或2.5D數據的MLLM不同，Spatial-MLLM利用雙編碼器架構，結合2D語義特徵和從視覺幾何模型中提取的3D結構特徵。此外，採用空間感知的幀採樣策略，在有限的算力下，優先選擇對空間推理至關重要的幀。該團隊還建立了一個Spatial-MLLM-120k數據集進行模型訓練。實驗結果顯示，Spatial-MLLM在多個真實世界的數據集上，於基於視覺的空間理解和推理任務中，都達到了最先進的水平。", "applications": ["**自動駕駛感知增強：** 透過分析行車記錄器影片，增強自動駕駛系統對周遭環境的空間理解，例如判斷車道線的曲率變化、預測其他車輛的行駛軌跡，以及偵測行人可能穿梭的潛在區域。", "**無人機巡檢故障診斷：** 無人機拍攝的橋樑或建築物影像，Spatial-MLLM 能自動識別結構的空間關係，判斷裂縫或鏽蝕的位置、大小和深度，輔助工程師進行更精確的故障診斷和維護。", "**室內導航與物品定位：** 利用手機或機器人相機拍攝的室內環境影像，Spatial-MLLM 可以建立室內地圖，幫助使用者找到特定物品的位置，或者提供更精準的室內導航指引，特別適用於大型購物中心或醫院。"], "pitch": "各位投資人，Spatial-MLLM 打破了傳統多模態模型對 3D 數據的依賴，僅憑 2D 影像就能實現強大的空間推理能力。這意味著我們能夠以更低的成本、更廣泛的應用場景，賦能各行各業。想想看，從自動駕駛到智慧城市，從智慧製造到遠程醫療，凡是需要理解和分析空間信息的領域，都能夠透過我們的技術大幅提升效率和精度。我們已經建立了專用數據集並在真實世界數據上驗證了模型的卓越性能。我們的目標是成為基於視覺的空間智能領域的領頭羊，並在巨大的市場中佔據主導地位。 現在是加入我們的最佳時機，讓我們一起重新定義視覺智能的未來！", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-31T17:11:13.139674"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：基於修正流變換器的免訓練多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一個全新的框架，能讓你使用 LoRA 模型進行多概念圖像編輯，無需重新訓練。它的核心發現是，Flux 風格的擴散變換器中，概念特定的特徵在降噪過程的早期就會激活空間上連貫的區域。LoRAShop 會為每個概念導出一個解耦的潛在遮罩，並僅在包含這些概念的區域內混合對應的 LoRA 權重。這樣就能將多個主題或風格無縫整合到原始場景中，同時保留全局背景、光照和細節。簡而言之，LoRAShop 讓你可以像使用 Photoshop 一樣，利用 LoRA 模型輕鬆編輯圖片，而且效果更好。", "applications": ["**電商商品圖客製化：** 商家可以快速將不同產品款式或顏色應用於現有商品圖，展示更多選項，提升消費者購買意願。", "**遊戲角色造型設計：** 遊戲開發者或玩家可以使用 LoRAShop 輕鬆修改角色外觀，例如更換髮型、服裝，打造獨一無二的角色形象。", "**建築設計概念圖呈現：** 建築師或設計師可以將不同的設計風格或元素（例如屋頂樣式、外牆材質）快速應用於現有的建築模型圖片，以便快速探索和展示設計方案。"], "pitch": "LoRAShop 解決了 AI 圖像編輯領域的一大痛點：多概念編輯需要大量訓練和調整。我們提供了一個免訓練、高效率、高保真度的解決方案，讓使用者能夠像使用 Photoshop 一樣輕鬆編輯圖像，添加多個主題或風格。這項技術具備極高的商業價值，尤其是在電商、遊戲、廣告、設計等領域，可以大幅提升內容創作效率，降低成本，並激發更多創意。LoRAShop 的潛在市場規模龐大，預計將引領新一波的 AI 圖像編輯革命，並為早期投資者帶來可觀回報。我們尋求資金支持，以加速產品開發、市場推廣和擴展應用場景。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-31T17:11:26.088169"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升多模態大型語言模型在視覺空間智能上的能力", "summary_zh": "本研究提出 Spatial-MLLM，一個能僅從 2D 影像進行空間推理的新框架。不同於以往依賴額外3D或2.5D資料的3D MLLM，Spatial-MLLM 利用視覺幾何基礎模型（Visual Geometry Foundation Model）的結構先驗知識，透過雙編碼器架構：一個提取語義特徵的2D視覺編碼器，以及一個提取3D結構特徵的空間編碼器，整合兩種特徵以增強空間理解。同時，研究也提出了一種空間感知幀取樣策略，選取影片序列中空間資訊最豐富的幀，確保模型能在有限的Token長度下專注於空間推理。透過 Spatial-MLLM-120k 資料集的訓練，Spatial-MLLM 在多個真實世界資料集上，展現了視覺空間理解和推理任務的頂尖效能。", "applications": ["**智慧導航：** 結合行車記錄器影像，即時判斷道路狀況、障礙物距離、車道線位置，提供更精準的導航輔助，甚至支援自動駕駛決策。", "**建築設計與室內規劃：** 從照片或影片快速生成建築物的3D模型，並能根據使用者需求進行空間規劃建議，例如家具擺放、動線優化等。", "**AR/VR 應用：** 透過手機鏡頭捕捉到的影像，即時建立周遭環境的3D地圖，提升AR/VR體驗的真實感和互動性，例如在遊戲中根據真實環境生成關卡。"], "pitch": "Spatial-MLLM 代表了 MLLM 在空間理解上的重大突破，它僅需2D影像輸入，就能進行準確的3D空間推理，省去對額外3D資料的依賴。這開啟了廣泛的商業應用可能性，從智慧導航、建築設計到 AR/VR 體驗，都能顯著提升。我們的技術具有以下優勢：\n\n*   **成本效益：** 僅需2D影像即可運作，降低了資料蒐集和處理的成本。\n*   **廣泛適用性：** 可應用於各種場景，只要有影像資料，就能進行空間理解。\n*   **技術領先：** 突破了傳統 MLLM 的限制，在空間推理方面擁有顯著優勢。\n\n我們正在尋找投資夥伴，共同將 Spatial-MLLM 推向市場，抓住視覺空間智能領域的巨大商機，將其應用於自動駕駛、智慧城市、工業自動化等高潛力領域，打造下一個十億級獨角獸。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-31T18:17:19.138520"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流轉換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop是一個首創的框架，利用LoRA模型進行多概念圖像編輯，無需重新訓練。 它基於一個關鍵發現：在Flux風格擴散轉換器中，概念特定的轉換器特徵在降噪過程的早期激活空間上連貫的區域。 LoRAShop利用這個特性，在預先進行的前向傳遞中，為每個概念導出解耦的潛在遮罩，並僅在概念周圍的區域內混合相應的LoRA權重，從而無縫地將多個主體或風格融入原始場景，同時保留全局背景、光照和精細細節。 實驗表明，LoRAShop在身份保留方面優於其他方法。通過消除重新訓練和外部約束，LoRAShop將個性化擴散模型變成一個實用的“帶LoRA的Photoshop”工具，並為組合視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**個性化商品設計:** 用戶上傳產品照片，並提供多種風格或設計元素的LoRA，即可快速生成不同風格的產品預覽圖，例如：將一件T-shirt用不同藝術風格渲染。", "**角色定制遊戲道具:** 遊戲玩家可以輕鬆將自己或朋友的照片轉換成遊戲角色，並自由混搭不同的服裝、武器LoRA，創造獨一無二的遊戲體驗。", "**創意廣告生成:** 廣告公司無需聘請大量模特或設計師，即可快速生成各種風格的廣告素材，例如：將同一位演員置於不同場景和服裝中，展示產品的不同使用場景。"], "pitch": "LoRAShop 解決了AI圖像生成領域中個性化定制的痛點，無需耗時耗力的重新訓練，即可實現多概念的無縫融合編輯。其核心價值在於顯著降低了圖像生成和編輯的門檻和成本，賦能普通用戶和專業人士進行更高效、更具創意的內容創作。 在商業模式上，可以考慮以下幾種： 1. SaaS訂閱：提供雲端服務，按照生成圖片的數量或功能使用時間收費； 2. API接口：將LoRAShop的功能打包成API，供其他應用或平台集成； 3. LoRA市場：建立一個LoRA交易平台，讓創作者可以分享和銷售自己訓練的LoRA模型。 其市場潛力巨大，涵蓋電商、遊戲、廣告、社交媒體等多個領域，有望成為AI圖像生成領域的關鍵基礎設施。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-31T18:17:34.659507"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI對齊的扭曲：偏好優化真的在優化偏好嗎？", "summary_zh": "大型語言模型在預訓練後會根據成對比較與人類偏好對齊。然而，現有的對齊方法（如基於PPO的RLHF和DPO）假設模型是對齊單一的偏好模型，但實際上使用者擁有不同的偏好。因此，這些對齊方法是否能讓模型在平均意義上滿足使用者需求尚不明朗。本文引入了對齊方法的「扭曲」概念，它衡量了最佳可實現的平均效用與學習策略的平均效用之間的最壞情況比例。研究表明，Nash Learning from Human Feedback 在各種條件下都能達到最佳的最小最大扭曲，而 RLHF 和 DPO 則會遭受嚴重的扭曲，甚至可能無限放大。", "applications": ["**個性化推薦系統：** 針對不同使用者偏好調整推薦策略，例如電影、音樂、商品推薦，避免單一偏好導致的推薦偏差，提高使用者滿意度。", "**多樣化AI助手：** 開發能根據不同使用者個性化調整回答風格和訊息呈現方式的AI助手，避免AI助手表現出過於統一或偏頗的價值觀，提升使用者接受度和信任度。", "**公平公正的政策制定：** 在公共政策制定中，使用AI模型評估不同政策方案對不同群體的影響，避免模型僅基於單一偏好群體進行優化，導致對部分群體的不公平待遇，促進社會公平。"], "pitch": "我們正在解決AI對齊領域的核心問題：如何讓AI模型真正理解並尊重使用者多樣化的偏好。現有方法的缺陷導致AI模型容易扭曲使用者偏好，造成不公平和不滿。我們的研究提出了一種更穩健的對齊方法，能夠更好地處理使用者偏好的多樣性，從而避免偏見，提高使用者體驗。這項技術的商業價值巨大，可以應用於個性化推薦、AI助手和公共政策等領域，打造更公平、更值得信賴的AI系統。我們正在尋求投資，以加速我們的技術商業化進程，成為AI對齊領域的領導者，並引領AI技術走向更可持續和負責的未來。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-05-31T19:10:11.453993"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在基於視覺的空間智能方面的能力", "summary_zh": "Spatial-MLLM 是一個新的框架，它能讓多模態大型語言模型（MLLM）僅僅從 2D 圖像或影片中學習並進行空間推理，無需額外的 3D 或 2.5D 資料。它採用雙編碼器架構，一個負責提取語義特徵，另一個則從視覺幾何基礎模型中提取 3D 結構特徵，並整合這些特徵。此外，還提出了一種空間感知幀採樣策略，在有限的 token 長度下，優先選擇對空間推理至關重要的幀。透過 Spatial-MLLM-120k 資料集進行訓練，實驗證明 Spatial-MLLM 在各種基於視覺的空間理解和推理任務中取得了領先的成果。", "applications": ["**智慧型手機導航與 AR 應用：** 手機透過鏡頭即時分析周遭環境的空間結構，提供更精準的 AR 導航指引，例如在室內商場、機場等地。", "**自動駕駛：** 利用車載攝影機判斷道路結構、交通標誌、行人位置等，提升自動駕駛系統對複雜路況的理解能力，降低事故風險。", "**機器人自主導航：** 讓機器人僅透過視覺資訊就能在未知環境中自主導航，適用於倉儲物流、清潔服務、醫療輔助等領域。"], "pitch": "Spatial-MLLM 解決了 MLLM 在僅有 2D 視覺輸入下進行空間推理的瓶頸，開闢了全新的應用場景。它的雙編碼器架構和空間感知幀採樣策略，能有效提升空間理解能力，並已在實際數據集上驗證其優越性。這項技術在自動駕駛、AR 導航、機器人自主導航等領域擁有巨大的商業潛力。我們相信 Spatial-MLLM 將成為新一代空間智能應用的核心引擎，為各行業帶來顛覆性的創新，帶來可觀的投資回報。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-31T19:10:27.633756"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流變換器進行免訓練的多概念圖像生成和編輯", "summary_zh": "LoRAShop 是一個新的框架，能使用 LoRA 模型進行多概念圖像編輯，無需重新訓練。它基於一個關鍵發現：在 Flux 風格的擴散變換器中，概念特定的變換器特徵會在去噪過程的早期激活空間上一致的區域。LoRAShop 利用這個發現，為每個概念導出一個解耦的潛在遮罩，並僅在限定概念的區域內混合相應的 LoRA 權重。這樣產生的編輯能將多個主體或風格無縫地整合到原始場景中，同時保留全局上下文、光照和細節。實驗表明，LoRAShop 比基準模型更好地保留了身份。透過消除重新訓練和外部約束，LoRAShop 將個性化的擴散模型變成了一個實用的“LoRA版 Photoshop”工具，並為組合視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**快速風格化人像：** 用戶可以輕鬆將朋友的照片套用特定藝術家的風格，例如將照片變成印象派風格，無需專業技能。", "**商品設計變體：** 設計師可以快速生成不同顏色、材質或功能的產品概念圖，例如改變椅子的顏色、材質或者增加扶手，加速設計迭代流程。", "**遊戲角色個性化：** 遊戲開發者或玩家可以輕鬆創建具有特定風格或特徵的遊戲角色，例如給角色添加特定的服裝、髮型或武器，增強遊戲體驗。"], "pitch": "LoRAShop 是一個突破性的圖像編輯技術，無需重新訓練即可實現多概念的無縫融合和編輯，大幅降低了使用個性化模型的門檻，使其更易於大眾化應用。其潛在商業價值體現在以下幾個方面：\n\n*   **提高效率，降低成本：** 設計師、藝術家等專業人士可以利用LoRAShop快速生成多種設計方案，顯著縮短開發週期，降低成本。\n*   **赋能创意，拓展市场：** 使得非专业人士也能轻松实现复杂的图像编辑和创作，拓宽了创意内容的市场。\n*   **平台整合，增加收益：** 可以整合到现有的图像编辑软件、设计平台或游戏引擎中，为用户提供更强大的功能，增加平台收益。\n*   **个性化定制，提升用户体验：** 可用于电商平台，用户可以通过个性化定制图像生成工具来定制商品外观，提升用户体验和购买意愿。\n\nLoRAShop有望成为下一代图像编辑的核心技术，具有巨大的市场潜力，值得投资。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-31T19:10:53.452859"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在基於視覺的空間智能中的能力", "summary_zh": "現有的多模態大型語言模型(MLLM)在2D視覺任務上表現優異，但在空間智能方面仍有不足。Spatial-MLLM提出一個新框架，僅從2D圖像或影片進行視覺空間推理。它利用預訓練的2D視覺編碼器提取語義特徵，並從視覺幾何模型骨幹初始化一個空間編碼器，提取3D結構特徵。透過連接器整合兩種特徵，提升空間理解能力。此外，還提出了空間感知的幀抽樣策略，專注於對空間推理至關重要的幀。透過在Spatial-MLLM-120k數據集上的訓練，模型在各種真實世界的數據集上，於基於視覺的空間理解和推理任務中，實現了最先進的性能。", "applications": ["**自動駕駛汽車導航：** 利用車載攝像頭拍攝的2D圖像，感知周圍環境的3D空間結構，例如判斷路徑可通行性，避免碰撞。", "**智慧家居機器人：** 透過掃地機器人的攝像頭理解房間的布局，更有效地規劃清潔路線，避開障礙物。", "**醫療影像分析：** 醫生可以利用2D的X光或CT掃描影像，透過模型重建3D結構，更精準地診斷病情，例如腫瘤的位置和大小。"], "pitch": "Spatial-MLLM 解決了多模態大型語言模型在空間智能上的瓶頸，僅需2D視覺輸入即可實現精準的3D空間推理。這項技術開闢了多個高價值應用領域，包括自動駕駛、機器人、醫療影像等。我們的模型超越了現有依賴額外3D或2.5D數據的方案，降低了成本和複雜性。透過獨特的雙編碼器架構和空間感知幀抽樣策略，Spatial-MLLM 在空間理解和推理任務中達到領先水準。 我們已建立 Spatial-MLLM-120k 數據集並進行了監督式微調和 GRPO 訓練，確保模型具有高度的實用性與可靠性。我們相信，Spatial-MLLM 將成為視覺空間智能領域的關鍵推動者，為各行業帶來顛覆性創新，帶來巨大的商業回報。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-31T20:13:50.270471"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：基於修正流轉換器的免訓練多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一個創新的圖像編輯框架，它利用 LoRA 模型實現多概念的圖像編輯，無需重新訓練模型。它觀察到擴散轉換器中特定概念的特徵會在去噪過程的早期激活空間上連貫的區域。基於這個發現，LoRAShop為每個概念導出一個解耦的潛在遮罩，並僅在概念區域內融合相應的 LoRA 權重。 這樣可以在保留全局上下文、光照和細節的同時，將多個主體或風格無縫地融入原始場景。LoRAShop 在身份保留方面優於其他基線方法，無需重新訓練或外部約束，使個性化擴散模型成為實用的 `LoRA 版 Photoshop`，為構圖視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**個性化頭像生成器：** 用戶可以上傳幾張個人照片，LoRAShop 就能快速生成各種風格的頭像，例如卡通、油畫、未來風格等，且保留用戶的臉部特徵。", "**產品設計快速迭代：** 設計師可以輸入產品草圖和風格描述（例如：未來主義、簡約、奢華），LoRAShop 能快速生成多種設計變體，協助設計師快速探索各種設計方向，並根據反饋進行調整。", "**創意廣告素材生成：** 廣告公司可以利用 LoRAShop 將不同的概念元素（例如：產品、場景、人物）融合到一起，快速生成吸引眼球的廣告素材，並根據目標受眾的喜好進行個性化定製。"], "pitch": "LoRAShop 解決了個性化圖像生成和編輯領域的關鍵痛點，無需耗時耗力的模型重新訓練，就能實現多概念的精準控制和編輯。這項技術將釋放巨大的商業價值，尤其是在以下幾個方面：\n\n*   **大幅降低圖像生成成本：** 對於需要大量個性化圖像的行業（例如：電商、遊戲、廣告），LoRAShop 可以顯著降低成本，提高效率。\n*   **提高用戶參與度：** 個性化的內容更容易吸引用戶的注意力，提高用戶的參與度和轉化率。\n*   **開闢新的商業模式：** 基於 LoRAShop 的圖像編輯服務可以作為獨立產品或 API 提供，或者集成到現有的設計工具中，創造新的商業模式。\n\nLoRAShop 的技術優勢和潛在市場規模使其成為一個極具吸引力的投資標的。 我們相信，LoRAShop 有潛力成為 AI 圖像生成領域的領導者。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-31T20:14:09.567388"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在基於視覺的空間智能方面的能力", "summary_zh": "這篇論文提出Spatial-MLLM，一個新的框架，旨在提升多模態大型語言模型 (MLLM) 在純粹 2D 視覺輸入下的空間推理能力。不同於依賴額外 3D 或 2.5D 數據的現有方法，Spatial-MLLM 利用視覺幾何基礎模型中的結構先驗知識，通過雙編碼器架構提取語義和空間特徵。它還採用了空間感知幀採樣策略，在有限的token長度下，選擇對空間推理至關重要的幀。作者構建了 Spatial-MLLM-120k 數據集，並通過監督微調和 GRPO 進行訓練。實驗結果表明，Spatial-MLLM 在各種真實世界數據集上的視覺空間理解和推理任務中取得了最先進的性能。", "applications": ["**智能駕駛輔助系統：** Spatial-MLLM 可以利用行車記錄器或車載鏡頭的 2D 影像，更精確地理解周圍環境的空間結構，例如檢測路邊的坑洞、預測行人的行動軌跡、或是判斷車輛與障礙物之間的距離，從而提升駕駛安全性。", "**室內導航與定位：** 在僅有 2D 影像的情況下，Spatial-MLLM 可以幫助機器人或使用者在室內環境中進行導航，例如在商場、博物館或辦公室中，透過手機鏡頭進行定位與路線規劃。", "**智慧城市監控：** 利用現有的城市監控錄像，Spatial-MLLM 可以分析人群的行為模式，檢測異常事件（例如跌倒、打架），並提供空間相關的資訊，例如事故發生的確切位置，協助城市管理人員更有效地維護公共安全。"], "pitch": "Spatial-MLLM 解決了現有 MLLM 在 2D 視覺輸入下空間推理能力不足的痛點，開創了基於純 2D 影像實現高精度空間理解的新途徑。其潛在商業價值巨大：\n\n*   **市場規模廣闊：** 涵蓋智能駕駛、智慧城市、零售、物流等眾多領域，可整合至現有產品或服務中，提升其智能化水平。\n*   **技術壁壘高：** 依賴於對視覺幾何模型的深入理解和創新的雙編碼器架構，先發優勢明顯。\n*   **數據驅動增長：** Spatial-MLLM-120k 數據集的建立，為模型訓練提供了堅實基礎，未來可通過不斷擴充數據集，持續提升模型性能。\n\n我們相信，Spatial-MLLM 將成為下一代視覺智能的關鍵技術，有望在多個行業引發變革，帶來可觀的經濟效益。投資 Spatial-MLLM，就是投資未來。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-31T21:11:58.357177"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流轉換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop是一個創新的框架，它能利用LoRA模型來編輯圖像中的多個概念，無需重新訓練。它基於對Flux風格擴散轉換器內部特徵交互模式的觀察：概念特定的轉換器特徵在去噪過程的早期，會在空間上激活連貫的區域。LoRAShop利用這一點，在先前的正向傳遞中導出每個概念的解耦潛在遮罩，並僅在包含個性化概念的區域內混合相應的LoRA權重。這樣，編輯就能將多個主體或風格無縫整合到原始場景中，同時保留全局上下文、光照和細節。LoRAShop在保持身份一致性方面優於其他方法。通過消除重新訓練和外部限制，LoRAShop將個性化的擴散模型轉變為實用的“帶LoRA的Photoshop”工具，並為組合視覺敘事和快速創意迭代開闢了新途徑。", "applications": ["**個性化商品定製：** 用戶可以上傳自己的寵物照片，然後使用LoRAShop將寵物融入到不同的場景中，例如讓寵物出現在電影海報中、身穿超級英雄服裝等，定制個性化的T恤、馬克杯或其他商品。", "**風格化照片編輯：** 將自己的照片和特定藝術家的畫風融合，快速生成具有名畫風格的自拍照，例如將自己的照片變成梵高的星空風格，或者莫奈的印象派風格，用於社交媒體分享或個人收藏。", "**虛擬服裝試穿：** 將目標服裝的LoRA模型與用戶自拍融合，預覽服裝穿搭效果，無需實際試穿即可了解是否合身，提升網購體驗，降低退貨率。"], "pitch": "LoRAShop代表了圖像編輯領域的重大突破，它提供了一種無需重新訓練、快速且高效的多概念圖像編輯解決方案。想象一下，一個“AI藝術家”隨時待命，可以根據您的描述創造出獨一無二的圖像。LoRAShop的商業價值體現在以下幾個方面：\n\n*   **降低內容創作成本：** 傳統的圖像編輯需要專業技能和大量時間，LoRAShop能顯著降低這兩者，讓更多人能夠參與到內容創作中。\n*   **提升用戶參與度：** 個性化定製和趣味編輯功能能夠激發用戶的創造力，並提高用戶的活躍度和留存率。\n*   **開闢新的商業模式：** LoRAShop可以作為獨立的圖像編輯工具出售，也可以集成到現有的社交媒體、電商平台或設計軟件中，形成新的商業生態。\n\n我們相信，LoRAShop有潛力顛覆現有的圖像編輯市場，並在數字內容創作領域創造巨大的商業價值。我們正在尋找戰略合作夥伴和投資者，共同將LoRAShop推向更廣闊的市場。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-31T21:12:16.665260"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升基於視覺空間智慧的多模態大型語言模型能力", "summary_zh": "Spatial-MLLM 是一個新穎的框架，旨在僅從 2D 圖像或影片中提升多模態大型語言模型（MLLM）的空間推理能力。與傳統依賴 3D 或 2.5D 資料的 MLLM 不同，Spatial-MLLM 利用視覺幾何基礎模型中的結構先驗知識。它採用雙編碼器架構，一個提取語義特徵，另一個提取 3D 結構特徵，並結合空間感知的幀採樣策略，專注於對空間推理至關重要的幀。透過監督式微調和 GRPO 在 Spatial-MLLM-120k 資料集上訓練，實驗證明 Spatial-MLLM 在各種基於視覺的空間理解和推理任務中取得了最先進的性能。", "applications": ["**智慧導航：** 協助視障人士或在複雜環境中導航的機器人，透過攝影鏡頭分析周遭環境，提供更精確的空間資訊。", "**自動駕駛：** 提升自動駕駛系統對道路和交通狀況的空間感知能力，例如辨識路障、預測其他車輛的行駛軌跡，從而提高安全性。", "**建築設計與室內規劃：** 協助建築師或室內設計師快速理解空間關係，例如從影片或照片中生成 3D 模型，評估設計方案的可行性。"], "pitch": "Spatial-MLLM 解決了現有多模態大型語言模型在 2D 視覺輸入下空間推理能力不足的痛點。它利用視覺幾何基礎模型的結構先驗知識，無需額外的 3D 或 2.5D 數據，就能顯著提升空間理解能力。這項技術的應用前景廣泛，從智慧導航到自動駕駛，再到建築設計，都有巨大的商業潛力。透過授權核心技術、提供客製化解決方案或開發相關應用，我們可以將 Spatial-MLLM 轉化為具有高成長潛力的商業模式，搶佔新一代空間智慧市場的先機。 我們正在尋找資金來擴大數據集、優化模型性能，並加速在各個垂直領域的落地應用，以實現規模化的商業價值。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-31T22:12:18.319556"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流變換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop是首個使用LoRA模型進行多概念圖像編輯的框架。它基於一項關鍵發現：在Flux風格的擴散變換器中，特定概念的變換器特徵會在去噪過程早期激活空間連貫的區域。LoRAShop利用這個特性，從先前的正向傳遞中為每個概念導出解耦的潛在掩碼，並僅在限定概念的區域內混合相應的LoRA權重。 這樣產生的編輯能將多個主題或風格無縫整合到原始場景中，同時保留全局背景、光照和細節。實驗表明，LoRAShop比基線方法更能保持身份一致性。通過消除重新訓練和外部約束，LoRAShop將個性化擴散模型轉變為實用的'LoRA版Photoshop'工具，並為組合式視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**個人化頭像與表情包生成：** 使用者可輕鬆將自己或朋友的風格融入到不同的藝術風格、服裝或背景中，創造獨特的頭像或表情包。", "**產品設計原型快速迭代：** 設計師可以快速嘗試將不同的產品組件、材質或顏色組合在一起，生成逼真的產品渲染圖，加速設計流程。", "**虛擬試穿與家居裝飾預覽：** 用戶可以將自己的照片或房間照片上傳，模擬穿戴不同服飾或擺放不同家具的效果，提前預覽購物體驗。"], "pitch": "LoRAShop是一款顛覆性的圖像編輯工具，它利用免訓練的LoRA模型，讓多概念圖像生成與編輯變得前所未有的簡單高效。想像一下，無需耗時的訓練，就能將任何概念融入圖像中，創造出無限可能的視覺內容。這不僅降低了使用門檻，也極大地加速了創意迭代的週期。在創投眼中，LoRAShop的核心價值在於其**低成本、高效率、易擴展**的特性。它能廣泛應用於社交娛樂、電商、設計、教育等領域，催生出個性化內容生成、虛擬體驗、智能設計工具等一系列創新產品。我們相信，LoRAShop將重塑圖像編輯的未來，並在快速增長的AI生成內容市場中佔據領先地位。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-31T22:12:41.241578"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在基於視覺的空間智能方面的能力", "summary_zh": "本研究提出 Spatial-MLLM，一種僅從2D圖像或影片進行空間推理的新架構。不同於以往需要額外3D數據的模型，Spatial-MLLM 採用雙編碼器架構：一個預訓練的2D視覺編碼器提取語義特徵，另一個空間編碼器（基於視覺幾何模型初始化）提取3D結構特徵。透過連接器將兩者整合，提升空間理解能力。此外，模型還採用空間感知的幀抽樣策略，選取影片中對空間推理至關重要的幀。透過 Spatial-MLLM-120k 數據集進行訓練，實驗證明其在各種真實世界數據集中，於基於視覺的空間理解和推理任務上表現卓越。", "applications": ["**自動駕駛/機器人導航：** 讓機器人或自駕車僅透過車載鏡頭拍攝的2D影像就能理解周遭環境的3D結構，進行更精確的導航和避障。", "**室內設計/虛擬實境：** 僅需上傳房間照片或影片，就能自動生成3D模型，方便進行虛擬裝修、空間規劃，或在VR環境中進行導覽。", "**醫學影像分析：** 協助醫生從2D的X光片、CT掃描等影像中，更準確地判斷病灶的3D位置和形狀，提升診斷的準確性。"], "pitch": "各位投資人，我們團隊開發了 Spatial-MLLM，一種突破性的空間智能模型，僅需2D圖像或影片即可實現精準的3D空間理解。這解決了傳統方法需要昂貴3D數據的痛點，大幅降低了成本並擴展了應用場景。試想一下，自動駕駛、室內設計、醫療診斷等領域，都能因為Spatial-MLLM而變得更高效、更智能。我們擁有一支強大的技術團隊，並建立了專用的數據集，在多個真實世界數據集中都取得了領先的成果。我們堅信 Spatial-MLLM 將在空間智能領域引發革命，帶來巨大的商業價值和投資回報。我們正在尋求戰略合作夥伴和投資人，共同將 Spatial-MLLM 推向市場，打造一個更智能的未來。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-05-31T23:12:56.429844"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流轉換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop是一種新型框架，利用LoRA模型進行多概念圖像編輯，無需重新訓練。 它基於對 Flux 式擴散轉換器內部特徵交互模式的觀察：概念特定的轉換器特徵在去噪過程的早期階段激活空間上連貫的區域。LoRAShop 在前向傳遞中導出每個概念的分離潛在遮罩，並僅在限制個性化概念的區域內混合相應的LoRA權重。 這樣編輯後的圖像能將多個主題或風格無縫整合到原始場景中，同時保留全局上下文、光照和細節。實驗表明，LoRAShop 在身份保留方面優於基線方法。 通過消除重新訓練和外部約束，LoRAShop將個性化的擴散模型變成一個實用的“LoRA版Photoshop”工具，並為組合視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**線上產品客製化：** 用戶上傳自己的寵物照片和喜歡的風格（例如梵谷風格），LoRAShop可以將寵物融入梵谷風格的畫作中，生成獨一無二的客製化手機殼、T恤等產品。", "**影視特效快速製作：** 電影製作人員可以快速將多個演員融入同一場景，並改變角色的服裝風格，大幅節省特效製作的時間和成本。", "**建築設計概念圖生成：** 設計師可以將多個建築風格（例如現代簡約、古典巴洛克）融合到現有照片中，快速生成不同風格的建築概念圖，方便與客戶溝通。"], "pitch": "LoRAShop正在顛覆圖像編輯領域，它利用免訓練的多概念圖像生成與編輯技術，為創意產業帶來革命性的效率提升和成本降低。 想像一下，無需漫長的重新訓練，就能輕鬆將多種風格和元素融合到圖像中。這不僅僅是一個工具，而是一個平台，一個加速創意迭代、實現個性化表達的平台。 LoRAShop的潛在市場巨大，涵蓋線上客製化、影視特效、建築設計等廣闊領域。 我們正在尋找投資，以擴大我們的研發團隊，優化算法，並建立一個易於使用的雲端平台，讓每個人都能輕鬆使用LoRAShop。我們的目標是成為圖像編輯領域的領先者，讓創意無處不在，讓每個人都能成為藝術家。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-05-31T23:13:17.040185"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI對齊的扭曲：偏好最佳化真的能最佳化偏好嗎？", "summary_zh": "大型語言模型在預訓練後，會透過成對比較與人類偏好對齊。現有的對齊方法（如基於PPO的RLHF和DPO）建立在與單一偏好模型對齊的假設上，然而它們被部署在使用者偏好多樣化的環境中。因此，這些對齊方法是否能讓使用者平均滿意都難以確定。本文引入「扭曲」的概念，衡量對齊方法在最差情況下，最佳平均效用與模型學習策略的平均效用之間的比例。研究發現，Nash Learning from Human Feedback (NLHF) 在各種條件下都能達到最佳的最小最大扭曲，而RLHF和DPO則表現出更嚴重的扭曲，甚至可能無限大。", "applications": ["**個人化學習平台：** 根據學生的學習風格和偏好調整教學內容和方法，避免採用對某些學生不利的通用策略。", "**推薦系統：** 針對不同用戶群體，設計不同的推薦演算法，以減少推薦結果的偏差，提升整體滿意度。例如，音樂、電影、書籍推薦等。", "**智能助理：** 針對不同使用者的習慣和需求，客製化回應方式和建議，避免提供對特定使用者無效或錯誤的資訊。"], "pitch": "想像一個AI，它自以為理解你的需求，但實際上只是在迎合大多數人的口味，而犧牲了你獨特的偏好。這項研究揭示了現有AI對齊方法的重大缺陷：它們無法有效地處理使用者偏好的多樣性，導致AI決策出現偏差。我們提出的解決方案，Nash Learning from Human Feedback (NLHF)，能夠更公平、更有效地滿足不同使用者的需求，從而提高使用者滿意度、降低誤解風險，並開闢個人化AI應用的新領域。這項技術的商業價值體現在：更精準的目標客戶定位、更有效的產品推薦、以及更高品質的客戶服務，最終帶來更強大的品牌忠誠度和更高的投資回報率。在一個日益重視個性化體驗的世界裡，掌握這項技術將使我們在AI市場中佔據領先地位。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-06-01T01:17:48.970283"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在視覺空間智能方面的能力", "summary_zh": "現有的多模態大型語言模型（MLLM）在二維視覺任務上表現出色，但在空間智能方面仍有挑戰。Spatial-MLLM 提出了一種新穎的框架，僅從二維圖像或影片中進行視覺空間推理。它採用雙編碼器架構，一個提取語義特徵，另一個從視覺幾何模型中提取3D結構特徵，並整合這些特徵以增強空間理解。此外，還提出了一種空間感知的幀採樣策略，在推理時選擇影片中空間信息最豐富的幀。通過在 Spatial-MLLM-120k 數據集上進行訓練，該模型在各種真實世界數據集上實現了最先進的視覺空間理解和推理性能。", "applications": ["**自動駕駛輔助：** 透過車載攝影機的2D影像，即時理解周遭環境的3D結構，提升路徑規劃和避障能力，例如：判斷路邊斜坡的陡峭程度、精確預測車輛與行人之間的距離。", "**智慧家庭應用：** 透過監視器或掃地機器人的鏡頭，理解室內空間佈局，幫助機器人更有效地導航和執行任務，例如：準確判斷沙發底下是否有障礙物，或識別物品在房間中的具體位置。", "**醫療影像分析：** 從2D醫學影像（如X光片）中推斷出3D器官結構，輔助醫生診斷疾病，例如：更精準地判斷肺部腫瘤的位置和大小，或評估骨骼的彎曲程度。"], "pitch": "Spatial-MLLM 解決了多模態大型語言模型在視覺空間智能方面的瓶頸，無需額外的3D或2.5D數據，僅憑2D影像即可進行高精度的空間推理。這項技術具備廣泛的應用潛力，尤其在自動駕駛、智慧家庭、醫療影像等領域。我們開發的架構和數據集已驗證了其卓越的性能，預計能顯著提升相關應用的智能化水平。投資 Spatial-MLLM，您將站在 AI 賦能現實世界應用的最前沿，搶佔數十億美元規模的市場。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-06-01T01:18:05.958460"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：基於修正流轉換器的免訓練多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一個創新的圖像編輯框架，它利用預訓練的 LoRA 模型實現多概念圖像編輯，無需重新訓練。核心原理是觀察到 Flux 風格擴散轉換器中，概念特定的特徵在降噪過程早期就激活空間上連貫的區域。LoRAShop 可以提取每個概念的解耦潛在遮罩，並僅在概念區域內混合相應的 LoRA 權重，從而無縫地將多個主體或風格整合到原始場景中，同時保留全局上下文、光照和精細細節。效果比現有方法更好，且因為省去重新訓練，讓個性化的擴散模型更易用，像是一個內建 LoRA 的 Photoshop。", "applications": ["**個性化商品設計：** 用戶上傳自己和寵物的照片，LoRAShop 可以將他們的形象無縫融合到各種產品樣式（例如：T恤圖案、手機殼設計、馬克杯裝飾）中，實現高度個性化的商品定制。", "**虛擬試穿/試戴：** 用戶上傳自己的照片，LoRAShop 可以模擬穿戴不同風格的服裝、眼鏡、珠寶等的效果，讓用戶在購買前就能看到真實的搭配效果，提升購物體驗。", "**藝術風格轉換與融合：** 將多種藝術風格融合到一張照片中，例如將照片中的人物置於梵高的星夜背景中，同時保留人物原有的表情和細節，創造獨特的藝術作品。"], "pitch": "LoRAShop 正在革新圖像編輯市場。我們提供了一個免訓練、高效且易於使用的多概念圖像編輯解決方案，大幅降低了個性化圖像生成的門檻。想像一下，一個可以根據你的想像力，將多個主題或風格無縫融合的圖像編輯工具，且無需耗時的重新訓練。我們的技術不僅優於現有方案，更打開了廣闊的商業前景：從個性化商品定制到虛擬試穿，再到藝術創作，LoRAShop 都具有顛覆市場的潛力。 我們尋求資金以擴大研發團隊，完善產品功能，並積極拓展B2B和B2C市場。透過LoRAShop，我們將重新定義圖像編輯的未來，打造一個更具創造力和個性化的視覺世界。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-06-01T01:18:22.070828"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升多模態大型語言模型在基於視覺的空間智能中的能力", "summary_zh": "現有的多模態大型語言模型(MLLM)在處理2D視覺任務上表現優異，但在空間智能方面仍有進步空間。本論文提出Spatial-MLLM框架，僅使用2D視覺信息進行空間推理。不同於傳統的視頻MLLM依賴針對語義理解優化的視覺編碼器，Spatial-MLLM採用雙編碼器結構：一個提取語義特徵的2D視覺編碼器，以及一個從視覺幾何模型主幹初始化的空間編碼器，以提取3D結構特徵。此外，論文還提出了一種空間感知的幀採樣策略，選擇視頻序列中包含空間信息的關鍵幀，以優化模型在有限的token長度下的空間推理能力。透過在Spatial-MLLM-120k數據集上進行訓練，模型在各種真實世界數據集上展現了最先進的空間理解和推理能力。", "applications": ["**自動駕駛導航：** 從行車記錄器影片中理解道路幾何結構，幫助車輛做出更準確的決策，例如變換車道或避開障礙物。即使在沒有精確地圖的情況下也能運作。", "**室內導航與定位：** 利用手機相機拍攝的影片或照片，幫助人們在大型建築物（如商場或醫院）內找到方向，定位特定店鋪或設施。", "**智慧家居環境理解：** 透過監控攝影機影像分析房間的空間佈局，例如家具位置、物品擺放等，進而提供更智能化的控制，例如自動調整照明或空調。"], "pitch": "Spatial-MLLM突破了多模態大型語言模型在空間智能上的瓶頸，僅需2D視覺輸入即可實現精準的3D空間推理。這解決了過往需仰賴額外3D數據的限制，大幅降低了應用門檻。想像一下，自動駕駛系統不再完全依賴高精地圖，室內導航能擺脫對複雜感測器的需求，智慧家居能更深入地理解用戶的空間環境。這項技術的商業價值巨大，可應用於自動駕駛、機器人導航、智慧城市、AR/VR、以及各種需要空間理解的場景。其簡化空間資訊獲取的方式，降低成本，擴大應用範圍，極具投資潛力，有望成為下一代空間智能的基礎設施。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-06-01T03:27:53.788934"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：基於修正流轉換器的免訓練多概念圖像生成與編輯", "summary_zh": "LoRAShop是一個嶄新的框架，使用LoRA模型進行多概念圖像編輯。它基於一個關鍵發現：在Flux風格的擴散轉換器中，特定概念的轉換器特徵會在去噪過程的早期激活空間上連貫的區域。LoRAShop利用這個特性，在預先正向傳播中為每個概念導出一個解耦的潛在遮罩，並且僅在包含待個性化概念的區域內混合相應的LoRA權重。 這樣產生的編輯可以無縫地將多個主題或風格整合到原始場景中，同時保留全局上下文、光照和精細細節。實驗表明，LoRAShop 比基準方法能更好地保持身份。由於消除了重新訓練和外部約束，LoRAShop 將個性化的擴散模型轉變為一個實用的“LoRA版的Photoshop”工具，並為組合成像視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**快速角色服裝更換：** 遊戲設計師可以快速為遊戲角色更換服裝，無需重新訓練模型，節省大量時間和資源，快速嘗試不同設計。", "**個性化家居裝修：** 用戶可以上傳客廳照片，然後使用LoRAShop 將不同風格的沙發、地毯等物件無縫添加到圖片中，預覽裝修效果，幫助他們做出更好的購買決策。", "**時尚產品搭配：** 線上服裝購物平台可以利用LoRAShop讓顧客上傳自己的照片，然後將不同服裝單品“穿”在身上，展示搭配效果，提升購物體驗和轉化率。"], "pitch": "LoRAShop 是一項顛覆性的圖像編輯技術，它利用LoRA模型和獨特的遮罩方法，實現了無需重新訓練的多概念圖像生成和編輯。 這意味著更快的迭代速度、更低的成本和更廣泛的應用場景。 我們相信 LoRAShop 有潛力成為圖像編輯領域的 Game Changer， 尤其是在遊戲、電商、廣告和內容創作等行業。 想象一下，一個設計師可以在幾分鐘內生成數百個不同風格的角色概念圖，一個電商平台可以讓用戶實時預覽家居裝修效果。LoRAShop 提供了一個極具吸引力的商業模式， 我們可以通過提供 SaaS 服務、API 授權或垂直領域的解決方案來獲取收入。 我們正在尋找投資者，共同將 LoRAShop 推向市場， 引領圖像編輯的未來。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-06-01T03:28:13.943714"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在基於視覺的空間智能中的能力", "summary_zh": "本研究提出Spatial-MLLM，一種新型框架，能僅從2D圖像或影片中進行基於視覺的空間推理。它採用雙編碼器架構：一個預訓練的2D視覺編碼器提取語義特徵，另一個空間編碼器（從視覺幾何基礎模型的骨幹初始化）提取3D結構特徵。透過連接器整合兩者，強化空間理解。此外，提出空間感知幀採樣策略，選取影片序列中空間資訊最豐富的幀，即使在Token長度有限的情況下，也能聚焦於對空間推理至關重要的幀。Spatial-MLLM在多個真實世界數據集上，於各種基於視覺的空間理解和推理任務中，均取得了最先進的性能。", "applications": ["自動駕駛：分析行車記錄器影片，判斷路況複雜度、預測潛在危險（例如：判斷路口是否有死角），提升自動駕駛安全性。", "居家安全監控：透過監視器畫面，判斷老人或小孩是否跌倒、或是有入侵者進入，並即時發出警報。", "建築設計與室內裝潢：根據2D平面圖或照片，自動生成3D模型，方便客戶預覽設計效果，並進行虛擬漫遊。"], "pitch": "Spatial-MLLM解決了多模態大型語言模型在僅依賴2D視覺資訊進行空間推理方面的瓶頸。它在自動駕駛、安防監控、建築設計等領域具有廣泛的應用前景，能有效提升效率和安全性。我們相信Spatial-MLLM可以成為這些行業的關鍵技術，帶來巨大的商業價值。其核心創新點在於從2D視覺信息中提取精準的3D空間結構信息，這意味著不需要昂貴的3D感測器，大大降低了成本。此外，我們的空間感知幀採樣策略在資源有限的情況下也能提供高精度的空間推理結果，這在邊緣計算設備上具有巨大的優勢。我們正在尋求投資以加速模型的開發和部署，並將其應用於更廣泛的行業領域。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-06-01T04:30:18.298614"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：基於修正流Transformer的免訓練多概念圖像生成與編輯", "summary_zh": "LoRAShop是首個利用LoRA模型進行多概念圖像編輯的框架。它基於對Flux風格擴散Transformer內部特徵交互模式的觀察：概念特定的Transformer特徵在去噪過程的早期階段，會激活空間上連貫的區域。LoRAShop利用這一點，在預先的正向傳遞中為每個概念導出解耦的潛在遮罩，並僅在包含個性化概念的區域內混合相應的LoRA權重。這樣產生的編輯能將多個主體或風格無縫整合到原始場景中，同時保留全局上下文、光照和精細細節。實驗表明，LoRAShop比基線方法更能保持身份一致性。通過消除重新訓練和外部約束，LoRAShop將個性化的擴散模型轉變為實用的“基於LoRA的Photoshop”工具，並為組合式視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**個性化頭像生成：** 用戶可以上傳多張照片，讓模型學習他們的臉部特徵和風格偏好，然後生成各種不同情境和服裝的個性化頭像，例如用於社交媒體、遊戲或線上會議。", "**產品設計與原型製作：** 設計師可以快速將不同的設計元素和風格應用到產品圖片上，例如更換沙發的材質、改變汽車的颜色或添加新的装饰，而无需重新渲染整个图像，从而加速设计迭代过程。", "**電影特效與場景創建：** 電影製作人可以將不同的角色、場景元素和視覺風格無縫地整合到現有鏡頭中，例如將演員的臉部換成年輕版本、改變背景環境或者添加科幻特效，而无需进行昂贵的重新拍摄。"], "pitch": "LoRAShop賦予圖像編輯全新能力，透過創新的LoRA整合技術，大幅降低了多概念編輯的門檻和成本。想像一下，無需長時間重新訓練模型，就能輕鬆將不同風格、角色或元素融入既有圖像中，這對內容創作者、廣告商、電商平台、以及電影特效產業來說，都是革命性的提升。其潛在商業價值體現在：(1)顯著降低內容生產成本，提升效率；(2)賦予使用者更強大的圖像控制能力，創造高度個性化的內容；(3)加速產品設計迭代，搶佔市場先機。此外，LoRAShop的技術還能應用於虛擬試穿、室內設計等領域，應用範圍極廣。我們相信，LoRAShop將引領新一代的圖像編輯工具，具備巨大的市場潛力和投資價值。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-06-01T04:30:36.680825"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升視覺空間智慧中多模態大型語言模型的能力", "summary_zh": "Spatial-MLLM 是一種新的框架，旨在僅從2D圖像和影片中提升多模態大型語言模型（MLLM）的空間推理能力。傳統的3D MLLM需要額外的3D或2.5D數據，而Spatial-MLLM則利用預訓練的視覺幾何模型的結構先驗，使用雙編碼器架構：一個提取語義特徵的2D視覺編碼器，以及一個提取3D結構特徵的空間編碼器。此外，還提出了空間感知幀採樣策略，確保模型在有限的 token 長度下，專注於對空間推理至關重要的幀。透過在Spatial-MLLM-120k數據集上的訓練，Spatial-MLLM 在各種真實世界的數據集上，在視覺空間理解和推理任務中都達到了最先進的性能。", "applications": ["**自動駕駛導航：** 僅憑攝像頭拍攝的2D畫面，協助汽車理解周圍環境的3D結構，例如判斷路徑的可行性，避免障礙物。", "**室內設計輔助：** 根據室內照片，推斷房間的3D佈局，並提供家具擺放建議或虛擬裝修方案。", "**機器人操作：** 讓機器人能夠僅通過視覺輸入，理解和操作複雜的3D環境，例如在倉庫中揀選物品或執行精確組裝任務。"], "pitch": "Spatial-MLLM 解決了多模態大型語言模型（MLLM）在僅有2D視覺輸入下，缺乏足夠空間理解能力的痛點。 我們利用創新的雙編碼器架構和空間感知幀採樣策略，在不需要額外3D數據的情況下，顯著提升了模型在視覺空間推理任務上的性能。 這開啟了 MLLM 在自動駕駛、機器人、室內設計等領域的廣泛應用前景。 我們的模型性能已經在真實數據集上得到了驗證，且具有可擴展性和易於部署的優勢。 投資 Spatial-MLLM，您將擁抱一個擁有巨大商業潛力的 AI 技術，並在快速發展的視覺空間智慧領域佔據領先地位。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-06-01T05:14:05.066909"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：利用修正流轉換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一種新的圖像編輯框架，它利用 LoRA 模型，可以在不需重新訓練的情況下，對圖像進行多概念編輯。它的核心思想是：在 Flux 風格的擴散轉換器中，特定概念的特徵會在去噪過程的早期階段，激活空間上連貫的區域。LoRAShop 會先提取每個概念的解耦潛在遮罩，然後只在包含這些概念的區域內混合相應的 LoRA 權重。這樣就能在保留全局上下文、光照和細節的同時，將多個主體或風格無縫地整合到原始場景中。實驗證明，LoRAShop 在身份保留方面優於其他方法。它無需重新訓練或外部約束，將個人化的擴散模型轉變為實用的『LoRA 版 Photoshop』工具，為組合式視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**快速創建帶有特定風格或對象的圖像：** 例如，設計師可以快速將產品照片與特定風格或人物結合，生成廣告素材。", "**個性化圖像編輯：** 用戶可以輕鬆地在照片中添加或修改特定對象，例如將自己的寵物添加到風景照片中。", "**虛擬試穿/試戴：** 用戶可以將不同款式的衣服、眼鏡等添加到自己的照片中，查看效果，而無需實際試穿/試戴。"], "pitch": "LoRAShop 是一種突破性的圖像編輯技術，它利用 LoRA 模型實現了免訓練的多概念圖像生成和編輯。相比於傳統的圖像編輯方法，LoRAShop 極大地降低了使用門檻和成本，使用者無需專業技能或大量的訓練數據，就能夠輕鬆地創建出高品質的個性化圖像。這使得 LoRAShop 在廣告、電商、設計、社交媒體等多個領域具有巨大的商業潛力。想像一下，廣告公司可以快速生成高度個性化的廣告素材，電商平台可以提供更吸引人的商品展示，設計師可以更高效地完成設計項目，社交媒體用戶可以更輕鬆地創建獨特的內容。LoRAShop 的核心價值在於其簡便性、高效性和創造性，它將為圖像編輯領域帶來革命性的變革，並為投資者帶來豐厚的回報。我們正在尋找戰略合作夥伴，共同將 LoRAShop 打造成為圖像編輯領域的領導者。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-06-01T05:14:19.014764"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在基於視覺的空間智能方面的能力", "summary_zh": "現有的多模態大型語言模型（MLLM）在2D視覺任務上表現出色，但在空間智能方面仍有提升空間。本論文提出 Spatial-MLLM 框架，僅需2D圖像或影片就能進行視覺空間推理。與傳統MLLM不同，Spatial-MLLM 利用視覺幾何基礎模型提取 3D 結構特徵，結合預訓練的 2D 視覺編碼器提取語義特徵。此外，還提出空間感知幀採樣策略，選取對空間推理至關重要的幀。透過 Spatial-MLLM-120k 資料集進行訓練，實驗證明 Spatial-MLLM 在多個真實世界的數據集上，於基於視覺的空間理解和推理任務中達到最先進的性能。", "applications": ["**自動駕駛:** 分析行車紀錄器影片，判斷車輛與行人、障礙物的空間關係，預測潛在危險，提升自動駕駛安全性。", "**無人機巡檢:** 分析無人機拍攝的建築物或橋樑影像，自動檢測裂縫、腐蝕等結構性問題，提升巡檢效率與準確性。", "**室內導航:** 結合手機相機與 Spatial-MLLM，提供更精確的室內導航，尤其是在 GPS 訊號弱的環境下，例如大型購物中心或地下停車場。"], "pitch": "Spatial-MLLM 解決了現有 MLLM 在空間智能方面的瓶頸，僅憑 2D 影像即可實現強大的空間推理能力。這打開了全新的商業應用，例如在自動駕駛領域，可以提升系統對複雜交通場景的理解，降低事故風險。在無人機巡檢領域，則可大幅降低人力成本，並提高檢測精度。此外，在室內導航、AR/VR 等領域也有廣闊的應用前景。Spatial-MLLM 的技術優勢和潛在市場規模巨大，具備極高的商業投資價值。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-06-01T06:19:19.457255"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流變換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop是一個無需重新訓練的圖像編輯框架，它利用LoRA模型，透過觀察擴散變換器中特徵交互模式，發現概念特定的變換器特徵在去噪過程的早期會激活空間上連貫的區域。LoRAShop藉此為每個概念導出一個解耦的潛在遮罩，並僅在概念邊界區域內融合相應的LoRA權重，從而在保留全局上下文、光照和細節的同時，將多個主題或風格無縫集成到原始場景中。實驗證明LoRAShop在身份保留方面優於其他方法。它消除了重新訓練和外部約束，將個性化的擴散模型轉變為實用的“LoRA版Photoshop”，並為組合式視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**個性化電子商務產品視覺化：** 用戶可以輕鬆地將自己的寵物、家人或朋友加入到產品展示圖片中，例如將寵物添加到家具廣告中，以展示家具在“真實”家庭環境中的效果。", "**快速生成多樣化的廣告素材：** 廣告設計師可以快速生成多個廣告版本，在同一背景中添加不同的產品或風格，進行A/B測試，找到效果最好的廣告素材。", "**協助非專業人士進行圖像創作：** 普通用戶無需專業技能，即可將不同的概念和風格融合到同一張圖片中，創造出個性化的藝術作品、社交媒體內容或個人化禮品。"], "pitch": "LoRAShop 是一項革命性的圖像編輯技術，它透過免訓練的方式實現多概念圖像生成和編輯，解決了傳統方法需要大量重新訓練的問題。 其“LoRA版Photoshop”的定位，不僅簡化了專業設計師的工作流程，更降低了圖像創作的門檻，讓普通用戶也能輕鬆創造出個性化的視覺內容。 從電子商務產品視覺化、廣告素材快速迭代到個人化圖像創作，LoRAShop 擁有廣泛的應用場景， 潛在商業價值巨大。 我們相信，LoRAShop 有機會成為圖像編輯領域的下一代標準，並在視覺內容創作領域引發新的浪潮。投資 LoRAShop，就是投資圖像編輯的未來。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-06-01T06:19:33.383515"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI校準的扭曲：偏好最佳化是否真的在最佳化偏好？", "summary_zh": "大型語言模型在預訓練後會基於成對比較結果來校準人類偏好。然而，現有的校準方法（如基於PPO的RLHF和DPO）都假設只與單一偏好模型校準，但實際上使用者擁有多元偏好。因此，這些方法是否能產生平均而言滿足使用者需求的模型仍然不明朗。本研究基於社會選擇理論，透過Bradley-Terry模型來模擬使用者的比較，引入了校準方法的扭曲概念：即最佳可實現平均效用與模型學習策略的平均效用之間的最差情況比率。 研究表明，Nash Learning from Human Feedback在不同條件下都能實現minimax最佳扭曲，而RLHF和DPO則會遭受更嚴重的扭曲，甚至可能出現無界扭曲。", "applications": ["**個人化推薦系統：** 針對不同使用者提供客製化電影、音樂或商品的推薦，避免單一偏好模型導致的推薦偏差。", "**多樣化內容生成：** 生成不同風格、觀點和主題的文章、故事或圖片，以滿足不同使用者的內容需求，而非僅僅是迎合大眾口味。", "**協作機器人設計：** 設計能夠根據團隊成員的不同偏好和工作風格進行協作的機器人，例如在會議安排、任務分配等方面考慮成員的個人習慣和時間安排。"], "pitch": "目前AI校準方法在面對使用者多元偏好時存在嚴重問題，導致模型無法有效滿足個體需求，產生偏差甚至錯誤。我們的研究揭示了這些問題，並提出了一種更穩健的解決方案，能顯著降低校準扭曲。 想像一下，一個能真正理解並滿足不同使用者需求的AI模型，可以大幅提升使用者體驗，在推薦系統、內容生成、智能助理等領域創造巨大的商業價值。 我們的技術能幫助企業打造更公平、更個性化的AI產品，贏得使用者信任，並在競爭激烈的市場中脫穎而出。我們正尋求投資，加速技術開發和商業化，將更符合人性的AI帶入千家萬戶。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-06-01T07:12:56.298148"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升多模態大型語言模型在基於視覺的空間智能方面的能力", "summary_zh": "Spatial-MLLM 是一種新的框架，旨在僅從 2D 圖像和影片中提升多模態大型語言模型 (MLLM) 的空間推理能力。與傳統依賴 3D 或 2.5D 數據的模型不同，Spatial-MLLM 利用視覺幾何基礎模型的結構先驗知識，採用雙編碼器架構：一個提取語義特徵的 2D 視覺編碼器，以及一個提取 3D 結構特徵的空間編碼器。此外，它還使用一種空間感知的幀採樣策略，在推理時選擇影片序列中對空間推理至關重要的幀。Spatial-MLLM 在多個真實世界數據集上取得了最先進的性能。", "applications": ["**自動駕駛導航：** 汽車可以通過攝像頭捕獲的 2D 圖像和影片，更精準地理解周圍環境的空間結構，例如判斷車道線的曲率、預測其他車輛的運動軌跡，從而提升導航的安全性和可靠性。", "**室內機器人清潔：** 掃地機器人可以利用 Spatial-MLLM 理解房間的 3D 結構，例如判斷障礙物的高度和位置，規劃更有效的清潔路徑，避免碰撞和遺漏。", "**虛擬實境 (VR) 和擴增實境 (AR)：** 在 VR/AR 環境中，模型可以理解使用者周圍的空間環境，並將虛擬物體更自然地融入到現實世界中，提升使用者沉浸感和互動體驗。"], "pitch": "Spatial-MLLM 解決了多模態大型語言模型在基於 2D 視覺的空間推理方面的瓶頸。現有的 3D MLLM 依賴額外的 3D 或 2.5D 數據，限制了其應用場景。Spatial-MLLM 僅需 2D 輸入，就能實現卓越的空間理解能力，開闢了廣闊的商業應用前景。例如，可以應用於自動駕駛、機器人、VR/AR 等領域，大幅提升這些領域的智能化水平。其核心技術，例如雙編碼器架構和空間感知的幀採樣策略，具有很強的技術壁壘。通過與現有平台或服務集成，Spatial-MLLM 可以快速商業化，搶佔市場先機。我們相信，Spatial-MLLM 將引領下一代基於視覺的空間智能技術，帶來巨大的商業價值。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-06-01T07:13:11.219935"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：基於修正流轉換器，無需訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop是一個創新的圖像編輯框架，利用LoRA模型實現多概念圖像編輯。它基於對Flux風格擴散轉換器內部特徵交互模式的觀察：概念特定的轉換器特徵在去噪過程的早期就激活了空間相干區域。LoRAShop利用這個特性，在正向傳遞中為每個概念導出一個解耦的潛在遮罩，並僅在限定概念的區域內混合相應的LoRA權重。這樣產生的編輯能夠將多個主體或風格無縫整合到原始場景中，同時保留全局背景、光照和細節。實驗證明，LoRAShop比基準模型更好地保留了身份資訊。通過消除重新訓練和外部約束，LoRAShop將個性化的擴散模型轉變為一個實用的“基於LoRA的Photoshop”工具，並為構圖視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**個性化商品設計：** 用戶可以上傳自己的照片（例如寵物、家人），LoRAShop 可以將這些元素無縫添加到商品圖片中，例如T恤、馬克杯或手機殼，讓用戶快速預覽定制效果。", "**虛擬試穿與造型：** 用戶可以上傳自己的照片，然後使用 LoRAShop 將不同的服裝、髮型或妝容應用到照片上，實現在虛擬環境中試穿和造型，而無需實際更換服裝或化妝。", "**情境式內容生成：** 廣告商或內容創作者可以輕鬆地將特定的人物、產品或風格整合到現有的圖片或影片中，快速創建符合特定情境的視覺內容，例如將新款汽車加入到城市夜景圖片中。"], "pitch": "LoRAShop 是下一代圖像編輯技術，它基於免訓練的多概念編輯能力，打破了傳統圖像編輯的壁壘。想像一下，一個無需專業技能的使用者就能夠輕鬆將多個主題無縫融入圖像，實現高度客製化的視覺內容。這不僅降低了圖像編輯的門檻，也極大地提升了效率。其商業價值體現在：\n\n*   **大幅降低創作成本：** 企業無需耗費大量資源進行素材拍攝和後期製作，即可快速生成客製化行銷內容。\n*   **提高使用者參與度：** 個性化定制功能將吸引更多用戶參與內容創作，形成更強的社群連結。\n*   **開闢新的商業模式：** 基於 LoRAShop 的應用程式或服務，例如個性化商品設計平台、虛擬試穿 App 等，都具有巨大的市場潛力。我們相信 LoRAShop 將引領圖像編輯領域的革命，為各行各業帶來顛覆性的創新機會。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-06-01T07:13:29.536148"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在視覺空間智能中的能力", "summary_zh": "這篇論文提出了一個名為Spatial-MLLM的新框架，旨在提升多模態大型語言模型（MLLM）在純2D視覺輸入下的空間推理能力。 不同於依賴額外3D數據或2.5D數據的現有方法，Spatial-MLLM利用預訓練的視覺幾何模型，並採用雙編碼器架構，分別提取圖像的語義特徵和3D結構特徵。 此外，論文還提出了一種空間感知幀採樣策略，以選擇包含豐富空間信息的關鍵幀。 透過在Spatial-MLLM-120k數據集上進行訓練，模型在各種真實世界的數據集上，於視覺空間理解和推理任務中取得了領先的性能。", "applications": ["**自動駕駛導航：** 基於車載攝像頭拍攝的2D影像，Spatial-MLLM可以重建周圍環境的3D結構，輔助車輛更準確地理解道路、行人和其他車輛的位置關係，提升導航的準確性和安全性。", "**智能家居物件擺放：** 用戶拍攝房間的2D照片，Spatial-MLLM可以分析房間的空間結構，並根據用戶的需求，推薦最佳的家具擺放位置，或者模擬不同擺放方案的效果，提供更好的家居設計體驗。", "**醫療影像分析：** 基於X光片或CT掃描等2D醫學影像，Spatial-MLLM可以重建人體器官的3D結構，輔助醫生進行疾病診斷和手術規劃，提高診斷的準確性和手術的成功率。"], "pitch": "Spatial-MLLM代表了多模態AI的一個重大突破，它讓AI能夠僅僅基於2D視覺信息，像人類一樣理解和推理3D空間。這開啟了大量新的商業機會。想像一下，我們的技術可以嵌入到自動駕駛系統中，顯著提高其安全性和可靠性；也可以應用於智能家居領域，為用戶提供更個性化和智能化的家居設計服務；更可以革新醫療影像分析，幫助醫生更準確地診斷和治療疾病。 Spatial-MLLM的核心優勢在於無需昂貴的3D傳感器或複雜的數據採集流程，僅僅利用現有的2D圖像或視頻就能實現高精度的空間推理，這大大降低了應用成本，加速了市場普及。 我們正在尋找投資者，共同將Spatial-MLLM推向市場，引領下一代基於視覺的空間智能應用。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-06-01T08:17:01.826486"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：基於修正流轉換器的免訓練多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一種全新的圖像編輯框架，利用 LoRA 模型實現多概念編輯。它發現 Flux 風格擴散轉換器中的特徵交互模式：概念特定的轉換器特徵在去噪過程早期激活空間上連貫的區域。LoRAShop 利用此發現，在預先的正向傳遞中，為每個概念導出一個解耦的潛在遮罩，並僅在邊界概念的區域內混合相應的 LoRA 權重。這樣編輯後的圖像能夠無縫地將多個主體或風格整合到原始場景中，同時保留全局上下文、光照和精細細節。實驗表明，LoRAShop 在身份保留方面優於現有方法。由於無需重新訓練和外部約束，LoRAShop 將個人化擴散模型轉變為實用的『基於 LoRA 的 Photoshop』工具，並為組合式視覺敘事和快速創意迭代開闢了新途徑。", "applications": ["**個性化照片修復與增強：** 使用者可以輕鬆地將照片中的特定人物或物體進行風格轉換，例如將老照片中的人物換上現代服裝，同時保持臉部特徵不變。", "**快速生成客製化商品素材：** 電商業者可以利用 LoRAShop 快速生成多種不同風格的商品圖片，例如將一個杯子圖片換上不同的設計圖案，用於商品展示和行銷活動。", "**遊戲角色和場景快速迭代設計：** 遊戲開發者可以使用 LoRAShop 快速修改遊戲角色或場景的風格，例如將一個角色從寫實風格轉換為卡通風格，用於概念設計和原型驗證。"], "pitch": "LoRAShop 是一種革命性的圖像編輯技術，它無需重新訓練模型，即可實現多概念的圖像編輯，極大地降低了使用門檻和成本。這項技術的核心優勢在於其獨特的潛在遮罩提取和 LoRA 權重混合機制，能夠精確控制編輯區域，實現細膩且逼真的編輯效果。其潛在商業價值巨大，可以應用於電商、遊戲、廣告、設計等眾多領域，為這些行業帶來效率提升和創意爆發。設想一下，一個電商平台可以利用 LoRAShop 快速生成數百種不同風格的商品圖片，吸引更多客戶；一個遊戲公司可以利用 LoRAShop 加速遊戲角色的迭代設計，降低開發成本；一個廣告公司可以利用 LoRAShop 快速生成各種創意廣告素材，提高廣告效果。LoRAShop 不僅僅是一種技術，更是一種賦能工具，它將極大地改變圖像編輯的未來，為各行各業帶來無限可能。我們相信 LoRAShop 具有成為圖像編輯領域領頭羊的潛力，值得大力投資。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-06-01T08:17:17.166525"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升基於視覺的空間智能中 MLLM 的能力", "summary_zh": "這篇論文介紹了一種新的框架 Spatial-MLLM，旨在提升多模態大型語言模型 (MLLM) 在純 2D 視覺資料上的空間推理能力。與過去需要額外 3D 或 2.5D 資料的模型不同，Spatial-MLLM 使用雙編碼器架構，一個提取語義特徵，另一個從視覺幾何模型中提取 3D 結構特徵。通過結合這些特徵以及一種空間感知幀採樣策略，Spatial-MLLM 即使在資料有限的情況下也能專注於對空間推理至關重要的幀。該研究還創建了 Spatial-MLLM-120k 資料集進行訓練，並在多個真實世界資料集上驗證了其卓越的性能。", "applications": ["自動駕駛：根據車載攝像頭提供的 2D 圖像，推斷周圍環境的 3D 結構和空間關係，提升導航和避障能力。", "智慧家居：通過分析監控錄影或照片，理解室內空間佈局和物體位置，實現更精確的語音助手控制和自動化功能。", "醫療影像分析：從 CT 或 MRI 的 2D 切片中重建 3D 模型，輔助醫生進行疾病診斷和手術規劃。"], "pitch": "Spatial-MLLM 解決了 MLLM 在 2D 視覺資料上進行空間推理的關鍵痛點，無需額外的 3D 資料。這項技術賦予了 MLLM 廣泛的應用潛力，特別是在自動駕駛、智慧家居和醫療影像等領域。其雙編碼器架構和空間感知幀採樣策略實現了更高效、準確的空間理解。我們創建的 Spatial-MLLM-120k 資料集將加速模型的訓練和應用落地。我們相信 Spatial-MLLM 將成為視覺智能領域的領先技術，具有巨大的商業價值，包括授權許可、模型部署和定制化解決方案等多種盈利模式。早期投資將帶來顯著的回報，因為 Spatial-MLLM 將重塑 MLLM 在空間智能方面的應用，開創全新的市場機會。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-06-01T09:13:20.741597"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：基於校正流變換器之免訓練多概念圖像生成與編輯", "summary_zh": "LoRAShop是首個基於LoRA模型的多概念圖像編輯框架。它利用Flux風格擴散變換器中，概念特定特徵會在去噪早期激活空間連貫區域的特性，提取每個概念的解耦潛在遮罩，並僅在概念邊界區域內混合相應的LoRA權重。這使得圖像編輯能無縫整合多個主題或風格，同時保留全局上下文、光照和細節。實驗表明，LoRAShop在身份保留方面優於其他方法。它無需重新訓練和外部約束，將個性化擴散模型轉化為實用的“LoRA版Photoshop”，為組合式視覺故事和快速創意迭代開闢了新途徑。", "applications": ["**一鍵風格轉換個人頭像：** 用戶可以上傳自己的照片，然後選擇多種LoRA風格（例如：卡通、油畫、賽博龐克），LoRAShop就能快速生成風格轉換後的個人頭像，無需等待或複雜設置。", "**商品圖片背景快速更換：** 電商商家可以利用LoRAShop快速更換商品圖片的背景，例如：將商品圖片從普通背景更換成充滿節日氣氛的背景，或是根據不同銷售平台調整背景風格，提高商品吸引力。", "**故事板快速生成：** 漫畫家或動畫師可以利用LoRAShop快速生成故事板，通過添加不同的LoRA概念（例如：不同角色、不同場景風格），快速嘗試不同的視覺效果，加快創作流程。"], "pitch": "LoRAShop解決了圖像編輯領域的兩大痛點：個性化和易用性。無需重新訓練，就能將多個概念無縫融入圖像，突破了傳統方法的限制。這意味著更低的計算成本、更快的迭代速度和更廣泛的應用場景。我們的商業價值體現在：1）赋能创意工作者，让他们能够更高效地创作视觉内容，加速创意产业的发展；2）为电商企业提供更便捷的图像处理工具，提升商品吸引力，增加销售额；3）革新个人用户的内容创作体验，让每个人都能轻松创造个性化的视觉作品。我们相信，LoRAShop有潜力成为下一代图像编辑的基础设施，构建一个全新的视觉内容创作生态。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-06-01T09:13:34.939314"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI對齊的扭曲：偏好優化真的能優化偏好嗎？", "summary_zh": "大型語言模型在預訓練後，會透過成對比較來對齊人類偏好。目前最先進的對齊方法，像是基於PPO的RLHF和DPO，都假設對齊的是單一偏好模型，但實際上用戶的偏好是多樣的。因此，這些方法是否能產生平均而言讓用戶滿意的模型都還不清楚。本文借鑒社會選擇理論，透過個體的布拉德利-特里模型（Bradley-Terry models）來建模用戶的比較，並引入了一種對齊方法的「扭曲」概念：最佳可實現的平均效用與學習到的策略的平均效用之間的最差情況比率。研究表明，Nash Learning from Human Feedback 在各種情況下都能達到最佳的minimax扭曲，而RLHF和DPO則會遭受顯著的扭曲，甚至在特定條件下達到無界扭曲。", "applications": ["**個性化教育平台：** 根據學生的學習風格和偏好，調整學習內容和方法。目前的AI輔導系統可能只考慮了普遍的學習模式，而忽略了學生的個別差異，造成學習效果不佳。", "**產品推薦系統：** 根據用戶過去的購買歷史和瀏覽行為，提供更精準的產品推薦。避免推薦過於同質化的產品，或是只基於流行趨勢的推薦，更能滿足用戶的個性化需求。", "**人資招募系統：** 根據職位需求和應聘者的技能、個性，進行更全面的匹配。目前的系統可能只關注硬性技能，而忽略了軟實力以及與公司文化的契合度。"], "pitch": "現今AI模型，尤其在大型語言模型領域，都面臨著對齊人類偏好這個挑戰。我們的研究揭示了現有對齊方法的缺陷，並提出了一種更精準、更能考慮多元偏好的解決方案。這個技術能有效提升AI模型的用戶滿意度，減少負面影響，並開創個性化服務的新紀元。想像一下，一個能真正理解並滿足每個用戶獨特需求的AI助手，這將在教育、電商、醫療等各個領域帶來顛覆性的改變。我們正在打造一個更具包容性和個性化的AI未來，這不僅是一個技術突破，更是一個巨大的商業機會。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-06-01T10:13:49.553886"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "空間-MLLM：提升MLLM在基於視覺的空間智能方面的能力", "summary_zh": "本論文提出一個名為Spatial-MLLM的新框架，旨在提升多模態大型語言模型（MLLM）在僅基於2D視覺輸入下的空間推理能力。 不同於依賴額外3D或2.5D數據的模型，Spatial-MLLM通過雙編碼器架構，結合預訓練的2D視覺編碼器和從視覺幾何模型初始化的空間編碼器，分別提取語義特徵和3D結構特徵。此外，還提出了一種空間感知幀採樣策略，以在有限的token長度下選擇對空間推理至關重要的幀。通過在Spatial-MLLM-120k數據集上進行訓練，該模型在多個真實世界數據集上展示了最先進的視覺空間理解和推理性能。", "applications": ["**自動駕駛導航：** 通過分析車載攝像頭拍攝的2D視頻，實時理解周圍環境的3D空間結構，提升導航準確性，尤其是在缺乏高精度地圖的區域。", "**室內機器人清潔：** 使清潔機器人能夠僅通過攝像頭掃描房間，就能理解房間的布局和障礙物位置，從而更有效地規劃清潔路徑，避開障礙。", "**建築設計輔助：** 建築師可以使用2D照片或視頻，讓模型理解建築物的空間結構，並自動生成3D模型，加速設計流程和降低成本。"], "pitch": "Spatial-MLLM解決了多模態大語言模型在僅基於2D視覺輸入下進行空間推理的痛點，填補了市場空白。其雙編碼器架構和空間感知幀採樣策略顯著提升了空間理解能力，使其在自動駕駛、機器人、建築等領域具有廣闊的商業前景。相較於需要昂貴3D或2.5D數據的方案，Spatial-MLLM僅需2D圖像，降低了部署成本和數據獲取門檻，更易於大規模應用。 我們相信Spatial-MLLM具有顛覆傳統視覺空間智能的潛力，能夠為投資者帶來豐厚的回報。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-06-01T10:14:01.403420"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流轉換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop是一個免訓練的多概念圖像編輯框架，基於LoRA模型。它發現擴散轉換器中，概念特定的特徵會在去噪過程早期激活空間上連貫的區域。因此，LoRAShop可以針對每個概念提取分離的潛在遮罩，並僅在概念邊界區域內融合相應的LoRA權重。這樣產生的編輯能將多個主題或風格無縫整合到原始場景中，同時保留全局上下文、光照和細節。 LoRAShop無需重新訓練和外部約束，使個性化擴散模型成為實用的「LoRA版Photoshop」，為組合視覺故事和快速創意迭代開闢了新途徑。", "applications": ["**虛擬試穿/試戴：** 將不同的衣服、髮型或配飾（LoRA模型代表）無縫整合到用戶自拍照中，讓用戶在購買前預覽效果。", "**個性化藝術創作：** 將不同的藝術風格（LoRA模型代表）融合到用戶提供的照片或草圖中，快速生成獨特的藝術作品，例如將風景照片轉化為梵谷風格。", "**遊戲角色定制：** 玩家可以將自己的臉部特徵（LoRA模型代表）添加到遊戲角色中，或者將不同遊戲角色的風格融合，創造獨一無二的虛擬化身。"], "pitch": "LoRAShop 重新定義了圖像編輯的遊戲規則。它提供了一個無需重新訓練的、高效、精準的多概念圖像編輯解決方案，類似於一個強大的 'LoRA版Photoshop'。想像一下，過去需要專業設計師數小時才能完成的複雜圖像編輯，現在只需幾分鐘就能完成，而且無需耗費高昂的訓練成本。這將徹底改變圖像設計、內容創作和電子商務等領域。其潛在商業價值巨大，我們可以將其授權給Adobe等大型圖像編輯軟體公司、線上零售平台、遊戲開發商，或者直接打造一個基於 LoRAShop 的雲端圖像編輯服務平台，提供高度個性化、快速且低成本的圖像編輯體驗。由於其效率和易用性，LoRAShop 有望成為圖像生成和編輯領域的行業標準。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-06-01T10:14:14.108527"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI對齊的失真：偏好優化真的能優化偏好嗎？", "summary_zh": "大型語言模型在預訓練後會根據成對比較結果與人類偏好對齊。現有對齊方法，如基於PPO的RLHF和DPO，假設與單一偏好模型對齊，但實際應用場景中用戶偏好各異。因此，這些方法是否能讓模型平均滿足用戶的需求，甚至是一個多元化對齊的最低要求，都尚不清楚。本研究借鑒社會選擇理論，通過個人Bradley-Terry (BT) 模型模擬用戶比較，引入了對齊方法的「失真」概念：即最佳可實現平均效用與學習策略的平均效用之間的最壞情況比率。 研究表明，Nash Learning from Human Feedback 具有最佳的minimax失真，而RLHF和DPO在高偏好差異下，失真程度可能很高甚至無界。", "applications": ["**個性化推薦系統：** 理解不同用戶偏好之間的差異，並設計更公平且多樣化的推薦策略，避免演算法只針對特定群體優化，忽略了其他用戶的需求。", "**醫療診斷輔助工具：** 在考慮不同醫生診斷偏好（例如，對於風險的容忍度）的情況下，設計更健壯的診斷建議，降低誤診率，提供更客觀的評估。", "**公共政策決策：** 通過分析不同利益相關者的偏好，制定更能平衡各方利益的政策，避免少數人的偏好主導決策，導致社會不公。"], "pitch": "我們發現現有AI對齊方法在處理多元化偏好時存在嚴重問題，可能導致演算法失真，無法真正滿足用戶需求。我們的研究提出了衡量和解決這種失真的方法。這項技術可以應用於各種領域，例如個性化推薦、醫療診斷和公共政策制定，確保AI系統在面對不同用戶偏好時，能够更公平、更有效率地工作。這將提升用戶體驗、降低風險，並提高AI的可信度和社會價值。我們的Nash Learning方法能有效降低失真，具備廣闊的商業化潛力，例如在推薦引擎、內容生成平台和自動駕駛等領域應用，提升使用者滿意度並實現差異化競爭。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-06-01T11:10:55.541609"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升基於視覺空間智慧的多模態大型語言模型能力", "summary_zh": "這篇論文提出了一個名為 Spatial-MLLM 的新框架，旨在提升多模態大型語言模型（MLLM）基於視覺的空間推理能力，特別是在只有 2D 影像或影片輸入的情況下。Spatial-MLLM 的核心概念是利用視覺幾何基礎模型中強大的結構先驗知識。它使用雙編碼器架構，一個提取語義特徵，另一個提取 3D 結構特徵，並結合空間感知的幀採樣策略，選擇影片序列中空間資訊最豐富的幀，以提升模型在有限 token 長度下的空間推理能力。研究團隊還建立了 Spatial-MLLM-120k 數據集，並進行了監督微調和 GRPO 訓練。實驗結果表明，Spatial-MLLM 在多個真實世界數據集上，於基於視覺的空間理解和推理任務中取得了最先進的性能。", "applications": ["**自動駕駛/機器人導航：** 透過單純的攝影機影像，讓汽車或機器人理解周遭環境的空間結構，進行更精準的導航與路徑規劃，尤其是在 GPS 訊號不佳或環境複雜的區域。", "**建築設計/室內設計：** 從 2D 平面圖或照片自動生成 3D 模型，幫助設計師快速預覽設計效果，並提供空間規劃建議，提升設計效率。", "**醫療影像分析：** 醫生可以基於 X 光片或 CT 掃描影像，利用 Spatial-MLLM 更準確地判斷組織結構和病灶位置，輔助診斷和手術規劃。"], "pitch": "Spatial-MLLM 突破了現有 MLLM 對 3D 或 2.5D 數據的依賴，僅憑 2D 視覺輸入即可實現強大的空間理解和推理能力。這開啟了許多商業機會，尤其是在自動駕駛、機器人、建築設計、醫療影像等領域。想像一下，無需昂貴的 LiDAR 或深度感測器，就能讓自動駕駛汽車更安全可靠；或者讓建築師僅需上傳 2D 平面圖，就能立即獲得逼真的 3D 模型和智能設計建議。Spatial-MLLM 的潛在商業價值巨大，我們正在尋找投資者和合作夥伴，共同將這項技術推向市場，改變人們與空間互動的方式。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-06-01T11:11:08.831495"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：利用修正流變換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一個利用 LoRA 模型進行多概念圖像編輯的全新框架。它基於一個關鍵發現：Flux 風格擴散變換器中的概念特定特徵，在去噪過程早期就會激活空間上一致的區域。LoRAShop 利用這個發現，在正向傳遞中為每個概念提取解耦的潛在遮罩，並且僅在限定概念的區域內混合對應的 LoRA 權重。這使得編輯可以無縫地將多個主體或風格融入到原始場景中，同時保留全局上下文、光照和細節。實驗表明，LoRAShop 相比基線方法能更好地保留身份資訊。通過消除重新訓練和外部約束，LoRAShop 將個人化擴散模型轉變為一個實用的 `基於 LoRA 的 Photoshop` 工具，並為構圖視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**客製化商品設計：** 用戶可以輕鬆將多個個人化元素（例如寵物、標語、風格）融入產品圖片，例如 T 恤、手機殼或海報，無需設計師協助。", "**虛擬室內裝潢：** 用戶可以將不同的家具風格、材質或顏色應用到同一張房間照片上，快速預覽不同裝潢方案的效果，並將多種風格無縫融合。", "**圖像生成輔助創作：** 藝術家和設計師可以快速生成包含特定風格、角色和場景的圖像變體，用於故事板、概念設計或視覺原型製作，加速創作流程。"], "pitch": "LoRAShop 解決了目前圖像編輯領域中需要大量訓練和專業知識才能實現多概念、個人化編輯的問題。它讓用戶無需編碼或繁瑣操作，就能像使用 Photoshop 一樣編輯圖像，並融入多個個人化元素。這將大幅降低圖像編輯的門檻，潛在市場廣闊，包括電商、廣告、娛樂和設計等領域。其免訓練的特性降低了運算成本，更易於部署和擴展。透過授權、API 整合或平台內建，LoRAShop 有望成為圖像編輯和生成領域的顛覆性技術，具有極高的商業價值。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-06-01T11:11:21.899339"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在視覺空間智能方面的能力", "summary_zh": "這篇論文提出了一個名為Spatial-MLLM的新框架，專注於提升多模態大型語言模型（MLLM）在僅有2D視覺輸入（如圖片或影片）下的空間推理能力。與以往依賴額外3D或2.5D數據的模型不同，Spatial-MLLM的核心在於利用視覺幾何基礎模型中強大的結構先驗知識。它採用雙編碼器架構，一個提取語義特徵，另一個則由視覺幾何模型初始化，提取3D結構特徵。同時，論文還提出了一種空間感知的幀採樣策略，只選擇視頻序列中對空間推理至關重要的幀，以提高效率。Spatial-MLLM在多個真實世界數據集上取得了最先進的性能。", "applications": ["**自動駕駛導航：** 利用車載攝像頭拍攝的2D影像，Spatial-MLLM可以理解周圍環境的空間結構，輔助車輛進行更精確的導航和避障，尤其是在GPS信號弱或沒有的情況下。", "**建築設計與室內設計：** 從2D藍圖或照片中推斷出3D結構，幫助設計師快速生成3D模型，並進行虛擬漫遊，方便客戶預覽設計效果。", "**醫療影像分析：** 從CT或MRI的2D切片圖像中重建3D器官模型，輔助醫生進行疾病診斷和手術規劃，提高診斷準確性和手術成功率。"], "pitch": "Spatial-MLLM代表了視覺空間智能領域的重大突破，解決了傳統MLLM在處理僅有2D視覺數據時空間推理能力不足的問題。它不僅提升了現有MLLM的性能，更開闢了新的商業機會。考慮到自動駕駛、建築設計、醫療影像等領域對精準空間理解的巨大需求，Spatial-MLLM擁有廣闊的應用前景。我們相信，通過將Spatial-MLLM集成到現有的產品和服務中，我們可以大幅提升其智能化水平和用戶體驗。我們的團隊正在尋求[金額]的種子輪融資，用於擴大數據集規模、優化模型性能，以及建立戰略合作夥伴關係，以快速推動Spatial-MLLM的商業化進程，成為視覺空間智能領域的領先者。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-06-01T12:24:34.258999"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：利用校正流變換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop是首個利用LoRA模型進行多概念圖像編輯的框架。它基於對Flux式擴散變換器內部特徵交互模式的觀察：概念特定的變換器特徵在去噪過程早期激活空間上連貫的區域。LoRAShop利用這個觀察結果，在先前的正向傳遞中為每個概念導出解耦的潛在遮罩，並僅在限定概念的區域內混合相應的LoRA權重。 這樣產生的編輯能將多個主體或風格無縫地整合到原始場景中，同時保留全局上下文、光照和細節。 實驗證明，與基線相比，LoRAShop 具有更好的身份保留能力。 透過消除重新訓練和外部約束，LoRAShop 將個人化的擴散模型轉變為實用的「搭載LoRA的Photoshop」工具，並為組合視覺故事講述和快速創意迭代開闢了新的途徑。", "applications": ["**個人化圖像編輯：** 使用者可以輕鬆地將自己的寵物、家人或特定風格添加到現有的照片中，無需專業技能或漫長的訓練過程，實現高度客製化的圖像創作。", "**廣告設計：** 廣告商可以快速生成包含特定產品和目標受眾的各種圖像變體，用於A/B測試或針對不同市場的需求進行定制，大幅提高廣告素材的製作效率。", "**遊戲資產創建：** 遊戲開發者可以使用LoRAShop快速生成具有一致風格和多個物件的遊戲資產，例如人物、武器、環境元素等，加速遊戲開發流程。"], "pitch": "LoRAShop 代表了圖像生成與編輯領域的重大突破，它提供了一種無需重新訓練即可對圖像進行多概念編輯的革命性方法。 其核心優勢在於無需昂貴的計算資源或專業知識即可實現高度個性化的圖像創作，這將極大地改變圖像編輯工具的市場格局。 我們相信 LoRAShop 有潛力成為圖像編輯領域的 Adobe Photoshop，並將顛覆廣告、設計、遊戲等行業。 我們正在尋求投資以加速產品開發、擴大團隊規模並建立戰略合作夥伴關係，以將 LoRAShop 推向全球市場，並成為下一個圖像創作領域的獨角獸。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-06-01T12:24:48.523864"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI校準的失真：偏好最佳化真的在最佳化偏好嗎？", "summary_zh": "大型語言模型在預訓練後，會根據成對比較結果來校準人類偏好。現有的校準方法，例如基於PPO的RLHF和DPO，都假設是與單一偏好模型對齊，但實際上使用者擁有多元偏好。因此，這些方法是否能讓模型滿足使用者的平均需求，甚至連最基本的多樣性校準都無法確定。本研究借鑒社會選擇理論，透過個別的Bradley-Terry模型來模擬使用者的比較，引入了校準方法的「失真」概念：即最佳可實現的平均效用與學習策略的平均效用之間的最差情況比率。研究發現，Nash Learning from Human Feedback在各種條件下表現出穩健的最優失真，而RLHF和DPO則表現出較差甚至無界的失真。", "applications": ["**個人化推薦系統：** 理解使用者多元偏好的差異，避免推薦結果過於單一，提供更符合使用者個別需求的內容。", "**醫療診斷輔助系統：** 在診斷決策過程中，考慮不同醫生的診斷偏好和專業知識，避免系統過度依賴單一判斷標準。", "**多方協作的AI助手：** 在團隊協作中，考慮不同成員的偏好和工作習慣，設計更具彈性的AI助手，提升協作效率。"], "pitch": "現有AI校準方法無法有效處理使用者多元偏好，導致模型效用大幅降低。我們的研究提出了「失真」概念，能更準確評估校準效果，並找到更優的校準方法。我們可以將此技術應用於各類AI應用，例如推薦系統、醫療診斷、協作工具等，大幅提升使用者滿意度與AI效用。這不僅能優化現有產品，更可開發新一代更能適應多元使用者需求的AI服務，打造更具競爭力的商業模式。我們正在尋找投資者，共同將這項研究成果商業化，引領AI校準的新方向，搶佔市場先機。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-06-01T13:22:36.344204"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升多模態大型語言模型在視覺空間智能中的能力", "summary_zh": "本論文提出Spatial-MLLM，一個創新的框架，旨在提升多模態大型語言模型（MLLM）在僅有2D視覺資訊下的空間推理能力。與傳統依賴額外3D或2.5D數據的模型不同，Spatial-MLLM利用預訓練的視覺幾何基礎模型提取3D結構特徵，並結合2D視覺編碼器提取的語義特徵，透過一個連接器將兩者融合。此外，還提出了空間感知幀採樣策略，確保模型在有限的token長度下，專注於對空間推理至關重要的幀。在Spatial-MLLM-120k數據集上進行訓練後，模型在多個現實世界的數據集上取得了最先進的性能，證明了其在視覺空間理解和推理方面的強大能力。", "applications": ["**自動駕駛輔助：** 分析行車紀錄器影像，即時判斷周遭環境的空間結構，預測潛在的碰撞風險，並提供駕駛員更精準的輔助資訊，例如更精準的車道維持或自動變換車道建議。", "**室內導航與機器人導航：** 透過手機相機或機器人自身的視覺系統，理解室內空間布局，幫助使用者或機器人在複雜的環境中導航，例如在大型購物中心找到特定店鋪，或引導倉庫機器人更有效率地搬運貨物。", "**虛擬實境（VR）/擴增實境（AR）應用：** 根據使用者視角和周遭環境的2D影像，即時重建出更精確的3D空間模型，提升VR/AR體驗的沉浸感和互動性，例如在AR遊戲中更自然地與虛擬物件互動，或在VR環境中進行更逼真的空間探索。"], "pitch": "Spatial-MLLM解決了多模態大型語言模型在缺乏3D資訊下進行空間推理的痛點，為機器視覺領域帶來了突破性的進展。其核心價值在於能夠僅憑2D影像實現高精度的空間理解，這為許多應用場景打開了新的可能性。團隊已經驗證了其在多個真實世界數據集上的卓越性能，證明了其技術的可行性和領先性。我們相信，Spatial-MLLM將成為推動自動駕駛、機器人導航、VR/AR等領域發展的關鍵技術，具備巨大的商業潛力。投資Spatial-MLLM，就是投資未來的空間智能。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-06-01T13:22:51.641558"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：基於校正流轉換器的免訓練多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一個無需重新訓練的模型，就能利用 LoRA 模型進行多概念圖像編輯的新框架。它基於 Flux 風格擴散轉換器中的特徵互動模式，發現概念特定的轉換器特徵會在去噪過程早期激活空間上連貫的區域。LoRAShop 利用這個發現，為每個概念導出一個解耦的潛在遮罩，並僅在概念周圍的區域內混合相應的 LoRA 權重，實現將多個主題或風格無縫整合到原始場景中，同時保留全局上下文、光照和細節。它避免了重新訓練和外部約束，將個性化的擴散模型變成了一個實用的“LoRA 版 Photoshop”工具，為組合視覺敘事和快速創意迭代開闢了新途徑。", "applications": ["**電商產品圖片生成：** 電商平台可以利用 LoRAShop 無需重新拍攝產品照片，就能快速將商品與不同風格或場景融合，創建吸睛的廣告素材。例如，將一款手錶放在海灘、辦公室或派對等不同場景中展示。", "**個性化頭像生成：** 使用者可以上傳幾張自己的照片，然後使用 LoRAShop 將自己變成不同風格的頭像，例如動漫風格、油畫風格或賽博龐克風格，並能精細控制臉部特徵和表情。", "**遊戲角色設計：** 遊戲開發者可以利用 LoRAShop 快速生成具有不同風格和裝備的遊戲角色，並方便地編輯角色的外觀，例如更改髮型、服裝或武器，加速遊戲開發流程。"], "pitch": "LoRAShop 解決了圖像編輯領域的痛點，提供了一個無需重新訓練即可快速、高效地進行多概念圖像生成和編輯的解決方案。想像一下，一個圖像編輯工具，可以像使用 Photoshop 一樣簡單，但卻能利用 LoRA 模型實現高度個性化的圖像創作，而無需耗時的重新訓練。這對於電商、遊戲、廣告等行業都具有巨大的商業價值。我們的競爭優勢在於：1. 免訓練，大幅降低成本和時間；2. 精準控制，能精確地編輯圖像中的特定區域和概念；3. 高品質，保持全局上下文和細節的一致性。我們將通過 SaaS 模式向企業和個人提供服務，並通過 API 接口與現有圖像編輯平台集成，快速佔領市場，成為下一代圖像創作工具的領導者。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-06-01T13:23:07.949828"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI對齊的失真：偏好最佳化真的能最佳化偏好嗎？", "summary_zh": "大型語言模型在預訓練後，會根據成對比較與人類偏好對齊。目前最先進的對齊方法，例如基於PPO的RLHF和DPO，都建立在與單一偏好模型對齊的假設上，但實際上使用者擁有多樣化的偏好。因此，這些對齊方法是否能產生平均而言滿足使用者的模型，還不清楚。透過社會選擇理論和使用者的Bradley-Terry模型，我們引入了對齊方法的「失真」概念：最佳可實現的平均效用，與學習到的策略的平均效用之間的最差情況比率。這個「失真」概念有助於區分不同的對齊方法：來自人類回饋的納什學習，實現了minimax最佳失真，而RLHF和DPO的失真較高，甚至可能在完整設定下達到無限大。", "applications": ["**個人化推薦系統：** 根據每位使用者的獨特偏好提供客製化內容，避免過度簡化導致的內容同質化。", "**協作機器人設計：** 在團隊中，機器人可以學習並適應不同成員的工作風格和偏好，提高協作效率。", "**客製化醫療照護：** 利用AI分析患者的生活習慣、病史和個人偏好，制定更精確有效的治療方案。"], "pitch": "我們解決了AI對齊領域一個核心問題：現有方法未能有效處理使用者偏好的多樣性，導致模型表現不佳甚至產生負面影響。我們的研究揭示了主流方法的缺陷，並提出了更穩健的解決方案。想像一下，一個可以真正理解並滿足每位使用者獨特需求的AI，這將徹底改變人機互動的方式。我們的技術不僅能提升AI產品的用戶體驗和滿意度，還能開闢全新的商業模式，例如超個人化推薦、智能助理和客製化服務。投資我們的團隊，您將站在下一代AI技術的最前沿，抓住一個潛力無限的市場。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-06-01T14:11:50.913740"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在基於視覺的空間智能方面的能力", "summary_zh": "Spatial-MLLM是一種新型框架，旨在僅從2D圖像或影片中提升多模態大型語言模型(MLLM)的空間推理能力。它使用雙編碼器架構，一個用於提取語義特徵，另一個從視覺幾何模型初始化用於提取3D結構特徵。此外，它還採用了空間感知幀採樣策略，在推理時選擇具有空間信息的幀。通過在Spatial-MLLM-120k數據集上進行訓練，模型在各種現實世界數據集上展現了最先進的視覺空間理解和推理性能。", "applications": ["自動駕駛系統：根據車載攝像頭的2D圖像，判斷道路、交通標誌和行人位置，實現更精準的導航和避障。", "機器人導航：機器人僅通過攝像頭提供的2D影像，就能理解周圍環境的3D結構，規劃路徑並避開障礙物。", "室內設計：使用者上傳房間照片，系統即可自動生成3D模型，方便進行虛擬裝修和家具擺放。", "醫學影像分析：從2D的X光片或CT掃描影像重建3D結構，輔助醫生診斷疾病。", "運動分析：從2D影片中分析運動員的動作，評估姿勢和技巧，提供訓練建議。"], "pitch": "Spatial-MLLM解決了現有MLLM在僅有2D視覺輸入下空間推理能力不足的痛點。它結合了語義和空間特徵提取，並通過空間感知採樣策略提升效率。這項技術能夠廣泛應用於自動駕駛、機器人、室內設計等領域，具有巨大的商業潛力。我們正在尋找投資者，共同將Spatial-MLLM商業化，打造下一代具備卓越空間智能的AI解決方案，搶佔市場先機。相比其他依賴3D或2.5D數據的方案，Spatial-MLLM更具成本效益和易用性，更能滿足真實應用場景的需求，且擁有獨家空間感知採樣策略，具備技術壁壘。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-06-01T14:12:02.781958"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：基於修正流轉換器的免訓練多概念圖像生成與編輯", "summary_zh": "LoRAShop是一個創新的圖像編輯框架，利用LoRA模型實現多概念編輯，無需重新訓練模型。其核心是觀察到Flux風格擴散轉換器中，特定概念的特徵在去噪過程早期就會激活空間上連續的區域。LoRAShop透過提取這些區域的隱藏遮罩，並在這些區域內混合相應的LoRA權重，從而實現精確的局部編輯，同時保留全局情境、光照和細節。相比其他方法，LoRAShop能更好地保持原始圖像的身份。", "applications": ["**個人化相片編輯:** 將自己或朋友的樣貌加入現有照片中，例如將自己加入到電影海報或風景照中，並調整風格。", "**產品設計迭代:** 設計師可以快速將不同風格或功能的配件（如輪胎、顏色、材質）添加到產品圖像中，進行快速視覺化設計迭代，例如汽車或家具設計。", "**內容創作輔助:** 漫畫家或插畫家可以快速將多個角色和背景融合，創建複雜的場景，或快速嘗試不同的風格，提高創作效率。"], "pitch": "LoRAShop提供了一個免訓練、高效、精準的多概念圖像編輯解決方案，解決了傳統圖像編輯流程耗時耗力、個人化程度低的痛點。想像一下，使用者無需專業技能即可輕鬆製作出個人化的圖像內容，企業可以更高效地進行產品設計和營銷素材創作。其商業價值體現在：\n\n*   **B2C 方面：** 可整合至相片編輯App，提供差異化競爭優勢，吸引更多使用者，提升使用者黏著度。\n*   **B2B 方面：** 可授權給廣告公司、設計工作室等，提升其內容創作效率和服務質量。\n*   **長期來看：** LoRAShop的技術架構具有高度的擴展性，未來可應用於視頻編輯、3D模型生成等領域，具有廣闊的市場前景。我們尋求投資，將LoRAShop打造成圖像生成和編輯領域的領先平台。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-06-01T14:12:16.124978"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升多模態大型語言模型在視覺空間智能上的能力", "summary_zh": "這篇論文介紹了一種名為 Spatial-MLLM 的新型框架，旨在提升多模態大型語言模型 (MLLM) 在僅依賴 2D 視覺輸入（例如圖片或影片）時的空間推理能力。與傳統方法不同，Spatial-MLLM 利用視覺幾何基礎模型的結構先驗知識，採用雙編碼器架構：一個提取語義特徵的 2D 視覺編碼器和一個提取 3D 結構特徵的空間編碼器。此外，論文還提出了一種空間感知的幀採樣策略，可以在有限的 token 長度下，選擇對空間推理至關重要的幀。通過在 Spatial-MLLM-120k 數據集上進行訓練，Spatial-MLLM 在多個真實世界數據集上，於各種基於視覺的空間理解和推理任務中，都達到了最先進的性能。", "applications": ["**自動駕駛與導航：** 提升自動駕駛系統對周遭環境的理解，例如判斷車輛間距離、行人位置，並做出更精確的行車決策。透過分析影片或圖片，提供更可靠的導航輔助，尤其是在複雜或缺乏明確標記的路況下。", "**機器人操作與家居控制：** 讓機器人能更好地理解和操作三維空間，例如在複雜的家居環境中移動、抓取物品，或者執行精確的組裝任務。使用者也可以透過手機或平板上的 2D 影像，更直觀地控制機器人。", "**虛擬實境 (VR) 與擴增實境 (AR)：** 為 VR/AR 應用程式提供更逼真的空間感知能力，例如在虛擬環境中更自然地移動、與虛擬物件進行互動，或者在 AR 應用程式中精確地將虛擬物件疊加到真實世界中。"], "pitch": "各位投資人，Spatial-MLLM 解決了多模態大型語言模型在空間智能上的瓶頸，這是一個巨大的潛在市場。現有的解決方案依賴於額外的 3D 數據，限制了其應用範圍。Spatial-MLLM 僅需 2D 視覺輸入，就能實現卓越的空間推理能力，大幅降低了成本和複雜性。想像一下，將這項技術應用於自動駕駛，可以顯著提高安全性；應用於機器人，可以實現更智能的自動化。Spatial-MLLM 具備廣泛的應用前景，並已在多個真實世界數據集上證明了其領先的性能。我們相信，Spatial-MLLM 將會徹底改變視覺空間智能領域，為我們帶來可觀的商業回報。這是一個值得您投資的明日之星。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-06-01T15:13:05.836700"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流轉換器，無需訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一個創新的圖像編輯框架，它利用 LoRA 模型實現多概念編輯，且無需重新訓練。 透過觀察 Flux 風格的擴散轉換器，LoRAShop 發現概念特定的特徵在去噪過程早期就能激活空間上連貫的區域。 因此，LoRAShop 能夠在初始的正向傳播中，為每個概念提取出解耦的潛在遮罩，並僅在包含這些概念的區域內融合相應的 LoRA 權重。 這樣就能在保留全局背景、光照和細節的同時，將多個主體或風格無縫地融入原始場景中。 實驗表明，LoRAShop 在身份保持方面優於其他方法。 它消除了重新訓練和外部約束，將個性化的擴散模型轉變為實用的「LoRA 版 Photoshop」，為組合視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**個性化產品設計：** 想像一下，使用者可以上傳幾張貓的照片，然後 LoRAShop 就能將貓的特徵融入到各種產品的設計中，例如 T 恤、手機殼或馬克杯，生成獨一無二的個性化商品。", "**創意內容生成：** 內容創作者可以快速將不同的藝術風格和主題元素混合，生成獨特且引人入勝的視覺內容，例如混合賽博龐克風格和古董攝影風格的圖片，用於社群媒體貼文或廣告活動。", "**虛擬試穿與場景模擬：** 在電商平台中，使用者可以上傳自己的照片，然後 LoRAShop 將不同的服裝或髮型「試穿」到照片上，或者將家具融入到自己的房間照片中，模擬擺放效果，提升購物體驗。"], "pitch": "LoRAShop 提供了一種突破性的方式來生成和編輯圖像，無需耗時的重新訓練，這為創意產業帶來了巨大的潛力。 其核心優勢在於：1) 顯著降低了生成高品質圖像所需的資源和時間；2) 提供了高度的個性化和定制化能力，滿足了使用者對於獨特視覺內容的需求；3) 打破了傳統圖像編輯工具的限制，釋放了無限的創意可能性。 我們相信，LoRAShop 能夠成為內容創作、電商、設計等領域的關鍵技術，具有巨大的商業價值，並有望催生新的應用場景和商業模式。 例如，我們可以將 LoRAShop 的技術授權給電商平台，幫助他們提供個性化的產品推薦和虛擬試穿服務；或者與設計公司合作，打造基於 AI 的設計工具，提升設計效率和創意水平。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-06-01T15:13:23.572180"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI對齊的失真：偏好最佳化真的能最佳化偏好嗎？", "summary_zh": "大型語言模型在預訓練後，會基於成對比較與人類偏好對齊。然而，現行的對齊方法（例如基於PPO的RLHF和DPO）假設對齊的是單一偏好模型，但使用者偏好其實是多樣化的。因此，這些方法是否能讓模型在平均程度上滿足使用者，甚至都還不清楚。我們借鑒社會選擇理論，並透過個別的Bradley-Terry模型來建模使用者的比較，引入了對齊方法的「失真」概念：即最佳可實現平均效用與學習策略的平均效用之間的最壞情況比率。研究發現，Nash Learning from Human Feedback (NLHF) 在多種情境下都能達到最小最大最佳失真，而 RLHF 和 DPO 則可能遭受顯著更高的失真，甚至在某些情況下是無限大的失真。", "applications": ["**個性化推薦系統：** 理解不同使用者的偏好差異，避免推薦千篇一律的商品或服務，提高使用者滿意度和點擊率。", "**自動駕駛系統：** 學習不同駕駛員的駕駛習慣和偏好，例如激進程度、舒適度等，提供更個性化的駕駛體驗。", "**醫療決策支持系統：** 考慮不同病患的治療偏好和風險承受能力，幫助醫生制定更符合病患需求的治療方案。"], "pitch": "我們解決了AI對齊領域的核心問題：如何確保AI模型在多樣化使用者偏好下，真正實現最佳化。現有的RLHF和DPO方法存在嚴重的失真問題，可能導致AI模型無法有效滿足使用者需求。我們的研究揭示了這些方法的缺陷，並展示了Nash Learning from Human Feedback (NLHF) 的優越性。通過投資於基於NLHF的AI對齊技術，我們可以開發出更可靠、更個性化、更符合倫理道德的AI產品。這將在個性化推薦、自動駕駛、醫療健康等領域帶來巨大的商業價值，並提升AI的整體可用性和可信度，避免AI失控帶來的潛在風險。我們提出的方法具有更強的魯棒性和可擴展性，可以有效地處理大規模、高維度的使用者偏好數據，為AI的商業化應用奠定堅實的基礎。 我們正在尋求種子輪投資，以進一步完善我們的技術，並將其應用於實際場景中。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-06-01T16:15:38.282707"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升視覺基礎空間智慧的多模態大型語言模型能力", "summary_zh": "現有的多模態大型語言模型在二維視覺任務上表現出色，但在提升空間智慧方面仍具挑戰。本論文提出 Spatial-MLLM，一個創新的框架，僅利用二維影像進行視覺基礎空間推理。不同於依賴額外3D或2.5D數據的3D模型，Spatial-MLLM採用雙編碼器架構：一個預訓練的2D視覺編碼器提取語義特徵，另一個空間編碼器（基於視覺幾何模型）提取3D結構特徵。最後，利用空間感知的幀採樣策略，在推理時選擇包含更多空間資訊的幀，確保模型能在有限的token長度下專注於關鍵幀。透過 Spatial-MLLM-120k 資料集訓練和監督式微調，Spatial-MLLM 在各種真實世界數據集上展現了卓越的空間理解和推理能力。", "applications": ["**自動駕駛：** 在只有前方攝影機數據的情況下，更準確地理解車輛周圍的環境，判斷障礙物的距離和位置，提高行車安全性。", "**機器人導航：** 讓機器人僅憑視覺數據，就能在複雜的環境中自主導航，例如倉庫、工廠或家庭環境，避免碰撞。", "**建築設計：** 從室內影片或圖片中，自動生成3D模型，或評估現有設計方案的空間利用率和視覺效果，輔助設計師進行優化。"], "pitch": "Spatial-MLLM 解決了多模態大型語言模型在空間智慧方面的核心痛點，特別是在缺乏3D數據的情況下。其雙編碼器架構和空間感知採樣策略，使其在2D視覺輸入下也能實現高精度的空間推理。這項技術具備廣泛的應用前景，從自動駕駛到機器人再到建築設計，都有機會顛覆現有的工作流程。Spatial-MLLM 的商業價值體現在降低對額外3D數據的依賴，提升效率和精度，並開創新的商業模式，例如視覺基礎的自動化導航、空間分析和設計工具，以及基於影片的3D模型重建服務。透過對 Spatial-MLLM 的投資，我們能搶佔下一代空間智慧的技術制高點，並在眾多領域中獲得領先優勢。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-06-01T16:16:00.835210"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流變換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一個創新的圖像編輯框架，它利用 LoRA 模型實現多概念圖像編輯，無需重新訓練。核心原理是觀察到 Flux 風格擴散變換器中的特徵交互模式：概念特定的變換器特徵在去噪過程的早期階段就能激活空間上連貫的區域。LoRAShop 因此能夠為每個概念提取解耦的潛在遮罩，並僅在概念邊界區域內融合相應的 LoRA 權重。這樣產生的編輯能將多個主體或風格無縫集成到原始場景中，同時保留全局上下文、光照和細節。實驗表明，LoRAShop 在身份保持方面優於現有方法。通過消除重新訓練和外部約束，LoRAShop 將個性化擴散模型轉變為實用的「基於 LoRA 的 Photoshop」工具，為組合視覺敘事和快速創意迭代開闢了新途徑。", "applications": ["**個性化圖像生成：** 用戶可以輕鬆地將自己的照片或寵物添加到現有的藝術作品或場景中，創建獨一無二的圖像。", "**產品設計與原型製作：** 設計師可以快速地將不同的設計元素或材質應用到產品模型上，進行視覺化的設計迭代，無需耗時的渲染。", "**內容創作與行銷：** 行銷人員可以將品牌元素或代言人無縫地融入到不同的廣告場景中，生成高度個性化的行銷素材，提升廣告效果。"], "pitch": "LoRAShop 重新定義了圖像編輯，讓使用者無需專業技能和龐大算力，就能輕鬆打造高度個性化且高品質的圖像。其免訓練、多概念編輯的特性，大幅降低了使用門檻和時間成本，將徹底顛覆圖像生成和編輯的市場。我們相信，LoRAShop 有潛力成為新一代的視覺創作工具，在設計、行銷、娛樂等領域擁有廣闊的應用前景。現在投資 LoRAShop，您將搶先佈局 AI 圖像生成的未來，並從中獲得巨大的商業回報。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-06-01T16:16:19.344663"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升 MLLM 在基於視覺的空間智能中的能力", "summary_zh": "這篇論文介紹了 Spatial-MLLM，一個新的框架，旨在提升多模態大型語言模型 (MLLM) 在純粹 2D 視覺數據上的空間推理能力。過去的 3D MLLM 仰賴額外的 3D 或 2.5D 數據來理解空間，但在只有 2D 圖像或影片的情況下就無法使用。Spatial-MLLM 採用雙編碼器架構：一個預訓練的 2D 視覺編碼器提取語義特徵，另一個基於視覺幾何模型骨幹的空間編碼器提取 3D 結構特徵。透過連接器將這兩種特徵整合到統一的視覺標記中，以增強空間理解。此外，還提出了一種空間感知幀採樣策略，在推理時選擇影片序列中具有空間資訊的幀，即使在有限的標記長度下也能確保模型專注於對空間推理至關重要的幀。研究團隊建立了一個名為 Spatial-MLLM-120k 的數據集，並使用監督微調和 GRPO 在其上訓練模型。實驗證明，Spatial-MLLM 在各種基於視覺的空間理解和推理任務中都達到了最先進的性能。", "applications": ["**自動駕駛輔助系統：** 透過分析車載攝影機的 2D 影像，Spatial-MLLM 可以更準確地理解周圍的交通環境，例如預測其他車輛的行駛軌跡、判斷障礙物的距離和位置，進而提升自動駕駛的安全性和可靠性。", "**室內導航和機器人：** 在缺乏 3D 地圖的情況下，Spatial-MLLM 可以僅利用攝像頭提供的 2D 影像，幫助機器人在室內環境中進行導航和定位，例如引導使用者到指定的房間，或協助機器人完成清潔、搬運等任務。", "**運動分析與訓練：** 透過分析運動員的 2D 影片，Spatial-MLLM 可以提取運動員的姿勢、動作和空間關係等信息，幫助教練和運動員更好地了解運動表現，並制定更有效的訓練計劃。"], "pitch": "Spatial-MLLM 解決了 MLLM 在僅有 2D 視覺輸入時，缺乏空間推理能力的問題，這是一個尚未被充分挖掘的市場。其潛在商業價值體現在：\n\n*   **廣泛的應用場景：** 從自動駕駛、機器人到運動分析，Spatial-MLLM 的應用範圍非常廣泛，覆蓋了多個高成長的產業。\n*   **優異的性能表現：** 論文實驗證明，Spatial-MLLM 在空間理解和推理任務中表現出色，優於現有技術。\n*   **技術壁壘：** 雙編碼器架構和空間感知幀採樣策略構成了較高的技術壁壘，競爭對手難以快速模仿。\n*   **數據優勢：** Spatial-MLLM-120k 數據集的建立為模型訓練提供了堅實的基礎。\n\n我們相信，Spatial-MLLM 具有巨大的商業化潛力，可以成為自動駕駛、機器人和運動分析等領域的關鍵技術。透過與相關企業合作，我們可以將 Spatial-MLLM 整合到他們的產品和服務中，創造巨大的經濟效益。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-06-01T17:11:38.084836"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流變換器進行免訓練的多概念圖像生成和編輯", "summary_zh": "LoRAShop 是一個創新的框架，無需重新訓練，就能用 LoRA 模型進行多概念圖像編輯。它基於對 Flux 風格擴散變換器內部特徵交互模式的觀察：概念特定的變換器特徵在去噪過程的早期激活空間上連貫的區域。LoRAShop 利用這個觀察結果，從先前的正向傳遞中為每個概念導出一個解耦的潛在遮罩，並僅在邊界包含要個性化的概念的區域內混合相應的 LoRA 權重。這樣產生的編輯將多個主題或風格無縫地整合到原始場景中，同時保留全局上下文、光照和精細細節。實驗表明，LoRAShop 在身份保留方面優於基準模型。通過消除重新訓練和外部約束，LoRAShop 將個性化的擴散模型轉變為實用的“帶有 LoRA 的 Photoshop”工具，並為組合視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**個性化商品設計：** 用戶可以輕鬆將自己的寵物、家人或個人風格融入到現有的產品圖片中，比如在手機殼、T恤或咖啡杯上添加獨特元素，预览效果。", "**創意內容生成：** 內容創作者可以快速將多個概念或風格合併到圖片中，例如將動漫人物置於真實場景中，或將不同藝術風格融合到一張畫作中，加速創作流程。", "**虛擬試穿/試妝：** 用戶可以將不同款式的服裝或妝容應用到自己的照片上，無須實際試穿或化妝，即可預覽效果，提升購物體驗。"], "pitch": "LoRAShop 顛覆了圖像編輯領域，提供無需重新訓練的多概念圖像生成和編輯能力。想像一下，一個無需專業技能，任何人都可以快速修改、組合圖像的平台。它的免訓練特性大幅降低了使用門檻，而强大的編輯能力則賦予了使用者極高的創作自由。針對電商、內容創作和社交媒體市場，LoRAShop 的商業潛力巨大：提供個性化定制服務、加速內容生成效率、以及提升用戶參與度。我們相信 LoRAShop 將成為圖像編輯領域的 Game Changer，並將快速佔領市場。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-06-01T17:11:55.621255"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升基於視覺空間智能的多模態大型語言模型能力", "summary_zh": "這篇論文提出了一個名為 Spatial-MLLM 的新框架，旨在提升多模態大型語言模型（MLLM）在純粹基於2D視覺資訊下的空間推理能力。現有的3D MLLM通常依賴額外的3D或2.5D數據來獲取空間感知，限制了它們在只有2D輸入（如圖像或影片）場景下的應用。Spatial-MLLM採用雙編碼器架構：一個預訓練的2D視覺編碼器提取語義特徵，以及一個由視覺幾何基礎模型骨幹初始化的空間編碼器提取3D結構特徵。然後，一個連接器將這兩種特徵整合到統一的視覺令牌中，以增強空間理解。此外，論文還提出了一種空間感知幀採樣策略，用於在推論時選擇影片序列中空間信息豐富的幀。研究團隊創建了 Spatial-MLLM-120k 數據集，並使用監督式微調和 GRPO 對模型進行訓練。實驗證明，Spatial-MLLM 在各種基於視覺的空間理解和推理任務中取得了最先進的性能。", "applications": ["**自動駕駛導航輔助：** 根據車載攝影機拍攝的2D影片，模型能推斷出道路的3D結構、其他車輛的位置和速度，輔助駕駛系統做出更安全的決策。", "**室內機器人導航：** 機器人僅依靠攝影機輸入，就能理解房間的佈局、家具的位置，實現更精確的導航和避障，無需額外的雷射雷達等3D感測器。", "**虛擬實境/擴增實境場景理解：** 模型能理解VR/AR應用中，使用者看到的2D影像所代表的3D空間結構，讓使用者與虛擬環境互動更自然、更沉浸。"], "pitch": "Spatial-MLLM 解決了多模態大型語言模型在純2D視覺資訊下的空間推理瓶頸，具有巨大的商業潛力。想像一下，自動駕駛系統不再需要昂貴的 LiDAR 就能精確導航，室內機器人能在複雜環境中靈活移動，AR/VR 體驗更加真實沉浸。Spatial-MLLM 通過利用視覺幾何基礎模型，有效提取 2D 圖像中的 3D 空間資訊，大幅降低了對額外 3D 感測器的依賴，降低了成本並擴大了應用範圍。我們已經證明了其在多個現實世界數據集上的卓越性能，並建立了專用數據集 Spatial-MLLM-120k 來支持模型的訓練和優化。我們正在尋求投資，以進一步開發和商業化 Spatial-MLLM，目標是將其整合到各個領域，革新基於視覺的空間智能應用，並在這個快速成長的市場中取得領導地位。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-06-01T18:17:59.518493"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：利用修正流轉換器實現免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一個創新的框架，專為使用 LoRA 模型進行多概念圖像編輯而設計。它基於一個關鍵發現：在 Flux 風格的擴散轉換器中，概念特定的轉換器特徵會在去噪過程的早期階段激活空間上連貫的區域。LoRAShop 利用這個特性，在先前的前向傳遞中為每個概念導出一個解耦的潛在遮罩，並且僅在包含要個性化的概念的區域內混合相應的 LoRA 權重。 這種方法能將多個主體或風格無縫地整合到原始場景中，同時保留全局上下文、光照和精細細節。實驗表明，LoRAShop 相比於基準線具有更好的身份保留效果。通過消除重新訓練和外部約束，LoRAShop 將個性化的擴散模型轉變為實用的「LoRA 版 Photoshop」，並為組合式視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**客製化商品設計：** 想像一下，你可以上傳一張你家寵物的照片，然後讓 LoRAShop 將牠無縫融入各種商品設計中，例如 T 恤、馬克杯或手機殼，而且寵物的細節和風格都能完美保留。", "**廣告素材快速生成：** 廣告商可以快速地將不同的產品或人物添加到現有的場景中，以測試不同的廣告概念，而無需重新訓練模型或聘請專業修圖師。", "**建築設計模擬：** 建築師可以使用 LoRAShop 將不同的傢俱、裝飾品或甚至建築風格融入到建築設計草圖中，以快速展示不同的設計方案，並獲得客戶的回饋。"], "pitch": "LoRAShop 徹底改變了圖像編輯和生成領域，讓使用者能夠以前所未有的方式輕鬆創造和客製化圖像。 其免訓練的特性大幅降低了門檻，使得非專業人士也能輕鬆使用。 這項技術具有廣泛的應用潛力，從電子商務到廣告，再到建築設計，都能大幅提升效率和創造力。 我們相信 LoRAShop 具有成為圖像編輯領域新標準的潛力，並能為早期投資者帶來豐厚的回報。 我們的商業模式將包括軟體授權、雲端服務以及與設計平台的整合。 潛在市場規模巨大，涵蓋了所有需要圖像編輯和生成服務的行業。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-06-01T18:18:30.105847"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升多模態大型語言模型在視覺空間智能上的能力", "summary_zh": "這篇論文介紹了一個名為Spatial-MLLM的新框架，旨在提升多模態大型語言模型（MLLM）在僅基於2D視覺輸入的空間推理能力。不同於以往需要額外3D或2.5D數據的3D MLLM，Spatial-MLLM利用預訓練的視覺幾何基礎模型提取3D結構特徵，並結合傳統的2D視覺編碼器提取語義特徵。通過雙編碼器架構和空間感知的幀採樣策略，Spatial-MLLM能夠在有限的token長度下，專注於對空間推理至關重要的幀。論文作者還構建了一個名為Spatial-MLLM-120k的數據集，並通過監督微調和GRPO訓練模型。實驗結果表明，Spatial-MLLM在各種真實世界的數據集上，於基於視覺的空間理解和推理任務中，都取得了最先進的性能。", "applications": ["**自動駕駛：** 在僅依靠攝像頭影像的情況下，協助判斷車輛與周圍環境的相對位置和距離，例如預測行人軌跡、識別交通標誌等，提升行車安全。", "**機器人導航：** 讓機器人能夠在未知環境中僅憑視覺資料進行精準導航，例如在倉儲物流中，機器人可以根據攝像頭畫面理解貨架的位置和貨物擺放，實現自主揀貨。", "**擴增實境（AR）：** 透過手機鏡頭，將虛擬物件精準地放置在真實場景中，例如在室內設計應用中，使用者可以透過手機即時看到家具擺放在家中的效果，並進行空間規劃。"], "pitch": "Spatial-MLLM解決了多模態大型語言模型在僅有2D視覺輸入時，空間推理能力不足的痛點。透過創新的雙編碼器架構和空間感知的幀採樣策略，顯著提升了模型在各種空間理解和推理任務上的性能。這項技術的商業價值巨大，尤其是在自動駕駛、機器人導航和擴增實境等領域。Spatial-MLLM不僅能降低對額外3D數據的需求，還能提升系統的魯棒性和泛化能力，具有顯著的競爭優勢。投資Spatial-MLLM，就是投資未來視覺空間智能的發展，搶佔市場先機。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-06-01T19:10:31.722501"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流變換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop是首個使用LoRA模型進行多概念圖像編輯的框架。它基於一個關鍵發現：在Flux風格的扩散變換器中，特定概念的變換器特徵會在去噪過程早期激活空間上連貫的區域。LoRAShop利用這一點，在先前的正向傳遞中，為每個概念導出一個解耦的潛在遮罩，並僅在包含要個性化概念的區域內混合相應的LoRA權重。最終的編輯結果能無縫地將多個主體或風格融入原始場景，同時保留全局上下文、光照和精細細節。實驗表明，LoRAShop相比基線模型，能更好地保留身份。通过消除重新训练和外部约束，LoRAShop将个性化的扩散模型变成了一个实用的“LoRA版本的Photoshop”工具，并为组合视觉叙事和快速创意迭代开辟了新的途径。", "applications": ["**客製化商品設計：** 讓使用者輕鬆將寵物、家人或喜愛的風格融入商品設計中，例如手機殼、馬克杯、T恤等，無需專業設計技能。", "**圖像修復與增強：** 無損地將舊照片中的人物替換為更清晰或風格化的版本，同時保留背景的原始細節與氛圍。", "**遊戲角色與場景創建：** 快速生成具有特定風格和元素的遊戲角色與場景，例如將真實人物照片轉換為具有奇幻風格的角色，並將其放置在遊戲世界中。"], "pitch": "LoRAShop解決了現有個性化圖像編輯技術需要大量訓練和受到限制的問題，提供了一個免訓練、快速、高精度的多概念圖像編輯方案。透過將預訓練的 LoRA 模型與巧妙的遮罩技術結合，LoRAShop 為創作者開啟了無限的可能，讓他們能夠輕鬆地將各種概念融入圖像中，同時保留原始的細節和上下文。這項技術的商業價值巨大，涵蓋了電商、遊戲、娛樂等多個領域。想像一下，使用者可以輕鬆地客製化商品、創建個性化的遊戲角色，或是在社交媒體上分享獨一無二的藝術作品。 LoRAShop 的低門檻和高效性將吸引大量使用者，並為我們帶來豐厚的回報。 我們相信 LoRAShop 有潛力成為圖像編輯領域的革命性技術，並在市場上佔據領導地位。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-06-01T19:10:46.622831"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在基於視覺的空間智能上的能力", "summary_zh": "現有的多模態大型語言模型(MLLM)在處理2D視覺任務上表現出色，但在空間智能方面仍有提升空間。Spatial-MLLM提出一種新穎的框架，僅使用2D視覺資訊進行空間推理。其核心在於利用前饋視覺幾何基礎模型中強大的結構先驗知識。該模型採用雙編碼器架構，一個用於提取語義特徵的2D視覺編碼器，以及一個從視覺幾何模型主幹初始化用於提取3D結構特徵的空間編碼器。通過連接器整合兩者，並引入空間感知的幀採樣策略，在推理時選擇具有空間信息量的關鍵幀。研究團隊還構建了Spatial-MLLM-120k數據集進行模型訓練。實驗結果表明，Spatial-MLLM在多種實際應用場景中，於基於視覺的空間理解和推理任務上均達到最佳性能。", "applications": ["**智慧無人機巡檢：** 無人機在橋樑、電塔等基礎設施巡檢時，透過影像判讀結構缺陷，並能精確定位缺陷位置，生成維護報告。", "**自動駕駛汽車：** 提高車輛對周圍環境的空間感知能力，尤其是在惡劣天氣或複雜路況下，更精準地識別行人、車輛及交通標誌，提升行車安全。", "**室內導航與機器人：** 幫助機器人在複雜的室內環境中精確導航，例如工廠、醫院或倉庫，實現高效的物料搬運和導覽服務。"], "pitch": "Spatial-MLLM解決了多模態大型語言模型(MLLM)在空間智能上的瓶頸，透過純2D視覺資訊實現精確的3D空間理解。相較於依賴額外3D數據的解決方案，Spatial-MLLM更具成本效益和應用彈性。其在無人機、自動駕駛、機器人等領域擁有廣闊的應用前景。透過授權、數據服務或垂直領域解決方案，Spatial-MLLM具備成為下世代空間智能基礎設施的潛力，可為投資者帶來豐厚回報。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-06-01T20:14:13.823490"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流轉換器進行免訓練多概念圖像生成和編輯", "summary_zh": "LoRAShop 是一個全新的框架，能使用 LoRA 模型進行多概念圖像編輯，無需重新訓練。它基於一個關鍵發現：在 Flux 風格的擴散轉換器中，針對不同概念的特徵會在去噪過程的早期階段激活空間上連貫的區域。LoRAShop 利用這個特性，在預先的前向傳遞中為每個概念導出一個解耦的潛在遮罩，然後僅在包含個性化概念的區域內混合相應的 LoRA 權重。這樣產生的編輯能夠無縫地將多個主體或風格融入原始場景，同時保留全局上下文、光照和精細細節。實驗表明，LoRAShop 比基準線在身份保留方面表現更好。通過消除重新訓練和外部約束，LoRAShop 將個性化的擴散模型轉變為實用的“基於 LoRA 的 Photoshop”工具，並為組合視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**電商商品圖像客製化：** 用戶可以輕鬆將自己的寵物或喜歡的圖案添加到商品圖片中，例如在 T 恤上印上寵物肖像，或者將房間的照片轉換為不同風格的裝潢。", "**遊戲角色快速設計：** 遊戲開發者可以快速將不同的服裝、武器和特效添加到現有的角色模型中，而無需重新訓練或複雜的建模過程，加速遊戲開發流程。", "**個人化藝術創作與社群分享：** 用戶可以將自己或朋友的照片融入到名畫風格中，或是將多張照片融合在一起，創造獨一無二的藝術作品並在社群媒體上分享。"], "pitch": "LoRAShop 正在顛覆圖像編輯領域，它讓用戶可以像使用 Photoshop 一樣輕鬆地進行多概念圖像編輯，但完全無需重新訓練模型。這項技術的商業潛力巨大，尤其是在電商、遊戲開發、廣告設計和個人化內容創作等領域。想像一下，電商平台可以讓用戶個性化產品圖片以提高轉化率，遊戲公司可以快速迭代角色外觀，廣告商可以輕鬆生成多樣化的廣告素材。LoRAShop 降低了圖像編輯的門檻，加速了創意產出，並為個性化內容開闢了無限可能。其“免訓練”的特性極大地降低了成本和時間，使其具有高度的可擴展性。我們正在尋找投資者，共同將 LoRAShop 打造成圖像編輯領域的領先平台，引領 AI 驅動的創意革命。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-06-01T20:14:28.217987"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升多模態大型語言模型在視覺空間智能方面的能力", "summary_zh": "Spatial-MLLM 是一個新的框架，專注於純粹基於2D視覺觀察進行空間推理。它採用雙編碼器架構：一個預訓練的2D視覺編碼器提取語義特徵，另一個從視覺幾何模型的骨幹初始化而來的空間編碼器提取3D結構特徵。透過連接器整合這些特徵，提升模型的空間理解能力。此外，它還使用空間感知的幀採樣策略，在推理時選擇包含豐富空間信息的幀，即使在token長度有限的情況下，模型也能專注於對空間推理至關重要的幀。透過在Spatial-MLLM-120k數據集上的訓練，Spatial-MLLM 在各種真實世界數據集上實現了最先進的視覺空間理解和推理性能。", "applications": ["**輔助駕駛：** 透過分析行車記錄器影片，即時判斷周圍車輛的距離、速度和相對位置，提升自動駕駛決策的安全性與準確性，避免碰撞。", "**室內導航與物體定位：** 利用手機鏡頭拍攝的影像，在沒有GPS信號的室內環境中，精準定位使用者位置，並導航至特定目標，同時辨識並標示房間內的家具、設施等物件。", "**虛擬實境（VR/AR）環境互動：** 讓使用者在VR/AR環境中，僅透過視覺資訊與虛擬物件進行更自然的互動，例如根據使用者觀察角度自動調整物件的遮蔽關係，提升沉浸感和互動性。"], "pitch": "Spatial-MLLM 解決了多模態大語言模型在僅有2D視覺輸入時，缺乏空間推理能力的瓶頸。它突破性地利用視覺幾何模型提取3D結構信息，賦予模型更強大的空間理解能力。這意味著在不需要額外3D數據的前提下，就能實現更精準的空間感知和推理。想像一下，將這項技術應用於輔助駕駛、無人機導航、零售分析或機器人導航，都能顯著提升效率和安全性。我們團隊已經證明Spatial-MLLM在多個真實世界數據集上的卓越性能。 我們相信Spatial-MLLM具有巨大的商業潛力，能為各行各業帶來革命性的變革，並希望與貴公司合作，共同開創視覺智能的新時代。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-06-01T21:12:04.505728"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流變換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一個創新的框架，它能利用 LoRA 模型在圖像中進行多概念編輯，而無需重新訓練模型。它基於一個關鍵發現：在 Flux 風格的擴散變換器中，概念特定的特徵會在去噪過程的早期激活空間連貫的區域。LoRAShop 利用這個特性，在先前的正向傳遞中為每個概念導出一個分離的潛在遮罩，並僅在邊界包含個性化概念的區域內混合相應的 LoRA 權重。這樣產生的編輯能將多個主體或風格無縫地整合到原始場景中，同時保留全局上下文、光照和精細細節。LoRAShop 優於其他基準模型，並通過消除重新訓練和外部約束，將個性化擴散模型變成一個實用的“帶 LoRA 的 Photoshop”工具，為構圖視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**個性化商品設計：** 用戶可以上傳自家寵物照片，然後使用 LoRAShop 將寵物融入到各種商品設計中，例如將寵物印在 T 恤、馬克杯或手機殼上，並且風格可變，讓商品更具獨特性。", "**虛擬試穿/搭配：** 用戶可以上傳自己的照片，然後使用 LoRAShop 將不同款式的衣服或配飾“穿”在身上，模擬試穿效果，方便用戶選擇最適合自己的商品，提升購物體驗。", "**快速生成廣告素材：** 廣告設計師可以快速將產品融入到各種風格的場景中，生成多樣化的廣告素材，例如將新款汽車融入到科幻風格的城市背景中，或者將食品產品融入到節日主題的场景中，提高廣告效率。"], "pitch": "LoRAShop 代表了圖像生成和編輯領域的重大突破，它解決了傳統個性化模型需要大量重新訓練的痛點。我們打造了一個免訓練、易於使用的平台，讓用戶能夠快速、高效地將多個概念融入圖像中，創造出獨特的視覺內容。其商業價值體現在：一、加速產品設計迭代，降低設計成本；二、提升用戶體驗，增加用戶黏性；三、賦能內容創作者，開闢新的商業模式。 我們相信，LoRAShop 將成為內容創作、電商、廣告等領域的必備工具，具有巨大的市場潛力。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-06-01T21:12:18.229358"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升視覺空間智能中多模態大型語言模型的能力", "summary_zh": "Spatial-MLLM 是一個新穎的框架，旨在純粹基於 2D 視覺觀察提升多模態大型語言模型 (MLLM) 的空間推理能力。它利用雙編碼器架構，結合預訓練的 2D 視覺編碼器提取語義特徵，以及從視覺幾何模型骨幹初始化的空間編碼器提取 3D 結構特徵。此外，還提出了空間感知幀採樣策略，在推理時選擇視頻序列中包含空間信息的關鍵幀。透過 Spatial-MLLM-120k 數據集上的訓練，該模型在各種真實世界的數據集上，於視覺空間理解和推理任務中表現出最先進的性能。", "applications": ["**智慧導航助手：** 利用行車記錄器或手機鏡頭提供的 2D 影像，即時分析路況和交通號誌，協助駕駛做出更精確的路線規劃和決策，例如預測轉彎半徑、判斷車輛間距、識別潛在危險。", "**建築物室內導航：** 僅需提供室內空間的 2D 平面圖或幾張照片，即可建立精確的 3D 室內空間模型，並引導使用者在大型建築物（如醫院、博物館、購物中心）內找到目的地。", "**機器人空間理解與定位：** 使機器人能夠僅憑攝像頭拍攝的 2D 影像，理解周圍環境的空間結構，例如判斷物體的遠近、大小、以及相互關係，進而實現自主導航和目標物體的抓取。"], "pitch": "Spatial-MLLM 解決了傳統 MLLM 在空間理解上的瓶頸，無需額外的 3D 或 2.5D 數據，僅憑 2D 視覺輸入就能實現高精度的空間推理。這項技術突破開啟了許多商業可能性。想像一下，我們可以將它應用於自動駕駛，提升導航精準度，減少事故發生；也能用於零售業，優化店內商品擺放，提升顧客購物體驗；甚至能讓機器人在複雜的環境中工作，例如倉庫管理、災害救援等。Spatial-MLLM 的核心優勢在於其算法的創新性和數據集的高質量，這將使其在競爭激烈的 AI 市場中脫穎而出，並帶來巨大的商業價值。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-06-01T22:12:58.927874"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流變換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop 是一個新框架，利用預先訓練好的 LoRA 模型，實現多概念圖像編輯，無需重新訓練。它觀察到 Flux 風格擴散變換器中，概念特定的特徵在降噪早期就激活了空間相干區域。LoRAShop 藉此為每個概念導出一個解耦的潛在遮罩，並僅在限定概念的區域內混合相應的 LoRA 權重。這樣產生的編輯可以將多個主題或風格無縫整合到原始場景中，同時保留全局上下文、光照和精細細節。LoRAShop 提供比其他方法更好的身份保留，且無需重新訓練，使其成為一個實用的「LoRA 版 Photoshop」，開闢了構圖視覺敘事和快速創意迭代的新途徑。", "applications": ["電商商品圖快速客製化：允許使用者輕鬆將特定風格或人物融入商品圖片，例如將自家寵物融入馬克杯設計。", "虛擬試穿/試戴：使用者可以上傳自己的照片，將不同的服裝或配件無縫添加到照片中，實現虛擬試穿/試戴效果。", "遊戲角色設計：遊戲開發者可以使用 LoRAShop 快速創建和修改遊戲角色外觀，例如將特定風格或紋理添加到角色服裝上。"], "pitch": "LoRAShop 是一種革命性的圖像編輯技術，透過免訓練的 LoRA 模型，讓使用者能夠輕鬆地將多個概念整合到圖像中，實現高度客製化和精準的編輯。與傳統方法相比，LoRAShop 降低了技術門檻，節省了大量時間和成本。其潛在商業價值巨大，可應用於電商、時尚、遊戲、廣告等領域，提供更具吸引力、更具個性化的視覺內容，並大幅提升創作效率。想像一下，一個讓所有人都能成為圖像編輯大師的工具，這就是 LoRAShop 的願景，一個充滿無限可能的市場。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-06-01T22:13:15.589714"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI對齊的扭曲：偏好優化真的能優化偏好嗎？", "summary_zh": "大型語言模型在預訓練後，會基於成對比較與人類偏好對齊。現有的對齊方法，如基於PPO的RLHF和DPO，假設能與單一偏好模型對齊，但實際上使用者偏好多元。這導致這些方法是否真的能讓使用者平均滿意都成問題，這可是多元對齊的最低要求。本研究借鑒社會選擇理論，用Bradley-Terry模型模擬使用者比較，提出對齊方法的「扭曲」概念：即最佳平均效用與學習策略的平均效用之間的最差情況比率。研究發現，Nash Learning from Human Feedback能達到極小化極大扭曲，而RLHF和DPO則會遭受較大的扭曲，甚至在完整設置中可能出現無界扭曲。", "applications": ["**個人化推薦系統：** 根據不同使用者的偏好，調整推薦內容，避免推薦引擎過度依賴單一偏好模型而產生偏差。", "**多方協作的AI輔助工具：** 在設計AI協作工具時，考慮不同利益相關者的偏好，避免AI偏向特定群體，導致不公平的結果。", "**法律和政策制定的AI輔助：** 在法律和政策制定過程中，使用AI輔助分析不同人群的偏好，確保政策制定能夠平衡各方利益，減少社會衝突。"], "pitch": "現今AI對齊技術，如RLHF和DPO，在面對使用者偏好多樣性的現實情況下，容易產生嚴重偏差，導致使用者體驗不佳甚至產生負面影響。我們的研究揭示了這種偏差的量化指標「扭曲」，並證明了Nash Learning from Human Feedback 在降低偏差方面的優勢。這代表著，我們可以打造更公平、更符合使用者需求的AI系統。透過投資採用Nash Learning的新一代AI對齊技術，我們可以大幅提升使用者滿意度，降低潛在的法律風險，並在快速發展的AI市場中建立競爭優勢。想像一下，一個能夠真正理解並滿足不同使用者需求的AI，它的商業價值是無限的！這不僅僅是一個技術投資，更是一項建立信任、提升品牌價值並引領行業變革的策略投資。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-06-01T23:12:44.619625"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升多模態大型語言模型在基於視覺的空間智能上的能力", "summary_zh": "Spatial-MLLM 是一個新型框架，旨在僅利用 2D 視覺資訊提升多模態大型語言模型（MLLM）的空間推理能力。它採用雙編碼器架構，一個負責提取圖像語義特徵，另一個（基於視覺幾何模型初始化）負責提取 3D 結構特徵。透過連接器將兩者整合，模型就能更好地理解空間關係。此外，Spatial-MLLM 還提出了一種空間感知幀採樣策略，能有效選擇影片中對空間推理至關重要的幀。實驗證明，Spatial-MLLM 在各種真實世界的數據集上，都展現了最先進的基於視覺的空間理解和推理性能。", "applications": ["**自動駕駛/機器人導航：** 利用道路影像和影片，讓自動駕駛汽車或機器人更精準地理解周圍環境，進行更安全的導航和路徑規劃，即使沒有額外的 3D 感測器。", "**室內設計/建築規劃：** 從室內照片或影片中提取空間結構資訊，輔助設計師快速生成3D模型或模擬不同設計方案的空間效果，提升設計效率和品質。", "**醫學影像分析：** 分析 2D 的醫學影像（如X光片、超聲波），協助醫生判讀病灶的空間位置和結構，提高診斷準確性，尤其是在需要判斷三維結構但只有二維影像的情況下。"], "pitch": "想像一下，你擁有一台機器學習模型，它能像人一樣，僅憑借 2D 的圖像或影片，就能理解複雜的空間關係。Spatial-MLLM 正是這樣一個突破性的技術，它將視覺幾何的強大能力與大型語言模型的推理能力相結合，開啟了全新的應用場景。在自動駕駛、智慧城市、建築設計、甚至醫療診斷等領域，Spatial-MLLM 都有巨大的潛力。我們可以將其授權給汽車製造商、建築公司、醫療機構，或開發基於 Spatial-MLLM 的 SaaS 平台，為各行各業提供更智能、更高效的空間理解解決方案。其核心優勢在於：降低對昂貴 3D 數據的依賴，節省成本；提升視覺理解的精度和廣度，提供更可靠的决策支持；加速模型部署和迭代，搶佔市場先機。我們相信，Spatial-MLLM 將成為未來空間智能領域的關鍵技術，為投資者帶來豐厚的回報。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-06-01T23:12:59.754646"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：利用修正流轉換器進行免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop是一個全新的框架，能用LoRA模型進行多概念圖像編輯，無需重新訓練。它基於一個重要發現：在Flux風格的擴散轉換器中，特定概念的特徵會在去噪過程的早期激活空間上連貫的區域。LoRAShop利用這個特性，在預先的正向傳遞中為每個概念導出一個解耦的潛在遮罩，然後僅在圍繞要個性化的概念的區域內混合相應的LoRA權重。如此產生的編輯能夠將多個主體或風格無縫整合到原始場景中，同時保留全局背景、光照和精細細節。實驗表明，LoRAShop在保持身份一致性方面優於其他方法。通過消除重新訓練和外部約束，LoRAShop將個性化的擴散模型轉變為一個實用的「LoRA版Photoshop」工具，並為組合視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**電商產品客製化：** 讓顧客上傳自家寵物的照片，快速生成穿戴不同服飾或置身於不同場景中的商品圖片，例如印有寵物穿著太空服的馬克杯或手機殼。", "**室內設計快速提案：** 設計師將客戶提供的房屋照片，快速加入不同的家具、牆面顏色或裝飾風格，生成多種設計方案供客戶選擇，省去繁瑣的建模和渲染過程。", "**藝術創作輔助：** 藝術家結合不同風格的藝術家作品（例如梵谷的星空和莫內的睡蓮），快速生成獨特的藝術作品，探索新的藝術表現形式，加速創作過程。"], "pitch": "LoRAShop解決了圖像編輯中多概念融合的痛點，無需耗時的重新訓練，大幅降低了使用門檻，使其成為一個真正實用的工具。其商業價值體現在以下幾個方面：\n\n*   **降低成本，提升效率：** 免訓練的特性可以顯著節省計算資源和時間，降低圖像生成和編輯的成本。\n*   **拓展應用場景：** 使得個性化內容創作更加容易，為電商、設計、藝術等多個行業帶來新的應用可能性。\n*   **創造新的商業模式：** 基於LoRAShop的服務可以為用戶提供個性化的圖像創作和編輯服務，例如客製化商品圖片、設計方案、藝術作品等，產生新的盈利模式。\n\n想像一下，未來每個人都可以輕鬆定制獨一無二的圖像，無論是生成夢想中的家園設計圖，還是創作個性化的紀念品，LoRAShop都有潛力顛覆現有的圖像編輯市場，成為下一代視覺創作的基石。我們相信，LoRAShop將會是一個極具潛力的投資項目。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-06-01T23:13:15.858142"}
{"query": "AI", "id": "2505.23749v1", "url": "http://arxiv.org/abs/2505.23749v1", "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?", "summary": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.", "authors": ["Paul Gölz", "Nika Haghtalab", "Kunhe Yang"], "published_date": "2025-05-29", "title_zh": "AI對齊的失真：偏好最佳化真的在最佳化偏好嗎？", "summary_zh": "現今大型語言模型透過人類偏好比較進行對齊。主流對齊方法，如基於PPO的RLHF和DPO，假設對齊單一偏好模型，但實際應用場景中，使用者偏好各異。這導致這些方法是否能滿足使用者的平均需求都成問題。本研究引入「失真」概念，衡量最佳可實現平均效用與學習策略平均效用之間的最差情況比率。研究表明，Nash Learning from Human Feedback達到最小最大最佳失真，而RLHF和DPO在高Beta值下會產生顯著的失真，甚至無限失真，具體取決於比較對的取樣方式。", "applications": ["**個性化推薦系統：** 理解使用者偏好差異，避免推薦系統只針對大眾口味，造成小眾愛好者體驗不佳。", "**自動駕駛決策：** 考慮不同駕駛風格和安全偏好，避免自動駕駛系統只學習到單一駕駛模式，忽略了舒適度或其他因素。", "**教育內容客製化：** 針對不同學習風格和知識水平的學生，提供客製化的學習材料和進度，避免一刀切的教學方式，提升學習效率和效果。"], "pitch": "想像一下，你正在建立一個基於AI的產品，但它只針對特定人群的偏好進行了優化，導致大部分使用者體驗不佳甚至完全無法使用。我們提出的研究揭示了現有AI對齊方法的潛在問題，並提出了更穩健的解決方案，如Nash Learning from Human Feedback。這意味著你的產品可以更好地滿足不同使用者的需求，擁有更廣泛的市場，降低因偏好差異導致的用戶流失風險。基於此研究，我們可以開發新一代的AI對齊技術，打造真正以人為本的AI應用，從而獲得巨大的商業價值。我們不僅僅是在對齊AI，更是在對齊你的產品和市場。", "audio": "audios/2505.23749v1.mp3", "timestamp": "2025-06-02T01:08:28.801938"}
{"query": "Foundation Model", "id": "2505.23747v1", "url": "http://arxiv.org/abs/2505.23747v1", "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.", "authors": ["Diankun Wu", "Fangfu Liu", "Yi-Hsin Hung", "Yueqi Duan"], "published_date": "2025-05-29", "title_zh": "Spatial-MLLM：提升MLLM在基於視覺空間智能方面的能力", "summary_zh": "現有的多模態大型語言模型（MLLM）在2D視覺任務表現優異，但在空間智能方面仍有挑戰。Spatial-MLLM是一種新的框架，僅基於2D視覺觀察進行空間推理。它利用雙編碼器架構，一個提取語義特徵，另一個從視覺幾何模型提取3D結構特徵。此外，提出了一種空間感知的幀採樣策略，在推理時選擇具有空間信息量的幀，以在有限的token長度下，讓模型專注於對空間推理至關重要的幀。通過Spatial-MLLM-120k數據集進行訓練，並在多個真實世界數據集上進行了廣泛的實驗，證明Spatial-MLLM在各種基於視覺的空間理解和推理任務中取得了最先進的性能。", "applications": ["**自動駕駛：** 透過車載攝影機影像，即時判斷路況、障礙物距離、行車路線等，提升駕駛安全性與自動駕駛能力。", "**建築設計與室內導航：** 根據平面圖或影片，快速理解建築物的空間結構，提供室內導航和物件定位服務。", "**醫療影像分析：** 從2D的醫療影像（如X光、MRI）中推斷出3D結構，幫助醫生更準確地診斷和定位病灶。"], "pitch": "Spatial-MLLM解決了多模態大型語言模型在空間智能方面的瓶頸，僅需2D視覺輸入即可進行精準的空間推理。這項技術擁有廣泛的應用前景，尤其是在自動駕駛、建築設計和醫療影像等領域。相較於需要額外3D數據的方案，Spatial-MLLM更具成本效益和易用性。我們認為Spatial-MLLM具備顛覆視覺智能產業的潛力，並將在未來數年內創造巨大的商業價值。我們尋求資金投入，以加速模型開發、擴大數據集規模，並將Spatial-MLLM應用於更廣泛的產業領域，打造一個領先的視覺智能平台。", "audio": "audios/2505.23747v1.mp3", "timestamp": "2025-06-02T01:08:45.968329"}
{"query": "Diffusion Model", "id": "2505.23758v1", "url": "http://arxiv.org/abs/2505.23758v1", "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers", "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.", "authors": ["Yusuf Dalva", "Hidir Yesiltepe", "Pinar Yanardag"], "published_date": "2025-05-29", "title_zh": "LoRAShop：使用修正流變換器實現免訓練的多概念圖像生成與編輯", "summary_zh": "LoRAShop是一個全新的框架，它使用LoRA模型進行多概念圖像編輯，無需重新訓練。它基於一個關鍵發現：Flux風格的擴散變換器中，概念特定的特徵會在去噪過程早期激活空間上連貫的區域。LoRAShop利用這個特性，在預先的向前傳遞中為每個概念導出一個解耦的潛在遮罩，並僅在包含要個性化概念的區域內混合相應的LoRA權重。這樣得到的編輯可以將多個主體或風格無縫地整合到原始場景中，同時保留全局上下文、光照和細節。實驗表明，LoRAShop在保持主體身份方面優於現有方法。通過消除重新訓練和外部約束，LoRAShop將個性化的擴散模型轉變為一個實用的“帶有LoRA的Photoshop”工具，並為組合視覺敘事和快速創意迭代開闢了新的途徑。", "applications": ["**虛擬試穿/裝飾：** 在照片中更換衣服的顏色、款式，或者在房間照片中擺放不同的傢俱，快速預覽效果。", "**產品客製化：** 允許客戶上傳商品圖片，並添加個性化的圖案、文字或設計元素，實時生成客製化預覽圖。", "**遊戲角色或場景設計：** 快速生成不同風格或配備的角色或場景，方便遊戲設計師進行創意探索和原型設計。"], "pitch": "LoRAShop解決了現有圖像編輯工具在處理多概念編輯時需要大量重新訓練或受到外部約束的問題。它免訓練的特性，使其更快速、更具成本效益。其商業價值體現在：首先，它能大幅降低設計師和藝術家的工作量，提高生產力。其次，它能為產品客製化、虛擬試穿等應用提供更便捷、更精確的解決方案，提升用戶體驗和銷售額。最後，LoRAShop的技術可以授權給其他圖像處理軟件公司或平台，擴大市場覆蓋範圍，成為圖像編輯領域的Game Changer。", "audio": "audios/2505.23758v1.mp3", "timestamp": "2025-06-02T01:09:16.299606"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "人類可以直觀地在3D空間中構圖和安排場景進行攝影。但當AI圖像生成器從文本或圖像提示創建圖像時，它們能否以類似的3D空間感知能力規劃場景呢？我們提出了GenSpace，一個新的基準測試和評估流程，以全面評估當前圖像生成模型的空間感知能力。此外，使用通用視覺語言模型（VLMs）的標準評估經常無法捕捉到詳細的空間錯誤。為了解決這個挑戰，我們提出了一個專門的評估流程和指標，它使用多個視覺基礎模型重建3D場景幾何結構，並提供更準確且更符合人類認知的空間保真度指標。我們的研究結果表明，雖然AI模型可以創建視覺上吸引人的圖像並遵循一般指令，但它們在物體放置、關係和測量等具體的3D細節方面表現不佳。我們總結了當前最先進的圖像生成模型在空間感知方面的三個核心限制：1）物體透視理解，2）自我中心-以環境為中心的轉換，以及3）度量測量遵從性，突顯了改進圖像生成中空間智能的可能方向。", "applications": ["**室內設計預覽：** 輸入房間大小和家具描述，AI生成逼真的房間設計圖，讓使用者在購買前就能看到擺設效果，避免空間規劃錯誤。", "**AR遊戲開發：** 根據遊戲腳本，AI自動生成符合物理規則的遊戲場景，例如正確放置障礙物和敵人，提高遊戲真實感和沉浸感。", "**建築設計輔助：** 建築師輸入建築物尺寸和風格要求，AI生成不同角度和光照條件下的建築效果圖，協助快速評估設計方案的可行性。"], "pitch": "GenSpace 正在揭示 AI 圖像生成領域的一大瓶頸：缺乏準確的空間感知能力。儘管現有模型能產生看似精美的圖像，但在處理複雜的 3D 空間關係時卻捉襟見肘。這意味著我們現在看到的AI設計圖往往是不切實際的，AR/VR體驗也因不真實的空間互動而大打折扣。GenSpace 的基準測試和評估體系，能夠精準量化 AI 模型的空間智能缺陷，並為未來的研究方向提供清晰的指引。投資 GenSpace 類似的技術，等於投資於下一代具備『空間感知』的AI模型。這將解鎖龐大的商業潛力，包括：更逼真、更可靠的虛擬內容創作（遊戲、元宇宙）、更高效的設計輔助工具（建築、產品設計）以及更智能的機器人應用（自動駕駛、物流）。想像一下，一個能夠真正理解並模擬物理世界的 AI，它將如何顛覆我們的生活和工作方式！", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-02T03:16:45.138684"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通往通用化神經符號學習之路應由基石模型鋪就", "summary_zh": "神經符號學習旨在解決訓練神經網路處理複雜推理任務時遇到的挑戰，並增加可解釋性、可靠性和效率等優點。傳統的神經符號學習方法會結合符號程式來訓練神經模型，但面臨許多限制，使其僅能處理簡單的問題。另一方面，純神經的基石模型現在透過提示而非訓練即可達到最先進的性能，但它們通常不可靠且缺乏可解釋性。我們提出一種稱為「神經符號提示」的方法，透過符號程式補充基石模型，為這些模型應用於複雜推理任務提供了一種途徑。這引發了一個問題：在基石模型的時代，作為神經符號學習一部分的專用模型訓練扮演什麼角色？為了探討這個問題，我們強調了傳統神經符號學習在計算、數據和程式方面導致泛化問題的三個陷阱。本立場文件認為，基石模型能夠實現通用化的神經符號解決方案，為實現神經符號學習的最初目標提供了一條途徑，而無需從頭開始訓練的缺點。", "applications": ["**智慧客服**：利用基石模型理解用戶提問，結合符號程式分析產品知識庫和FAQ，提供更精準、可解釋的答案，解決傳統客服只能提供預設回答或模糊答覆的問題。", "**醫療診斷輔助**：基石模型分析病患病歷和醫學文獻，符號程式則負責邏輯推理和規則驗證，輔助醫生進行診斷決策，降低誤診率，並提供診斷依據的可解釋性。", "**金融風險評估**：基石模型學習市場數據和新聞資訊，符號程式則應用預設的風險模型和監管規則，對投資組合進行風險評估，並提供風險分析報告，協助投資者做出更明智的決策。"], "pitch": "我們正在開發下一代AI引擎，結合基石模型和符號程式，打造更可靠、可解釋且通用的智能解決方案。傳統神經網路黑盒子難以應用於高風險領域，而我們的神經符號提示技術，能賦予AI系統更強大的推理能力和透明的決策過程。初期目標鎖定金融、醫療和法律等領域，這些行業對於AI的準確性和可解釋性要求極高，而我們的技術正好能滿足這些需求。我們預計透過SaaS訂閱模式獲利，並與各行業的領導企業建立戰略合作夥伴關係，共同推動AI技術的商業化應用，打造一個百億美元級別的獨角獸企業。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-02T03:17:03.683477"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：使用組合式多視角擴散生成可動畫的細緻3D人像", "summary_zh": "現有的圖片到3D人像生成方法難以產生適用於真實世界應用的高度細緻、可動畫的人像。我們提出AdaHuman，這是一個創新的框架，可以從單張自然圖片生成高保真、可動畫的3D人像。 AdaHuman 包含兩個關鍵創新：（1）一個姿態條件的3D聯合擴散模型，可在任意姿勢下合成一致的多視角圖像，並在每個擴散步驟中重建相應的3D高斯 Splats (3DGS)；（2）一個組合式3DGS精煉模塊，通過圖像到圖像的精煉來增強局部身體部位的細節，並使用一種新穎的裁剪感知相機光線圖將它們無縫整合，產生一個有凝聚力的細緻3D人像。 這些組件使 AdaHuman 能夠生成高度逼真的標準 A 姿勢人像，且自我遮擋最小，從而可以使用任何輸入動作進行綁定和動畫。 在公共基準測試和自然圖像上的廣泛評估表明，AdaHuman 在人像重建和重新擺姿勢方面顯著優於最先進的方法。 代碼和模型將公開發布以供研究使用。", "applications": ["**個人化虛擬化身/數位分身:** 使用者提供一張照片，即可生成高度逼真的3D虛擬化身，用於視訊會議、社交媒體、遊戲或元宇宙等應用，不再需要複雜的建模過程。", "**線上服裝試穿/時尚設計:** 讓消費者可以上傳自己的照片，在生成的3D人像上試穿各種服裝，查看穿著效果，減少退貨率，並讓設計師能更快速地在不同人像上測試設計。", "**客製化健身與健康應用:** 透過使用者上傳照片，生成3D人像，提供更精確的運動建議、體態分析，並客製化健身計畫，提升運動效果與動機。"], "pitch": "AdaHuman解決了目前3D人像生成技術在細節和動畫方面的瓶頸。其組合式多視角擴散技術帶來了前所未有的真實度和可動畫性，具有巨大的商業潛力。我們不僅能以更低的成本和更高的效率生產出高品質的3D人像，更能將其應用於快速增長的元宇宙、電商、健身健康等領域，創造多元商業模式。想像一下，每個人都能擁有一個逼真的數位分身，隨時隨地進行互動和體驗。AdaHuman將成為3D內容創作的未來，我們相信它能顛覆現有的產業生態，帶來巨大的投資回報。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-02T03:17:21.589676"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace: 空間感知圖像生成基準測試", "summary_zh": "人類在3D空間中構圖和安排場景的能力很強。這篇論文提出了GenSpace，一個新的基準測試和評估流程，旨在全面評估現有圖像生成模型在根據文字或圖像提示生成圖像時的空間感知能力。研究發現，雖然AI模型可以生成視覺上吸引人的圖像並遵循一般指令，但在物體放置、關係和測量等特定3D細節方面表現不佳。研究歸納了現有圖像生成模型在空間感知方面的三個核心限制：物體透視理解、以自我為中心的空間轉換，以及度量測量一致性，並強調了改進圖像生成空間智能的可能方向。", "applications": ["虛擬室內設計：使用者可以文字描述房間的佈局和家具，AI生成室內設計圖，並可調整家具位置、大小等，進行客製化設計預覽。", "遊戲場景設計：遊戲開發者可以透過文字或概念圖，讓AI生成遊戲場景，包含地形、建築、物體擺放等，加速遊戲開發流程。", "廣告素材製作：廣告商可以描述產品放置的場景和角度，讓AI生成符合需求的廣告圖像，提高廣告的吸引力。"], "pitch": "GenSpace揭示了當前AI圖像生成模型在空間感知上的不足，這同時也創造了巨大的商業機會。我們能利用GenSpace的評估體系，開發出更精確、更具空間智慧的圖像生成引擎，應用於虛擬實境、室內設計、遊戲開發、廣告行銷等領域。想像一下，一個能完美理解3D空間關係的AI，能夠根據客戶需求，精準生成逼真的場景和產品圖像，大幅降低設計成本，並提升效率和創造力。這不僅僅是一個圖像生成工具，更是一個連接虛擬與現實的橋樑，具有極高的市場潛力和投資價值。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-02T04:24:54.948283"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "邁向通用型神經符號學習之路，應由基石模型鋪就", "summary_zh": "神經符號學習旨在解決神經網路在複雜推理任務訓練上的挑戰，並提供可解釋性、可靠性和效率等優勢。傳統方法會將神經模型與符號程式聯合訓練，但面臨許多限制，使其只能處理簡單問題。另一方面，純神經的基石模型現在透過提示而非訓練，達到了最先進的性能，但往往不可靠且缺乏可解釋性。本文提出神經符號提示，即用符號程式來補充基石模型，為這些模型在複雜推理任務中的應用提供了一種途徑。這也引發了一個問題：在基石模型時代，神經符號學習中專門模型訓練的角色是什麼？我們強調傳統神經符號學習在計算、數據和程式方面導致泛化問題的三個陷阱。本文認為，基石模型能夠實現通用型神經符號解決方案，提供了一條通往實現神經符號學習最初目標的道路，而無需從頭開始訓練。", "applications": ["**智能客服:** 利用大型語言模型理解用戶意圖，並透過符號程式執行複雜的查詢和決策，例如退款、訂單修改等，提升客服效率和準確性。", "**金融風險評估:** 基於歷史數據，使用大型語言模型提取關鍵風險因素，並透過符號程式建立風險模型，提升風險評估的精準度和可解釋性，符合監管要求。", "**自動化程式碼生成:** 根據自然語言描述，利用大型語言模型生成程式碼骨架，並透過符號程式驗證和優化程式碼邏輯，降低程式開發的門檻和成本。"], "pitch": "傳統神經符號學習在通用性上遇到了瓶頸，我們正在透過結合基石模型與符號程式，打造更強大的推理能力。想像一下，一個能夠像人類專家一樣思考的AI，它不僅能理解複雜的任務，還能提供清晰的解決方案和解釋。我們的神經符號提示技術，能有效解決基石模型不可靠和缺乏解釋性的問題，為企業級應用打開了新的大門。這項技術的商業潛力巨大，可以應用於智能客服、金融風險評估、自動化程式碼生成等各個領域，大幅提升效率、降低成本。我們尋求投資，共同將這項突破性技術推向市場，引領下一代AI的發展。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-02T04:25:11.061905"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：基於組合式多視角擴散的可動畫細緻3D人體生成", "summary_zh": "現有的圖像到3D頭像生成方法難以產生高細節、可動畫且適用於現實應用的頭像。我們提出 AdaHuman，一個從單張真實場景圖像生成高保真、可動畫3D頭像的新框架。AdaHuman 包含兩項關鍵創新：(1)一個姿態條件3D聯合擴散模型，在每個擴散步驟中合成任意姿態下的一致多視角圖像，並同時進行相應的3D高斯散布（3DGS）重建；(2)一個組合式3DGS細化模塊，通過圖像到圖像的細化來增強局部身體部位的細節，並使用一種新穎的裁剪感知相機光線圖無縫地整合它們，產生一個有凝聚力且細緻的3D頭像。這些組件使 AdaHuman 能夠生成高度逼真的標準 A 字型頭像，且自我遮擋最少，從而可以使用任何輸入動作進行綁定和動畫處理。在公共基準測試和真實場景圖像上的大量評估表明，AdaHuman 在頭像重建和重新定位方面顯著優於最先進的方法。代碼和模型將公開提供用於研究目的。", "applications": ["**虛擬試穿：** 消費者可以上傳一張照片，生成自己的3D頭像，並在線上虛擬試穿衣服，查看不同服裝的搭配效果，減少退貨率並提升購物體驗。", "**客製化遊戲角色：** 玩家可以上傳一張自拍照，立即創建一個高度個性化的遊戲角色，無需複雜的建模過程，提升遊戲的沉浸感和趣味性。", "**遠程醫療諮詢：** 病患可以上傳照片，創建3D頭像，醫生可以透過遠端操控頭像，進行初步的身體評估，提升遠程醫療的效率和精準度。"], "pitch": "AdaHuman解決了現有3D頭像生成技術的痛點，能夠從單張圖像快速生成高細節、可動畫的3D頭像。這項技術具有巨大的商業潛力，能夠顛覆服裝電商、遊戲娛樂、遠程醫療等行業。想像一下，消費者可以輕鬆創建自己的數位分身，體驗個性化的購物、娛樂和醫療服務。AdaHuman的技術優勢在於其創新的擴散模型和細化模塊，能夠生成更逼真、更易於操作的3D頭像。我們相信，AdaHuman將引領下一代虛擬形象技術的發展，並為投資者帶來豐厚的回報。市場規模龐大，應用場景廣闊，AdaHuman的商業價值不容小覷。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-02T04:25:27.635479"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "本研究提出GenSpace，一個評估圖像生成模型空間感知能力的基準測試。現有AI模型在生成圖像時，雖然視覺效果不錯，也能遵循一般指令，但在物體擺放、關係和尺寸等3D細節上表現不佳。GenSpace透過重建3D場景幾何結構，提供更準確且符合人類認知的空間真實性指標。研究發現現有模型在物體透視理解、自我中心與外中心轉換，以及尺寸測量準確性方面存在三大核心限制。", "applications": ["**室內設計預覽：** 用戶描述房間配置，AI生成不同風格的擺設方案，並能精準呈現物件的空間關係與尺寸，協助用戶視覺化裝修後的樣貌。", "**虛擬試穿：** 用戶上傳個人照片，AI將衣服精準地疊加在照片上，並根據用戶體型和空間位置調整透視，提供更真實的穿搭效果。", "**遊戲場景生成：** 開發者描述遊戲場景，AI自動生成符合描述且物件空間關係合理的3D場景，加速遊戲開發流程。"], "pitch": "GenSpace的研究點出了AI圖像生成模型在空間感知上的短板，這正是我們創業的機會。我們將開發更精準的空間感知圖像生成引擎，聚焦解決現有模型在3D細節上的缺陷。首先，我們鎖定室內設計與虛擬試穿市場，提供更真實、更符合用戶需求的視覺化體驗。透過與現有設計軟體或電商平台整合，快速搶佔市場。未來，我們可將技術應用於遊戲開發、建築設計等更廣泛的領域，打造一個具備高度空間智能的圖像生成平台，具備巨大的商業潛力。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-02T05:15:24.046515"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通用神經符號學習之路應以基礎模型鋪就", "summary_zh": "神經符號學習旨在解決訓練神經網絡進行複雜推理任務時遇到的挑戰，並額外提供可解釋性、可靠性和效率。傳統的神經符號學習方法結合神經模型和符號程式進行訓練，但它們面臨重大挑戰，使其僅限於簡單問題。另一方面，純神經基礎模型現在通過提示而非訓練就能達到最先進的性能，但它們通常不可靠且缺乏可解釋性。用符號程式補充基礎模型，我們稱之為神經符號提示，提供了一種將這些模型用於複雜推理任務的方法。這樣做引出了一個問題：在基礎模型的時代，作為神經符號學習一部分的專用模型訓練扮演什麼角色？為了探索這個問題，我們強調了傳統神經符號學習在計算、數據和程式方面導致泛化問題的三個陷阱。這篇立場論文認為，基礎模型可以實現通用的神經符號解決方案，提供了一條實現神經符號學習最初目標的途徑，而無需從頭開始訓練的缺點。", "applications": ["**智能客服系統：** 使用神經符號提示可以讓客服系統更準確地理解客戶的複雜問題，並基於明確的知識庫（符號程式）提供更可靠和可解釋的解決方案，例如退貨政策、常見問題解答等。", "**醫療診斷輔助：** 基礎模型可以根據症狀描述生成可能的診斷，而神經符號提示可以利用醫學知識庫（符號程式）驗證和完善診斷結果，提高診斷的準確性和可信度。", "**程式碼自動生成：** 基礎模型可以根據需求描述生成程式碼片段，而神經符號提示可以利用程式語言的語法規則和API文檔（符號程式）驗證和優化生成的程式碼，減少bug並提高程式碼品質。"], "pitch": "想像一下，你可以結合大型語言模型的強大能力和符號推理的精確性，打造一個既能理解複雜問題，又能提供可靠且可解釋答案的AI系統。我們的方法，神經符號提示，正是要解決這個問題。傳統的神經符號學習過於依賴從頭開始的訓練，成本高且效果差。現在，利用現成的基礎模型，我們可以大幅降低開發成本，加速產品上市。我們的技術具有廣泛的應用前景，從智能客服到醫療診斷，再到自動化程式碼生成，都能大幅提高效率和準確性。我們正在構建的是下一代AI引擎，它不僅更智能，而且更值得信賴。這是一個巨大的市場機會，我們正在尋找有遠見的投資者，與我們一起開創AI的未來。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-02T05:15:41.234376"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：基於可組合多視角擴散模型的可動畫化細緻3D人體生成", "summary_zh": "AdaHuman是一個能從單張照片生成高擬真、可動畫3D人體模型的全新框架。它利用姿態條件3D聯合擴散模型，在任意姿態下合成一致的多視圖圖像，並同步進行3D高斯潑濺重建。此外，AdaHuman還包含一個可組合的3D高斯潑濺細化模塊，通過圖像到圖像的細化增強局部身體部位的細節，並利用一種新型的裁剪感知相機光線圖無縫整合它們，產生一個連貫且細緻的3D人體模型。這使AdaHuman能夠生成高度逼真的標準A字形人體模型，且自我遮擋最小化，從而可以使用任何輸入動作進行綁定和動畫製作。實驗證明，AdaHuman在人體模型重建和姿勢重構方面顯著優於現有技術。", "applications": ["**虛擬試衣間：** 用戶上傳一張照片，即可看到自己在不同服裝下的3D虛擬形象，模擬真實試穿效果，提升線上購物體驗。", "**遊戲角色客製化：** 玩家只需提供一張照片，就能快速創建一個高度相似的遊戲角色，避免冗長的捏臉過程，增加遊戲代入感。", "**遠程醫療/健身指導：** 醫生或教練可以通過3D模型分析患者/學員的姿態，提供更精準的遠程診斷和運動指導，不受地理位置限制。"], "pitch": "AdaHuman解決了目前3D人體建模領域精度和易用性的痛點。僅需一張照片，即可快速生成高精度、可動畫的3D人體模型，大幅降低了建模成本和時間。其潛在商業價值巨大，涵蓋電商、遊戲、娛樂、醫療等多個領域。試想一下，如果電商平台能提供逼真的虛擬試穿體驗，或遊戲公司能讓玩家輕鬆創建自己的遊戲角色，市場前景將無可限量。我們相信，AdaHuman將引領3D人體建模技術的革命，並為相關產業帶來顛覆性的變革。我們正在尋求資金，以加速產品開發、拓展應用場景，並建立行業領導地位。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-02T05:15:58.220271"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "人類能在3D空間中直覺地構圖和安排場景，以進行攝影。那麼，先進的AI圖像生成器在根據文本或圖像提示創建圖像時，是否也能以類似的3D空間感知來規劃場景？ 我們提出了GenSpace，這是一個新的基準測試和評估流程，旨在全面評估當前圖像生成模型的空間感知能力。此外，使用通用視覺語言模型（VLM）的標準評估經常無法捕捉到詳細的空間錯誤。為了解決這個挑戰，我們提出了一個專門的評估流程和指標，它使用多個視覺基礎模型重建3D場景幾何，並提供更準確且與人類對齊的空間保真度指標。我們的研究結果表明，雖然AI模型可以創建視覺上吸引人的圖像，並且可以遵循一般指示，但它們在物體放置、關係和測量等具體的3D細節方面存在困難。我們總結了當前最先進的圖像生成模型在空間感知方面的三個核心限制：1）物體透視理解，2）自我中心-他者中心轉換，3）度量測量一致性，突出了提高圖像生成空間智能的可能方向。", "applications": ["**室內設計虛擬擺設：** 用戶輸入房間描述（例如：『簡約風格客廳，需要一個L型沙發，一個落地燈，和一張圓形茶几』），AI生成不同擺設方案的3D空間渲染圖，讓用戶快速預覽效果。", "**遊戲場景快速建模：** 遊戲開發者輸入場景描述（例如：『中古世紀風格的城堡，周圍環繞護城河，遠處是連綿山脈』），AI生成初步的場景模型，開發者再進行細節調整，大幅縮短開發時間。", "**教育訓練的虛擬環境創建：** 醫學系學生練習手術時，透過文字描述（例如：『模擬手術室，病人躺在手術台上，需要無影燈和手術器械台』），AI生成逼真的手術室環境，提供更真實的訓練體驗。"], "pitch": "GenSpace 開發了一個突破性的基準測試，揭示了現有圖像生成AI在3D空間感知方面的不足。 這不僅是學術上的進展，更是商業上巨大的機會。試想一下：我們正在打造一個可以理解空間關係的AI引擎，它可以根據用戶的指令，精準地生成符合要求的3D場景。 這項技術的應用範圍極其廣泛，從室內設計、遊戲開發到教育訓練，都能大幅提升效率，降低成本，並帶來更優質的用戶體驗。 我們的專利技術和評估方法將幫助我們在空間感知AI領域建立領先地位，吸引各產業的合作夥伴，並最終塑造下一代的3D內容創作方式。 我們正在尋找投資者，一起抓住這個千載難逢的機會，共同打造一個空間感知的AI未來。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-02T06:21:03.621924"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通用化神經符號學習之路：應以基礎模型鋪路", "summary_zh": "神經符號學習旨在解決訓練神經網路處理複雜推理任務時所面臨的挑戰，並具備可解釋性、可靠性和效率等優點。傳統神經符號學習方法將神經模型與符號程式結合訓練，但面臨著將其限制在簡化問題上的重大挑戰。另一方面，純神經基礎模型現在通過提示而非訓練來達到最先進的性能，但它們通常不可靠且缺乏可解釋性。用符號程式補充基礎模型，我們稱之為神經符號提示，提供了一種將這些模型用於複雜推理任務的方法。這引發了一個問題：在基礎模型的時代，作為神經符號學習一部分的專門模型訓練有什麼作用？為了探討這個問題，我們重點介紹了傳統神經符號學習在計算、數據和程式方面導致泛化問題的三個陷阱。這篇立場論文認為，基礎模型能夠實現可泛化的神經符號解決方案，提供了一條實現神經符號學習的原始目標的途徑，而無需從頭開始訓練的缺點。", "applications": ["**智慧客服：** 使用者提出複雜問題（例如：退貨政策+特定商品），系統結合大型語言模型理解語意，並透過符號邏輯處理退貨條件，提供精準的答案，避免模型隨機回答錯誤資訊。", "**醫療診斷輔助：** 醫師輸入症狀描述，系統結合大型語言模型理解病徵，再透過符號規則庫（例如：疾病診斷手冊）進行推理，提供可能的診斷建議，並說明推理過程，增加醫師的信任度。", "**自動化合約審閱：** 程式分析合約條款，結合大型語言模型理解文字，並透過預定義的符號規則判斷條款是否符合公司政策或法律規範，自動標記風險點並提出修改建議，減少法律顧問的人工審查時間。"], "pitch": "我們正在開發下一代AI推理引擎，結合大型語言模型的理解能力和符號邏輯的精確性。 傳統的神經網路難以處理複雜的規則和約束，而我們的技術利用基礎模型作為知識庫，並通過符號規則進行推理，實現可解釋、可靠且高效的決策。 商業價值在於：1) 解決現有AI在複雜場景下的不可靠性問題；2) 賦能各行業（如金融、法律、醫療）進行更精準、更透明的決策；3) 降低AI開發和維護成本，加速AI在各領域的落地。我們相信這項技術將成為企業級AI的核心基石，帶來巨大的商業潛力。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-02T06:21:19.016356"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：基於組合式多視角擴散的可動畫化精細3D人體生成", "summary_zh": "AdaHuman 是一個新的框架，可以從單張照片生成高保真、可動畫化的3D頭像。它利用姿態條件的3D聯合擴散模型來合成在任意姿勢下的連貫多視角圖像，並同時重建3D高斯濺射 (3DGS)。此外，它還使用組合式3DGS細化模組，通過圖像到圖像的細化來增強局部身體部位的細節，並利用新型的裁剪感知相機射線圖將它們無縫集成，從而生成連貫、精細的3D頭像。AdaHuman 在頭像重建和重新定位方面顯著優於現有技術。", "applications": ["**虛擬試衣間：** 消費者可以上傳自己的照片，生成3D頭像，然後在線上試穿衣服，查看穿著效果，避免退換貨的麻煩。", "**遊戲角色客製化：** 玩家可以上傳自己的照片，快速生成高度相似的遊戲角色，增加遊戲的沉浸感和個性化體驗。", "**遠程醫療：** 醫生可以利用患者的照片生成3D頭像，進行更精準的遠程診斷和康復指導，尤其適用於皮膚科和整形外科。"], "pitch": "AdaHuman 解決了目前3D頭像生成技術在細節和動畫方面的瓶頸，提供了一個從單張照片生成高保真、可動畫化3D頭像的突破性方案。其市場潛力巨大，涵蓋電商、遊戲、醫療、娛樂等多個領域。想像一下，消費者可以利用手機鏡頭創建自己的3D頭像，在虛擬世界中試穿衣服、參加會議、甚至是與已故親人重聚。AdaHuman的技術不僅僅是視覺效果的提升，更代表著人機互動和虛擬體驗的革新。我們相信，AdaHuman有潛力成為下一代虛擬化身技術的領導者，並為投資者帶來可觀的回報。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-02T06:21:30.197528"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "人類能夠直覺地在3D空間中構圖和安排場景以進行攝影。然而，當先進的AI圖像生成器從文字或圖像提示中創建圖像時，它們能否以類似的3D空間感知能力來規劃場景？ 我們提出了 GenSpace，這是一個新的基準測試和評估流程，旨在全面評估當前圖像生成模型的空間感知能力。 此外，使用通用視覺語言模型 (VLM) 的標準評估通常無法捕捉到詳細的空間錯誤。 為了應對這一挑戰，我們提出了一種專門的評估流程和指標，該流程利用多個視覺基礎模型重建3D場景幾何，並提供更準確、更符合人類認知的空間保真度指標。 我們的研究結果表明，雖然 AI 模型可以創建具有視覺吸引力的圖像並可以遵循一般指令，但它們在物體放置、關係和測量等具體的 3D 細節方面存在困難。 我們總結了當前最先進的圖像生成模型在空間感知方面的三個核心局限性：1）物體透視理解，2）自我中心-異體中心變換，以及 3）度量測量遵循，突出了改進圖像生成空間智能的可能方向。", "applications": ["**室內設計模擬：** 用戶可以使用文字描述房間配置（例如：在沙發旁邊放一張圓桌），AI生成圖像後，可以檢查AI是否正確理解物體之間的空間關係和尺寸比例，協助用戶設計更符合實際需求的房間。", "**虛擬試穿/試戴：** 用戶上傳個人照片，並描述想要試穿的衣服/飾品，AI生成試穿/試戴效果圖。可以評估AI是否能正確處理透視關係，例如：項鍊是否正確地懸掛在脖子上，或帽子是否符合頭型。", "**遊戲場景設計：** 遊戲開發者可以使用文字描述遊戲場景（例如：在一座山谷的中央，有一條小河流過），利用GenSpace評估AI生成的地形、物件佈局是否合理，例如：河流的流向、山的高度等是否符合物理規則和美學考量。"], "pitch": "GenSpace解決了目前圖像生成AI在空間感知能力上的不足，這個問題限制了AI在許多高價值領域的應用。我們的基準測試和評估流程，能夠幫助開發者更精確地診斷和改進AI模型，提升其空間推理能力。這將直接促成更逼真、更可控的圖像生成，在室內設計、AR/VR、遊戲開發、電商等領域帶來巨大的商業潛力。透過提供更可靠的空間感知能力評估，GenSpace有機會成為圖像生成領域的黃金標準，吸引大量AI開發者使用，並產生持續的數據和洞見，建立長期競爭優勢。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-02T07:14:46.173330"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "邁向通用神經符號學習之路應以基礎模型鋪就", "summary_zh": "神經符號學習旨在解決訓練神經網路處理複雜推理任務時遇到的挑戰，並增強可解釋性、可靠性和效率。傳統的神經符號學習方法會結合符號程式訓練神經模型，但它們面臨重大挑戰，限制了它們只能處理簡單問題。另一方面，純神經基礎模型現在透過 Prompting 而非訓練達到最先進的效能，但它們通常不可靠且缺乏可解釋性。透過符號程式補充基礎模型，我們稱之為神經符號 Prompting，提供了一種使用這些模型來完成複雜推理任務的方法。這樣做引發了一個問題：在基礎模型的時代，作為神經符號學習一部分的專門模型訓練扮演什麼角色？為了探討這個問題，我們重點介紹了傳統神經符號學習在計算、數據和程式方面導致泛化問題的三個陷阱。這篇立場文件認為，基礎模型能夠實現通用的神經符號解決方案，提供了一條在沒有從頭開始訓練缺點的情況下，實現神經符號學習原始目標的途徑。", "applications": ["**醫療診斷輔助系統：** 利用基礎模型理解病歷和醫學知識，並結合符號推理進行診斷，例如判斷疾病種類、評估風險因子等。相比單純的深度學習模型，此方法能提供更清晰的診斷依據，方便醫生理解和驗證。", "**法律文件分析與合規性檢查：** 基礎模型理解法律條文，符號推理引擎則根據特定案例進行邏輯推理，判斷文件是否符合相關法律規定，提高法律工作者的效率，降低錯誤風險。", "**智能程式碼偵錯與優化：** 利用基礎模型理解程式碼意圖，符號推理引擎分析程式碼邏輯結構，找出潛在的錯誤和性能瓶頸，並提供修改建議，輔助開發者更快地完成程式碼開發。"], "pitch": "我們正在開發一種基於基礎模型的神經符號學習平台，旨在賦能各行各業的AI應用。傳統的神經符號學習方案需要大量的訓練數據和專業知識，難以推廣。我們的平台利用現成的基礎模型，通過神經符號Prompting的方式，大幅降低了開發成本和技術門檻，能夠快速構建可解釋、可靠且高效的AI系統。我們已驗證了在醫療、法律和程式碼分析等領域的應用潛力，市場規模巨大。我們的團隊擁有深厚的AI背景和豐富的產品開發經驗，尋求Pre-Seed輪投資，加速產品迭代，搶占市場先機，成為下一代通用AI平台的領導者。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-02T07:15:02.131202"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：基於組合式多視角擴散的可動畫化細緻3D人體生成", "summary_zh": "現有的圖像到3D頭像生成方法難以生成高度細緻、可動畫且適用於真實世界應用的頭像。AdaHuman 提出了一個新的框架，可以從單張真實圖像生成高保真的可動畫3D頭像。 AdaHuman 包含兩項關鍵創新：(1) 一個姿態條件的3D聯合擴散模型，可以合成任意姿態下的一致多視角圖像，並在每個擴散步驟中進行相應的3D高斯散佈（3DGS）重建；(2) 一個組合式3DGS細化模塊，通過圖像到圖像的細化來增強局部身體部位的細節，並使用一種新的裁剪感知相機光線圖將它們無縫集成，從而生成一個有凝聚力的細緻3D頭像。這些組件使 AdaHuman 能夠生成高度逼真的標準 A 姿態頭像，且自我遮擋最少，從而能夠使用任何輸入運動進行綁定和動畫處理。在公共基準和真實圖像上的大量評估表明，AdaHuman 在頭像重建和重新定位方面顯著優於最先進的方法。程式碼和模型將公開發布以供研究。", "applications": ["**個性化虛擬化身創建:** 使用者上傳一張照片，即可快速生成高度逼真的個人虛擬化身，用於線上會議、遊戲、社交媒體等，免去繁瑣的建模過程。", "**虛擬試衣間:** 服裝品牌可以使用 AdaHuman 技術讓顧客上傳照片，生成可活動的3D模型，進行線上試穿，更真實地模擬穿著效果，提高線上購物體驗。", "**動畫角色生成:** 動畫製作人員可以利用 AdaHuman 從單張角色設計圖快速生成可動畫的3D模型，簡化建模流程，加速動畫製作週期。"], "pitch": "AdaHuman 解決了 3D 頭像生成領域的痛點，即難以從單張圖像生成高質量、可動畫的頭像。我們的技術突破在於結合了姿態條件的3D擴散模型和組合式細化模塊，能夠顯著提升頭像的真實感和可操作性。市場潛力巨大，涵蓋娛樂、電商、教育等多個領域。例如，在遊戲和社交領域，用戶可以輕鬆創建個性化的虛擬化身，提升沉浸式體驗；在電商領域，虛擬試衣間可以有效提升購買轉化率。我們的競爭優勢在於技術領先，能夠生成比現有方案更精細、更逼真的模型，並且擁有更強的動畫適應性。團隊具備深厚的 3D視覺和機器學習背景。我們正在尋求種子輪融資，以擴大研發團隊，加速產品商業化落地，搶佔市場先機。預期透過B2B授權API或者提供 SaaS服務的方式實現盈利，並在未來成為元宇宙時代不可或缺的基礎設施。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-02T07:15:22.347224"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成能力評估基準", "summary_zh": "人類能直觀地在3D空間中構圖和安排場景以進行攝影。但當前的人工智慧圖像生成模型，在根據文本或圖像提示生成圖像時，是否也能像人類一樣具備3D空間感知能力，並進行場景規劃呢？我們提出 GenSpace，一個新的基準測試和評估流程，旨在全面評估當前圖像生成模型的空間感知能力。此外，使用通用視覺語言模型（VLM）的標準評估經常無法捕捉到細節的空間錯誤。為了應對這一挑戰，我們提出了一種專門的評估流程和指標，它使用多個視覺基礎模型重建3D場景幾何，並提供更準確、更符合人類認知的空間保真度指標。我們的研究結果表明，雖然人工智慧模型可以生成視覺上吸引人的圖像並遵循一般指令，但它們在物體放置、關係和測量等具體的3D細節方面存在困難。我們總結了當前最先進的圖像生成模型在空間感知方面的三個核心限制：1) 物體透視理解，2) 以自我為中心/以對象為中心的轉換，3) 度量測量一致性，並強調了提高圖像生成中空間智能的可能方向。", "applications": ["**虛擬試衣間/家具擺設預覽:** 顧客可以輸入尺寸和風格描述，AI生成包含顧客照片或房間照片的虛擬預覽，讓他們在購買前看到實際效果，避免尺寸不合或風格不搭的問題。", "**建築設計輔助:** 建築師可以快速生成不同設計方案的3D視覺效果圖，並且可以根據實地拍攝的照片，將建築模型無縫整合到真實環境中，用於演示和客戶溝通。", "**遊戲地圖生成/內容創作:** 遊戲開發者可以輸入場景描述，AI自動生成符合要求的遊戲地圖，甚至可以根據玩家的遊玩行為和偏好，動態調整地圖佈局和內容，提升遊戲體驗。"], "pitch": "GenSpace 專注於解決圖像生成領域在3D空間理解方面的核心痛點，其商業價值巨大。目前的AI圖像生成模型雖然視覺效果出色，但在空間關係的理解上存在明顯缺陷，這限制了其在許多行業的應用。GenSpace 提出的評估基準和改進方向，將加速開發出真正理解空間的AI圖像生成模型。基於此技術，我們可以打造出更逼真的虛擬實境體驗、更精準的建築設計輔助工具、以及更智能化的遊戲內容創作平台。這些應用不僅能降低成本、提高效率，還能創造全新的商業模式。例如，在電商領域，可以實現高精度的虛擬試穿，大幅降低退貨率；在房地產領域，可以打造身臨其境的線上看房體驗，提升銷售轉化率。因此，GenSpace 的相關技術具有極高的投資價值，有望在未來幾年內帶來指數級的市場增長。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-02T08:20:43.371561"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通往可泛化神經符號學習之路，應以基石模型鋪就", "summary_zh": "神經符號學習旨在解決神經網路在複雜推理任務上的訓練難題，並增加可解釋性、可靠性和效率。傳統方法會結合神經模型和符號程序進行訓練，但存在限制，使其僅能處理簡單問題。另一方面，純神經基石模型現在透過Prompting而非訓練達到最先進的性能，但往往不可靠且缺乏可解釋性。本文提出「神經符號提示 (Neuro-Symbolic Prompting)」的概念，透過符號程序來補充基石模型，使其能夠處理複雜推理任務。在基石模型的時代，專門的模型訓練在神經符號學習中扮演什麼角色？本文探討了傳統神經符號學習在計算、數據和程序方面的三個陷阱，導致泛化問題。本文認為，基石模型能實現可泛化的神經符號解決方案，為實現神經符號學習的最初目標提供了一條途徑，避免了從頭開始訓練的缺點。", "applications": ["**醫療診斷輔助：** 利用基石模型理解病歷文本，結合符號程序進行疾病推理和診斷，提供更可靠和可解釋的診斷建議，降低誤診率。", "**智能客服與法律諮詢：** 將基石模型用於理解用戶問題，再用符號程序分析法律條文和相關案例，提供精準且可解釋的法律建議，節省人力成本。", "**金融風險評估：** 使用基石模型處理新聞和市場數據，結合符號程序建立風險模型，進行更精確的風險預測和評估，降低投資風險。"], "pitch": "我們正在開創神經符號學習的新時代，透過結合基石模型和符號程序的「神經符號提示」技術，解決傳統神經符號學習的泛化難題。想像一下，能夠創造出既擁有基石模型強大理解能力，又具備符號程序可解釋性的 AI 系統。這將帶來巨大的商業價值：從醫療診斷、法律諮詢到金融風險評估，我們的技術都能提供更可靠、更精準的決策支持，大幅降低錯誤率和運營成本。我們正在尋找合作夥伴，共同將這項革命性技術商業化，重塑AI的未來，實現更值得信賴和可解釋的人工智慧。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-02T08:20:59.185704"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：基於組合式多視角擴散的可動畫化細緻3D人體生成", "summary_zh": "AdaHuman是一個創新的框架，能從單張真實照片生成高保真、可動畫化的3D人體模型。它採用了姿態條件化的3D關節擴散模型，在每個擴散步驟合成一致的多視角圖像，並同步進行3D高斯潑濺(3DGS)重建。此外，它還包含一個組合式3DGS細化模組，通過圖像到圖像的細化來增強局部身體部位的細節，並使用一種新的感知裁剪的相機射線圖無縫地整合它們，產生一個連貫且細緻的3D人體模型。這使得AdaHuman能夠生成高度逼真的標準A字形姿勢模型，最大程度地減少自我遮擋，從而可以對其進行綁定並使用任何輸入動作進行動畫處理。在公共基準測試和真實照片上的廣泛評估表明，AdaHuman在人體模型重建和重新擺姿勢方面顯著優於最先進的方法。", "applications": ["**虛擬試衣間：** 用戶上傳一張照片，就能立即生成一個精確的3D人體模型，用於在線上試穿各種服裝，查看不同款式和尺寸的上身效果，避免退換貨問題。", "**遊戲角色客製化：** 玩家可以上傳自己的照片，快速生成一個高度相似的3D遊戲角色，增加遊戲的沉浸感和個性化體驗。", "**AI健身教練：** 將用戶的照片轉換為3D模型，AI教練可以更精確地評估用戶的姿勢，提供個性化的健身指導，避免運動損傷。"], "pitch": "AdaHuman解決了從單張圖像生成高質量、可動畫化3D人體模型的痛點，這在元宇宙、遊戲、電商等領域具有廣闊的應用前景。目前市場上缺乏能在細節和動畫能力上達到AdaHuman水平的解決方案。我們的核心競爭力在於創新的多視角擴散模型和組合式細化模組，能夠生成逼真且易於操作的3D模型。商業模式可以包括：(1) SaaS服務，向企業提供3D人體模型生成API，按使用量或訂閱收費；(2) 與遊戲公司合作，提供角色客製化解決方案；(3) 與電商平台合作，提供虛擬試衣間服務。初期重點拓展電商和遊戲市場，並逐步向醫療、教育等領域延伸。預計在三年內實現盈利，並具有指數級增長的潛力。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-02T08:21:22.816726"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "人類擅長在3D空間中構圖和安排場景進行攝影。本研究提出GenSpace，一個新的基準測試和評估流程，旨在全面評估當前圖像生成模型在從文本或圖像提示生成圖像時的空間感知能力。研究發現，雖然AI模型能產生視覺上吸引人的圖像並遵循總體指示，但在物體放置、關係和測量等特定3D細節方面表現不佳。研究總結了當前最先進圖像生成模型在空間感知方面的三個核心限制：1) 物體透視理解，2) 自我中心-客體中心轉換，以及3) 度量測量遵循。這些發現指出了提升圖像生成中空間智能的可能方向。", "applications": ["**虛擬室內設計:** 用戶可以通過文字描述房間佈局和家具擺放，AI根據空間感知生成真實感圖片，協助設計師或用戶進行室內裝潢方案的預覽和修改。", "**遊戲場景設計:** 遊戲開發者可以利用AI快速生成符合特定空間規則和物理特性的遊戲場景，例如，根據“佈滿碎石的陡峭山坡”生成可攀爬的地形，節省美術製作時間。", "**機器人導航訓練:** 利用AI生成各種具有複雜空間關係的虛擬環境，用於訓練機器人的導航和避障能力，提升機器人在真實世界中的適應性。"], "pitch": "GenSpace 解決了目前AI圖像生成缺乏精準空間感知的問題，這限制了其在諸如室內設計、遊戲開發、機器人導航等高潛力市場的應用。我們提供的基準測試和評估流程能夠量化並改善AI模型的空間智能。通過授權許可或提供API服務，GenSpace能夠幫助企業開發更真實、更實用的AI圖像生成應用，搶佔市場先機。我們的護城河在於：1）我們擁有獨特的空間感知評估方法；2）我們正在建立不斷更新的基準數據集；3）我們能提供優化空間智能的技術諮詢服務。短期內，我們可以針對特定行業提供定制化的空間感知圖像生成解決方案。長期來看，隨著AI能力的提升，GenSpace 將成為空間智能領域的領導者，為元宇宙、自動駕駛等下一代技術的發展提供關鍵支持。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-02T09:16:16.460464"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "邁向通用化神經符號學習之路：應由基石模型鋪就", "summary_zh": "神經符號學習旨在解決訓練神經網路處理複雜推理任務時遇到的挑戰，並帶來可解釋性、可靠性和效率等額外優勢。傳統上，神經符號學習方法會結合符號程式來訓練神經模型，但它們面臨重大挑戰，使其只能解決簡單問題。另一方面，純神經的基石模型現在透過提示而非訓練來達到最先進的性能，但它們通常不可靠且缺乏可解釋性。利用符號程式補充基石模型，我們稱之為神經符號提示，提供了一種使用這些模型進行複雜推理任務的方法。這引發了一個問題：在基石模型時代，作為神經符號學習一部分的專門模型訓練扮演什麼角色？為了探討這個問題，我們強調了傳統神經符號學習在計算、資料和程式方面導致泛化問題的三個陷阱。這篇立場文件認為，基石模型能夠實現通用化的神經符號解決方案，從而提供了一條實現神經符號學習最初目標的途徑，而無需從頭開始訓練的缺點。", "applications": ["**自動生成法規合規報告：** 律師或法規顧問可以使用該系統，根據輸入的法規條文和公司運營數據，自動生成合規報告，並突出潛在的違規風險，大幅降低合規成本和時間。", "**智能故障診斷與維修：** 工程師可以使用該系統，結合設備的監控數據和維修手冊，快速診斷故障原因，並提供維修步驟建議，提高維修效率和準確率，尤其適用於複雜的工業設備。", "**個性化教育輔導：** 學生在使用學習平台時，系統可以根據其學習記錄和知識點掌握情況，動態生成個性化的學習路徑和練習題，並提供針對性的講解，提高學習效果和學習效率。"], "pitch": "我們正在利用基石模型的力量，重新定義神經符號學習。傳統的神經符號學習成本高昂、難以泛化，而我們的神經符號提示方法，結合了基石模型的強大推理能力和符號程式的可解釋性，能以更低的成本、更高的效率解決複雜問題。想像一下，一個可以自動生成合規報告、診斷機器故障、甚至提供個性化教育的AI。這就是我們的願景。我們相信，我們的技術將在法律、製造、教育等領域產生顛覆性影響，為企業和個人帶來巨大的價值，具有極高的商業潛力，值得投資。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-02T09:16:32.655756"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：使用可組合多視角擴散生成可動畫的細緻3D人體", "summary_zh": "AdaHuman 是一個新的框架，能夠從單張真實照片生成高擬真且可動畫的 3D 頭像。 它透過兩個主要創新實現這一點：一是姿勢條件的 3D 關節擴散模型，可在任意姿勢下合成一致的多視圖圖像，並在每個擴散步驟重建對應的 3D 高斯 Splats（3DGS）；二是可組合的 3DGS 優化模組，透過圖像到圖像的優化來增強局部身體部位的細節，並使用新的作物感知相機光線圖將它們無縫整合，從而生成一個有凝聚力的細緻 3D 頭像。 這些組件使 AdaHuman 能夠生成高度逼真的標準 A 姿勢頭像，且具有最小的自我遮擋，從而可以使用任何輸入運動進行綁定和動畫製作。 在公開基準測試和真實照片上的廣泛評估表明，AdaHuman 在頭像重建和姿勢重定方面顯著優於最先進的方法。", "applications": ["虛擬試穿：使用者可以上傳自己的照片，在虛擬環境中試穿衣服，提前預覽穿搭效果，大幅減少退貨率。", "客製化遊戲角色：玩家可以將自己的照片轉換成高擬真的 3D 遊戲角色，增強遊戲沉浸感和個人化體驗。", "遠端會議與虛擬活動：使用者可以使用高度逼真的 3D 頭像參與遠端會議和虛擬活動，提供更生動和個性化的溝通方式，減少視訊會議的疲勞感。"], "pitch": "AdaHuman 解決了從單張照片生成高品質可動畫 3D 頭像的痛點，其技術領先於現有方案，在虛擬試穿、遊戲、遠端會議等領域擁有巨大的商業潛力。 透過授權模型、提供雲端服務（avatar-as-a-service）或與相關產業龍頭合作，可以快速佔領市場。其獨特的姿勢條件擴散模型和可組合優化模組，確保了生成的 3D 頭像具備高度的細節和逼真度，這是消費者願意付費的關鍵。 我們預計 AdaHuman 技術將徹底改變 3D 頭像生成領域，並為使用者帶來前所未有的個人化體驗，因此具備極高的投資價值。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-02T09:16:45.876057"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "這篇論文介紹了一個名為GenSpace的基準測試，用來評估AI圖像生成模型在創造圖像時的空間感知能力，像是模型能否理解物體在3D空間中的位置、關係和尺寸。研究發現，雖然AI模型能生成好看的圖像並遵循大致的指令，但在處理精確的3D細節上仍有困難。論文指出了模型在物體透視理解、自我中心與以他人為中心轉換，以及尺寸精確度上存在三大限制，並提出了改進方向。", "applications": ["**虛擬室內設計：** 讓使用者能透過文字描述來生成具有空間合理性的室內設計圖，方便規劃家具擺放和空間利用。", "**遊戲開發：** 自動生成符合遊戲場景空間邏輯的物件擺放，加速關卡設計和美術製作流程。", "**建築規劃輔助：** 輔助建築師快速生成不同視角的建築效果圖，並確保建築物及其周圍環境的空間關係準確。"], "pitch": "現今AI圖像生成模型雖然強大，但在空間感知方面仍有缺陷，GenSpace基準測試能精確量化這些缺陷。這是一個龐大的商業機會，因為精確的空間感知能力是許多應用場景的關鍵。透過投資於解決GenSpace中指出的問題，我們可以解鎖圖像生成技術在虛擬實境、遊戲開發、建築設計、甚至是機器人導航等領域的巨大潛力。想像一下，一個能理解3D空間並生成完全符合物理規則圖像的AI，將顛覆設計和創造的模式，創造出巨大的市場價值。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-02T10:15:23.891891"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通往泛化神經符號學習之路應由基礎模型鋪就", "summary_zh": "神經符號學習旨在解決訓練神經網絡進行複雜推理任務的挑戰，並提供可解釋性、可靠性和效率等優勢。傳統的神經符號學習方法結合神經模型和符號程序進行訓練，但面臨重大挑戰，限制其應用於簡單問題。另一方面，純神經基礎模型現在通過提示（prompting）而非訓練達到最先進的性能，但它們通常不可靠且缺乏可解釋性。我們提出使用符號程序來補充基礎模型，稱之為神經符號提示，這提供了一種使用這些模型進行複雜推理任務的方法。這引發了一個問題：在基礎模型的時代，神經符號學習中專門模型訓練的作用是什麼？為了探討這個問題，我們強調了傳統神經符號學習在計算、數據和程序方面導致泛化問題的三個陷阱。這篇立場文件認為，基礎模型可以實現可泛化的神經符號解決方案，從而提供了一條在沒有從頭開始訓練缺點的情況下，實現神經符號學習最初目標的途徑。", "applications": ["**自動程式碼生成與除錯：** 基於自然語言描述自動生成程式碼，並利用符號推理進行錯誤檢測與修正，大幅提升開發效率。", "**智能醫療診斷：** 結合病歷數據、醫學知識庫和基礎模型，利用神經符號提示進行疾病診斷和治療方案推薦，提高診斷準確率並減少誤診。", "**金融風險評估：** 運用基礎模型分析市場趨勢和財務數據，再結合符號規則進行風險評估和預測，提升金融機構的風險管理能力。"], "pitch": "我們正在開發一種基於基礎模型的神經符號學習平台，通過結合大規模預訓練模型和符號推理，解決傳統神經網絡在可解釋性、可靠性和泛化性上的不足。我們的核心技術 '神經符號提示' 讓基礎模型能夠進行複雜的推理任務，例如自動程式碼生成、智能醫療診斷和金融風險評估。相較於從頭開始訓練的神經符號學習方法，我們大幅降低了計算和數據需求，並提高了模型的泛化能力。我們的商業價值在於：1. 提供SaaS服務，讓企業快速部署AI推理能力，無需大量訓練數據和專家知識。2. 透過API整合，讓現有應用程式無縫銜接我們的神經符號推理引擎。3. 在垂直領域提供客製化解決方案，解決特定產業的複雜推理問題。我們正在尋求種子輪融資，以擴大團隊、加速產品開發並開拓市場。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-02T10:15:39.991659"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：基於組合式多視角擴散的可動畫化細緻3D人體生成", "summary_zh": "AdaHuman 是一種全新的框架，能夠從單張真實照片生成高保真、可動畫化的 3D 頭像。它利用基於姿態條件的 3D 聯合擴散模型，在任意姿態下合成一致的多視角圖像，並同步進行 3D 高斯濺射 (3DGS) 重建。此外，還引入了組合式 3DGS 細化模塊，通過圖像到圖像的細化來增強局部身體部位的細節，並利用一種新型的裁剪感知相機光線映射將它們無縫集成，從而生成一個連貫細緻的 3D 頭像。 簡而言之，AdaHuman 能從單張照片快速產生高品質、可動的3D人物模型。", "applications": ["**個性化遊戲角色創建：** 玩家可以上傳一張自拍照，快速生成一個完全可動畫化的遊戲角色，並自由定制服裝和動作。", "**虛擬試衣間：** 用戶可以上傳自己的照片，看到穿著不同服裝的3D形象，模擬線上試衣體驗，有效降低退貨率。", "**AI健身教練：** 根據用戶提供的照片生成 3D 模型，AI可以評估運動姿勢的正確性，提供更精準的個人化健身指導。"], "pitch": "AdaHuman 解決了現有圖像到 3D 頭像生成技術在細節呈現和動畫化方面的不足，能夠從單張照片生成高保真、可動畫化的 3D 頭像。 這項技術的應用前景廣闊，尤其在遊戲、電商、健身等領域具有巨大的商業潛力。 我們相信 AdaHuman 將革新內容創作方式，為用戶帶來更沉浸式、個性化的互動體驗。 團隊將持續優化模型，並探索更多商業合作機會，目標是將 AdaHuman 打造成領先的 3D 頭像生成解決方案，佔領相關市場的領導地位。 預計透過B2B和B2C模式獲利，如提供API服務給遊戲公司，或推出個人化頭像生成訂閱服務。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-02T10:16:08.038204"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "這篇論文提出 GenSpace，一個全新的基準測試和評估流程，用以全面評估現有圖像生成模型在空間感知方面的能力。研究發現，雖然AI模型能生成視覺上吸引人的圖像，並遵循一般指令，但在物件放置、關係和測量等具體的3D細節上表現不佳。研究指出目前最先進的圖像生成模型在空間感知方面存在三個核心限制：1) 物件透視理解，2) 以自我為中心到以他人為中心的轉換，以及 3) 度量測量遵從性。這些發現為改進圖像生成中的空間智能指出了可能的方向。", "applications": ["**虛擬室內設計：** 用戶可以描述他們想要的房間佈局（例如：“一張書桌放在窗戶旁邊，旁邊有一個綠色盆栽”），AI 會生成具有空間一致性的房間圖像，避免出現透視錯誤或物件擺放不合理的情況。", "**遊戲開發場景建模：** 遊戲設計師可以利用 AI 快速生成符合特定空間規則和風格的場景，例如：“一個廢棄的城市街道，佈滿了碎石和倒塌的建築物”，並保證物件之間的距離、大小比例合理。", "**輔助攝影：** 根據文本描述或草圖，AI 可以在取景時提供建議，例如：“將太陽放置在畫面的黃金分割點上”，或者“確保背景與人物的距離足夠，以營造景深效果”，幫助攝影師拍攝出更具空間感的照片。"], "pitch": "GenSpace 揭示了當前圖像生成模型在空間感知方面的局限性，這為開發下一代空間智能 AI 開闢了巨大的商業機會。我們提供的技術能更準確地評估和改進 AI 的空間理解能力，從而創造更逼真、更有用的圖像生成應用。我們的價值主張在於：1) 降低設計成本：在虛擬室內設計、遊戲開發等領域，可以大幅降低對人工建模的需求，提高效率。2) 提升用戶體驗：更真實、空間一致的圖像能提供更沉浸式的體驗，提高用戶滿意度。3) 開啟全新應用：空間感知的 AI 可以應用於導航、機器人等領域，賦予它們更強的環境理解能力。我們正在尋找投資，以擴大我們的研究團隊，開發更先進的評估工具和算法，並將我們的技術商業化，成為空間智能 AI 領域的領導者。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-02T11:12:56.436593"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通往通用化神經符號學習之路應以基礎模型鋪設", "summary_zh": "神經符號學習旨在解決訓練神經網路進行複雜推理任務時所面臨的挑戰，並提供可解釋性、可靠性和效率等優勢。傳統的神經符號學習方法通常將神經模型與符號程序結合訓練，但它們面臨著使其受限於簡單問題的重大挑戰。另一方面，純神經基礎模型現在透過提示（Prompting）而非訓練達到最先進的性能，但它們通常不可靠且缺乏可解釋性。利用符號程序來補充基礎模型，我們稱之為神經符號提示，提供了一種使用這些模型進行複雜推理任務的方法。這樣做引發了一個問題：在基礎模型時代，作為神經符號學習一部分的專門模型訓練扮演什麼角色？為了探索這個問題，我們強調了傳統神經符號學習在計算、數據和程序方面導致泛化問題的三個陷阱。這篇立場文件認為，基礎模型能夠實現通用化的神經符號解決方案，從而提供了一條實現神經符號學習的最初目標的途徑，而無需從頭開始訓練的缺點。", "applications": ["**智能客服系統：** 客服系統能理解客戶複雜的問題，不僅基於關鍵字，還能進行推理，例如理解語氣中的不滿，並根據公司政策和產品知識提供更精準、客製化的解決方案。", "**自動化程式碼除錯：** AI能分析程式碼，不僅找出錯誤，還能理解程式碼的邏輯，並提供修正建議，甚至自動生成修正程式碼。這將大幅縮短開發時間，降低除錯成本。", "**醫療診斷輔助：** AI能結合病患的病歷、檢驗數據和醫療知識庫，進行更精準的診斷和治療建議。例如，根據患者的症狀和病史，推理出潛在的疾病，並建議進一步檢查項目。"], "pitch": "想像一下，一個能像人類專家一樣思考的AI，但比人類專家更快、更精準。我們正在打造基於基礎模型的下一代神經符號學習技術，讓AI不僅僅是數據處理器，更是真正的問題解決者。我們的技術能讓AI理解複雜的邏輯關係，做出可解釋的決策，並且能夠快速適應新的情境。這意味著在智能客服、自動化開發、醫療診斷等領域，都能實現前所未有的效率和準確性。我們解決了傳統神經符號學習的泛化問題，並且避免了從頭開始訓練的巨大成本。這是一個巨大的市場機會，我們有信心能引領AI的發展方向，創造巨大的商業價值。投資我們，你將投資於AI的未來！", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-02T11:13:12.133246"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：利用組合式多視圖擴散生成可動畫的細緻3D人體", "summary_zh": "AdaHuman 是一個創新的框架，能從單張真實世界圖像生成高擬真、可動畫的3D頭像。它結合了兩個主要創新：一個姿態條件化的3D聯合擴散模型，可以在任意姿勢下合成一致的多視圖圖像，並在每個擴散步驟中進行相應的3D高斯 Splats (3DGS) 重建；以及一個組合式的3DGS精煉模塊，透過圖像到圖像的精煉來增強局部身體部位的細節，並使用一種新的感知裁剪的相機射線圖將它們無縫整合，生成一個有凝聚力的細緻3D頭像。AdaHuman能夠生成高度逼真的標準A-pose頭像，並且具有最小的自我遮擋，使其能夠與任何輸入運動進行綁定和動畫。在公共基準測試和真實世界圖像上的廣泛評估表明，AdaHuman 在頭像重建和重新定位方面顯著優於最先進的方法。", "applications": ["**個人化虛擬化身創建：** 用戶上傳一張照片，即可快速生成自己的3D頭像，用於VR/AR社交、遊戲或會議等。", "**時尚產業的虛擬試穿：** 消費者可以上傳自己的照片，在線上試穿不同服裝，看到衣服穿在自己身上的3D效果。", "**影視動畫角色的快速生成：** 根據演員的照片或概念圖，快速創建可動畫的3D角色模型，降低製作成本並縮短時間。"], "pitch": "AdaHuman解決了3D頭像生成領域的一個核心痛點：高品質、可動畫性。現有方案要麼細節不足，要麼難以動畫。AdaHuman 的技術突破使其能從單張照片快速生成逼真且可動畫的3D頭像，這在VR/AR、遊戲、時尚、影視等多個領域具有巨大的商業價值。其核心優勢在於：更低的製作成本（單張圖片生成），更高的生成效率（快速生成），以及更廣泛的應用場景（可動畫性）。我們相信 AdaHuman 有潛力成為3D頭像生成領域的領先技術，並帶來顛覆性的創新。我們尋求合作夥伴，共同開發面向消費級和企業級市場的應用產品，打造3D數位世界的基礎設施。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-02T11:13:25.559883"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "這篇論文介紹了GenSpace，一個新的基準測試和評估流程，用來全面評估當前圖像生成模型在空間感知方面的能力。研究發現，雖然AI模型可以生成視覺上吸引人的圖像並遵循一般指令，但在物體放置、關係和測量等具體3D細節方面存在困難。研究歸納出當前最先進圖像生成模型在空間感知方面的三個核心限制：1)物體透視理解，2)自我中心-對象中心轉換，3)度量測量遵守。這些發現突顯了改進圖像生成中空間智慧的可能方向。", "applications": ["**室內設計模擬：** 使用者可以輸入房間描述（例如：「現代簡約風客廳，落地窗旁有一張米色沙發，牆上掛著抽象畫」），AI生成器可以根據空間關係和尺寸生成逼真的室內設計圖，方便使用者預覽裝修效果。", "**虛擬活動場景創建：** 活動主辦方可以描述活動場地（例如：「海邊派對，沙灘上有DJ台，周圍擺放著遮陽傘和躺椅」），AI生成器可以生成活動場景圖，用於宣傳和規劃。", "**輔助建築設計：** 建築師可以輸入建築物的基本描述（例如：「現代風格別墅，帶有游泳池和花園」），AI生成器可以生成不同角度和光照下的建築物效果圖，幫助建築師進行設計和展示。"], "pitch": "GenSpace的研究揭示了圖像生成AI在空間感知上的不足，這為我們提供了一個巨大的商業機會。我們可以利用這些研究成果，開發更精準、更符合人類空間直覺的圖像生成引擎，應用於室內設計、遊戲開發、建築設計等領域。想像一下，使用者只需簡單描述，就能生成高度逼真的3D空間場景，這將極大地提高生產力，降低成本。我們的圖像生成引擎將不僅僅是繪圖工具，更是空間設計和規劃的強大助手。 我們可以透過建立垂直領域的 SaaS 服務，或是授權圖像生成引擎給現有的平台，快速搶佔市場。隨著元宇宙和AR/VR技術的發展，對於逼真3D空間內容的需求將會爆炸性增長，GenSpace的相關技術將擁有巨大的潛在商業價值。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-02T12:27:47.336917"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通往通用化神經符號學習之路應以基石模型鋪平", "summary_zh": "神經符號學習旨在解決訓練神經網路進行複雜推理任務時的挑戰，並提供可解釋性、可靠性和效率等優勢。傳統的神經符號學習方法結合訓練神經模型和符號程序，但面臨重大挑戰，使其僅限於簡單的問題。另一方面，純神經基石模型現在通過提示而非訓練達到最先進的性能，但它們通常不可靠且缺乏可解釋性。通過符號程序補充基石模型，我們稱之為神經符號提示，提供了一種將這些模型用於複雜推理任務的方法。這樣做引出了一個問題：在基石模型的時代，作為神經符號學習一部分的專門模型訓練扮演著什麼角色？為了探討這個問題，我們強調了傳統神經符號學習在計算、數據和程序方面導致泛化問題的三個陷阱。這篇立場論文認為，基石模型能夠實現可泛化的神經符號解決方案，為實現神經符號學習的最初目標提供了一條道路，而無需從頭開始訓練的缺點。", "applications": ["**法律文件審查：** 使用基石模型輔助分析法律條文，結合符號邏輯判斷合約是否符合特定法規，例如隱私條款是否符合GDPR標準。避免律師需要逐字逐句檢查。", "**醫療診斷輔助：** 基於醫學知識庫的符號規則，結合基石模型分析病人的病歷和影像資料，判斷病人的疾病風險，提供初步的診斷建議。醫師可以更快速地做出判斷。", "**智慧客服：** 利用基石模型理解用戶問題，再透過預先定義好的符號規則決定回覆策略，例如查詢訂單狀態、更改地址等等。提升客服效率並降低人力成本。"], "pitch": "我們正在開發基於基石模型的神經符號框架，解決傳統神經符號學習的泛化問題。想像一下，一個能像律師一樣審閱合約，像醫生一樣協助診斷，像資深客服一樣應對客戶問題的AI，而這一切無需耗費巨額數據和算力從頭訓練。我們的技術利用基石模型強大的語言理解能力，結合可解釋的符號規則，打造出可靠、可驗證、高效的AI解決方案。這不僅降低了AI開發和部署的成本，也大幅拓展了其應用範圍，潛在市場規模巨大，涵蓋法律、醫療、金融等各個行業。我們正在尋找投資者，共同打造下一代具有複雜推理能力的AI引擎，搶佔市場先機。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-02T12:28:05.669001"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：基於組合式多視角擴散的可動畫化細緻3D人體生成", "summary_zh": "AdaHuman 是一個從單張圖片生成高逼真度、可動畫化 3D 頭像的全新框架。它利用姿態條件 3D 關節擴散模型，在生成一致的多視角圖像的同時，逐步重建 3D 高斯濺射 (3DGS)。此外，它還採用組合式 3DGS 優化模組，通過圖像到圖像的優化來增強局部身體部位的細節，並使用創新的裁剪感知相機射線圖無縫整合這些細節，從而生成一個連貫且細緻的 3D 頭像。 AdaHuman 生成的頭像具有高度的真實感，並且姿態標準化，方便進行綁定和動畫製作，適用於各種運動輸入。", "applications": ["**個人化遊戲角色創建：** 玩家只需上傳一張自拍照，就能快速生成一個高度擬真的遊戲角色，並能根據遊戲需求進行客製化，大大提升遊戲體驗。", "**虛擬試衣與電商應用：** 消費者上傳照片，就能看到衣服穿在自己身上的效果，解決線上購物無法試穿的痛點，提高購買意願。", "**遠程醫療與復健：** 醫生可以根據患者的照片，生成3D模型，進行遠程診斷和復健計劃設計，減少患者往返醫院的負擔。"], "pitch": "AdaHuman解決了從單張圖片生成高品質、可動畫化3D人體模型的痛點，具有廣闊的商業應用前景。其核心優勢在於能夠以高效、低成本的方式生成高逼真度、易於動畫製作的3D頭像。這不僅降低了遊戲、電影、電商等行業對3D模型製作的門檻和成本，也為遠程醫療、虛擬現實等新興領域提供了強大的技術支持。通過授權核心技術、提供雲服務API、以及與相關行業進行深度合作，AdaHuman具備巨大的市場潛力，預計將在未來幾年內實現快速增長，成為3D內容創作領域的領先者。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-02T12:28:20.103352"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "GenSpace 是一項新的基準測試，旨在評估 AI 圖像生成模型在從文字或圖像提示創建圖像時，是否具有類似於人類的 3D 空間感知能力。研究發現，雖然這些模型能產生視覺上吸引人的圖像並遵循一般指令，但在物件放置、關係和尺寸等特定 3D 細節方面仍存在困難。GenSpace 提出了一種專門的評估流程和指標，利用多個視覺基礎模型重建 3D 場景幾何結構，從而提供更準確且更符合人類感知的空間逼真度評估。", "applications": ["**虛擬室內設計：**使用者可以輸入文字描述，例如「現代風格客廳，窗邊放一張綠色沙發，牆上掛一幅抽象畫」，讓 AI 生成逼真的室內設計圖，並能準確調整家具的尺寸和位置。", "**遊戲場景創建：**遊戲開發者可以利用 AI 快速生成多樣化的遊戲場景，例如「中世紀村莊，中心有一座水井，周圍環繞著房屋」，節省大量建模時間，並確保物件之間的空間關係合理。", "**自動駕駛模擬環境生成：**生成各種複雜的交通場景，例如「繁忙的十字路口，有汽車、行人、自行車」，用於訓練和測試自動駕駛系統，提高其在真實環境中的安全性。"], "pitch": "GenSpace 解決了圖像生成領域一個關鍵痛點：空間感知能力不足。目前的 AI 圖像生成模型雖然能產生逼真圖像，但對物件之間的空間關係理解不足，導致產出內容在空間上不合理。GenSpace 提供了一套客觀的基準測試和評估流程，能有效衡量和提升 AI 模型的空間感知能力。這項技術的商業價值體現在以下幾個方面：1. **提升現有圖像生成產品的品質和可用性：** 例如，可以將 GenSpace 的評估結果用於改進產品的訓練數據和算法，從而提升生成圖像的空間合理性，增強使用者體驗。2. **開闢新的應用場景：** GenSpace 可以促進在虛擬現實、遊戲開發、建築設計等領域的應用，創造新的商業機會。3. **構建相關服務：** 可以提供 GenSpace 評估和諮詢服務，幫助企業評估和改進其圖像生成模型。GenSpace 的核心價值在於提升圖像生成模型對 3D 空間的理解，並使其更接近人類的感知，這將帶來巨大的商業潛力。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-02T13:28:36.666928"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "邁向通用化神經符號學習之路應以基石模型鋪就", "summary_zh": "神經符號學習旨在解決訓練神經網路進行複雜推理任務的挑戰，並增加可解釋性、可靠性和效率。傳統方法同時訓練神經模型和符號程式，但面臨著限制其應用於簡單問題的挑戰。另一方面，純神經基石模型現在透過提示而非訓練達到最先進的性能，但它們通常不可靠且缺乏可解釋性。透過符號程式補充基石模型，我們稱之為神經符號提示，提供了一種利用這些模型進行複雜推理任務的方法。 這引發了一個問題：在基石模型的時代，作為神經符號學習一部分的專門模型訓練有什麼作用？ 為了探討這個問題，我們重點介紹了傳統神經符號學習在計算、數據和程式方面導致泛化問題的三個陷阱。 本立場文件認為，基石模型使通用化的神經符號解決方案成為可能，從而提供了一條實現神經符號學習最初目標的途徑，而無需從頭開始訓練的缺點。", "applications": ["**智能客服：** 基石模型理解使用者複雜問題，結合符號推理規則找出最佳解決方案，提供更精準、可解釋的客戶服務。", "**醫療診斷輔助：** 基石模型從病歷中提取關鍵資訊，結合醫療知識庫進行符號推理，輔助醫生進行更精確的診斷和治療方案制定。", "**金融風險評估：** 基石模型分析市場數據和新聞，結合風險評估模型進行符號推理，提供更可靠、易於理解的風險評估報告，協助投資決策。"], "pitch": "我們正在開創基於基石模型的新一代神經符號學習技術，解決了傳統方法泛化能力差、可解釋性不足的問題。 透過將基石模型的強大語言理解能力與符號推理的精確性結合，我們能夠為各種產業提供更可靠、更高效、更可解釋的AI解決方案。 我們的技術能大幅降低模型訓練成本，加速AI應用落地，並在智能客服、醫療診斷、金融風控等領域創造巨大商業價值。 我們相信，這項技術將成為未來AI發展的關鍵驅動力，我們正在尋找合作夥伴，共同打造AI驅動的未來。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-02T13:28:50.076496"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：利用組合式多視角擴散生成可動畫的細緻3D人體模型", "summary_zh": "AdaHuman 是一個創新的框架，它能從單張真實世界的照片生成高保真、可動畫的 3D 人體模型。它包含兩個關鍵創新：首先，一個姿態條件式 3D 聯合擴散模型，能合成在任意姿態下一致的多視角圖像，並在每個擴散步驟中重建對應的 3D 高斯潑濺 (3DGS)；其次，一個組合式 3DGS 細化模組，透過圖像到圖像的細化來增強局部身體部位的細節，並使用一種新的裁剪感知相機光線圖將它們無縫整合，產生一個連貫且細緻的 3D 人體模型。這使 AdaHuman 能夠生成高度逼真、標準 A 姿勢的人體模型，並具備最小的自我遮擋，從而可以使用任何輸入動作進行骨骼綁定和動畫製作。在公開基準測試和真實世界圖像上的廣泛評估表明，AdaHuman 在人體模型重建和姿勢調整方面顯著優於最先進的方法。", "applications": ["**個人化虛擬化身製作：** 用戶上傳一張照片，即可快速生成自己的高擬真 3D 化身，應用於遊戲、社交平台等虛擬世界。", "**服裝試穿與設計：** 服裝設計師可以根據單張模特照片，生成可任意變換姿勢的 3D 人體模型，用於模擬服裝的穿著效果和進行設計調整。", "**動畫與遊戲角色快速生成：** 動畫師和遊戲開發者可以使用現有角色照片快速生成高質量、可動畫的角色模型，大幅縮短製作時間。"], "pitch": "AdaHuman 解決了從單張圖像生成高品質、可動畫 3D 人體模型的關鍵痛點。 它的優勢在於顯著提升了生成模型的真實度和動畫適配性，克服了現有技術在細節處理和姿態控制上的局限。 從市場角度來看，AdaHuman 具有廣泛的商業應用前景，包括但不限於：個人化化身市場（社交、遊戲）、虛擬試穿與電商、電影動畫製作、以及 AI 驅動的內容創作平台。 通過提供 API 接口或 SDK，我們可以將這項技術賦能給各行各業，讓每個人都能輕鬆擁有自己的高擬真 3D 化身，或快速創建所需的 3D 角色。 我們的競爭優勢在於技術領先性和生成模型的品質，結合有效的商業模式和市場拓展，AdaHuman 具有巨大的增長潛力，有望成為 3D 人體模型生成領域的領先者。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-02T13:29:05.011752"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "GenSpace 是一項新的基準測試和評估流程，旨在全面評估當前圖像生成模型在創建圖像時的空間感知能力。研究發現，儘管AI模型可以生成視覺上吸引人的圖像並遵循一般指令，但在物體放置、關係和測量等具體的3D細節方面表現不佳。該研究揭示了當前最先進圖像生成模型在空間感知方面的三個核心局限：1) 物體透視理解，2) 以自我為中心到以物體為中心的轉換，以及 3) 度量測量遵守。研究結果為改進圖像生成中的空間智能提供了方向。", "applications": ["**室內設計預覽：** 用戶可以輸入文字描述（例如：「現代風格客廳，左邊放一張 L 型沙發，右邊靠窗放一盆大型盆栽」），AI 生成符合空間佈局的設計效果圖，幫助用戶預覽裝修效果，避免實際裝修後的空間比例不協調問題。", "**AR/VR遊戲場景創建：** 遊戲開發者可以利用該技術，根據文字描述快速生成符合物理規則和空間邏輯的遊戲場景，大幅縮短場景製作時間，提高遊戲開發效率。", "**電商商品展示：** 電商平台可以讓用戶透過文字描述，客製化商品在虛擬空間中的展示方式，例如：「在北歐風格的客廳裡，將我的新沙發放在壁爐旁邊，並展示它的多個角度」，提升用戶購物體驗，增加購買慾望。"], "pitch": "GenSpace 解決了 AI 圖像生成領域在空間感知方面的關鍵瓶頸，為打造更真實、更符合用戶需求的圖像生成應用奠定了基礎。 我們的 benchmark 有助於引導開發者更有效地提升模型的空間智能，而我們的評估流程能精確度量模型表現。這項技術在室內設計、遊戲開發、電商等多個領域具有巨大的商業潛力。想像一下，用戶可以通過簡單的文字描述，快速生成逼真的空間效果圖，這將徹底改變傳統的設計和行銷模式。 我們團隊正在積極尋求合作夥伴，共同將這項技術商業化，搶佔市場先機，成為空間感知圖像生成領域的領先者。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-02T14:15:11.684397"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "邁向通用型神經符號學習之路應由基礎模型鋪就", "summary_zh": "神經符號學習旨在解決訓練神經網路進行複雜推理任務時遇到的挑戰，並增加可解釋性、可靠性和效率等優點。傳統的神經符號學習方法通常將神經模型與符號程序結合訓練，但面臨著重大挑戰，使其僅限於簡單的問題。另一方面，純粹的神經基礎模型現在通過提示而非訓練達到了最先進的性能，但它們通常不可靠且缺乏可解釋性。用符號程序補充基礎模型，我們稱之為神經符號提示，提供了一種使用這些模型進行複雜推理任務的方法。這樣做引出了一個問題：在基礎模型時代，作為神經符號學習一部分的專門模型訓練扮演什麼角色？為了探討這個問題，我們重點介紹了傳統神經符號學習在計算、數據和程序方面導致泛化問題的三個陷阱。這篇立場文件認為，基礎模型能夠實現通用型的神經符號解決方案，從而提供了一條實現神經符號學習的原始目標，而沒有從頭開始訓練的缺點的途徑。", "applications": ["**醫療診斷輔助：** 利用基礎模型理解病歷，再結合符號推理規則（例如：如果X症狀存在且Y檢查結果異常，則Z疾病的可能性很高），輔助醫生進行更精準的診斷，提高準確性並降低誤診率。", "**金融風險評估：** 基礎模型分析市場新聞和公司財報等信息，結合符號規則（例如：負債比率高於某個閾值，且盈利能力下降，則風險等級提高），更全面地評估貸款和投資的風險，幫助金融機構做出更明智的決策。", "**智能客服系統：** 基礎模型理解用戶意圖，並根據預定義的符號規則（例如：如果用戶詢問退貨政策，則輸出相關條款），提供更準確和個性化的客戶服務，提升用戶體驗並降低人工客服成本。"], "pitch": "我們正在構建下一代智能系統，它結合了基礎模型的強大語言理解能力和符號推理的嚴謹性。傳統神經符號學習過於依賴從頭訓練，泛化能力差，效率低下。我們的解決方案，神經符號提示，利用預訓練的基礎模型，並用符號程序進行補充，可以顯著提升推理能力、可解釋性和可靠性。這意味著，我們能以更低的成本，開發出能夠解決複雜問題的智能系統，例如在醫療、金融、客服等領域。我們預計將通過 SaaS 模式向企業提供解決方案，並通過專業服務協助客戶進行客製化部署，創造巨大的商業價值。我們的競爭優勢在於無需從頭訓練模型，快速迭代，以及更強的可解釋性，這將為我們在市場上贏得領先地位。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-02T14:15:39.575375"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：基於組合式多視角擴散的可動畫化精細3D人體生成", "summary_zh": "現有的圖像生成3D頭像的方法難以產生足夠精細、可動畫化的頭像，無法滿足實際應用需求。我們提出了 AdaHuman，一種從單張真實圖像生成高保真可動畫化3D頭像的新框架。AdaHuman 包含兩大創新：(1) 一個姿態條件3D聯合擴散模型，可合成任意姿態下的一致多視角圖像，並在每個擴散步驟進行對應的3D高斯濺射（3DGS）重建；(2) 一個組合式3DGS細化模塊，透過圖像到圖像的細化來增強局部身體部位的細節，並使用新的、具有裁剪感知能力的相機光線圖無縫整合它們，從而產生一個具有凝聚力的精細3D頭像。這些組件使 AdaHuman 能夠生成高度逼真的標準A-pose頭像，且自遮擋最小，從而可以使用任何輸入運動進行骨骼绑定和動畫製作。在公共基準測試和真實圖像上的廣泛評估表明，AdaHuman 在頭像重建和重新擺姿勢方面顯著優於最先進的方法。代碼和模型將公開用於研究目的。", "applications": ["**虛擬試衣間：** 顧客上傳一張照片，即可生成一個高度逼真的3D人體模型，試穿不同款式的服裝，查看穿著效果，並在不同的姿態下模擬服裝的動態效果，提升線上購物體驗。", "**客製化遊戲角色：** 玩家只需提供一張自拍照，即可快速創建一個與自己極為相似的遊戲角色，提升遊戲沉浸感和個性化體驗，讓玩家感覺自己真正置身於遊戲世界中。", "**虛擬社交與元宇宙：** 用戶創建自己的精細3D頭像，在元宇宙中進行互動、社交和娛樂，呈現更真實、更具表現力的自我，打破線上交流的隔閡感。"], "pitch": "AdaHuman 解決了生成高品質、可動畫化3D人體頭像的關鍵瓶頸，其技術優勢在於能從單張圖像快速生成精細且真實的3D模型。這項技術具有廣泛的商業應用潛力，包括：在服裝零售業中提供沉浸式虛擬試穿體驗，大幅提升線上銷售轉換率；在遊戲產業中創造高度客製化的遊戲角色，吸引更多玩家；在元宇宙領域提供更真實的虛擬化身，增強用戶沉浸感和參與度。投資AdaHuman，意味著投資於未來人機互動的核心技術，搶佔快速成長的虛擬經濟市場的先機。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-02T14:15:55.708564"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "人類能直觀地在3D空間中構圖和安排場景，以進行攝影。但現今的AI圖像生成器，在根據文字或圖像提示生成圖像時，能否展現類似的3D空間感知能力？ 我們提出了GenSpace，一個全新的基準測試和評估流程，以全面評估當前圖像生成模型的空間感知能力。 研究發現，AI模型生成的圖像雖然視覺上吸引人，且能遵循一般指示，但在物體放置、關係和尺寸等具體的3D細節方面表現不佳。 我們總結了當前最先進圖像生成模型在空間感知方面的三個核心限制：1) 物體透視理解，2) 以自我為中心到以他人為中心的轉變，以及3) 尺寸測量準確性，並強調了改進圖像生成空間智能的可能方向。", "applications": ["**虛擬室內設計助手：** 用戶可以通過文字描述或草圖，精確控制家具擺放位置、大小比例，並預覽真實效果，避免購買後發現不合適的情況。", "**遊戲關卡設計工具：** 遊戲開發者可以利用AI快速生成符合特定空間規則（例如重力、遮擋）的遊戲場景，加速關卡設計流程。", "**輔助攝影構圖訓練：** 提供AI反饋，分析用戶拍攝照片的空間透視、物體關係等，幫助攝影愛好者提升構圖技巧。"], "pitch": "GenSpace的研究揭示了現有AI圖像生成模型在空間感知方面的重大缺陷，這同時也創造了巨大的商業機會。 我們的評估基準能夠推動AI模型在空間理解方面的進步，催生更精準、更符合人類直覺的圖像生成技術。 透過針對GenSpace提出的問題進行技術突破，我們可以開發出能夠理解複雜空間關係的AI，進而革新室內設計、遊戲開發、建築設計等領域。 想像一下，用戶只需簡單描述，AI就能生成符合物理法則、完美呈現空間感的圖像，這將釋放巨大的創造力，並簡化複雜的設計流程。 我們相信，基於GenSpace的研究，我們可以打造出下一代具有空間智能的AI圖像生成引擎，在上述領域佔據領導地位，並創造巨大的商業價值。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-02T16:18:03.012723"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通往可泛化的神經符號學習之路應以基石模型鋪設", "summary_zh": "神經符號學習旨在解決神經網路在複雜推理任務訓練上的挑戰，並提供可解釋性、可靠性和效率等優勢。傳統神經符號學習方法結合訓練神經模型和符號程式，但面臨重大挑戰，使其僅限於簡單問題。另一方面，純神經基石模型現在透過提示而非訓練達到最先進的性能，但它們通常不可靠且缺乏可解釋性。使用符號程式補充基石模型，我們稱之為神經符號提示，提供了一種使用這些模型進行複雜推理任務的方法。這樣做引出了一個問題：在基石模型的時代，作為神經符號學習一部分的專業模型訓練扮演什麼角色？為了探討這個問題，我們重點介紹了傳統神經符號學習在計算、數據和程式方面導致泛化問題的三個陷阱。本文認為，基石模型能夠實現可泛化的神經符號解決方案，從而提供了一條在不從頭開始訓練的情況下實現神經符號學習原始目標的途徑。", "applications": ["**法律文件審閱：** 使用基石模型理解法律條文，並結合符號程式檢查合規性、識別潛在風險，簡化律師工作流程。", "**醫療診斷輔助：** 利用基石模型分析病患病歷，結合符號程式推理藥物交互作用、疾病關聯，為醫生提供更準確的診斷建議。", "**自動化程式碼生成：** 基石模型理解自然語言描述的需求，符號程式負責生成結構化的程式碼，降低開發門檻，加速軟體開發。"], "pitch": "我們正處於AI的轉捩點，基石模型展現了強大的推理能力，但缺乏可解釋性和可靠性，限制了其在高風險領域的應用。我們的神經符號提示方法，結合基石模型和符號程式的優勢，打造可泛化、可解釋、且可靠的AI解決方案。想像一下，一個能精確診斷疾病、審閱複雜法律文件、甚至自動生成程式碼的AI，這將顛覆醫療、法律和軟體開發等行業。 我們的創新技術，透過利用現有的基石模型，大幅降低了從頭訓練模型的成本和風險。我們尋求投資以加速產品開發，拓展應用場景，並將此技術推廣至更廣泛的市場，建立一個由可信賴的AI驅動的未來。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-02T16:18:23.519287"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：透過可組合的多視角擴散生成可動畫的精細3D人像", "summary_zh": "現有的圖片到3D頭像生成方法難以產生高度精細、可供動畫製作的頭像，無法滿足真實世界的應用需求。AdaHuman提出了一個新穎的框架，可以從單張真實環境照片生成高保真、可動畫的3D人像。AdaHuman包含兩個關鍵創新：（1）一個姿態條件化的3D聯合擴散模型，可以在任意姿態下合成一致的多視角圖像，同時在每個擴散步驟中重建對應的3D高斯潑濺(3DGS)；（2）一個可組合的3DGS精煉模塊，通過圖片到圖片的精煉來增強局部身體部位的細節，並使用一個新穎的裁剪感知相機光線圖無縫地整合它們，從而產生一個具有凝聚力的精細3D人像。這些組件使AdaHuman能夠生成高度逼真的標準A-pose頭像，並最大限度地減少自我遮擋，從而可以使用任何輸入運動進行裝配和動畫製作。在公共基準測試和真實環境圖片上的廣泛評估表明，AdaHuman在頭像重建和重新擺姿勢方面顯著優於最先進的方法。程式碼和模型將公開用於研究目的。", "applications": ["**虛擬試衣間：** 消費者上傳一張個人照片，即可生成一個精確的3D人像，用於線上試穿各種服裝，無需親自到實體店。", "**遊戲角色客製化：** 玩家只需上傳一張照片，就能快速創建一個高度逼真的遊戲角色，大幅提升遊戲的沉浸感和個人化體驗。", "**虛擬助理/虛擬偶像：** 企業可以基於員工或藝人的照片，創建具有高度擬真外觀的虛擬助理或虛擬偶像，用於線上客服、直播互動等，降低運營成本並提升品牌形象。"], "pitch": "AdaHuman解決了傳統3D人像生成技術在細節和動畫方面的瓶頸，僅需單張照片即可生成高品質、可動畫的3D人像，具有顛覆市場的潛力。其核心優勢在於高效的生成速度和高度的真實感，能夠廣泛應用於電商、遊戲、娛樂、教育等多個領域，帶來顯著的商業價值。想像一下，一個可以讓消費者在線上精準試穿衣服的App，一個讓遊戲玩家輕鬆創建逼真角色的平台，或是一個可以24小時在線提供服務的擬真虛擬助理。AdaHuman技術將賦能這些應用，開創全新的商業模式，搶佔先機。我們尋求投資，加速技術商業化，將AdaHuman打造成3D人像生成領域的領導者。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-02T16:18:45.686111"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "這篇論文提出了一個名為GenSpace的新基準測試和評估流程，用以評估當前圖像生成模型在根據文字或圖像提示生成圖像時的空間感知能力。研究發現，雖然AI模型能產生視覺上吸引人的圖像，並能遵循一般指示，但在物體放置、關係和測量等具體的3D細節方面仍存在困難。研究總結了當前最先進圖像生成模型在空間感知方面的三個核心限制：1) 物體透視理解，2) 自我中心-他者中心轉換，3) 度量測量遵守。研究旨在強調改善圖像生成中空間智能的可能方向。", "applications": ["**虛擬室內設計：** 用戶可以通過文字描述或草圖，讓AI生成逼真的室內設計圖，並精確調整家具擺放和尺寸比例，預覽裝修效果。", "**遊戲場景創建：** 遊戲開發者可以利用AI快速生成具有空間感的遊戲場景，例如：描述一個古老的城堡，AI會自動生成符合描述的城堡內部和外部場景，並保證物件之間的空間關係合理。", "**AR/VR內容生成：** 將文字描述轉換為AR/VR環境中的3D物件和場景，使用者可以通過語音或文字創造自己想要的虛擬世界，例如：描述一個「充滿熱帶魚的珊瑚礁」，即可在VR中體驗身歷其境的潛水感受。"], "pitch": "GenSpace 項目解決了圖像生成領域長期存在的空間感知問題，揭示了現有AI模型在3D空間理解上的瓶頸。基於此，我們將開發更智能的圖像生成引擎，應用於虛擬室內設計、遊戲場景創建和AR/VR內容生成等高潛力市場。我們獨特的評估體系和改進方向，將顯著提升圖像生成模型的逼真度和可用性，創造更豐富、更具沉浸感的用戶體驗。這不僅能加速內容創作效率，更能開闢全新的商業模式，例如：個性化定制化場景生成、智能設計助手等。我們相信，GenSpace 的技術突破將引領下一代圖像生成革命，帶來巨大的商業價值和社會影響。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-02T17:13:31.832781"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通往廣義神經符號學習之路，應由基礎模型鋪設", "summary_zh": "神經符號學習旨在解決神經網路在複雜推理任務上的訓練難題，並提供可解釋性、可靠性和效率等優勢。傳統方法將神經模型與符號程式共同訓練，但面臨限制其應用於簡單問題的挑戰。另一方面，純神經基礎模型透過提示而非訓練達到最先進的性能，但缺乏可靠性和可解釋性。本文提出利用符號程式補充基礎模型，即神經符號提示，將這些模型用於複雜推理任務。文章探討在基礎模型時代，作為神經符號學習一部分的專用模型訓練扮演的角色。透過強調傳統神經符號學習在算力、數據和程式方面的三個缺陷，導致了泛化問題。本文認為，基礎模型能夠實現廣義神經符號解決方案，提供了一條實現神經符號學習最初目標的途徑，避免了從頭訓練的缺點。", "applications": ["**醫療診斷輔助：** 利用基礎模型理解病歷，結合符號程式的推理能力，輔助醫生進行疾病診斷和治療方案制定，提升診斷準確性並提供可解釋的診斷依據。", "**金融風險評估：** 整合金融市場數據與規則，基礎模型負責處理非結構化數據（如新聞、社交媒體），符號程式進行合規性檢查和風險計算，提供更準確且可解釋的風險評估報告，減少人為偏誤。", "**智能合約自動驗證：** 使用基礎模型理解合約內容，符號程式進行漏洞檢測和邏輯驗證，確保合約的安全性和可靠性，防止惡意利用和錯誤執行。"], "pitch": "我們正在打造下一代人工智能引擎，它結合了基礎模型強大的語義理解能力和符號程式嚴謹的邏輯推理能力。想像一下，不再需要從零開始訓練模型，而是可以利用現有的大型語言模型，通過神經符號提示，賦予它們複雜的推理能力。這將大幅降低開發成本和時間，同時提高模型的可靠性和可解釋性。我們的技術適用於需要高精確度和透明度的各個行業，例如金融、醫療和法律。通過我們的平台，企業可以構建更智能、更可靠的AI解決方案，解決複雜的決策問題，並在快速變化的市場中保持領先地位。我們相信，這項技術將徹底改變AI的開發和應用方式，成為下一個AI浪潮的基石。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-02T17:13:52.591689"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：利用組合式多視角擴散生成可動畫的細緻3D人體", "summary_zh": "AdaHuman 是一個新的框架，可以從單張自然圖像生成高保真、可動畫的3D虛擬人像。它結合了兩個關鍵創新：一是姿勢條件的3D聯合擴散模型，可以合成任意姿勢下一致的多視角圖像，並在每個擴散步驟中重建相應的3D高斯散布(3DGS)。二是組合式的3DGS精煉模塊，通過圖像到圖像的精煉來增強局部身體部位的細節，並使用新的裁剪感知相機光線圖將它們無縫集成，從而產生一個具有凝聚力的詳細3D虛擬人像。這些組件使 AdaHuman 能夠生成高度逼真的標準A姿勢虛擬人像，具有最小的自我遮擋，從而能夠使用任何輸入動作進行綁定和動畫。在公共基準和自然圖像上的廣泛評估表明，AdaHuman 在虛擬人像重建和重新姿態方面都顯著優於最先進的方法。", "applications": ["**個人化遊戲角色創建：** 玩家可以上傳一張自己的照片，快速生成一個可以立即在遊戲中使用的、高度還原的3D角色，避免了漫長的捏臉過程。", "**虛擬試衣間：** 消費者可以上傳自己的照片，生成3D虛擬人像，在線上模擬試穿各種服裝，更直觀地看到穿搭效果，提升線上購物體驗。", "**遠程醫療與復健：** 醫療機構可以利用患者的照片，生成可動畫的3D模型，用於遠程評估患者的運動能力，監測復健進度，並提供個性化的復健指導。"], "pitch": "AdaHuman 解決了現有 3D 人體生成技術在細節和可動畫性上的痛點，能夠從單張照片生成高度逼真、可動畫的 3D 頭像。這項技術具有廣泛的應用前景，包括遊戲、電商、虛擬現實/擴增實境、以及醫療保健等領域。我們正在尋求投資，以加速技術開發，建立完善的應用生態系統，並搶占市場先機。潛在的商業價值包括：授權技術給遊戲公司和電商平台、開發個人化虛擬頭像服務、以及與醫療機構合作開發遠程醫療解決方案。我們的目標是將 AdaHuman 打造成為 3D 人體生成領域的領導者。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-02T17:14:15.628626"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "人類具備直覺地在3D空間中構圖和安排場景的能力，以進行攝影。然而，當先進的AI圖像生成器從文本或圖像提示創建圖像時，它們是否能以類似的3D空間感知來規劃場景？ 本文提出GenSpace，一個新穎的基準測試和評估流程，旨在全面評估當前圖像生成模型的空間感知能力。標準評估通常使用通用的視覺語言模型（VLM），但這些評估往往無法捕捉到詳細的空間錯誤。為了解決這個挑戰，我們提出了一個專門的評估流程和指標，利用多個視覺基礎模型重建3D場景幾何，並提供更準確、更符合人類感知的空間保真度指標。研究結果表明，雖然AI模型能夠創建視覺上吸引人的圖像並遵循一般指令，但它們在物體放置、關係和測量等具體的3D細節方面表現不佳。我們總結了當前最先進圖像生成模型在空間感知方面的三個核心限制：1) 物體透視理解，2) 自我中心-他者中心轉換，以及 3) 度量測量一致性，並強調了改進圖像生成空間智能的可能方向。", "applications": ["**虛擬實境(VR)內容創作：** 開發者可以利用此研究改善VR環境中物件的擺放和互動邏輯，例如讓虛擬家具擺放更自然、物件大小比例更真實，提高VR體驗的沉浸感。", "**建築設計輔助：** 設計師可以利用此研究來評估AI生成的建築設計方案，確保設計符合空間比例、物件擺放邏輯和結構安全等因素，加速設計流程並減少錯誤。", "**電商產品展示：** 商家可以利用此研究生成更逼真的產品展示圖，例如將產品擺放在符合空間透視和光影效果的居家場景中，提高消費者的購買意願。"], "pitch": "GenSpace解決了AI圖像生成在空間感知上的關鍵瓶頸，為市場帶來了新的機會。目前AI生成的圖像雖然美觀，但在空間理解上存在嚴重缺陷，限制了其應用範圍。GenSpace提供了一套完整的評估體系和改進方向，可應用於VR/AR、建築設計、電商等領域。透過與現有AI圖像生成模型整合，我們可以大幅提升生成內容的真實感和可用性，創造更沉浸式的用戶體驗。我們計劃將GenSpace打造成一個開放的基準測試平台，吸引更多開發者參與，共同推動AI空間智能的發展，並透過API服務、客製化模型訓練等方式實現商業價值。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-02T18:19:27.195633"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通往通用神經符號學習之路應以基石模型鋪設", "summary_zh": "神經符號學習旨在解決訓練神經網路處理複雜推理任務的挑戰，並帶來可解釋性、可靠性和效率的優勢。傳統的神經符號學習方法將神經模型與符號程序結合訓練，但面臨重大挑戰，限制了其應用範圍。另一方面，純神經的基石模型現在通過提示而非訓練達到了最先進的性能，但它們往往不可靠且缺乏可解釋性。我們提出神經符號提示，即利用符號程序補充基石模型，提供了一種使用這些模型處理複雜推理任務的方法。這引發了一個問題：在基石模型的時代，作為神經符號學習一部分的專門模型訓練扮演什麼角色？為了探討這個問題，我們強調了傳統神經符號學習在計算、數據和程序方面導致泛化問題的三個陷阱。本文認為，基石模型能夠實現通用的神經符號解決方案，提供了一條通往實現神經符號學習最初目標的道路，而無需從頭開始訓練的缺點。", "applications": ["**智能客服：** 利用基石模型理解用戶意圖，結合符號程序進行複雜問題的推理和解答，例如查詢醫療保險政策的複雜條款，提供準確且可解釋的答案。", "**自動化程式碼偵錯：** 基石模型分析程式碼，符號程序驗證邏輯正確性，幫助開發者快速找到並修復程式碼中的錯誤，提升程式碼品質和開發效率。", "**金融風險評估：** 基石模型分析市場數據，符號程序執行複雜的風險計算和模型驗證，提供更可靠、可解釋的風險評估報告，輔助投資決策。"], "pitch": "傳統神經符號學習受限於訓練數據和模型複雜度，難以泛化。我們的解決方案利用基石模型強大的泛化能力，結合符號程序的精確推理，克服了這些限制。我們將構建一個神經符號提示平台，提供API服務，讓企業可以快速將強大的推理能力集成到其現有系統中。市場對可解釋AI的需求日益增長，尤其是在金融、醫療等高風險領域。我們的技術能夠顯著提高AI模型的可靠性和可解釋性，從而抓住這個巨大的市場機會。我們預計通過訂閱API服務和定制化解決方案，實現高成長和高利潤。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-02T18:19:50.353040"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：基於組合式多視角擴散的可動畫化細緻3D人體生成", "summary_zh": "AdaHuman 是一個創新的框架，它可以從單張真實照片生成高度擬真且可動畫化的3D人物模型。它採用了兩個關鍵技術：一是基於姿態條件的3D聯合擴散模型，可以在任意姿態下合成一致的多視角圖像，並同步進行3D高斯潑濺(3DGS)重建；二是組合式3DGS細化模塊，通過圖生圖細化技術增強局部身體部位的細節，並使用創新的裁剪感知相機光線圖將它們無縫整合，生成一個完整且細緻的3D人物模型。這使得 AdaHuman 能夠生成高度逼真且具有標準A字姿勢的人物模型，最小化自我遮擋，方便後續的綁定和動畫製作。實驗證明，AdaHuman 在人物重建和姿態調整方面，都顯著優於現有的技術。", "applications": ["**虛擬試衣間：** 顧客只需上傳一張照片，即可生成自己的3D模型，在線上試穿各種服裝，更真實地了解服裝的上身效果，減少退貨率。", "**遊戲角色客製化：** 玩家可以上傳自己的照片，快速生成個性化的遊戲角色，提升遊戲體驗的沉浸感和代入感。", "**社交媒體虛擬化身：** 用戶可以創建高度逼真的3D虛擬化身，用於社交媒體互動、虛擬會議和內容創作，打造更個性化的線上形象。"], "pitch": "AdaHuman 解決了目前3D人物生成技術在細節和動畫能力上的瓶頸，實現了從單張照片生成高度逼真且可動畫化的3D模型。這項技術在虛擬試穿、遊戲角色客製化、社交媒體虛擬化身等領域具有廣闊的應用前景。其商業價值體現在：一方面，可以提高企業的效率和用戶體驗，降低成本；另一方面，可以開創全新的商業模式，例如個性化定制服務。通過技術授權、平台服務、內容合作等多種方式，AdaHuman 有望在 rapidly growing 的元宇宙和數字娛樂市場中佔據領先地位，帶來巨大的投資回報。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-02T18:20:14.698827"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "人類在攝影時能直覺地在3D空間中構圖和安排場景。本研究提出GenSpace，一個新的基準測試和評估流程，旨在全面評估當前圖像生成模型在從文本或圖像提示創建圖像時的空間感知能力。現有評估方法難以捕捉到細微的空間錯誤。因此，本研究提出一個專用的評估流程和指標，利用多個視覺基礎模型重建3D場景幾何結構，從而提供更準確、更符合人類認知的空間保真度指標。研究結果表明，雖然AI模型能創建視覺上吸引人的圖像並遵循一般指令，但它們在物件放置、關係和測量等特定3D細節方面存在困難。研究總結了當前最先進圖像生成模型在空間感知方面的三個核心局限性：1) 物件透視理解，2) 以自我為中心-以世界為中心的轉換，3) 度量測量一致性，並強調了改進圖像生成空間智能的可能方向。", "applications": ["**虛擬試穿/家居設計：** 透過指定空間大小、物件擺放位置等條件，AI生成模型協助使用者預覽服裝或家具在實際環境中的效果，更精準地進行購買決策。", "**建築設計輔助：** 建築師可以輸入設計概念和空間約束，AI協助生成多種方案，並提供視覺化的3D模型，加速設計流程並激發靈感。", "**遊戲場景生成：** 開發者可利用AI生成具有空間一致性和合理性的遊戲場景，大幅降低美術設計成本並提高開發效率。"], "pitch": "GenSpace 研究揭示了目前AI圖像生成在空間感知方面的瓶頸，也同時指出了改進方向。基於此，我們能開發更具空間理解力的AI圖像生成引擎，在虛擬試穿、建築設計、遊戲開發等多個領域創造巨大商業價值。我們的競爭優勢在於，我們掌握了評估和優化AI空間能力的關鍵技術，可以打造更逼真、更符合用戶需求的應用產品。初期市場可以瞄準需要精準空間感知的行業，如房地產、設計等，後續擴展到更廣泛的內容創作市場。投資回報將來自軟體授權、雲服務、定制開發等多種商業模式。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-02T19:11:24.944371"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通往通用化神經符號學習之路應以基礎模型鋪就", "summary_zh": "神經符號學習旨在解決神經網路在複雜推理任務訓練上的挑戰，並提供可解釋性、可靠性和效率等優點。傳統的神經符號學習方法通常將神經模型與符號程式結合訓練，但面臨著嚴峻的挑戰，使其僅限於簡單的問題。另一方面，純神經基礎模型現在通過Prompting而非訓練來達到最先進的性能，但它們通常不可靠且缺乏可解釋性。透過符號程式補充基礎模型，我們稱之為神經符號Prompting，提供了一種將這些模型用於複雜推理任務的方法。這樣做引發了一個問題：在基礎模型的時代，作為神經符號學習一部分的專用模型訓練有什麼作用？為了探討這個問題，我們重點介紹了傳統神經符號學習在計算、數據和程式方面導致泛化問題的三個陷阱。這篇立場論文認為，基礎模型能夠實現通用化的神經符號解決方案，為實現神經符號學習的最初目標提供了一條途徑，而無需從頭開始訓練的缺點。", "applications": ["**智慧醫療診斷輔助：** 利用基礎模型理解病歷，再結合符號程式進行推理，提供更精確、可解釋的診斷建議，降低誤診率。", "**自動化程式碼生成與修復：** 基礎模型學習程式碼邏輯，符號程式確保程式碼符合規範和安全標準，加速軟體開發並減少錯誤。", "**金融風險評估：** 基礎模型分析市場數據，符號程式根據既定規則進行風險評估和投資策略制定，提升決策效率和精準度。"], "pitch": "在AI的寒武紀大爆發時代，基礎模型展現了驚人的潛力，但也存在可靠性與可解釋性的瓶頸。我們的神經符號 Prompting方案，如同為基礎模型注入了邏輯的靈魂，使其不僅能處理複雜推理，還能保證結果的可追溯性。想像一下，一個能解釋決策過程的AI，在醫療、金融、法律等高風險領域的應用潛力無限。我們正在打造下一代AI引擎，其核心優勢是可解釋性、可靠性和效率，這將徹底改變企業決策模式，並創造巨大的商業價值。投資我們的方案，就是投資未來AI的基石。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-02T19:11:40.996566"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：利用組合式多視角擴散生成可動畫的細緻 3D 人物", "summary_zh": "AdaHuman 是一個創新的框架，能從單張照片生成高擬真、可動畫的 3D 人物模型。它結合了兩個關鍵技術：一、一個姿態條件式 3D 聯合擴散模型，能在任意姿態下合成一致的多視角影像，並在擴散過程中重建對應的 3D 高斯散點 (3DGS)；二、一個組合式的 3DGS 精煉模組，透過圖像到圖像的精煉來增強局部身體部位的細節，並使用一個新穎的、感知裁剪的相機光線地圖將它們無縫集成，產生一個連貫且細緻的 3D 人物模型。這個方法能產生高度逼真的標準 A 字姿勢人物模型，減少自我遮擋，方便後續的骨骼綁定和動畫製作。在公開基準測試和真實照片上的廣泛評估表明，AdaHuman 在人物重建和重新擺姿勢方面都顯著優於現有技術。", "applications": ["**虛擬試衣間：** 用戶上傳一張自拍照，就能看到自己穿著不同服裝的 3D 模型，並模擬行走、跳躍等動作，更直觀地了解服裝的穿著效果。", "**客製化遊戲角色：** 玩家上傳自己的照片，就能快速生成一個與自己相似的遊戲角色，並根據自己的喜好進行細節調整，增加遊戲的沉浸感和個性化體驗。", "**沉浸式遠程協作：** 在虛擬會議中，參與者可以將自己轉換為高度逼真的 3D 化身，更自然地進行互動和交流，提升遠程協作的效率和臨場感。"], "pitch": "AdaHuman 打破了從單張照片生成高品質、可動畫 3D 人物的技術瓶頸。其卓越的細節還原能力和動畫適應性，使其在虛擬形象、遊戲、電商、社交媒體等多個領域具有巨大的商業潛力。我們正在構建一個基於 AdaHuman 的 SaaS 平台，提供用戶友好的 3D 頭像生成和定制服務。這個平台將通過訂閱模式或按次使用模式，為個人用戶、企業和開發者提供價值。我們的目標是成為下一代 3D 頭像生成和動畫解決方案的領導者，抓住市場對高品質、可定制 3D 內容日益增長的需求。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-02T19:11:57.643612"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "人類可以直覺地在3D空間中構圖和安排場景以進行攝影。然而，先進的AI圖像生成器在根據文本或圖像提示創建圖像時，是否也能以類似的3D空間感知能力來規劃場景？ 我們提出了 GenSpace，這是一個新的基準測試和評估流程，旨在全面評估當前圖像生成模型的空間感知能力。此外，使用通用視覺語言模型（VLM）的標準評估通常無法捕捉到詳細的空間錯誤。為了應對這一挑戰，我們提出了一個專門的評估流程和指標，該流程使用多個視覺基礎模型重建3D場景幾何形狀，並提供更準確且更符合人類直覺的空間忠實度指標。我們的研究結果表明，雖然AI模型可以創建視覺上吸引人的圖像並可以遵循一般指令，但它們在物體放置、關係和測量等特定3D細節方面存在困難。我們總結了當前最先進的圖像生成模型在空間感知方面的三個核心限制：1)物體透視理解，2)自我中心-環境中心轉換，以及3)度量測量依從性，突顯了改進圖像生成中空間智能的可能方向。", "applications": ["**室內設計預覽：** 用戶輸入文字描述，例如「現代簡約風格客廳，落地窗前有一張灰色沙發，牆上掛著一幅抽象畫」，AI生成器可以生成多個符合描述且空間配置合理的室內設計方案供用戶參考。", "**虛擬導覽創造：** 根據旅遊景點的文字描述，AI生成器創建逼真的3D場景，讓用戶可以在虛擬環境中體驗未曾去過的地方，並提前規劃行程。", "**遊戲開發輔助：** 遊戲開發者可以使用該技術快速生成遊戲場景的初始原型，例如「一個茂密的森林，裡面有一條小溪和一座古老的橋」，並根據生成結果進行修改和完善。"], "pitch": "GenSpace解決了AI圖像生成中空間理解的關鍵瓶頸，為該領域提供了一個客觀的評估標準。我們創建了一個全新的評估流程和指標，能更準確地衡量AI在3D空間中的表現。想像一下，只需簡單的文字描述，就能生成高度真實且空間佈局合理的圖像，這將極大地簡化設計、內容創作和遊戲開發流程。 我們的商業價值體現在：第一，它能加速3D內容的創建，降低成本。第二，通過提升用戶體驗，增強產品的競爭力。第三，為相關產業，如室內設計、遊戲開發、虛擬旅遊等，提供新的商業模式。我們相信GenSpace能引領AI圖像生成進入一個全新的空間智能時代，成為相關領域的重要基礎設施，並帶來巨大的商業回報。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-02T20:16:08.174150"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通用神經符號學習之路應由基礎模型鋪就", "summary_zh": "神經符號學習旨在解決訓練神經網路執行複雜推理任務的挑戰，並提供可解釋性、可靠性和效率等優勢。傳統的神經符號學習方法將神經模型與符號程式共同訓練，但面臨著嚴峻的挑戰，使其僅限於解決簡單的問題。另一方面，純神經基礎模型現在通過提示而非訓練達到最先進的性能，但它們往往不可靠且缺乏可解釋性。使用符號程式補充基礎模型，我們稱之為神經符號提示，提供了一種將這些模型用於複雜推理任務的方法。這樣做引發了一個問題：在基礎模型的時代，作為神經符號學習一部分的專門模型訓練有什麼作用？為了探討這個問題，我們重點介紹了傳統神經符號學習在計算、數據和程式方面導致泛化問題的三個缺陷。這篇立場論文認為，基礎模型能夠實現可泛化的神經符號解決方案，從而提供了一條實現神經符號學習最初目標的途徑，而無需從頭開始訓練的缺點。", "applications": ["**醫療診斷輔助系統:** 利用基礎模型理解病歷文本，結合符號推理進行疾病診斷和治療方案建議，提高診斷準確性和效率。", "**自動化程式碼除錯與生成:** 基礎模型理解程式碼語意，結合符號推理進行程式碼錯誤分析和自動修復，或者根據需求自動生成程式碼。", "**智能客戶服務:** 基礎模型理解客戶問題，結合符號推理找到相關知識庫和解決方案，提供更精準和高效的客戶服務。"], "pitch": "傳統神經符號學習瓶頸已破，基礎模型時代迎來新機遇！我們基於神經符號提示技術，將大型語言模型的強大理解能力與符號推理的精確性和可解釋性完美結合，打造可泛化、可靠的AI解決方案。想像一下，一個能像人類專家一樣推理的AI系統，不僅能理解複雜情境，還能提供清晰透明的決策過程。這將顛覆醫療、金融、自動化等多個行業。我們的商業模式將基於授權、定制化解決方案和SaaS服務，搶佔先機，打造下一代AI引擎。 投資我們，您將參與重塑AI的未來，分享千億美元市場的紅利！", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-02T20:16:21.808641"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：基於組合式多視角擴散的可動畫化精細3D人體生成", "summary_zh": "AdaHuman 是一種新型框架，可以僅從單張真實照片生成高保真、可動畫的 3D 頭像。它包含兩項關鍵創新：首先，一個姿態條件化的 3D 聯合擴散模型，可以合成任意姿勢下的連貫多視圖圖像，並在每個擴散步驟中重建相應的 3D 高斯 Splats（3DGS）；其次，一個組合式的 3DGS 細化模塊，通過圖像到圖像的細化來增強局部身體部位的細節，並使用一種新型的裁剪感知相機光線圖將它們無縫整合，從而產生一個連貫且精細的 3D 頭像。這種方法可以生成高度逼真、標準 A 姿勢的頭像，最大限度地減少了自我遮擋，可以通過任何輸入動作進行裝配和動畫處理。 實驗表明，AdaHuman 在頭像重建和姿勢調整方面均顯著優於現有方法。", "applications": ["**個人化虛擬化身：** 用戶可以使用手機照片快速創建自己的 3D 頭像，用於虛擬會議、遊戲或社交媒體平台，甚至可以將自己「放入」VR/AR 環境中。", "**服裝試穿應用：** 網購服裝時，用戶可以創建自己的 3D 身體模型，在虛擬環境中試穿衣服，提高網購服裝的合身度和滿意度。", "**動畫製作簡化：** 動畫師可以使用該技術快速生成動畫角色，無需耗時的手動建模和綁定，大幅縮短動畫製作週期，降低製作成本。"], "pitch": "AdaHuman 代表了 3D 頭像生成領域的重大突破，解决了现有方法难以生成高细节、可动画的 3D 人体模型的难题。它的核心优势在于单张照片输入、快速生成、高保真度和易于动画化。这使其在多个领域具有巨大的商业潜力，包括游戏、社交媒体、电商、虚拟现实和远程协作。想象一下，每个人都可以拥有一个高度逼真的数字分身，用于在线互动和各种虚拟体验。我们相信 AdaHuman 有潜力成为构建元宇宙和下一代人机交互的关键技术，并带来巨大的经济价值和用户增长。我们正在寻求战略投资者，共同将这项技术推向市场，改变人们在线互动和表达自我的方式。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-02T20:16:36.405133"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "人類在攝影時能直觀地在3D空間中構圖和安排場景。那麼，先進的AI圖像生成器在根據文本或圖像提示創建圖像時，能否像人類一樣規劃具有類似3D空間感知能力的場景？我們提出了GenSpace，一個創新的基準測試和評估流程，用於全面評估當前圖像生成模型的空間感知能力。此外，使用通用視覺語言模型（VLM）的標準評估經常無法捕捉到詳細的空間錯誤。為了應對這個挑戰，我們提出了一個專門的評估流程和指標，該流程使用多個視覺基礎模型重建3D場景幾何結構，並提供更準確、更符合人類的空間保真度指標。我們的研究結果表明，雖然AI模型可以創建視覺上吸引人的圖像，並且可以遵循一般指令，但它們在物體放置、關係和測量等特定3D細節方面存在困難。我們總結了當前最先進的圖像生成模型在空間感知方面的三個核心限制：1) 物體透視理解，2) 自我中心-異體中心變換，以及3) 度量測量依從性，突出了改進圖像生成中的空間智能的可能方向。", "applications": ["**室內設計輔助：** 讓使用者輸入想要的房間描述（例如：「簡約風客廳，左邊有落地窗，窗邊放盆栽」），AI生成不同視角的3D空間圖像，輔助設計師快速產生草稿，也讓使用者更直觀地看到設計方案。", "**虛擬導覽生成：** 根據景點描述文字，自動生成逼真的3D虛擬導覽影片，讓使用者在線上就能身歷其境地體驗旅遊景點。", "**遊戲場景快速原型設計：** 遊戲開發者可以透過輸入文字描述快速生成遊戲場景的原型，節省建模時間，並快速迭代不同的設計方案。"], "pitch": "GenSpace 是一個關鍵的圖像生成技術發展指標，揭示了現有AI模型在空間感知方面的不足，這意味著現有技術在一些高精度要求的應用場景中仍有局限性。我們提供的基準測試和評估流程，能夠幫助開發者更精準地了解模型的優缺點，並針對性地進行優化。未來，空間感知能力更強的圖像生成技術，將會在室內設計、虛擬實境、遊戲開發、廣告設計等領域帶來顛覆性的變革，創造巨大的商業價值。想像一下，能夠根據客戶的具體需求，自動生成高度逼真且空間關係準確的產品廣告圖片，或是在元宇宙中構建出完全符合使用者需求的個性化空間，這些都是巨大的市場機會。因此，對GenSpace的研究和基於此的技術開發，具有極高的投資價值。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-02T21:14:19.911738"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通往可泛化的神經符號學習之路，應由基石模型鋪設", "summary_zh": "神經符號學習旨在解決訓練神經網路以執行複雜推理任務時遇到的挑戰，並增加可解釋性、可靠性和效率。傳統的神經符號學習方法將神經模型與符號程式結合訓練，但面臨重大挑戰，使其僅限於簡單的問題。另一方面，純神經基石模型現在透過提示而非訓練達到最先進的性能，但它們通常不可靠且缺乏可解釋性。用符號程式補充基石模型（我們稱之為神經符號提示）提供了一種使用這些模型執行複雜推理任務的方法。這引出了一個問題：在基石模型的時代，作為神經符號學習一部分的專門模型訓練有什麼作用？為了探討這個問題，我們強調了傳統神經符號學習在計算、數據和程式方面導致泛化問題的三個陷阱。這篇立場論文認為，基石模型可以實現可泛化的神經符號解決方案，從而提供了一條在不從頭開始訓練的情況下，實現神經符號學習最初目標的途徑。", "applications": ["**醫療診斷輔助系統：** 利用基石模型理解醫學文本和患者數據，再用符號推理規則進行疾病診斷和治療方案推薦，提高診斷準確性和效率，並提供可解釋的推理過程。", "**金融風險評估：** 使用基石模型分析市場趨勢和公司財報，結合符號規則進行風險評估和投資建議，降低投資風險並提高收益。", "**法律文件審閱與合規檢查：** 利用基石模型理解法律條文和合同內容，結合符號規則進行合規性檢查和風險評估，大幅提高審閱效率並降低法律風險。"], "pitch": "傳統神經符號學習受限於數據和算力，難以泛化。現在，我們將基石模型與符號推理結合，打造更強大、可解釋、可靠的AI。我們的神經符號提示技術，能賦予基石模型複雜推理能力，開闢醫療、金融、法律等高價值領域的應用機會。想像一下，一個能診斷罕見疾病、精準預測市場風險、高效審閱法律文件的AI系統，將大幅提升效率、降低成本，並創造巨大的商業價值。這是一個顛覆性的機會，我們正在打造下一代AI的基石。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-02T21:14:39.924092"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：基於組合式多視角擴散的可動畫化細節3D人體生成", "summary_zh": "AdaHuman 是一個新穎的框架，能夠從單張真實環境照片生成高保真、可動畫化的 3D 頭像。它結合了兩個關鍵創新：(1) 一個姿態條件式的 3D 關節擴散模型，能在任意姿態下合成一致的多視角圖像，並在每個擴散步驟重建對應的 3D 高斯球 (3DGS)；(2) 一個組合式 3DGS 細化模組，通過圖生圖細化增強局部身體部位的細節，並使用新穎的裁剪感知相機光線圖無縫地整合它們，產生一個有凝聚力的細緻 3D 頭像。這些組件使 AdaHuman 能夠生成高度真實的標準 A 字形姿勢頭像，且自遮擋最小，從而可以使用任何輸入運動進行綁定和動畫製作。在公開基準和真實環境圖像上的廣泛評估表明，AdaHuman 在頭像重建和姿勢調整方面都顯著優於最先進的方法。", "applications": ["**虛擬試衣間：** 使用者上傳一張照片，即可生成逼真的 3D 頭像，模擬穿著不同服裝的效果，提供更直觀的線上購物體驗。", "**遊戲角色客製化：** 玩家可以上傳自己的照片，快速生成與自己相似的遊戲角色，增加遊戲的沉浸感和個性化體驗。", "**虛擬社交化身：** 在元宇宙或虛擬社交平台中，使用者可以使用 AdaHuman 創建逼真且可動畫化的化身，進行更自然的交流互動。"], "pitch": "AdaHuman 解決了從單張照片生成高質量、可動畫 3D 頭像的關鍵問題。其核心技術，即基於組合式多視角擴散模型的頭像生成，在頭像重建和姿勢調整方面具有顯著優勢，遠超目前市面上的解決方案。我們看到在電商、遊戲和元宇宙領域的巨大應用潛力，尤其是在虛擬試穿、個性化遊戲角色創建和沉浸式虛擬社交方面。隨著元宇宙的發展，人們對高品質、可定制化身的需求將會爆炸式增長。 AdaHuman 不僅能夠滿足這種需求，更可能成為未來虛擬身份的核心技術基礎，具有極高的商業價值和投資回報。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-02T21:14:59.831772"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "這篇論文提出了 GenSpace，一個新的基準測試和評估流程，用於全面評估當前圖像生成模型的空間感知能力。研究發現，雖然AI模型可以產生視覺上吸引人的圖像並遵循一般指令，但在物體放置、關係和測量等具體的3D細節方面存在困難。論文總結了當前最先進圖像生成模型在空間感知方面的三個核心局限：1)物體透視理解，2)自我中心-環境中心轉換，3)度量測量遵循，並強調了改進圖像生成空間智能的可能方向。", "applications": ["**室內設計虛擬佈置：** 讓用戶可以透過文字描述或圖片，利用AI生成不同家具擺設的空間模擬圖，方便用戶在實際購買前預覽效果，並確保家具尺寸比例符合空間。", "**遊戲場景設計輔助：** 遊戲設計師可以利用AI快速生成場景概念圖，並調整物件位置和大小，以探索不同的場景佈局，加速遊戲開發流程。", "**AR/VR空間規劃：** 將真實世界掃描成3D模型後，使用者可以透過文字描述或圖片，指示AI在空間中放置虛擬物件，進行AR/VR體驗的空間規劃和設計，例如模擬博物館展覽佈置。"], "pitch": "GenSpace的研究揭示了當前圖像生成模型在空間感知上的不足，這代表著市場上存在巨大的機會。我們將利用GenSpace基準測試的洞察，開發更精準的空間感知AI模型，專注於解決諸如室內設計、遊戲開發和AR/VR空間規劃等應用場景的痛點。我們的優勢在於，我們不僅能生成視覺上吸引人的圖像，更重要的是，我們能確保圖像在空間上的真實性和一致性，避免AI生成的不協調或不合理的場景。透過垂直整合GenSpace評估流程和改進後的生成模型，我們將能提供更具商業價值的解決方案，重新定義空間設計和視覺化體驗，並在快速成長的元宇宙和數位內容創作市場中佔據領先地位。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-02T22:13:38.141323"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通往可泛化神經符號學習之路應以基礎模型鋪就", "summary_zh": "神經符號學習旨在解決訓練神經網路進行複雜推理任務時遇到的挑戰，並帶來可解釋性、可靠性和效率等優勢。傳統的神經符號學習方法通常將神經模型與符號程序結合訓練，但它們面臨著顯著的挑戰，使其僅限於簡單的問題。另一方面，純神經基礎模型現在透過提示而非訓練達到最先進的性能，但它們通常不可靠且缺乏可解釋性。透過符號程序補充基礎模型，我們稱之為神經符號提示，提供了一種使用這些模型進行複雜推理任務的方法。這樣做引發了一個問題：在基礎模型時代，作為神經符號學習一部分的專門模型訓練扮演什麼角色？為了探討這個問題，我們強調了傳統神經符號學習在計算、數據和程序方面導致泛化問題的三個陷阱。本文認為，基礎模型能夠實現可泛化的神經符號解決方案，從而提供了一條實現神經符號學習最初目標的途徑，而無需從頭開始訓練的缺點。", "applications": ["**自動化法律文件審查：** 結合法律條文（符號）與大型語言模型，自動分析合約、訴訟文件，找出潛在風險與不合規之處，大幅提升律師工作效率。", "**智慧醫療診斷輔助：** 將病患病歷數據（符號）結合影像辨識模型（基礎模型），協助醫生更快更準確地診斷疾病，提升醫療品質並減少誤診率。", "**客製化教育內容生成：** 根據學生的學習進度與偏好（符號），利用大型語言模型生成客製化的教材、練習題與學習計畫，提升學習成效與動機。"], "pitch": "傳統神經符號學習受限於複雜度，而純神經網路模型缺乏可解釋性。我們結合兩者優勢，利用基礎模型強大的預訓練能力，再輔以符號規則進行引導，打造更可靠、可解釋且泛用性更廣的AI解決方案。想像一下，一個可以自動審閱法律文件、輔助醫生診斷疾病，甚至為每個學生打造客製化學習計畫的AI。這不僅僅是技術升級，而是對AI應用方式的顛覆性變革，將為各行各業帶來巨大的效率提升與價值創造，具有極高的商業潛力。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-02T22:13:51.650912"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：具備可動畫細節的組合式多視角擴散 3D 人體生成", "summary_zh": "現有圖像到 3D 頭像生成方法難以產生高細節、可動畫的頭像，不適用於實際應用。AdaHuman 是一種新的框架，可以從單張真實世界圖像生成高保真、可動畫的 3D 頭像。它包含兩個關鍵創新：(1) 一個姿態條件的 3D 聯合擴散模型，可以在任意姿態下合成一致的多視角圖像，並在每個擴散步驟中進行對應的 3D 高斯飛濺 (3DGS) 重建；(2) 一個組合式 3DGS 完善模組，通過圖像到圖像的完善來增強局部身體部位的細節，並使用一種新的感知裁剪的相機光線圖無縫地整合它們，從而產生一個有凝聚力且細節豐富的 3D 頭像。這些組件使 AdaHuman 能夠生成高度逼真的標準 A-pose 頭像，具有最小的自我遮擋，從而可以使用任何輸入動作進行綁定和動畫製作。在公共基準測試和真實世界圖像上的廣泛評估表明，AdaHuman 在頭像重建和重新姿態方面顯著優於最先進的方法。代碼和模型將公開用於研究目的。", "applications": ["個人化虛擬試穿：使用者上傳一張照片，即可生成個人化的 3D 頭像，在線上商店試穿衣服，查看穿搭效果。", "遊戲角色客製化：遊戲開發者可以利用此技術，讓玩家上傳照片快速創建與自己相似的遊戲角色，增加遊戲沉浸感。", "社交媒體頭像生成：使用者上傳一張照片，即可生成逼真的 3D 頭像，用於社交媒體、元宇宙等虛擬環境中，提升個人品牌形象。"], "pitch": "AdaHuman 解決了圖像到 3D 頭像生成領域長期存在的痛點，即難以生成高細節、可動畫的頭像。其獨特的組合式多視角擴散技術，能夠從單張照片生成逼真且可高度客製化的 3D 頭像，具有廣泛的商業應用前景。試想一下，我們將能為電子商務平台提供革命性的虛擬試穿體驗，大幅降低退貨率；為遊戲公司提供前所未有的角色客製化選項，提升玩家參與度；甚至為社交媒體平台創造全新的用戶互動模式。AdaHuman 的技術壁壘高，市場潛力巨大，在娛樂、時尚、社交等多個領域都有機會成為領先者，具備極高的投資價值。我們正在尋找具有遠見卓識的投資者，共同將 AdaHuman 打造成下一代 3D 頭像生成技術的領導者。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-02T22:14:06.133532"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "人類天生擅長在3D空間中構圖和安排攝影場景。但現在的AI圖像生成器，在根據文字或圖像提示生成圖像時，也能擁有相似的3D空間感知能力嗎？我們提出了GenSpace，一個全新的基準測試和評估流程，旨在全面評估現有圖像生成模型的空間感知能力。由於通用視覺語言模型（VLMs）的標準評估經常無法捕捉到細微的空間錯誤，我們提出了一個專門的評估流程和指標，利用多個視覺基礎模型重建3D場景幾何結構，從而提供更準確、更符合人類直覺的空間保真度指標。研究結果表明，雖然AI模型可以生成視覺上吸引人的圖像，並遵循一般指令，但在物體放置、關係和尺寸等具體的3D細節上表現不佳。我們總結了目前最先進圖像生成模型在空間感知方面的三個核心局限性：1) 物體透視理解，2) 以自我為中心的分配轉換，以及 3) 度量測量依從性，並突出了改進圖像生成空間智能的可能方向。", "applications": ["**虛擬室內設計預覽：** 用戶提供房屋描述和偏好，AI生成帶有家具擺放的3D預覽圖，幫助用戶快速了解裝修效果，避免實際擺放的空間問題。", "**遊戲場景設計輔助：** 遊戲開發者輸入文字描述，AI生成包含正確透視和物件比例的遊戲場景草圖，加速遊戲地圖設計流程。", "**建築規劃視覺化：** 建築師輸入建築設計參數，AI生成逼真的3D模型，讓客戶更直觀地了解建築的空間布局和外觀，提升溝通效率。"], "pitch": "GenSpace提供了一個關鍵的圖像生成評估框架，揭示了現有AI在空間理解上的局限性，預示著更精確、更可控的圖像生成技術的巨大潛力。我們正在構建一個平台，解決AI在生成圖像時空間感知不足的問題，這對於許多行業至關重要。試想一下，一個設計師只需輸入簡單的描述，就能立即獲得符合透視和空間比例的3D模型，節省大量時間和成本。我們提供的解決方案將從根本上改變設計、建築、遊戲開發等行業的工作流程，創造一個價值數十億美元的市場。我們的核心優勢在於獨特的評估方法和對空間理解的深刻理解，我們將通過技術授權、SaaS平台和API集成等多種方式實現商業化，期待您的投資。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-02T23:13:10.283310"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通往可泛化神經符號學習之路，應以基石模型鋪就", "summary_zh": "神經符號學習旨在解決神經網路在複雜推理任務訓練中的挑戰，並提供可解釋性、可靠性和效率等優勢。傳統的神經符號學習方法將神經模型與符號程式結合訓練，但面臨限制其應用於簡單問題的挑戰。另一方面，純神經基石模型現在透過提示而非訓練達到最先進的效能，但它們往往不可靠且缺乏可解釋性。我們提出「神經符號提示」，透過符號程式補充基石模型，為利用這些模型進行複雜推理任務提供了一種途徑。 這引發了一個問題：在基石模型的時代，作為神經符號學習一部分的專用模型訓練有什麼作用？為了探討這個問題，我們重點介紹了傳統神經符號學習在計算、資料和程式方面導致泛化問題的三個陷阱。這篇立場論文認為，基石模型能夠實現可泛化的神經符號解決方案，為實現神經符號學習的原始目標提供了一條途徑，且避免了從頭開始訓練的缺點。", "applications": ["**法律文件審閱：** 利用基石模型理解法律術語，並透過符號邏輯進行合規性檢查，自動識別潛在風險。", "**醫療診斷輔助：** 基石模型學習醫學知識，結合患者症狀和檢查結果（符號數據），推理並提供診斷建議，輔助醫生進行決策。", "**自動化程式碼生成與驗證：** 基石模型基於自然語言描述生成程式碼片段，再利用符號驗證工具檢查程式碼的正確性和安全性。"], "pitch": "想像一下，擁有一種既能像人類一樣理解複雜概念，又能像電腦一樣精確推理的AI。我們的神經符號提示技術，巧妙地結合了基石模型強大的語言理解能力和符號程式嚴謹的邏輯推理能力，突破了傳統AI的瓶頸。我們將從法律、醫療、軟體開發等高價值領域切入，解決以往AI無法處理的複雜問題，大幅提高效率、降低成本、並降低錯誤率。這不僅是一個技術突破，更是一個巨大的市場機會，我們有信心成為神經符號AI領域的領導者，為各行各業帶來變革性的影響，並創造巨大的投資回報。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-02T23:13:27.847077"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：透過組合式多視角擴散生成可動畫化的精細3D人像", "summary_zh": "現有的圖像到3D人像生成方法難以產生細節豐富、適合動畫且能應用於真實世界的頭像。我們推出 AdaHuman，一個創新的框架，可以從單張自然圖像生成高保真、可動畫化的 3D 人像。AdaHuman 包含兩項關鍵創新：(1) 一個姿態條件 3D 關節擴散模型，它能合成任意姿態下一致的多視角圖像，同時在每個擴散步驟重建對應的 3D Gaussian Splats (3DGS)；(2) 一個組合式 3DGS 細化模組，透過圖像到圖像的細化來增強局部身體部位的細節，並使用一種新穎的、感知裁剪的相機光線圖無縫地整合它們，從而產生一個有凝聚力的精細 3D 人像。這些組件使 AdaHuman 能夠生成高度逼真的標準 A 姿勢人像，且具有最小的自我遮擋，從而能夠使用任何輸入動作進行綁定和動畫製作。在公開基準和自然圖像上的廣泛評估表明，AdaHuman 在人像重建和重新定位方面顯著優於最先進的方法。代碼和模型將公開供研究之用。", "applications": ["**個人化虛擬化身:** 用戶可以上傳一張照片，快速生成可用於視訊會議、遊戲和社交媒體的個人化3D虛擬化身。", "**服裝試穿:** 服裝品牌可以利用此技術讓顧客上傳照片，生成3D試穿模型，無需實際穿戴即可預覽服裝效果，提升網購體驗。", "**動畫角色製作:** 動畫工作室或個人創作者可以使用此技術快速生成具有特定外貌和姿態的角色，節省建模時間，提高製作效率。"], "pitch": "AdaHuman 解決了現有3D人像生成技術精度不足、動畫效果不佳的痛點，提供從單張照片生成高保真、可動畫3D人像的創新解決方案。其核心優勢在於多視角擴散和局部細化，保證了人像細節和動作的真實性。商業潛力巨大，可廣泛應用於個人化虛擬化身、電商服裝試穿、遊戲和動畫角色製作等領域。通過API授權、訂閱服務或技術轉讓，可以快速實現商業化。初期可以瞄準遊戲公司、社交平台和電商平台等B端客戶，長期來看，C端市場的潛力也十分可觀。考慮到元宇宙的發展趨勢，AdaHuman有望成為下一代人像生成技術的領先者，具有極高的投資價值。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-02T23:13:49.122185"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "本研究提出GenSpace，一個新的基準測試和評估流程，旨在全面評估目前圖像生成模型在空間感知方面的能力。研究發現，雖然AI模型能生成視覺上吸引人的圖像並遵循一般指令，但在物件放置、關係和測量等具體3D細節上表現不佳。主要限制包括物件透視理解、自我中心-世界中心轉換，以及度量測量遵循。GenSpace旨在幫助改善圖像生成中的空間智能。", "applications": ["**室內設計預覽：** 讓使用者輸入房間描述，AI能生成不同家具擺設的3D空間圖像，幫助他們選擇最佳方案。", "**遊戲關卡設計：** 遊戲設計師可以輸入場景描述，AI快速生成初步的關卡佈局，節省大量手動調整的時間。", "**建築規劃視覺化：** 建築師或開發商輸入建築描述和周圍環境，AI生成逼真的建築融入場景的模擬圖，用於宣傳或審批。"], "pitch": "GenSpace研究揭示了目前AI圖像生成在空間理解上的局限性，但同時也指出了巨大的商業潛力。想像一下，我們能提供更精準、更可控的3D空間生成引擎，應用於室內設計、遊戲開發、建築規劃等領域。透過授權我們的技術或提供基於GenSpace技術的雲端服務，我們可以賦能各行業，大幅提升生產力，創造新的視覺體驗。早期投資者將有機會參與塑造下一代空間感知AI的發展，在快速成長的生成式AI市場中佔據領先地位。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-03T01:06:37.095674"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "邁向通用神經符號學習之路應由基礎模型鋪就", "summary_zh": "神經符號學習旨在解決訓練神經網路處理複雜推理任務時的挑戰，並提供可解釋性、可靠性和效率等優勢。傳統的神經符號學習方法通常結合訓練神經模型和符號程式，但面臨重大挑戰，使其僅限於簡單問題。另一方面，純神經基礎模型現在透過提示（Prompting）而非訓練達到最先進的性能，但它們通常不可靠且缺乏可解釋性。本文提出利用符號程式補充基礎模型，稱為神經符號提示（Neuro-Symbolic Prompting），為這些模型用於複雜推理任務提供了一種途徑。本文認為，在基礎模型的時代，作為神經符號學習一部分的專門模型訓練有什麼作用？為了探討這個問題，我們強調了傳統神經符號學習在計算、數據和程式方面導致泛化問題的三個陷阱。本立場論文認為，基礎模型能夠實現通用的神經符號解決方案，提供了一條實現神經符號學習最初目標的途徑，而無需從頭開始訓練的缺點。", "applications": ["**智慧客服與法律諮詢：** 利用基礎模型理解客戶或當事人提出的問題，並透過神經符號提示，連結到法律知識庫或相關法條，給予更精確且可解釋的答案，提升服務品質與效率。", "**程式碼自動生成與錯誤偵測：** 基礎模型可根據自然語言描述或需求，生成初步的程式碼，並透過神經符號提示，利用形式驗證工具或規則檢查，找出潛在的錯誤或安全漏洞，加速開發流程並提高程式碼品質。", "**醫療診斷輔助系統：** 基礎模型可以分析病患的症狀描述和病歷資料，並透過神經符號提示，結合醫學知識庫和診斷規則，提供更精確的診斷建議和治療方案，協助醫生做出更明智的決策。"], "pitch": "我們正在構建下一代 AI 推理引擎，它結合了基礎模型的強大泛化能力和符號程式的可解釋性。傳統的 AI 模型在複雜推理任務中表現不佳，且難以理解其決策過程。我們的神經符號提示技術，利用基礎模型理解問題，再透過符號程式進行邏輯推理，最終提供可靠且可解釋的答案。這項技術的商業價值巨大，涵蓋醫療、金融、法律等各個領域，可以大幅提升工作效率、降低錯誤率，並提供更安全可靠的 AI 解決方案。我們團隊擁有頂尖的 AI 研究背景和豐富的工程經驗，我們相信可以將這項技術成功商業化，成為 AI 領域的領頭羊。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-03T01:07:01.543325"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：透過組合式多視角擴散生成可動畫化的高細節3D人體", "summary_zh": "現有從圖像生成3D頭像的方法難以產生高細節且可動畫化的頭像，不適用於實際應用。我們提出了AdaHuman，一個從單張真實圖像生成高保真可動畫3D頭像的新框架。AdaHuman包含兩項關鍵創新：（1）一個姿態條件化的3D聯合擴散模型，它在每個擴散步驟中合成任意姿態的一致性多視角圖像，並同時進行對應的3D高斯潑濺（3DGS）重建；（2）一個組合式3DGS精煉模組，透過圖像到圖像的精煉增強局部身體部位的細節，並使用一種新的裁剪感知相機光線圖無縫地整合它們，從而產生一個連貫的高細節3D頭像。這些組件使AdaHuman能夠生成高度逼真的標準A姿勢頭像，具有最小的自我遮擋，可以使用任何輸入動作進行綁定和動畫處理。在公共基準測試和真實圖像上的廣泛評估表明，AdaHuman在頭像重建和重新定位方面顯著優於最先進的方法。代碼和模型將公開提供用於研究目的。", "applications": ["虛擬試衣間：消費者只需上傳一張照片，就能看到自己穿著不同服裝的3D模型，並模擬各種動作，提高線上購物的體驗。", "遊戲角色客製化：玩家可以上傳自己的照片，快速生成遊戲中的3D角色，並根據喜好進行細節調整，增加遊戲的沉浸感和個性化。", "遠程醫療復健：醫生可以利用患者的照片創建3D模型，並根據患者的康復進度模擬運動，提供個性化的復健方案和指導，提升復健效果。"], "pitch": "AdaHuman 解決了 3D 頭像生成領域的一個關鍵痛點：高品質、可動畫化的頭像製作成本高昂且耗時。透過我們的創新技術，我們能夠從單張圖像快速生成高細節、易於動畫化的 3D 模型，大幅降低了製作成本和時間。想像一下，遊戲公司不再需要聘請大量建模師，電商平台可以為每個用戶提供個性化的虛擬試衣體驗。 AdaHuman 的商業潛力巨大，我們正在尋找合作夥伴，共同將這項技術推向市場，應用於遊戲、電商、醫療等領域，開創 3D 頭像應用的新紀元。我們的競爭優勢在於高效、低成本和高質量，這將為我們帶來巨大的市場佔有率。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-03T01:07:24.865512"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "這篇論文介紹了一個名為GenSpace的新基準測試和評估流程，旨在評估當前圖像生成模型在根據文字或圖像提示生成圖像時的空間感知能力，也就是說，它們是否能像人類一樣，在3D空間中直觀地構圖和安排場景。 研究發現，雖然AI模型能生成視覺上吸引人的圖像，並能遵循一般指令，但在物件放置、關係和測量等具體的3D細節上表現不佳。 研究強調了當前最先進圖像生成模型在空間感知方面的三個核心限制：物件透視理解、以自我為中心的到以物體為中心的轉換，以及度量測量遵循，並指出了改進圖像生成空間智能的可能方向。", "applications": ["**虛擬室內設計：** 用戶只需輸入描述，AI就能生成符合空間尺寸和風格要求的室內設計方案，並能精確預覽傢俱擺放效果，大幅簡化設計流程。", "**遊戲開發：** 遊戲開發者可以快速生成具有真實空間關係的遊戲場景，例如，根據描述生成一個「充滿古代遺跡和危險陷阱的叢林」，AI能自動生成符合該描述，且空間邏輯合理的遊戲地圖。", "**廣告設計：** 廣告商可以根據產品描述和目標受眾，利用AI生成具有視覺衝擊力和空間感的廣告圖片，例如，推銷汽車時，AI能生成一輛汽車在特定環境（例如，險峻的山路）中行駛的逼真圖片，突顯汽車的性能。"], "pitch": "GenSpace的研究成果揭示了目前AI圖像生成技術在空間感知方面的局限性，但也同時指明了未來發展方向。 我們的商業價值體現在，我們可以利用這些研究成果，開發出更智能、更精確的空間感知圖像生成引擎。 這個引擎將能廣泛應用於室內設計、遊戲開發、廣告製作、教育訓練等領域，極大地提高效率和降低成本。 想像一下，設計師不再需要耗費大量時間手動調整3D模型，而是可以通過我們的AI引擎，快速生成符合需求的設計方案。 遊戲開發者可以輕鬆創建豐富且真實的遊戲世界。 廣告商可以製作更具吸引力且更有效的廣告素材。 我們的產品將會顛覆這些行業的工作流程，創造巨大的商業價值。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-03T03:12:36.641032"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通往可泛化神經符號學習的道路應由基礎模型鋪設", "summary_zh": "神經符號學習旨在解決訓練神經網絡進行複雜推理任務時遇到的挑戰，並提供可解釋性、可靠性和效率等額外優勢。傳統的神經符號學習方法將神經模型與符號程序結合訓練，但面臨重大挑戰，使其僅限於簡單問題。另一方面，純神經基礎模型現在通過提示而非訓練達到最先進的性能，但它們通常不可靠且缺乏可解釋性。通過符號程序補充基礎模型（我們稱之為神經符號提示）提供了一種使用這些模型進行複雜推理任務的方法。這樣做就引出了一個問題：在基礎模型的時代，作為神經符號學習一部分的專用模型訓練有什麼作用？為了探討這個問題，我們重點介紹了傳統神經符號學習在計算、數據和程序方面導致泛化問題的三個陷阱。這篇立場論文認為，基礎模型能夠實現可泛化的神經符號解決方案，從而提供了一條實現神經符號學習最初目標的途徑，而無需從頭開始訓練的缺點。", "applications": ["**智慧醫療診斷輔助:** 結合病歷資料（符號）與醫學影像分析（基礎模型），更準確地診斷疾病，並提供可解釋的診斷依據。", "**自動化合約審閱與生成:** 利用法律知識庫（符號）和自然語言處理模型（基礎模型）自動審閱合約，發現潛在風險，並根據需求自動生成合約條款。", "**智能客服系統:** 結合產品知識庫（符號）與大型語言模型（基礎模型），提供更準確、更具上下文理解力的客服回覆，有效解決客戶問題。"], "pitch": "我們正在開發一種革命性的神經符號學習平台，利用大型語言模型（LLM）等基礎模型，結合符號推理能力，解決傳統神經網絡在複雜推理、可解釋性上的瓶頸。我們的核心技術'神經符號提示'，無需從頭訓練模型，大幅降低了成本和時間。想像一下，一個能像專家一樣分析數據、進行推理並提供可解釋決策的人工智慧。我們的平台能廣泛應用於金融、醫療、法律等領域，顯著提升效率、降低風險。 我們認為我們的技術具有巨大的商業潛力，能顛覆目前的AI市場，成為下一代AI技術的領導者。 投資我們，您將參與構建未來AI的基石。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-03T03:12:53.004684"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：利用組合式多視角擴散生成可動畫的精細 3D 人物", "summary_zh": "AdaHuman 是一種新的框架，可以從單張照片生成高度逼真且可動畫的 3D 頭像。它利用姿態條件的 3D 聯合擴散模型，在任意姿態下合成一致的多視角圖像，並同步進行 3D 高斯 Splats (3DGS) 重建。接著，透過組合式 3DGS 修正模組，利用圖像到圖像的修正來增強局部身體部位的細節，並使用創新的裁剪感知相機光線圖無縫整合，產生連貫且細緻的 3D 頭像。這使得 AdaHuman 能夠生成高真實度的標準 A 姿勢頭像，並且可以輕鬆進行骨骼綁定和動畫。相較於現有方法，AdaHuman 在頭像重建和重新擺姿勢方面都有顯著提升。", "applications": ["**個性化虛擬試穿：**使用者上傳一張個人照片，即可生成 3D 頭像，用於線上試穿服裝，查看不同服裝在自己身上的效果，減少退換貨率。", "**遊戲角色客製化：**玩家上傳一張照片，就能快速創建與自己相似的遊戲角色，提升遊戲代入感和個性化體驗。", "**虛擬活動和社交：**使用者可以生成自己的 3D 頭像參與線上會議、虛擬演唱會或社交活動，提升互動體驗和真實感，並可透過表情與動作呈現更豐富的個人情感。"], "pitch": "AdaHuman 解決了目前 3D 頭像生成技術在細節和動畫方面的痛點。 我們開發了一種創新方法，可以從單張圖像創建高度逼真、可動畫的 3D 頭像，打破了虛擬角色創建的門檻。 想像一下，電商平台能讓顧客線上試穿服裝，大幅降低退貨率； 遊戲公司能讓玩家輕鬆打造個性化角色，提升玩家黏著度；元宇宙平台則能提供更真實的社交體驗。 AdaHuman 的商業價值體現在它能廣泛應用於各個領域，並帶來顯著的成本節約和用戶體驗提升。 我們擁有領先的技術和清晰的商業化路徑，目標是成為 3D 頭像生成領域的領導者。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-03T03:13:07.797263"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成能力基準測試", "summary_zh": "這篇論文提出了一個名為GenSpace的新基準測試和評估流程，旨在全面評估現有圖像生成模型在空間感知方面的能力。研究發現，雖然AI模型可以生成視覺上吸引人的圖像並遵循一般指令，但在物體放置、關係和測量等具體的3D細節上表現不佳。研究總結了目前最先進的圖像生成模型在空間感知方面的三個核心限制：1) 物體透視理解，2) 自我中心-客體中心轉換，以及 3) 度量測量遵守。這項研究為改進圖像生成中的空間智能提供了方向。", "applications": ["**虛擬室內設計：** 根據客戶需求，生成符合空間規劃和尺寸的室內設計圖，減少設計師和客戶溝通成本，並提供更直觀的視覺體驗。", "**遊戲場景設計：** AI自動生成具有空間邏輯的遊戲場景，例如根據描述創建符合物理規則的城市或迷宮，加速遊戲開發流程。", "**3D 物件擺放模擬：** 在電商平台上，讓用戶可以透過AR/VR功能，模擬家具或裝飾品在家中的擺放效果，提升購物體驗並減少退貨率。"], "pitch": "GenSpace benchmark揭示了圖像生成AI在空間理解上的不足，這是一個巨大的機會！ 我們可以開發基於GenSpace的空間感知增強模型，解決目前AI在物體擺放、透視關係和尺寸把握上的問題。 這項技術將賦能虛擬室內設計、遊戲開發、電商AR/VR等領域，讓用戶獲得更真實、更符合空間邏輯的圖像生成體驗。 我們提供的不僅僅是圖像，更是空間智能，這將徹底改變內容創作和交互方式，具有巨大的商業潛力，絕對是下一代AI圖像生成技術的關鍵突破點。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-03T04:21:22.587165"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通往泛化神經符號學習之路應由基礎模型鋪就", "summary_zh": "神經符號學習旨在解決訓練神經網路處理複雜推理任務時遇到的挑戰，並帶來可解釋性、可靠性和效率等額外優勢。傳統的神經符號學習方法將神經模型與符號程序結合訓練，但面臨重大挑戰，使其僅限於解決簡單問題。另一方面，純神經基礎模型現在透過提示而非訓練達到最先進的性能，但它們通常不可靠且缺乏可解釋性。我們提出一種稱為神經符號提示的方法，透過符號程序補充基礎模型，為將這些模型用於複雜推理任務提供了一種途徑。這引發了一個問題：在基礎模型的時代，作為神經符號學習一部分的專門模型訓練扮演什麼角色？為了探討這個問題，我們強調了傳統神經符號學習在計算、數據和程序方面導致泛化問題的三個陷阱。本立場論文認為，基礎模型能夠實現可泛化的神經符號解決方案，為實現神經符號學習的最初目標提供了途徑，而無需從頭開始訓練的缺點。", "applications": ["**智能客服升級：** 現有智能客服雖然能回答一般問題，但對於複雜的產品技術問題或跨部門協調，往往無法有效解決。透過結合基礎模型和符號推理，客服系統能更準確理解客戶需求，並利用內部的產品知識庫和規則進行推理，最終提供更精準的解決方案，提升客戶滿意度。", "**醫療診斷輔助：** 醫生可以利用結合基礎模型和醫學知識庫的神經符號系統，輸入病患的症狀、病史和檢查結果。系統能自動從大量醫學文獻中提取相關信息，並運用符號推理判斷可能的疾病，並提供相應的診斷和治療建議，輔助醫生做出更明智的決策。", "**自動化程式碼偵錯：** 開發者可以利用神經符號系統分析程式碼，系統透過基礎模型理解程式碼的意圖，然後運用符號推理找出潛在的錯誤和漏洞，並提供修改建議。這能大幅提升程式碼的品質和開發效率。"], "pitch": "各位投資人，我們正在開發一項革命性的技術，將徹底改變 AI 的應用方式。傳統神經網路在處理複雜推理任務時存在局限性，而我們的神經符號提示技術，巧妙地結合了基礎模型和符號推理的優勢。想像一下，AI不再是個黑盒子，而是能清晰地解釋它的決策過程，並且能更可靠地解決複雜問題。這項技術能廣泛應用於各個領域，包括智能客服、醫療診斷和自動化程式碼偵錯等，市場潛力巨大。我們相信，透過您的投資，我們可以將這項技術推向市場，打造一個更智能、更可靠的未來。我們的商業模式將基於SaaS服務，向企業客戶提供可定制化的神經符號提示解決方案，並收取訂閱費用。我們預計在三年內達到盈虧平衡，並在五年內實現指數級增長。感謝您的聆聽，期待與您攜手開創 AI 的新時代！", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-03T04:21:43.250602"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：基於組合式多視角擴散的可動畫化細緻3D人體生成", "summary_zh": "現有的從圖像生成3D頭像的方法難以產生適合真實世界應用的高度細緻且可動畫化的頭像。AdaHuman 是一個新穎的框架，可以從單張真實場景圖像生成高保真、可動畫化的3D頭像。AdaHuman 包含兩個關鍵創新：(1) 一個姿態條件3D聯合擴散模型，它在每個擴散步驟中合成任意姿態下一致的多視圖圖像，並同時進行對應的3D高斯潑濺 (3DGS) 重建；(2) 一個組合式3DGS細化模塊，通過圖像到圖像的細化來增強局部身體部位的細節，並使用一種新穎的裁剪感知相機光線圖將它們無縫集成，從而產生一個連貫的細緻3D頭像。這些組件使 AdaHuman 能夠生成高度逼真的標準 A 姿頭像，且具有最小的自遮擋，從而可以使用任何輸入運動進行綁定和動畫製作。在公共基準和真實場景圖像上的廣泛評估表明，AdaHuman 在頭像重建和重新擺姿勢方面顯著優於最先進的方法。代碼和模型將公開發布以供研究之用。", "applications": ["**虛擬試衣間：** 用戶上傳一張自己的照片，即可生成可動畫的3D頭像，在線上試穿不同款式的衣服，模擬真實穿著效果，減少退貨率。", "**遊戲角色客製化：** 遊戲玩家只需上傳一張自拍照，即可快速生成具有高度個人化的3D遊戲角色，不再受限於預設的角色模型。", "**個性化虛擬助手：** 將虛擬助手的外貌設計成用戶自己的形象，提供更親切、更具個人化的互動體驗，增強用戶黏性。"], "pitch": "AdaHuman解決了高品質3D人體模型快速生成的核心痛點，大幅降低了3D建模的成本和時間。想像一下，只需一張照片，就能擁有高度逼真且可動畫的數位化身。這在虛擬試穿、遊戲角色客製化、以及個性化虛擬助手等領域具有巨大的商業潛力。我們將持續優化模型，提升細節還原度，並探索更多應用場景，例如元宇宙的身份認證、以及影視製作中的快速角色原型設計。我們相信 AdaHuman 將重新定義3D人體模型的生成方式，並為相關產業帶來顛覆性變革。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-03T04:21:59.952062"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "人類在攝影時能直覺地在3D空間中構圖和安排場景。那麼，當AI圖像生成器從文本或圖像提示生成圖像時，它們能否以類似的3D空間感知能力來規劃場景呢？我們提出了GenSpace，一個新的基準測試和評估流程，旨在全面評估當前圖像生成模型的空間感知能力。此外，使用通用視覺語言模型(VLMs)進行的標準評估經常無法捕捉到詳細的空間錯誤。為了解決這個挑戰，我們提出了一個專門的評估流程和指標，它使用多個視覺基礎模型重建3D場景幾何，並提供更準確且與人類對齊的空間真實性度量。我們的研究結果表明，雖然AI模型可以創建具有視覺吸引力的圖像並遵循一般指令，但它們在諸如物體放置、關係和測量等具體的3D細節方面存在困難。我們總結了當前最先進的圖像生成模型在空間感知方面的三個核心局限性：1)物體透視理解，2)自我中心-他人中心轉換，以及3)度量測量依從性，突出了改進圖像生成中空間智能的可能方向。", "applications": ["室內設計：根據文字描述生成客廳、臥室等空間的3D渲染圖，方便使用者預覽裝修效果，並自動偵測設計是否符合空間尺寸規範，例如家具擺放是否合理。", "遊戲開發：快速生成具有特定空間佈局和物體擺放的遊戲場景，例如密室逃脫的房間、開放世界的城鎮，加速遊戲開發流程。", "輔助建築設計：設計師根據文字描述，例如「現代風格、帶有大片落地窗、能俯瞰海景的別墅」，快速生成多種方案，並檢測設計是否符合物理限制（例如，結構穩定性）。"], "pitch": "GenSpace 解決了當前 AI 圖像生成在空間理解上的關鍵瓶頸。我們開發的基準測試和評估工具，不僅能更精準地衡量 AI 模型在 3D 空間中的表現，更能引導 AI 在物件擺放、空間關係和尺寸精準度上的顯著提升。想像一下，未來 AI 不僅能生成美麗的圖像，更能創造出符合物理現實、擁有空間邏輯的虛擬世界。這在室內設計、遊戲開發、建築設計等領域擁有巨大潛力，大幅提升效率、降低成本，並帶來前所未有的設計自由度。我們的評估流程和指標，將成為相關產業的品質保證標準，擁有巨大的商業價值和市場前景。 我們能提供空間感知圖像生成的基石，從而解鎖新的應用和商業機會。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-03T05:15:10.241779"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通往通用型神經符號學習之路應以基石模型鋪設", "summary_zh": "神經符號學習旨在解決神經網路在複雜推理任務訓練上的挑戰，並提供可解釋性、可靠性和效率等優勢。傳統方法結合訓練神經模型和符號程式，但面臨使其僅限於簡單問題的重大挑戰。另一方面，純神經基石模型現在透過提示而非訓練達到最先進的性能，但它們通常不可靠且缺乏可解釋性。我們提出一種名為「神經符號提示」的方法，透過符號程式補充基石模型，為使用這些模型執行複雜推理任務提供了一種途徑。這引發了一個問題：在基石模型的時代，作為神經符號學習一部分的專門模型訓練扮演什麼角色？為了解決這個問題，我們重點介紹了傳統神經符號學習在計算、數據和程式方面導致泛化問題的三個陷阱。本立場論文認為，基石模型能夠實現可泛化的神經符號解決方案，從而提供了一條實現神經符號學習最初目標的途徑，而無需從頭開始訓練的缺點。", "applications": ["**自動化法律文件審閱：** 利用基石模型理解法律條款，並結合符號程式進行邏輯推理，例如判斷合約條款是否符合特定法律規定，提高審閱效率和準確性。", "**智能故障排除系統：** 使用基石模型分析設備的歷史數據和當前狀態，並透過符號程式建立故障診斷規則，快速找出故障原因並提供解決方案。", "**教育輔助系統：** 基石模型理解學生的問題，並利用符號程式生成逐步解題方案，提供個性化的學習指導，提高學習效率和理解深度。"], "pitch": "我們正在構建下一代AI大腦，結合基石模型的強大泛化能力與符號程式的精準推理能力，克服傳統AI的不可解釋性和可靠性問題。想像一下，一個能夠理解複雜情境、進行精確推理、且結果可驗證的AI，這將颠覆包括法律、金融、醫療等需要高度準確性和透明度的行業。我們的神經符號提示技術，讓AI不再是黑箱，而是可解釋、可控的智能助手，能大幅提升生產力，降低風險。現在投資，你將成為下一波AI浪潮的領跑者，佔據巨大的市場先機。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-03T05:15:27.211946"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：基於組合式多視角擴散的可動畫化細緻 3D 人物生成", "summary_zh": "現有的圖像到 3D 頭像生成方法難以產生高細節、可動畫化的頭像，無法滿足真實世界應用需求。我們提出了 AdaHuman，一個從單張真實圖片生成高保真可動畫化 3D 頭像的新框架。AdaHuman 包含兩大創新：(1) 一個姿態條件化的 3D 關節擴散模型，它能在任意姿態下合成一致的多視角圖像，並在每個擴散步驟中進行對應的 3D 高斯濺射 (3DGS) 重建；(2) 一個組合式的 3DGS 精煉模塊，通過圖像到圖像的精煉來增強局部身體部位的細節，並使用一種新的裁剪感知相機光線圖將它們無縫整合，產生一個連貫的細緻 3D 頭像。這些組件使 AdaHuman 能夠生成高度逼真、具有標準 A 字形姿態的頭像，且自遮擋最少，使其能夠使用任何輸入動作進行綁定和動畫製作。在公共基準測試和真實圖像上的大量評估表明，AdaHuman 在頭像重建和重新擺姿勢方面均顯著優於最先進的方法。代碼和模型將公開發布以供研究使用。", "applications": ["**線上虛擬試穿：** 讓消費者上傳一張自己的照片，就能在線上看到自己穿著不同服飾的 3D 效果，大幅提升購物體驗和降低退貨率。", "**客製化遊戲角色：** 玩家上傳照片即可快速生成高擬真度的遊戲角色，省去繁瑣的捏臉過程，打造獨一無二的遊戲體驗。", "**虛擬偶像生成與動畫製作：** 快速生成高質量的虛擬偶像，並輕鬆進行動畫製作，降低內容創作成本，加速元宇宙內容生態發展。"], "pitch": "AdaHuman 解決了圖像到 3D 頭像生成領域長期存在的難題：高細節、可動畫化的問題。我們的技術突破性地提升了頭像的真實感和可操作性，使其具備廣泛的商業應用潛力。從電商的虛擬試穿、遊戲的客製化角色，到元宇宙的虛擬偶像，AdaHuman 都能顯著提升用戶體驗、降低生產成本。我們預計未來市場對高質量 3D 頭像的需求將持續增長，AdaHuman 有望成為該領域的領頭羊，為投資者帶來豐厚的回報。我們不僅掌握了核心技術，還擁有清晰的商業化路徑，並計劃通過 SaaS 平台、授權合作等方式，將 AdaHuman 快速推向市場，搶佔先機。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-03T05:15:45.034517"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成能力基準測試", "summary_zh": "這篇論文介紹了GenSpace，一個新的基準測試和評估流程，旨在全面評估目前圖像生成模型在空間感知方面的能力。研究發現，雖然這些模型可以生成視覺上吸引人的圖像並遵循一般指令，但在處理具體的3D細節，例如物體放置、關係和測量方面存在困難。研究揭示了目前最先進的圖像生成模型在空間感知方面的三個核心限制：1) 物體透視理解；2) 自我中心-他者中心轉換；3) 測量標準一致性。研究結果為未來提升圖像生成中空間智能的方向提供了啟示。", "applications": ["**虛擬室內設計輔助：** 輸入房間照片和文字描述，AI可以協助調整家具擺設，但需確保物品之間的空間關係合理，例如桌子不能穿牆，椅子要方便使用。", "**遊戲場景生成與測試：** 遊戲開發者可以快速生成具有空間關係的遊戲場景，並利用GenSpace評估其空間合理性，避免出現玩家無法通過的BUG。", "**自動駕駛模擬環境生成：** 生成逼真的道路環境，包含車輛、行人、交通號誌等，並確保它們之間的距離、速度等符合真實世界的物理規則，用於訓練和測試自動駕駛系統。"], "pitch": "GenSpace 發現現有 AI 圖像生成在空間感知上的缺陷，這代表一個巨大的市場機會。想像一下，一個可以精準控制 3D 空間關係的圖像生成引擎，將能徹底改變設計、遊戲開發、自動駕駛模擬等產業。我們的基準測試與評估方法，不僅能協助 AI 模型開發者提升空間智能，更能成為各行各業驗證 AI 生成內容真實性的重要工具。我們計劃將 GenSpace 發展為一個 SaaS 平台，提供 API 讓企業能夠整合到他們的工作流程中，同時，也將與各領域的專家合作，不斷優化基準測試和評估指標，打造一個準確、可靠且易於使用的空間智能評估系統。這將大幅降低開發成本、提高效率，並創造一個更真實、更可信的 AI 生成世界，具有龐大的商業潛力。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-03T06:20:45.510934"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通往通用型神經符號學習之路，應以基石模型鋪就", "summary_zh": "神經符號學習旨在解決訓練神經網路處理複雜推理任務時遇到的挑戰，並提供可解釋性、可靠性和效率等優勢。傳統的神經符號學習方法會結合符號程式訓練神經模型，但它們面臨重大挑戰，使其僅限於簡單的問題。另一方面，純神經基石模型現在透過提示而非訓練達到了最先進的性能，但它們通常不可靠且缺乏可解釋性。我們提出神經符號提示，即用符號程式補充基石模型，為使用這些模型執行複雜推理任務提供了一種方法。這引發了一個問題：在基石模型的時代，作為神經符號學習一部分的專用模型訓練起什麼作用？為了探索這個問題，我們重點介紹了傳統神經符號學習在計算、數據和程式方面的三個陷阱，這些陷阱導致了泛化問題。這篇立場文件認為，基石模型可以實現可泛化的神經符號解決方案，提供了一條在沒有從頭開始訓練的缺點的情況下，實現神經符號學習最初目標的途徑。", "applications": ["**醫療診斷輔助系統:** 利用基石模型理解病歷，再透過符號規則進行邏輯推理，輔助醫生進行更精準的診斷，並解釋診斷結果的依據，提高可信度。", "**金融風險評估:** 基石模型分析市場數據和客戶信息，結合預設的風險模型和法規，進行更全面的風險評估，並提供清晰的風險解釋，符合監管要求。", "**自動程式碼生成與修復:** 基石模型理解人類自然語言描述的需求，結合程式語言的語法規則和邏輯，自動生成程式碼或修復程式碼缺陷，大幅提升開發效率。"], "pitch": "我們正在打造新一代人工智慧引擎，結合基石模型強大的感知能力和符號推理的精準性，突破傳統神經網路的局限，解決複雜推理任務。我們的神經符號提示技術，無需耗時耗力的從頭訓練，就能讓AI系統具備可解釋、可靠、且高效的推理能力。想像一下，一個能理解複雜醫療數據並做出精準診斷建議的AI醫生，或者一個能自動生成高品質程式碼的AI程式設計師。我們的技術將顛覆醫療、金融、程式設計等領域，創造巨大的商業價值。我們正在尋求資金，加速產品開發和市場拓展，引領人工智慧的下一個浪潮。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-03T06:21:00.301278"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：透過可組合多視角擴散生成可動畫化的精細3D人體", "summary_zh": "現有從圖像生成3D頭像的方法難以產生足夠精細、可動畫化的頭像，無法滿足實際應用需求。AdaHuman是一種新穎的框架，它僅需一張真實環境中的圖像，就能生成高保真、可動畫化的3D頭像。AdaHuman包含兩個關鍵創新：(1) 一個姿態條件的3D聯合擴散模型，能合成任意姿態下一致的多視角圖像，並在每個擴散步驟中進行對應的3D高斯濺射（3DGS）重建；(2) 一個可組合的3DGS細化模組，通過圖像到圖像的細化增強局部身體部位的細節，並使用一種新型的感知裁剪的相機射線圖無縫集成它們，從而產生一個有凝聚力的精細3D頭像。這些組件使AdaHuman能夠生成高度逼真的標準A字型姿勢頭像，具有最小的自遮擋，可以使用任何輸入運動進行裝配和動畫處理。在公開基準測試和真實圖像上的廣泛評估表明，AdaHuman在頭像重建和重新定位方面的表現明顯優於最先進的方法。程式碼和模型將公開提供用於研究目的。", "applications": ["**虛擬試穿與時尚購物：** 消費者只需上傳一張自拍照，即可生成自己的3D頭像，在線上試穿不同款式的服裝，看到真實的穿著效果，提升購物體驗並減少退貨率。", "**遊戲與虛擬實境角色定制：** 玩家可以輕鬆地將自己或朋友的照片轉換為遊戲或VR中的個性化角色，大幅降低角色創建的門檻，提升沉浸感和互動性。", "**遠程醫療與復健：** 醫生可以利用病患的照片創建3D頭像，用於評估身體姿態、運動範圍等，輔助診斷與制定復健計畫，無需病患親自到場。"], "pitch": "AdaHuman解決了目前3D頭像生成技術在精細度和動畫能力上的瓶頸，具有巨大的商業潛力。其核心技術，包括姿態條件的3D聯合擴散模型和可組合的3DGS細化模組，能夠快速且精確地將2D圖像轉換為高品質的、可動畫化的3D頭像。這項技術可廣泛應用於電商、遊戲、娛樂、醫療等多個領域，創造全新的商業模式與用戶體驗。 我們相信，通過將AdaHuman技術整合到現有的平台或應用中，可以顯著提升用戶參與度、增加營收並建立強大的競爭優勢。例如，與電商平台合作推出虛擬試穿服務，與遊戲公司合作提供個性化角色定制，與醫療機構合作開發遠程復健系統。 我們正在尋找策略投資者，共同將AdaHuman技術推向市場，打造下一代3D頭像生成引擎，引領虛擬世界體驗的變革。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-03T06:21:16.565512"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "人類能夠直觀地在3D空間中構圖和佈置場景進行攝影。然而，當AI圖像生成器從文本或圖像提示中創建圖像時，它們是否也能以類似的3D空間感知能力來規劃場景呢？我們提出了GenSpace，一個新穎的基準測試和評估流程，旨在全面評估當前圖像生成模型的空間感知能力。此外，使用通用視覺語言模型(VLM)的標準評估經常無法捕捉到詳細的空間錯誤。為了應對這個挑戰，我們提出了一個專門的評估流程和指標，它使用多個視覺基礎模型重建3D場景幾何形狀，並提供一個更準確且與人類對齊的空間保真度指標。我們的研究結果表明，雖然AI模型可以創建視覺上吸引人的圖像並遵循一般指令，但它們在諸如物體放置、關係和測量等特定3D細節方面存在困難。我們總結了當前最先進的圖像生成模型在空間感知方面的三個核心限制：1) 物體透視理解，2) 自我中心-以世界為中心的轉換，以及3) 度量測量依從性，突出了改進圖像生成中空間智能的可能方向。", "applications": ["**智能家居設計：** 用戶可以透過語音或文字描述想要的房間擺設（例如：「在窗戶旁邊放一個綠色盆栽，書桌在房間中央面向窗戶」），AI生成器生成多個符合空間佈局的房間設計圖，並允許用戶修改細節，最終幫助用戶視覺化並選擇最適合的設計方案。", "**虛擬現實/遊戲開發：** 開發者可以更快速地創建具有空間合理性的遊戲場景。例如，輸入「廢棄的城市街道，左邊有一棟高樓，右邊是河流，遠處有山」，AI能快速生成具備良好透視關係與空間感的場景基礎，開發者再進行細節打磨。", "**機器人導航與環境理解：** 提升機器人對周圍環境的空間理解能力。透過分析AI生成的不同場景，可以訓練機器人更好地識別物體之間的空間關係，例如「椅子在桌子旁邊」或「書在書架上」，從而提高導航效率和安全性。"], "pitch": "GenSpace 解決了 AI 圖像生成在 3D 空間理解上的關鍵瓶頸，這是一個快速成長且充滿潛力的市場。我們的基準測試和評估方法，能幫助開發者更精準地提升 AI 圖像生成器的空間感知能力。想像一下，一個能完美理解空間關係的 AI，將彻底改變设计、游戏、机器人等行业。通過授權 GenSpace 技術，我們能成為 AI 空間智能領域的領導者，為這些產業創造巨大的價值，並在高度競爭的市場中搶佔先機。我们正在构建下一代具备真正3D理解能力的AI，而 GenSpace 将是实现这一目标的基石。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-03T07:14:38.306396"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通往可泛化神經符號學習之路，應以基礎模型鋪設", "summary_zh": "神經符號學習旨在解決神經網路在複雜推理任務訓練上的挑戰，並增加可解釋性、可靠性和效率。傳統方法結合訓練神經模型和符號程序，但面臨限制其應用的問題。另一方面，純神經基礎模型透過Prompting而非訓練，達到頂尖效能，但缺乏可靠性和可解釋性。本文提出利用符號程序輔助基礎模型，稱為神經符號Prompting，以解決複雜推理任務。本文探討在基礎模型時代，神經符號學習中專門模型訓練的角色，並指出傳統方法在算力、數據和程序方面的三個陷阱，導致泛化問題。本文主張基礎模型能實現可泛化的神經符號解決方案，從而無需從頭開始訓練，即可達成神經符號學習的原始目標。", "applications": ["**智能客服：** 利用基礎模型處理複雜的客戶問題，並透過符號程序進行邏輯推理和決策，提供更精確和可解釋的回覆，例如處理退款、投訴等複雜流程。", "**法律文件審閱：** 基礎模型閱讀大量的法律文本，快速提取關鍵信息，而符號程序則檢查合規性、識別潛在風險，協助律師提高效率，減少人為錯誤。", "**醫療診斷輔助：** 基礎模型分析病患的病歷、檢驗報告等數據，並結合醫學知識庫（符號程序）進行診斷建議，提供給醫生參考，提高診斷準確性和效率。"], "pitch": "各位投資人，我們正處於AI技術的轉折點。傳統神經符號學習雖然理想，但受限於數據和算力，難以泛化。我們的創新方案，神經符號Prompting，巧妙結合了基礎模型的強大能力和符號程序的可解釋性，解決了這一痛點。想像一下，一個既能理解複雜語言，又能像專家一樣進行邏輯推理的AI系統，應用前景廣闊，從智能客服到醫療診斷，都能帶來顛覆性的效率提升和成本降低。我們相信，這項技術將引領下一代AI革命，成為各行業智能化的關鍵引擎，為投資者帶來豐厚回報。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-03T07:14:55.143669"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "人類能直觀地在3D空間中構圖和安排場景以進行攝影。然而，當前先進的AI圖像生成器在根據文字或圖像提示創建圖像時，能否像人類一樣具備3D空間感知能力來規劃場景？ 我們提出了GenSpace，一個創新的基準測試和評估流程，用於全面評估當前圖像生成模型的空間感知能力。此外，使用通用視覺-語言模型（VLM）的標準評估經常無法捕捉到詳細的空間錯誤。 為了解決這個挑戰，我們提出了一個專門的評估流程和指標，它使用多個視覺基礎模型重建3D場景幾何體，並提供更準確和更符合人類認知的空間保真度度量。 我們的研究結果表明，雖然AI模型可以創建視覺上吸引人的圖像並遵循一般指令，但它們在特定的3D細節方面存在困難，例如物體放置、關係和測量。 我們總結了當前最先進的圖像生成模型在空間感知方面的三個核心限制：1）物體透視理解，2）自我中心-以環境為中心的轉換，3）度量測量一致性，突出了改進圖像生成空間智能的可能方向。", "applications": ["室內設計App：用戶上傳房間照片，App能根據文字描述（例如“在沙發旁邊放一盞落地燈”）生成逼真的3D空間佈置，幫助用戶預覽裝修效果。", "遊戲開發工具：開發者可以透過文字或草圖快速生成符合特定空間關係的遊戲場景（例如“山頂上有一個村莊”），大幅縮短場景建模時間。", "虛擬導覽與訓練：創建具有真實空間感的虛擬導覽或訓練環境（例如模擬倉庫環境），提升沉浸感與學習效率。"], "pitch": "GenSpace 解決了 AI 圖像生成中長期被忽略的空間感知問題，提供了一個客觀的評估標準和改進方向。當前 AI 模型在空間細節上的缺陷阻礙了其在多個行業的應用。基於 GenSpace 的研究成果，我們能打造更精確、更具實用性的 AI 圖像生成工具。 我們預計，第一個能克服這些空間感知限制的圖像生成模型，將在室內設計、遊戲開發、教育訓練等領域產生革命性的影響。 我們可以開發基於 GenSpace 評估標準的 API 服務，幫助企業和開發者評估和優化他們的圖像生成模型，或者將更優秀的空間感知能力集成到現有產品中，從而獲得顯著的競爭優勢和市場份額。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-03T08:20:07.069294"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "邁向通用化神經符號學習之路應由基石模型鋪就", "summary_zh": "神經符號學習旨在解決訓練神經網路處理複雜推理任務時的挑戰，並增加可解釋性、可靠性和效率。傳統的神經符號學習方法將神經模型與符號程式結合訓練，但面臨諸多挑戰，使其僅限於簡單問題。另一方面，純神經的基石模型現在透過提示而非訓練達到最先進的性能，但它們通常不可靠且缺乏可解釋性。我們提出使用符號程式來補充基石模型，稱之為神經符號提示，為利用這些模型執行複雜推理任務提供了一種途徑。這引發了一個問題：在基石模型時代，作為神經符號學習一部分的專用模型訓練扮演著什麼角色？為了探討這個問題，我們重點介紹了傳統神經符號學習在計算、數據和程式方面導致泛化問題的三個陷阱。這篇立場文件認為，基石模型能夠實現通用化的神經符號解決方案，為實現神經符號學習的最初目標提供了一條途徑，而無需從頭開始訓練的缺點。", "applications": ["**智能客服：** 利用基石模型處理用戶提出的複雜問題，並結合符號程式進行邏輯推理，提供更精確、可解釋的回答，例如解釋保險條款、法律條文等。", "**醫療診斷輔助：** 基石模型分析患者病歷和症狀，符號程式則根據醫學知識庫進行推理，輔助醫生進行診斷，並解釋診斷邏輯，提高診斷準確性和可信度。", "**自動程式碼生成：** 利用基石模型理解用戶的需求描述，符號程式生成符合需求的程式碼片段，並能解釋程式碼的生成邏輯，降低程式設計門檻。"], "pitch": "我們正在打造基於基石模型的神經符號學習平台，解決傳統AI在複雜推理、可解釋性及可靠性上的痛點。相較於從頭訓練模型，我們的方法更高效、泛用性更高。想像一下，你可以用自然語言描述複雜的商業規則，我們的平台就能自動生成可執行的決策引擎，大幅提升企業自動化水平，降低運營成本。這是一個潛力巨大的市場，我們相信能夠成為下一代AI基礎設施的領先者，重塑各行各業的決策模式。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-03T08:20:22.429386"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：基於可組合多視角擴散的可動畫細緻3D人體生成", "summary_zh": "現有的從圖像到3D頭像生成的方法難以產生細節豐富、可動畫且適用於真實世界應用的頭像。我們提出了AdaHuman，一種從單張真實場景圖像生成高保真可動畫3D頭像的新框架。 AdaHuman包含兩項關鍵創新：(1)一個姿態條件的3D聯合擴散模型，在任意姿態下合成一致的多視角圖像，同時在每個擴散步驟中進行相應的3D高斯濺射(3DGS)重建；(2)一個可組合的3DGS精煉模塊，通過圖像到圖像的精煉增強局部身體部位的細節，並使用一種新穎的裁剪感知相機光線圖將它們無縫集成，產生一個有凝聚力的細緻3D頭像。這些組件使AdaHuman能夠生成高度逼真的標準A字形姿態頭像，且自遮擋最少，從而能夠使用任何輸入運動進行裝配和動畫。在公共基準和真實場景圖像上的廣泛評估表明，AdaHuman在頭像重建和重新擺姿勢方面均顯著優於最先進的方法。代碼和模型將公開提供以供研究。", "applications": ["**虛擬試衣間：** 消費者上傳一張全身照，就能看到自己穿不同衣服的3D效果，減少網購退貨率。", "**遊戲角色客製化：** 玩家上傳照片，快速生成高擬真度的遊戲角色，提升遊戲體驗。", "**VR/AR 社交互動：** 用戶上傳照片，創建個人專屬的3D虛擬化身，在VR/AR環境中進行更真實的社交互動。"], "pitch": "AdaHuman解決了創建高擬真度、可動畫3D頭像的痛點，其核心優勢在於單張照片快速生成和細節高度還原。市場需求旺盛，應用場景廣泛，尤其在電商、遊戲和社交領域潛力巨大。我們的商業模式包括技術授權、客製化頭像服務和與平台合作。相較於競爭對手，AdaHuman在生成速度和細節表現上更具優勢，有望成為行業領導者。我們相信AdaHuman能夠顛覆現有的3D頭像生成市場，創造巨大的商業價值。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-03T08:20:39.382022"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "人類能直觀地在3D空間中構圖和安排場景以進行攝影。那麼，先進的AI圖像生成器在根據文字或圖像提示創建圖像時，是否也能以類似的3D空間感知來規劃場景呢？我們提出了GenSpace，一個新穎的基準測試和評估流程，旨在全面評估當前圖像生成模型的空間感知能力。此外，使用通用視覺語言模型(VLM)的標準評估經常無法捕捉到詳細的空間錯誤。為了解決這個挑戰，我們提出了一種專用的評估流程和指標，它使用多個視覺基礎模型重建3D場景幾何形狀，並提供更準確且更符合人類認知的空間保真度指標。我們的研究結果表明，儘管AI模型可以創建具有視覺吸引力的圖像並遵循一般指令，但它們在特定的3D細節（如物體放置、關係和測量）方面存在困難。我們總結了當前最先進的圖像生成模型在空間感知方面的三個核心限制：1) 物體透視理解，2) 以自我為中心-以他人為中心的轉換，以及3) 度量測量一致性，突出了改進圖像生成中空間智能的可能方向。", "applications": ["**室內設計預覽：** 用戶輸入房間描述和家具樣式，AI 生成不同擺放方案的3D渲染圖，幫助用戶更直觀地比較和選擇。", "**虛擬導覽製作：** 根據景點描述和地圖資訊，AI 生成高度擬真的虛擬導覽影片，讓用戶在家也能體驗景點風光。", "**遊戲場景設計：** 遊戲開發者輸入場景描述，AI 生成初步的遊戲地圖和物件擺放，大幅縮短前期場景設計時間，並提供創意靈感。"], "pitch": "GenSpace 開發了一套評估AI圖像生成模型空間感知能力的基準測試，揭示了當前模型在3D場景理解上的不足。這意味著，現在AI生成的圖像在空間關係上仍然存在明顯的瑕疵。但這同時也代表了巨大的機會！通過改進AI的空間感知能力，我們可以打造更逼真、更具沉浸感的虛擬體驗，應用於室內設計、遊戲開發、廣告行銷等多個領域。我們的基準測試可以幫助開發者精準定位模型缺陷，加速技術突破。我們相信，隨著AI空間感知能力的提升，GenSpace將成為推動下一代視覺內容創作的關鍵力量，創造巨大的商業價值。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-03T09:15:42.630370"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通往通用化神經符號學習的道路應以基礎模型鋪就", "summary_zh": "神經符號學習旨在解決訓練神經網路以進行複雜推理任務時的挑戰，並增加可解釋性、可靠性和效率等優點。傳統的神經符號學習方法通常將神經模型與符號程序結合訓練，但它們面臨重大挑戰，限制了其應用於簡單問題。另一方面，純神經基礎模型現在通過提示而非訓練，達到了最先進的性能，但它們通常不可靠且缺乏可解釋性。使用符號程序補充基礎模型（我們稱之為神經符號提示）提供了一種將這些模型用於複雜推理任務的方法。這樣做引出了一個問題：在基礎模型的時代，作為神經符號學習一部分的專門模型訓練扮演著什麼角色？為了探索這個問題，我們重點介紹了傳統神經符號學習在計算、數據和程序方面導致泛化問題的三個陷阱。這篇立場文件認為，基礎模型能夠實現可泛化的神經符號解決方案，從而提供了一條在沒有從頭開始訓練缺點的情況下，實現神經符號學習最初目標的途徑。", "applications": ["**自動程式碼修復與生成：** 利用神經符號提示，基礎模型可以根據錯誤訊息和程式碼上下文，自動生成或修復程式碼，大幅提升開發效率，減少人工debug時間。", "**醫療診斷輔助系統：** 將醫療影像數據與醫學知識庫整合，基礎模型可以根據症狀描述和檢驗報告，進行初步診斷，並提供潛在的治療方案建議，輔助醫生進行更準確的判斷。", "**智慧客服與問題解決：** 客戶提出問題後，系統利用基礎模型理解問題，然後透過符號程序查閱相關知識庫或規則，提供精準且可解釋的解答，提升客戶滿意度。"], "pitch": "想像一下，一個能像人類專家一樣推理、解決問題的AI系統，它不僅聰明，而且可解釋、可靠。這就是神經符號學習的願景。但過去的方法需要耗費大量資源進行訓練，且難以泛化。我們的解決方案是將強大的基礎模型與精確的符號程序結合，打造更通用、更高效的神經符號系統。這項技術可以應用於各行各業，從自動化程式碼生成、到精準醫療、再到智慧客服，大幅提升效率、降低成本。我們正在建立一個平台，讓企業能夠輕鬆利用這些技術，釋放AI的真正潛力。投資我們，就是投資未來，一個AI不僅能思考，還能解釋其思考方式的未來。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-03T09:16:01.733111"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：基於組合式多視角擴散的可動畫化精細3D人體生成", "summary_zh": "AdaHuman是一種新型框架，能從單張自然環境下的圖像生成高擬真且可動畫化的3D人體模型。它利用姿態條件式3D聯合擴散模型，合成任意姿態下一致的多視角圖像，並在每個擴散步驟重建對應的3D高斯潑濺（3DGS）。此外，它還使用組合式3DGS細化模塊，通過圖生圖細化增強局部身體部位的細節，並使用新型的裁剪感知相機光線圖無縫整合它們，生成連貫的精細3D人體模型。AdaHuman能夠生成高度逼真的標準A字姿態人體模型，且自遮擋情況極少，從而可以使用任何輸入動作進行綁定和動畫製作。評估顯示，AdaHuman在人體模型重建和重新擺放方面顯著優於現有技術。", "applications": ["**虛擬試衣間：** 顧客上傳一張照片，就能看到自己穿上不同服飾的3D效果，甚至可以模擬走秀。", "**遊戲角色客製化：** 玩家只需上傳一張照片，就能快速生成個人化的遊戲角色，並能自由調整服裝、髮型等細節。", "**遠程健康諮詢：** 患者上傳照片後，醫生可創建3D模型，更精確地評估身體狀況，並進行遠程康復指導。"], "pitch": "AdaHuman解決了從單張圖片生成高品質、可動畫化3D人體模型的痛點，其技術突破在虛擬化身、遊戲、時尚、醫療等領域具有廣闊的商業前景。相較於現有技術，AdaHuman能生成更精細、更逼真的模型，顯著提升用戶體驗。我們計劃將這項技術授權給遊戲公司、電商平台、醫療機構等，並提供定制化服務。憑藉其優越的性能和廣泛的應用場景，AdaHuman有潛力成為3D人體建模領域的領頭羊，並為投資者帶來豐厚的回報。 投資 AdaHuman 就是投資下一代虛擬化身技術的未來。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-03T09:16:15.737589"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "人類能直覺地在3D空間中構圖和安排場景進行攝影。本文探討進階AI圖像生成器是否能在根據文字或圖像提示創建圖像時，以類似的3D空間感知能力來規劃場景。研究提出了GenSpace，一個新的基準測試和評估流程，用以全面評估當前圖像生成模型的空間感知能力。由於通用視覺語言模型（VLMs）的標準評估通常無法捕捉到詳細的空間錯誤，研究提出了一個專門的評估流程和指標，利用多個視覺基礎模型重建3D場景幾何，從而提供更準確、更符合人類直覺的空間忠實度指標。研究結果表明，雖然AI模型可以創建具有視覺吸引力的圖像並遵循一般指令，但它們在物體放置、關係和測量等特定3D細節方面存在困難。研究總結了當前最先進圖像生成模型在空間感知方面的三個核心限制：1) 物體透視理解，2) 自我中心-他中心轉換，以及 3) 指標測量一致性，並強調了改進圖像生成中空間智能的可能方向。", "applications": ["**虛擬實境（VR/AR）內容生成：** 利用GenSpace改善VR/AR環境中物體的空間佈局，使體驗更真實、更符合直覺，避免物體放置不合理或比例失調的問題。", "**建築設計與室內設計輔助：** GenSpace可協助建築師和設計師生成符合空間規範和美學要求的設計方案，快速生成不同風格和佈局的視覺化效果。", "**遊戲開發：** 自動生成符合遊戲世界觀和物理規則的場景，減少美術設計師的工作量，提升遊戲開發效率。"], "pitch": "GenSpace提供了一個量化圖像生成模型空間感知能力的標準，揭示了目前AI在3D空間理解上的局限性。透過持續提升空間感知能力，GenSpace將能賦能更真實、更具互動性的虛擬體驗，並大幅提升設計和遊戲開發等領域的效率。試想一下，一個能精準理解空間關係的AI，將會如何顛覆建築設計、虛擬實境和遊戲開發產業。我們相信，GenSpace不僅是一個評估工具，更是一個引領圖像生成技術走向成熟的關鍵基礎設施，具有巨大的商業潛力和戰略價值。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-03T10:15:37.570113"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通用型神經符號學習之路應以基礎模型鋪路", "summary_zh": "神經符號學習旨在解決訓練神經網路處理複雜推理任務時遇到的挑戰，並提供可解釋性、可靠性和效率等優點。傳統神經符號學習方法將神經模型與符號程序一起訓練，但面臨許多限制，使其只能處理簡單問題。另一方面，純神經基礎模型現在透過提示而非訓練達到最先進的性能，但通常不可靠且缺乏可解釋性。透過符號程序補充基礎模型，我們稱之為神經符號提示，提供了一種將這些模型用於複雜推理任務的方法。這樣做引發了一個問題：在基礎模型的時代，作為神經符號學習一部分的專業模型訓練扮演什麼角色？為了探討這個問題，我們重點介紹了傳統神經符號學習在計算、數據和程序方面導致泛化問題的三個陷阱。這篇立場論文認為，基礎模型能夠實現通用型神經符號解決方案，提供了一條實現神經符號學習最初目標的途徑，而無需從頭開始訓練的缺點。", "applications": ["**智能客服的問答推理：** 客戶提出複雜問題，例如「如果我同時購買A商品和B商品，並且使用折扣碼C，運費會怎麼計算？」，傳統客服系統可能需要複雜的規則引擎才能處理。透過神經符號提示，可以利用大型語言模型理解客戶意圖，並用符號程序進行準確的運算，最終給出正確答案。", "**法律條文的自動解釋與案例分析：** 法律條文通常晦澀難懂。利用神經符號提示，讓大型語言模型理解法律條文的含義，並結合符號程序進行邏輯推理，分析具體案例是否符合相關法律規定，輔助法律專業人士進行判斷。", "**醫療診斷輔助：**醫生輸入患者的症狀和檢測結果，利用神經符號提示，大型語言模型可以結合醫學知識庫和符號程序進行推理，判斷患者可能患有的疾病，並提供相關的診斷建議。這可以幫助醫生提高診斷效率和準確性。"], "pitch": "傳統神經符號學習的瓶頸在於難以訓練出泛用性強的模型。現在有了大型語言模型，我們可以利用『神經符號提示』，讓大型語言模型負責理解和生成，而符號程序負責精確的推理和計算。這就像給大型語言模型裝上了一個精確的計算器和嚴謹的邏輯引擎。這項技術的商業價值巨大，尤其是在需要高度準確性和可解釋性的領域，例如金融、法律、醫療等。我們可以基於此技術開發智能化的解決方案，大幅降低人工成本，提高決策效率，並創造新的商業模式。例如，智能合約審計、自動化法務諮詢、精準醫療診斷等，都是非常有潛力的應用方向。 我們正在尋找投資者，共同打造新一代的人工智能解決方案，讓AI不僅更聰明，也更可靠、更值得信賴。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-03T10:15:58.391972"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：基於組合式多視角擴散的可動畫化精細3D人體生成", "summary_zh": "AdaHuman 是一種從單張照片生成高保真、可動畫化3D人體模型的創新框架。它採用基於姿態條件的3D聯合擴散模型，能合成任意姿態下一致的多視角圖像，並在每一步重建3D高斯噴濺(3DGS)。此外，透過組合式3DGS精煉模組，利用圖像到圖像的精煉方式增強局部身體部位的細節，並透過一種新的裁剪感知相機光線映射將其無縫整合，產生一個連貫且細緻的3D人體模型。這使得AdaHuman能夠生成高度逼真的標準A字姿勢人體模型，減少自我遮擋，方便進行骨骼綁定和動畫製作。實驗證明，AdaHuman在人體重建和重新擺姿勢方面，顯著優於現有技術。", "applications": ["**個人化虛擬替身：** 用戶上傳一張照片即可快速生成逼真且可自由控制的3D虛擬替身，用於遊戲、社交平台或其他元宇宙應用中，不再受限於捏臉系統的限制。", "**服裝試穿與展示：** 電商平台可以利用此技術讓消費者上傳自己的照片，生成3D替身，模擬穿著不同服裝的效果，提供更真實的線上購物體驗。", "**遠程醫療與運動分析：** 醫生或教練可以通過患者或運動員的單張照片創建3D模型，用於評估姿勢、運動習慣，並進行遠程指導或治療方案的制定。"], "pitch": "AdaHuman解決了從單張圖像生成高品質、可動畫化3D人體模型的關鍵痛點。現有技術往往細節不足或難以動畫化，限制了其在元宇宙、遊戲、電商等領域的應用。AdaHuman的突破性技術，能夠快速、高效地生成逼真的3D人體模型，降低了3D角色創作的門檻，並提供更豐富的應用場景。 其潛在的商業價值巨大，包括授權模型生成技術給遊戲公司和社交平台，為電商平台提供個性化的服裝試穿服務，以及為醫療和運動健康領域提供3D人體建模解決方案。透過與各行業龍頭企業的戰略合作，AdaHuman有望成為3D人體建模領域的領導者，創造巨大的市場價值。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-03T10:16:16.277621"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "這篇論文提出了GenSpace，一個新的基準測試，用於評估AI圖像生成模型在空間感知方面的能力。現有的模型在生成圖像時，雖然能產生視覺上吸引人的圖片，並遵循大致的指令，但在物件擺放、關係和尺寸等具體的3D空間細節上表現不佳。研究指出，當前最先進的圖像生成模型在物件透視理解、自我中心-世界中心轉換以及尺寸測量準確性上存在三大局限，並為提升圖像生成中的空間智能指出了方向。", "applications": ["室內設計App：使用者可以輸入文字指令，例如「簡約風格的客廳，落地窗旁有一張灰色沙發，沙發上放兩個藍色抱枕」，AI根據指令生成逼真的3D空間圖像，讓使用者預覽裝修效果。", "遊戲開發：遊戲開發者可以利用AI快速生成具有空間關係的場景物件，例如「森林場景，一顆大樹旁邊有兩個蘑菇，前方有一條小路」，加速遊戲場景的設計流程。", "廣告設計：廣告商可以根據廣告文案，利用AI生成符合空間邏輯的產品展示圖，例如「高級手錶放在桌子上，旁邊是一杯咖啡，背景是落地窗外的城市夜景」，提升廣告的吸引力。"], "pitch": "GenSpace揭示了當前AI圖像生成在空間感知上的瓶頸，這是一個巨大的商機！我們的基準測試和評估方法，能幫助企業更精準地開發和優化具有空間理解能力的AI模型。想像一下，一個能根據文字描述，自動生成逼真且符合空間邏輯的3D模型的平台，將徹底改變室內設計、遊戲開發、廣告設計等行業。我們不僅提供技術，更提供一個行業標準，引領AI圖像生成走向更智能、更實用的未來。投資GenSpace，就是投資AI視覺的下一個爆發點！", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-03T11:12:51.958627"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通用型神經符號學習之路應由基石模型鋪就", "summary_zh": "神經符號學習旨在解決訓練神經網路進行複雜推理任務時的挑戰，並提供可解釋性、可靠性和效率等優勢。傳統的神經符號學習方法將神經模型與符號程式結合訓練，但面臨許多限制，使其只能處理簡單問題。另一方面，純神經基石模型現在透過提示而非訓練達到最先進的性能，但它們通常不可靠且缺乏可解釋性。 我們提出利用符號程式補充基石模型，稱為神經符號提示，提供了一種使用這些模型執行複雜推理任務的方法。 這引發了一個問題：在基石模型時代，作為神經符號學習一部分的專用模型訓練扮演什麼角色？ 為了探討這個問題，我們強調了傳統神經符號學習在計算、數據和程式方面導致泛化問題的三個陷阱。 本立場論文認為，基石模型能夠實現通用型的神經符號解決方案，提供了一條實現神經符號學習原始目標的途徑，而無需從頭開始訓練的缺點。", "applications": ["**智能客服系統：** 利用基石模型理解客戶問題，然後透過符號程式進行推理，例如查閱知識庫、計算費用等，提供更精確和個性化的答案。", "**醫療診斷輔助：** 基石模型分析病患數據（例如病歷、影像），再透過符號程式的規則進行推理，輔助醫生進行更準確的診斷，同時提供清晰的診斷邏輯。", "**金融風險評估：** 基石模型分析市場數據和客戶信息，然後透過符號程式的規則進行風險評估，例如計算信用評分、預測違約機率等，提供更可靠的風險管理。"], "pitch": "我們正在建立一個下一代的人工智慧平台，它結合了基石模型強大的理解能力和符號程式的可解釋性和可靠性，解決了傳統AI在複雜推理和泛化能力上的瓶頸。想想看，不需要大量標記數據就可以理解複雜規則並做出精確判斷的AI，這在醫療、金融、製造等行業具有巨大的顛覆潛力。我們的技術優勢在於神經符號提示，這能讓基石模型在特定領域擁有卓越的表現，同時保證輸出的可追溯性和可驗證性。我們尋求投資，以加速產品開發、擴大團隊，並搶佔市場先機，成為通用型神經符號學習領域的領導者。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-03T11:13:10.133263"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：基於組合式多視角擴散的可動畫細緻3D人體生成", "summary_zh": "現有的圖像到3D頭像生成方法難以生成細節豐富、可供動畫製作且適用於真實世界的頭像。AdaHuman 提出一個創新的框架，可以從單張真實圖片生成高保真、可動畫的3D頭像。它包含兩項關鍵創新：(1) 一個姿態條件的3D聯合擴散模型，可合成任意姿態下一致的多視角圖像，並在每個擴散步驟中同步進行3D高斯濺射(3DGS)重建；(2) 一個組合式3DGS細化模塊，通過圖像到圖像的細化來增強局部身體部位的細節，並使用一種新的裁剪感知相機光線圖將它們無縫集成，從而生成一個連貫細緻的3D頭像。這些組件使 AdaHuman 能夠生成高度逼真的標準A字姿勢頭像，並具有最小的自我遮擋，從而能夠使用任何輸入運動進行裝配和動畫製作。在公共基準和真實圖片上的廣泛評估表明，AdaHuman 在頭像重建和重新擺姿勢方面顯著優於最先進的方法。程式碼和模型將公開提供以供研究。", "applications": ["**虛擬試衣間：** 使用者上傳一張個人照片，即可生成自己的3D頭像，在線上試穿各種服裝，預覽效果，大幅提升網購體驗。", "**個性化遊戲角色：** 遊戲玩家可以通過上傳照片，快速創建與自己外貌相似的3D遊戲角色，增強遊戲沉浸感和個性化定制體驗。", "**遠程醫療/健身指導：** 醫生或健身教練可以基於患者或客戶的照片生成3D模型，遠程評估身體狀況、制定運動計畫，並提供視覺化的指導。", "**元宇宙化身：** 用户上传照片或视频，快速生成高质量的元宇宙化身，用于社交、娱乐和工作，提升用户在虚拟世界的存在感。"], "pitch": "AdaHuman 解決了創建高品質、可動畫的3D人體頭像的關鍵瓶頸，市場潛力巨大。 其技術優勢體現在細節豐富、可動畫以及僅需單張圖像輸入。這使得產品更容易被大眾接受和使用。 我們可以將其應用於電商、遊戲、醫療等多個領域，打造個性化的虛擬體驗。 從創投角度來看， AdaHuman 有潛力成為元宇宙基礎設施的重要組成部分，搶佔3D數位頭像市場的先機，並透過授權技術、訂閱服務等方式實現商業變現，具有極高的投資價值。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-03T11:13:35.837636"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "這篇論文提出了一個名為GenSpace的新基準測試，旨在評估AI圖像生成模型在創建圖像時的空間感知能力，特別是能否像人類一樣理解並安排3D空間中的物體。研究發現，儘管AI模型能生成視覺上吸引人的圖像並遵循一般指令，但在處理精確的3D細節（如物體放置、關係和測量）方面存在困難。該研究總結了當前最先進的圖像生成模型在空間感知方面的三個核心限制：物體透視理解、以自我為中心到以他人為中心的轉換，以及度量測量準確性，並為改善圖像生成中的空間智能指出了可能的方向。", "applications": ["**室內設計輔助：** 輸入房間尺寸和家具描述，AI生成逼真的3D房間模型，方便使用者預覽設計效果，並自動調整家具擺放位置。", "**遊戲場景生成：** 遊戲開發者可以通過文字描述快速生成各種場景，例如「森林裡有一棵高大的樹，旁邊有一個小木屋」，AI自動生成符合描述的3D場景。", "**虛擬試穿/試戴：** 用戶上傳自身照片，AI根據商品描述和用戶身材生成虛擬試穿/試戴效果圖，讓用戶更直觀地了解商品的上身效果。"], "pitch": "GenSpace 揭示了目前AI圖像生成在空間感知方面的瓶頸，這反過來孕育著巨大的商業機會。我們將基於 GenSpace 的研究成果，開發下一代空間智能圖像生成引擎，解決目前AI圖像生成在3D空間理解上的缺陷。我們的技術可以應用於室內設計、遊戲開發、電商等行業，提供更逼真、更可控的圖像生成服務。通過提升AI在物體透視、空間關係和尺寸測量方面的準確性，我們可以為用戶提供更具沉浸感和實用價值的體驗。例如，想像一下，你只需輸入幾句話，就能看到你理想的房子裝修效果，或是在遊戲中體驗到一個完全符合你想像的史詩級戰場。我們的技術將大幅降低創意內容的製作成本，並為各行各業帶來全新的可能性。我們正在尋找投資者，共同打造這個令人興奮的未來！", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-03T12:27:35.844764"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通往通用化神經符號學習之路應由基石模型鋪就", "summary_zh": "神經符號學習旨在解決訓練神經網路處理複雜推理任務時的挑戰，同時提供可解釋性、可靠性和效率等優勢。傳統神經符號學習方法將神經模型與符號程序結合訓練，但面臨重大挑戰，使其僅限於簡單問題。另一方面，純神經基石模型現在通過提示而非訓練達到最先進的性能，但它們通常不可靠且缺乏可解釋性。通過符號程序補充基石模型，我們稱之為神經符號提示，提供了一種將這些模型用於複雜推理任務的方法。這引發了一個問題：在基石模型的時代，作為神經符號學習一部分的專門模型訓練有何作用？為了探討這個問題，我們重點介紹了傳統神經符號學習在計算、數據和程序方面導致泛化問題的三個陷阱。本文認為，基石模型可以實現可泛化的神經符號解決方案，提供了一條實現神經符號學習原始目標的途徑，而無需從頭開始訓練的缺點。", "applications": ["**智能客服：** 利用基石模型理解複雜客戶問題，並結合符號程序進行推理和查詢知識庫，提供更準確且可解釋的回應，解決傳統AI客服的答非所問或無法處理複雜問題的困境。", "**醫療診斷輔助：** 基石模型分析病歷資料，符號程序結合醫療知識庫和診斷規則，輔助醫生進行診斷，降低誤診率，提高診斷效率，並提供診斷依據的可解釋性。", "**自動化程式碼生成：** 基石模型理解用戶需求，符號程序根據需求生成程式碼框架和邏輯，降低開發門檻，提高開發效率，並保證程式碼的可靠性與可維護性。"], "pitch": "我們正在開發基於基石模型的神經符號學習平台，旨在利用大規模預訓練模型的強大能力，結合符號推理的精確性和可解釋性，為各行業提供更通用、可靠和高效的人工智慧解決方案。傳統神經符號學習受限於數據和計算資源，難以泛化。我們的方案利用基石模型的預訓練知識，顯著降低了對專門訓練數據的需求，並利用符號程序實現精確推理和知識整合。初期聚焦於智能客服、醫療診斷輔助和自動化程式碼生成等高價值垂直領域，憑藉更優越的性能、可解釋性和更低的開發成本，我們預計將迅速佔領市場，並將平台擴展至更多應用場景，打造新一代AI基礎設施，具有極高的商業潛力。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-03T12:28:39.268473"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：基於組合式多視角擴散的可動畫化細緻3D人體生成", "summary_zh": "AdaHuman是一種新的框架，可以從單張真實照片生成高保真、可動畫化的3D頭像。它使用姿態條件的3D聯合擴散模型，在任意姿態下合成一致的多視角圖像，並逐步重建3D高斯潑濺 (3DGS)。此外，它還利用組合式的3DGS細化模組，透過圖生圖細化來增強局部身體部位的細節，並使用新的裁剪感知相機射線圖無縫整合這些細節，產生連貫且細緻的3D頭像。最終生成的標準A-pose頭像細節豐富，方便進行骨骼綁定和動畫製作。實驗結果顯示，AdaHuman在頭像重建和姿勢調整方面都顯著優於現有技術。", "applications": ["**虛擬試穿:** 消費者上傳一張自己的照片，即可在線上虛擬試穿各種服裝，更真實地預覽穿搭效果，降低網購退貨率。", "**遊戲角色客製化:** 玩家上傳照片快速創建個人化的遊戲角色，省去手動調整的繁瑣步驟，增強遊戲沉浸感。", "**遠程協作與會議:** 在視訊會議中，使用個人化3D頭像代替真實影像，保護個人隱私，同時提供更生動、更具表現力的溝通方式。"], "pitch": "AdaHuman解決了從單張照片生成高品質、可動畫3D人體頭像的難題，這在元宇宙、遊戲、電商等領域具有廣闊的應用前景。 我們相信，AdaHuman技術能大幅降低3D頭像製作的門檻和成本，讓更多人能夠擁有自己的虛擬化身，並創造全新的互動體驗。 我們的優勢在於生成模型的細節度和動畫適應性，顯著超越現有技術。 未來，我們計劃將模型部署到雲端，提供API服務，讓企業和個人開發者能輕鬆地將AdaHuman技術整合到自己的應用程式中。 這將創造一個龐大的潛在市場，帶來可觀的商業價值和回報。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-03T12:29:32.584079"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace: 空間感知圖像生成基準測試", "summary_zh": "人類能直覺地在3D空間中構圖和安排場景進行攝影。但進階AI圖像生成器，在根據文字或圖像提示創造圖像時，是否也能具備相似的3D空間感知能力來規劃場景？我們提出了GenSpace，一個嶄新的基準測試和評估流程，以全面評估當前圖像生成模型的空間感知能力。此外，使用通用視覺語言模型(VLM)的標準評估，往往無法捕捉到詳細的空間錯誤。為了解決這個挑戰，我們提出了一個專門的評估流程和指標，使用多個視覺基礎模型重建3D場景幾何，並提供一個更準確且更符合人類感知的空間保真度指標。我們的研究結果表明，雖然AI模型可以創建視覺上吸引人的圖像，並且能夠遵循一般指令，但它們在物體放置、關係和測量等特定3D細節上表現不佳。我們總結了當前最先進的圖像生成模型在空間感知方面的三個核心限制：1) 物體透視理解，2) 自我中心-非自我中心轉換，以及 3) 度量測量依從性，並強調了改進圖像生成中空間智能的可能方向。", "applications": ["室內設計應用：根據使用者文字描述，生成符合空間比例的室內設計圖，例如「在5坪房間擺放一張雙人床和一個書桌」。目前的AI往往會生成比例失真的圖片，導致設計參考價值降低。", "遊戲開發應用：快速生成符合物理規則和透視關係的遊戲場景，例如「中世紀城堡佇立在山頂，俯瞰著一片森林」。目前的AI生成場景可能出現城堡飄浮在空中，或建築物透視錯誤等問題。", "建築規劃應用：根據建築師的草圖和文字描述，生成建築物的3D模型和渲染圖，並能模擬不同光照條件下的效果。目前的AI可能難以準確呈現建築物的空間結構和光影效果。"], "pitch": "GenSpace 解決了當前 AI 圖像生成在空間感知方面的重大缺陷，為更真實、更具實用性的圖像生成開闢了道路。這項技術的商業價值巨大：首先，它能提升現有圖像生成工具的品質，吸引更多付費用戶。其次，GenSpace 能賦能垂直領域應用，例如室內設計、遊戲開發和建築規劃，創造全新的商業模式。例如，可以開發基於GenSpace的雲端設計平台，讓使用者輕鬆生成符合空間規範的室內設計方案，並與家具廠商合作，實現一鍵購買。我們相信，GenSpace 將成為下一代圖像生成技術的核心引擎，具有巨大的市場潛力和投資回報。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-03T13:29:12.617706"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通用型神經符號學習之路應以基石模型鋪設", "summary_zh": "神經符號學習旨在解決訓練神經網路進行複雜推理任務時遇到的挑戰，並提供可解釋性、可靠性和效率等優勢。傳統的神經符號學習方法將神經模型與符號程序結合訓練，但面臨著重大挑戰，使其僅限於解決簡單問題。另一方面，純神經基石模型現在透過提示而非訓練達到最先進的性能，但它們通常不可靠且缺乏可解釋性。用符號程序補充基石模型（我們稱之為神經符號提示）提供了一種利用這些模型執行複雜推理任務的方法。這引出了一個問題：在基石模型時代，作為神經符號學習一部分的專門模型訓練扮演什麼角色？為了探索這個問題，我們強調了傳統神經符號學習在計算、數據和程序方面導致泛化問題的三個缺陷。本立場文件認為，基石模型能夠實現通用型的神經符號解決方案，提供了一條實現神經符號學習最初目標的途徑，而無需從頭開始訓練的缺點。", "applications": ["**智能客服與法律諮詢：** 利用基石模型理解複雜的法律條文，再用符號程序進行邏輯推理，提供更精準、更可信賴的法律諮詢服務，避免傳統 AI 可能產生的誤導性答案。", "**醫療診斷輔助：** 基石模型可分析病歷和醫學影像，符號程序則用於執行診斷規則和藥物相互作用檢查，提供醫生更全面的資訊，輔助其做出更明智的醫療決策。", "**金融風險評估：** 基石模型可分析市場新聞和社交媒體數據，符號程序則用於計算風險指標和執行投資策略，幫助金融機構更有效地管理風險並做出投資決策。"], "pitch": "我們正在打造下一代 AI 推理引擎，不再受限於傳統神經網路的脆弱性和黑箱特性。我們利用預訓練基石模型的強大泛化能力，結合符號程序的邏輯推理能力，創造出更可靠、更可解釋的解決方案。這就像為 AI 裝上了一個強大的大腦和一套清晰的規則引擎。市場機會巨大，從法律、醫療到金融，任何需要複雜推理和決策的領域都是我們的潛在客戶。我們的優勢在於，我們能更快、更低成本地解決這些問題，並提供競爭對手無法比擬的可解釋性和可信賴性。我們尋求種子輪投資，用於擴大研發團隊，並在特定垂直領域進行概念驗證，證明我們的技術的商業價值。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-03T13:34:44.076887"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：利用组合式多视角扩散生成可动画的精细3D人体", "summary_zh": "現有圖像到3D頭像生成方法難以產生適用於現實應用、高度精細且可動畫的頭像。我們介紹了AdaHuman，一種從單張自然圖像生成高保真、可動畫3D頭像的新穎框架。AdaHuman包含兩項關鍵創新：(1) 一個姿態條件的3D聯合擴散模型，可在任意姿態下合成一致的多視角圖像，同時在每個擴散步驟重建相應的3D高斯濺射（3DGS）；(2) 一個組合式3DGS細化模塊，通過圖像到圖像的細化來增強局部身體部位的細節，並使用一種新穎的裁剪感知相機光線圖將它們無縫集成，從而產生一個連貫的詳細3D頭像。這些組件使AdaHuman能夠生成高度逼真的標準A字形頭像，且自遮擋最小，從而能夠使用任何輸入運動進行綁定和動畫處理。在公共基準和自然圖像上的廣泛評估表明，AdaHuman在頭像重建和重新擺姿勢方面顯著優於最先進的方法。代碼和模型將公開發布，供研究使用。", "applications": ["**虚拟试穿：** 消费者可以通过上传一张自拍照，立即看到穿着不同款式服装的3D虚拟形象，方便在线购物时更直观地了解服装的上身效果。", "**游戏角色定制：** 玩家可以上传自己的照片，快速生成一个高度逼真的游戏角色，增强游戏体验的沉浸感和个性化。", "**虚拟现实（VR/AR）社交：** 用户可以创建自己的高精度3D虚拟化身，用于在VR/AR环境中进行社交互动，让交流更具真实感和表达力。"], "pitch": "AdaHuman解决了现有3D虚拟化身生成技术的瓶颈，能够从单张照片生成高度精细、可动画的3D人体模型。 想象一下，一个能够即时创建逼真3D化身的未来。 这项技术拥有颠覆在线零售、游戏、社交等行业的潜力。通过授权用户创建个性化的数字身份，AdaHuman 正在引领数字体验的新时代。我们的竞争优势在于独特的组合式多视角扩散技术，显著优于市场上的其他解决方案，使其能够生成具有极高细节水平和动画保真度的头像。 我们正在寻找投资者来扩大我们的研究和开发团队，并加速将这项技术商业化。 凭借强大的技术、巨大的市场机会和清晰的盈利路径（包括API订阅服务、定制解决方案和授权），AdaHuman 必将成为 3D 化身生成领域的领导者，并为我们的投资者带来可观的回报。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-03T13:35:05.407084"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "人類能直覺地在3D空間中構圖和安排攝影場景。那麼，當先進的AI圖像生成器從文本或圖像提示中創建圖像時，它們是否也能以類似的3D空間感知來規劃場景呢？ 本文提出GenSpace，一個新的基準測試和評估流程，旨在全面評估當前圖像生成模型的空間感知能力。研究發現，AI模型雖然能生成視覺上吸引人的圖像並遵循一般指令，但在物體放置、關係和測量等具體的3D細節上表現不佳。研究總結了當前最先進的圖像生成模型在空間感知方面的三個核心限制：1) 物體透視理解，2) 以自我為中心到以他人為中心的轉換，以及3) 度量測量遵守，並強調了改進圖像生成中空間智能的可能方向。", "applications": ["**虛擬室內設計預覽：** 輸入房間尺寸和想擺放的家具，AI生成逼真的空間預覽圖，協助使用者在購買前評估家具擺放效果和整體風格。", "**遊戲場景快速原型設計：** 遊戲開發者快速輸入文字描述，如「佈滿廢墟的古老城市廣場，遠處有一座高聳的城堡」，AI生成符合空間邏輯和透視的場景草圖，加速場景設計流程。", "**建築設計錯誤檢測：** 設計師輸入建築藍圖，AI分析是否存在結構上的空間衝突，例如管道穿過樑柱，提前發現並修正設計錯誤。"], "pitch": "GenSpace的研究揭示了AI圖像生成在空間感知方面的局限性，這反而孕育了巨大的商業機會。我們將基於此研究，開發專注於空間智能的AI圖像生成引擎，解決目前AI在3D空間理解上的不足。首階段，我們可以與室內設計、遊戲開發、建築設計等行業合作，提供更精準、更可控的圖像生成服務，提高生產效率並降低錯誤率。長期來看，隨著空間感知的進一步提升，我們的技術可以應用於自動駕駛、機器人導航等更廣闊的領域，打造下一代具有空間理解能力的AI應用，搶佔市場先機。我們正在尋找具備遠見的創投夥伴，共同投資這項極具潛力的技術，引領AI圖像生成進入一個全新的空間智能時代。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-03T15:15:57.691668"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "邁向通用型神經符號學習之路應由基石模型鋪設", "summary_zh": "神經符號學習旨在解決訓練神經網路處理複雜推理任務的挑戰，同時增強其可解釋性、可靠性和效率。傳統的神經符號學習方法將神經模型與符號程序聯合訓練，但面臨重大挑戰，使其僅限於簡單問題。另一方面，純神經的基石模型現在透過提示而非訓練就能達到最先進的性能，但它們通常不可靠且缺乏可解釋性。利用符號程序來補充基石模型，我們稱之為神經符號提示，提供了一種將這些模型用於複雜推理任務的方法。這引出了一個問題：在基石模型時代，作為神經符號學習一部分的專業模型訓練扮演著什麼角色？為了探討這個問題，我們強調了傳統神經符號學習在計算、數據和程序方面導致泛化問題的三個陷阱。本文認為，基石模型能夠實現通用型神經符號解決方案，提供了一條實現神經符號學習最初目標的途徑，同時避免了從頭開始訓練的缺點。", "applications": ["**醫療診斷輔助：** 利用基石模型理解病歷文本，並結合符號規則（例如，疾病症狀關聯）來輔助醫生進行更準確的診斷和治療方案制定，提高醫療決策的可靠性和效率。", "**法律文件審閱與合規檢查：** 利用基石模型理解法律條款，並結合符號規則（例如，法律合規要求）來自動審閱法律文件，檢查合規性，降低人工審閱成本和風險，提高法律服務效率。", "**智能客服與產品推薦：** 利用基石模型理解用戶意圖，並結合符號規則（例如，產品特性與用戶需求的匹配）來提供個性化的產品推薦和客戶服務，提升用戶體驗和銷售轉化率。"], "pitch": "想像一下，一個AI可以像人類專家一樣思考，既能處理海量數據，又能進行嚴謹的邏輯推理。這就是神經符號學習的願景。傳統的神經符號學習需要大量針對特定任務的訓練，成本高昂且難以泛化。現在，基石模型出現了，它們已經接受過海量數據的訓練，擁有強大的語言理解和生成能力。我們的創新之處在於，利用基石模型作為基礎，並結合符號規則，打造出更強大、更可靠、更可解釋的AI系統。這將極大地降低開發成本，並加速AI在各行業的應用。我們的神經符號提示技術，將基石模型的強大能力與符號規則的精確性相結合，可以廣泛應用於醫療、法律、金融等需要高度可靠性和可解釋性的領域。我們預計，未來五年內，神經符號學習市場將達到數十億美元的規模。我們正在尋找投資者，共同引領這場AI技術革命，為各行各業帶來真正的智能化轉型。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-03T15:16:30.027069"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：使用組合式多視角擴散生成可動畫的精細3D人體", "summary_zh": "AdaHuman 是一種新穎的框架，可從單張真實環境照片生成高保真、可動畫的3D頭像。它結合了兩個關鍵創新：(1) 一個姿態條件式 3D 聯合擴散模型，能合成任意姿態下一致的多視角圖像，並在每個擴散步驟中同步進行 3D 高斯散佈 (3DGS) 重建；(2) 一個組合式 3DGS 精煉模組，透過圖生圖精煉增強局部身體部位的細節，並使用一種新穎的裁剪感知相機光線圖將它們無縫整合，產生一個連貫的精細 3D 頭像。這些元件使 AdaHuman 能夠生成高度逼真的標準 A-pose 頭像，且自我遮擋最小化，從而能夠使用任何輸入動作進行綁定和動畫製作。廣泛的公開基準測試和真實環境圖像評估表明，AdaHuman 在頭像重建和重新擺姿勢方面顯著優於最先進的方法。", "applications": ["**虛擬試穿：** 用戶上傳一張照片，就能生成自己的 3D 頭像，在線上商店中試穿衣服，查看衣服在不同姿態下的效果，提高網購滿意度並降低退貨率。", "**個性化遊戲角色：** 玩家上傳自己的照片，快速生成逼真的遊戲角色，增加遊戲沉浸感和個性化體驗。甚至可以創建基於親朋好友頭像的遊戲角色，增加趣味性。", "**虛擬偶像創建：** 藝人或品牌可以利用 AdaHuman 技術，輕鬆創建高品質的虛擬偶像，用於直播、廣告代言、以及其他數位內容創作，擴展品牌影響力。"], "pitch": "AdaHuman 解決了現有 3D 頭像生成技術在細節和動畫方面的瓶頸，帶來了全新的商業機會。我們提供從單張照片生成高精度、可動畫 3D 頭像的解決方案，大幅降低了 3D 頭像製作的成本和門檻。這將催生虛擬試穿、遊戲角色個性化、虛擬偶像等領域的革命性應用。 我們擁有領先的技術和清晰的商業模式，市場潛力巨大，是值得投資的明日之星。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-03T15:16:52.826555"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "這篇論文提出了一個名為GenSpace的新基準測試，用來評估目前AI圖像生成模型在生成圖像時對3D空間的感知能力。研究發現，雖然AI可以生成視覺上吸引人的圖像並遵循大致的指令，但在物體放置、關係和尺寸等具體的3D細節方面仍然存在困難。論文總結了當前最先進圖像生成模型在空間感知方面的三個主要限制：物體透視理解、自我中心-環境中心轉換，以及度量測量一致性，並指出改進圖像生成空間智能的可能方向。", "applications": ["**客製化室內設計預覽:** 使用者可以輸入文字描述（例如「一個面向大海的現代客廳，帶有白色沙發和落地窗」），AI生成工具根據使用者房間的實際尺寸和結構，生成逼真的室內設計方案預覽圖，確保物體的比例和放置符合現實。", "**虛擬導覽和歷史場景重建:** AI根據歷史文獻或考古數據，生成逼真且符合空間結構的歷史場景。使用者可以進行虛擬導覽，身歷其境地體驗過去的生活，而不用擔心場景的空間關係錯誤。", "**遊戲開發中的關卡設計輔助:** 遊戲開發者可以利用AI根據遊戲劇情和需求快速生成關卡地圖，並確保物體之間的互動符合物理規則和空間邏輯，從而節省設計時間和成本。"], "pitch": "GenSpace論文指出了AI圖像生成在空間感知上的不足，這代表著一個巨大的市場機會。我們的技術可以幫助AI更精確地理解和模擬3D空間，從而提高圖像生成的真實感和實用性。想像一下，你可以利用AI輕鬆創建高度真實的虛擬室內設計、歷史場景重建，甚至加速遊戲關卡設計。我們將專注於解決GenSpace論文中指出的三大問題，並將這些技術整合到現有的圖像生成平台或獨立應用中。透過授權我們的技術、提供客製化解決方案，或開發獨立產品，我們有潛力在室內設計、遊戲開發、教育娛樂等領域創造巨大的商業價值。我們相信，解決AI在空間感知上的盲點，將開啟一個全新的創意表達和應用領域。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-03T16:19:10.197633"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通往泛化型神經符號學習之路應由基石模型鋪就", "summary_zh": "神經符號學習旨在解決訓練神經網路進行複雜推理任務時的挑戰，並提供可解釋性、可靠性和效率等優勢。傳統的神經符號學習方法將神經模型與符號程式共同訓練，但面臨許多限制其應用於簡單問題的挑戰。另一方面，純神經基石模型現在透過提示而非訓練達到最先進的性能，但它們通常不可靠且缺乏可解釋性。我們認為，透過符號程式補充基石模型，即我們稱之為神經符號提示的方法，提供了一種使用這些模型進行複雜推理任務的方式。這也引出了問題：在基石模型的時代，作為神經符號學習一部分的專門模型訓練扮演什麼角色？為了探討這個問題，我們重點介紹了傳統神經符號學習在計算、數據和程式方面導致泛化問題的三個陷阱。本立場文件認為，基石模型使泛化型神經符號解決方案成為可能，從而提供了一條實現神經符號學習最初目標的途徑，而無需從頭開始訓練的缺點。", "applications": ["**醫療診斷輔助：** 基石模型分析病歷和影像，結合符號規則（例如，疾病之間的關聯、藥物禁忌）進行更精確的診斷和治療建議，並解釋推理過程，提升醫生對判斷的信心。", "**智能合約審計：** 使用基石模型理解合約內容，並利用符號程式驗證合約的邏輯和潛在漏洞，提高智能合約的安全性和可靠性。", "**程式碼自動生成與修正：** 基石模型基於自然語言描述生成程式碼框架，並通過符號規則檢查程式碼的語法和邏輯錯誤，提高程式碼開發效率和品質。"], "pitch": "我們正在開發下一代AI引擎，結合基石模型強大的理解能力和符號程式精確的推理能力，打造可解釋、可靠且泛用性更強的人工智慧系統。傳統神經符號學習受限於數據和計算資源，泛化能力差。而我們利用基石模型，無需大量從零開始的訓練，大幅降低開發成本，加速產品落地。目標市場包括醫療、金融、法律等對決策解釋性要求高的行業。相比於純神經網路方案，我們的方案更透明可控，更易於滿足監管要求。 我們尋求種子輪融資，用於完善神經符號提示技術，建立行業應用案例，搶佔市場先機。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-03T16:19:31.185257"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：基於組合式多視角擴散的可動畫細節化3D人體生成", "summary_zh": "AdaHuman 是一個創新的框架，它能從單張真實照片生成高擬真、可動畫的3D人體頭像。它採用了兩個關鍵技術：一是姿態條件的3D聯合擴散模型，在每個擴散步驟中，同步生成在任意姿勢下的多視角一致圖像以及對應的3D高斯潑濺 (3DGS) 重建；二是組合式3DGS細化模組，通過圖像到圖像的細化來增強局部身體部位的細節，並利用一種新穎的裁剪感知相機光線圖將它們無縫集成，從而產生一個連貫且細節豐富的3D頭像。這使得 AdaHuman 能夠生成高度逼真的標準A姿勢頭像，且自我遮擋最小化，方便進行骨骼綁定和任何輸入動作的動畫。在公開基準測試和真實照片上的廣泛評估表明，AdaHuman 在頭像重建和重新擺放方面顯著優於現有技術。", "applications": ["**虛擬試穿體驗：**使用者只需上傳一張照片，即可在虛擬環境中試穿各種衣服，看到 3D 模型在不同姿勢下的穿著效果，解決線上購物的尺寸和款式選擇問題。", "**客製化遊戲角色：**玩家可以基於自己的照片快速生成高細節度的 3D 遊戲角色，並進行個性化調整，提升遊戲的沉浸感和代入感。", "**遠程協作和虛擬會議：**在虛擬會議中，使用者可以使用自己的 3D 頭像進行互動，不再需要打開攝像頭，保護隱私的同時，也能更生動地表達自我。"], "pitch": "AdaHuman 解決了當前 3D 頭像生成技術在細節、真實度和可動畫性方面的瓶頸。其核心技術 – 組合式多視角擴散模型，大幅提升了生成頭像的品質和易用性。市場潛力巨大，可廣泛應用於電商、遊戲、社交、教育、醫療等領域。想像一下，一個提供 AI 頭像生成服務的平台，使用者可以輕鬆創建自己的 3D 化身，用於虛擬活動、線上購物甚至遠程醫療諮詢。這種個性化、高互動性的體驗將吸引大量使用者，並創造新的商業模式。我們的競爭優勢在於技術領先性、高效率和低成本，這使得我們能夠快速搶佔市場，並與現有的 3D 建模公司形成差異化競爭。我們尋求投資者，共同打造下一代 3D 人體化身生態系統，引領虛擬世界的發展。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-03T16:19:58.766853"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "人類能夠直觀地在3D空間中構圖和安排場景，以便進行攝影。然而，當AI圖像生成器根據文字或圖像提示創建圖像時，它們能否像人類一樣具有3D空間感知能力並規劃場景呢？ 本研究提出了GenSpace，一個新的基準測試和評估流程，旨在全面評估當前圖像生成模型的空間感知能力。研究發現，雖然AI模型可以生成視覺上吸引人的圖像並遵循一般指令，但在物體放置、關係和測量等具體的3D細節方面表現不佳。研究歸納了當前最先進圖像生成模型在空間感知方面的三個核心限制：1) 物體透視理解，2) 以自我為中心-以他人為中心的轉換，以及 3) 度量測量一致性，並強調了提高圖像生成空間智能的可能方向。", "applications": ["**虛擬試衣間：** 改善在線購物體驗，讓顧客能更真實地預覽服裝在自己身上的3D效果，解決尺寸和穿搭問題。", "**室內設計預覽：** 讓使用者透過文字描述快速生成不同風格的室內設計方案，並能精確調整傢俱擺放位置和大小，預覽裝修後的實際效果。", "**電影特效合成：** 協助特效團隊更精確地將虛擬元素整合到真實場景中，例如確保爆炸特效的比例和位置符合物理規則。"], "pitch": "GenSpace研究揭示了現有AI圖像生成模型在空間理解上的瓶頸，為開發更智能、更實用的圖像生成工具提供了方向。想像一下，未來使用者只需要簡單描述場景，AI就能生成具有逼真3D空間感的圖像，廣泛應用於電商、設計、娛樂等領域。我們的團隊將基於GenSpace的研究成果，開發專注於空間感知能力的圖像生成引擎，提供更精確、更可控的圖像生成服務。這不僅能大幅提升現有應用的使用者體驗，還能開創全新的商業模式，例如定制化的3D場景生成、虛擬現實內容創作等。我們正在尋找投資者，共同打造下一代圖像生成技術，引領視覺內容創作的革命。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-03T17:13:52.256199"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通往通用型神經符號學習之路，應由基石模型鋪砌", "summary_zh": "神經符號學習旨在解決訓練神經網路處理複雜推理任務的挑戰，並增加可解釋性、可靠性和效率。傳統方法通常聯合訓練神經模型和符號程式，但面臨重大挑戰，使其僅限於簡單問題。另一方面，純神經基石模型現在通過Prompting而非訓練達到最先進的性能，但往往不可靠且缺乏可解釋性。 我們提出利用符號程式來輔助基石模型，稱之為神經符號Prompting，提供了一種使用這些模型進行複雜推理任務的方式。這引發了一個問題：在基石模型的時代，作為神經符號學習一部分的專用模型訓練扮演什麼角色？ 為了解決這個問題，我們重點介紹了傳統神經符號學習在計算、數據和程式方面導致泛化問題的三個陷阱。這篇立場文件認為，基石模型能夠實現通用型的神經符號解決方案，為實現神經符號學習的最初目標提供了一條道路，且避免了從頭開始訓練的缺點。", "applications": ["**自動化法律文件審閱：** 基石模型透過神經符號Prompting，可以分析複雜的法律條款，並根據既定的法律規則（符號程式）判斷條款的合法性、潛在風險，並自動生成審閱報告，大幅提升律師的工作效率。", "**智慧醫療診斷輔助：** 結合基石模型對醫學影像的理解能力與疾病診斷規則（符號程式），輔助醫生進行更精確的疾病診斷。例如，根據X光片判斷骨折類型，同時考慮患者的病史和藥物相互作用，提供更全面的診斷建議。", "**自動駕駛決策優化：** 自動駕駛系統可以利用基石模型識別複雜的交通場景，並透過符號程式定義的交通規則（例如讓路規則、紅綠燈規則），快速做出安全且合理的駕駛決策，提高自動駕駛的安全性與可靠性。"], "pitch": "我們正在開創一個由基石模型驅動的神經符號學習新時代，它能克服傳統神經符號學習的局限性，實現真正通用的人工智慧。我們的神經符號Prompting技術，將基石模型強大的感知能力與符號程式的邏輯推理能力完美結合，帶來更強大的可解釋性、可靠性和泛化能力。這項技術有著巨大的商業潛力，可以應用於法律、醫療、金融等各個行業，解決複雜的決策問題，提升效率，降低風險。我們尋求合作夥伴共同開發和商業化這項突破性技術，搶佔下一代AI的市場先機，打造百億級獨角獸。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-03T17:14:23.044614"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：基於組合式多視角擴散的可動畫化精細3D人體生成", "summary_zh": "現有圖片到3D頭像生成方法難以產生適用於真實世界應用、高度精細且可動畫化的頭像。AdaHuman是一個創新框架，僅需一張真實環境下的圖片即可生成高保真、可動畫化的3D頭像。它包含兩個關鍵創新：(1)一個姿勢條件化的3D聯合擴散模型，可以合成任意姿勢下一致的多視角圖像，並在每個擴散步驟中進行相應的3D高斯濺射（3DGS）重建；(2)一個組合式3DGS細化模塊，通過圖像到圖像的細化增強局部身體部位的細節，並使用一種新的裁剪感知相機光線映射將它們無縫集成，從而產生一個連貫的精細3D頭像。這些組件使AdaHuman能夠生成具有最小自我遮擋的高度逼真的標準A字形頭像，從而可以使用任何輸入運動進行綁定和動畫。在公共基準測試和真實環境圖片上的大量評估表明，AdaHuman在頭像重建和姿勢調整方面顯著優於最先進的方法。代碼和模型將公開用於研究目的。", "applications": ["**虛擬試穿：** 用戶上傳一張照片，即可生成自己的3D頭像，在線上商店試穿各種服裝，看到真實的試穿效果，減少退貨率。", "**個人化遊戲角色：** 遊戲玩家可以使用自己的照片創建高度逼真的遊戲角色，增加遊戲的沉浸感和個性化體驗。", "**遠程協作與虛擬化身：** 在遠程會議或協作環境中，使用個人照片創建的3D化身代替傳統的視訊，提供更具表現力和沉浸感的溝通方式。"], "pitch": "AdaHuman解決了從單張圖片生成高品質、可動畫3D人體頭像的關鍵痛點。相比傳統方法，AdaHuman顯著提升了生成的精細度和真實感，並實現了便捷的動畫化。其商業價值體現在：(1)虛擬試穿領域：大幅提升線上購物體驗，降低退貨率；(2)遊戲與娛樂領域：實現高度個性化的遊戲角色定制，增強用戶黏性；(3)遠程協作領域：提供更逼真、更具表現力的溝通方式。基於AdaHuman的技術，我們可以打造一系列B2C和B2B產品，例如虛擬試衣平台、個人化遊戲角色生成器、沉浸式遠程協作工具等。此外，授權AdaHuman的底層技術給服裝品牌、遊戲公司和遠程協作平台也是一個可行的商業模式。隨著元宇宙和虛擬現實技術的發展，AdaHuman具有巨大的市場潛力，有望成為下一代3D頭像生成領域的領先者。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-03T17:14:48.586930"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "人類在攝影時能直覺地在3D空間中構圖和安排場景。那麼，先進的AI圖像生成器在根據文本或圖像提示創建圖像時，能否以類似的3D空間感知來規劃場景？ 我們提出了GenSpace，一個新穎的基準測試和評估流程，以全面評估當前圖像生成模型的空間感知能力。此外，使用通用視覺-語言模型（VLM）的標準評估通常無法捕捉到詳細的空間錯誤。為了應對這個挑戰，我們提出了一個專門的評估流程和指標，它使用多個視覺基礎模型重建3D場景幾何，並提供更準確和更符合人類直覺的空間真實性度量。 我們的研究結果表明，雖然AI模型可以創建視覺上吸引人的圖像並可以遵循一般指令，但它們在諸如物體放置、關係和測量等特定3D細節方面存在困難。 我們總結了當前最先進的圖像生成模型在空間感知方面的三個核心限制：1）物體透視理解，2）自我中心-以物體為中心的轉換，以及3）度量測量一致性，突出了改進圖像生成中空間智能的可能方向。", "applications": ["**智慧家居設計：** 根據文字描述（例如：“客廳裡沙發的左邊放一盞落地燈，右邊放一個小茶几”）自動生成3D空間布局圖，輔助使用者更直觀地看到設計效果。", "**遊戲開發：** 開發者可以透過文本描述快速生成場景原型，例如“一個佈滿古代遺跡的熱帶雨林”，大幅縮短場景建模時間。", "**虛擬實境（VR）/擴增實境（AR）體驗：** 根據使用者的位置和視角，即時生成或調整周圍的虛擬場景，提供更逼真和沉浸式的互動體驗。"], "pitch": "GenSpace 解決了 AI 圖像生成領域一個關鍵痛點：空間理解能力不足。目前的圖像生成技術擅長創造視覺上令人驚艷的畫面，但缺乏對物體間相對位置、透視關係和真實尺寸的精確把握。這限制了其在需要精準空間規劃的應用場景中的發展。GenSpace 提供了一套基準測試和評估工具，幫助開發者診斷和改進模型的空間感知能力。我們相信，透過提升 AI 的空間智能，能解鎖百億級別的市場機會，尤其是在智慧家居設計、遊戲開發、VR/AR 等領域。我們的差異化競爭優勢在於，我們不僅提供評估指標，還深入分析了現有模型的不足，並指出了明確的改進方向。我們尋求投資，以加速 GenSpace 的開發和商業化，打造下一代更智能、更實用的圖像生成技術。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-03T18:20:15.204503"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通往可泛化神經符號學習之路應以基石模型鋪就", "summary_zh": "神經符號學習旨在解決訓練神經網路處理複雜推理任務時的挑戰，並提供可解釋性、可靠性和效率等優勢。傳統的神經符號學習方法將神經模型與符號程式結合訓練，但面臨限制其應用於簡單問題的重大挑戰。另一方面，純神經基石模型現在通過提示而非訓練達到最先進的性能，但它們通常不可靠且缺乏可解釋性。透過符號程式補充基石模型，我們稱之為神經符號提示，提供了一種將這些模型用於複雜推理任務的方法。這引出了一個問題：在基石模型的時代，作為神經符號學習一部分的專門模型訓練扮演著什麼角色？為了探討這個問題，我們強調了傳統神經符號學習在計算、數據和程式方面導致泛化問題的三個陷阱。這篇立場論文認為，基石模型能夠實現可泛化的神經符號解決方案，為實現神經符號學習的原始目標提供了一條途徑，同時避免了從頭開始訓練的缺點。", "applications": ["**智慧客服系統：** 基石模型理解客戶複雜問題，符號程式處理規則和流程，確保答案準確且可解釋，提升客戶滿意度。", "**醫療診斷輔助：** 基石模型分析病歷和症狀，符號程式應用醫學知識庫，提供更可靠、可驗證的診斷建議，輔助醫生決策。", "**金融風險評估：** 基石模型分析市場數據和新聞，符號程式應用金融模型和監管規則，更精準地評估風險並提供合規建議。"], "pitch": "我們正在解決AI可解釋性和可靠性的根本問題。傳統神經網路就像黑盒子，難以理解其決策過程，而這限制了它們在高風險領域的應用。我們的神經符號提示技術結合了基石模型強大的語言理解能力和符號程式的邏輯推理能力，提供可解釋、可驗證且可泛化的解決方案。這使得AI可以安全可靠地應用於醫療、金融等高度監管行業，潛在市場巨大。我們的技術優勢在於不需要從頭開始訓練模型，而是利用現有的基石模型，大大降低了開發成本和時間。我們正在尋找資金加速產品開發和市場拓展，目標是成為AI可解釋性領域的領導者， unlocking AI的巨大潛力。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-03T18:20:35.784157"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：基於組合式多視角擴散的可動畫化精細3D人體生成", "summary_zh": "AdaHuman 是一個全新的框架，它能從單張真實環境的照片生成高保真、可動畫的 3D 人體模型。它結合了兩項關鍵創新：首先，一個姿態條件式的 3D 關節擴散模型，能夠在任意姿態下合成一致的多視角圖像，並同步重建對應的 3D 高斯潑濺 (3DGS)。其次，一個組合式的 3DGS 細化模塊，透過圖生圖細化來增強局部身體部位的細節，並使用創新的裁剪感知相機光線映射將它們無縫整合，生成連貫且細緻的 3D 人體模型。這讓 AdaHuman 能夠生成高度逼真的標準A字姿態模型，具有最小的自我遮擋，方便使用任何輸入動作進行綁定和動畫。在公開基準測試和真實圖像上的廣泛評估表明，AdaHuman 在人體模型重建和姿態調整方面顯著優於現有技術。", "applications": ["**虛擬試衣間：** 用戶上傳一張照片，就能在虛擬環境中試穿各種服裝，預覽效果逼真，減少退貨率，提升購物體驗。", "**遊戲角色客製化：** 玩家只需上傳一張照片，就能生成高度相似的3D遊戲角色，實現個性化遊戲體驗，降低建模成本。", "**動畫製作：** 動畫師可以快速生成逼真的人體模型，並通過動作捕捉或AI動畫工具賦予其生命，大幅提升動畫製作效率。"], "pitch": "AdaHuman 技術解決了目前圖像到3D人體模型生成領域的痛點，能以單張照片為基礎，快速且高精度地生成可動畫的3D人體模型，擁有巨大的商業潛力。想想看，虛擬試衣間可以大幅提升電商銷售額；個性化遊戲角色能有效提高玩家黏著度；而動畫工作室則能以前所未有的速度創造出令人驚艷的作品。我們正在打造的是新一代的數位化身引擎，應用範圍涵蓋電商、遊戲、娛樂、教育等各個領域。初期我們將聚焦授權核心算法與模型，與相關產業龍頭建立合作關係，快速搶佔市場先機。後續可發展成PaaS平台，提供API接口給開發者，並探索與元宇宙平台的深度整合，挖掘更大的商業價值。團隊擁有深厚的技術積累和市場洞察力，有信心將AdaHuman打造成為領先的3D人體生成解決方案，為投資者帶來豐厚的回報。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-03T18:20:57.628339"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "人類能直觀地在3D空間中構圖，但AI圖像生成模型是否也能在根據文字或圖像提示生成圖像時，進行類似的3D空間感知場景規劃？我們提出了GenSpace，一個全新的基準測試和評估流程，以全面評估目前圖像生成模型的空間感知能力。由於通用視覺語言模型（VLMs）的標準評估經常無法捕捉到詳細的空間錯誤，我們提出了一個專門的評估流程和指標，該流程使用多個視覺基礎模型重建3D場景幾何結構，並提供更準確且與人類對齊的空間真實性指標。我們的研究結果表明，雖然AI模型可以創建視覺上吸引人的圖像並遵循一般指令，但它們在物體放置、關係和測量等特定3D細節方面存在困難。我們總結了當前最先進的圖像生成模型在空間感知方面的三個核心限制：1) 物體透視理解，2) 以自我為中心的到以物體為中心的轉換，以及3) 度量測量遵守。這些發現突出了改進圖像生成中空間智能的可能方向。", "applications": ["**室內設計模擬：** 消費者可以輸入房屋描述和想要的家具風格，讓AI生成符合空間比例和風格的室內設計方案，預覽家具擺放效果，避免購買錯誤尺寸或風格不搭的家具。", "**遊戲地圖設計：** 遊戲開發者可以快速生成符合遊戲設定和物理規則的3D地圖原型，例如，指定「中世紀城堡，依山而建，周圍有河流環繞」，AI能生成符合該描述且物件比例合理的初步地圖，大幅縮短開發時間。", "**虛擬試穿/試戴：** 用戶上傳自己的照片，AI能將服裝或配飾逼真地呈現在用戶身上，確保比例協調，顏色搭配得當，提供更真實的線上購物體驗，降低退貨率。"], "pitch": "GenSpace解決了圖像生成AI在空間感知方面的瓶頸，這是一個巨大的潛在市場。雖然現有的AI能生成美觀的圖像，但在空間關係的理解上存在明顯不足。GenSpace的基準測試和評估流程能幫助開發者針對性地提升AI的空間智能，使其能應用於室內設計、遊戲開發、虛擬試穿等眾多高價值領域。我們的專利技術不僅能提高圖像的真實感，更能大幅提升用戶體驗和商業轉化率。試想一下，一個能精準生成符合實際空間比例的室內設計方案的AI，將徹底顛覆室內設計行業。我們尋求投資，將GenSpace打造成圖像生成AI領域的黃金標準，並加速其商業化落地，共同瓜分這塊巨大的市場蛋糕。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-03T19:11:37.954033"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通往可泛化神經符號學習的道路應由基石模型鋪就", "summary_zh": "神經符號學習旨在解決訓練神經網路處理複雜推理任務的挑戰，並提升可解釋性、可靠性和效率。傳統神經符號學習方法將神經模型與符號程式結合訓練，但這限制了它們在簡化問題上的應用。另一方面，純神經基石模型現在透過提示而非訓練達到最先進的性能，但它們通常不可靠且缺乏可解釋性。利用符號程式補充基石模型，我們稱之為神經符號提示，提供了一種將這些模型用於複雜推理任務的方法。這引出了一個問題：在基石模型的時代，作為神經符號學習一部分的專門模型訓練有什麼作用？為了探索這個問題，我們強調了傳統神經符號學習在計算、數據和程式方面導致泛化問題的三個陷阱。本立場文件認為，基石模型可以實現可泛化的神經符號解決方案，提供了一條實現神經符號學習最初目標的路徑，而無需從頭開始訓練的缺點。", "applications": ["**法律文件審閱自動化：** 利用大型語言模型理解法律條款，再用符號邏輯驗證文件的一致性和潛在風險，例如條款衝突或缺失。", "**醫療診斷輔助系統：** 基於症狀描述和醫療數據（基石模型），結合疾病診斷規則（符號程式）進行推理，提供更準確且可解釋的診斷建議，並解釋診斷的依據。", "**金融風險評估與合規檢查：** 利用市場數據和歷史交易數據（基石模型），結合金融法規和風險管理模型（符號程式），自動評估投資組合的風險，並檢查是否符合監管要求。"], "pitch": "我們正在打造新一代 AI 引擎，結合大型語言模型和符號推理，解決現有 AI 在複雜推理和可解釋性上的瓶頸。傳統神經符號學習過於依賴從零開始的訓練，導致泛化能力差。我們的方案利用現成的基石模型，透過神經符號提示實現知識的注入和推理能力的提升。這能大幅降低開發成本，提升模型的通用性和可靠性。想像一下，一個可以自動審閱法律文件、輔助醫療診斷或評估金融風險的 AI 引擎，市場潛力巨大。我們的技術可以應用於金融、法律、醫療等高價值行業，大幅提升效率並降低成本。我們正在尋求種子輪融資，加速產品開發和市場推廣，成為神經符號 AI 領域的領先者。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-03T19:11:58.810563"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：基於組合式多視圖擴散的可動畫化細緻3D人體生成", "summary_zh": "AdaHuman是一種新的框架，可以從單張真實照片生成高保真、可動畫化的3D人體模型。它使用了一種姿態條件的3D聯合擴散模型，能在任意姿態下合成一致的多視圖圖像，並同步進行3D高斯點雲重建。此外，它還採用了一種組合式的3D高斯點雲精煉模塊，通過圖像到圖像的精煉來增強局部身體部位的細節，並使用一種新型的裁剪感知相機光線映射將它們無縫集成，從而生成連貫且細緻的3D人體模型。這使得AdaHuman能夠生成高度逼真、標準A字姿態的人體模型，並可用於綁定和動畫製作。實驗證明，AdaHuman在人體模型重建和姿勢重塑方面都顯著優於現有技術。", "applications": ["**虛擬試衣間：** 顧客只需上傳一張照片，就能看到自己在不同服裝下的3D模型，從而更好地評估服裝的合身性和風格。", "**遊戲角色定制：** 玩家可以通過上傳自己的照片，快速生成高度逼真的遊戲角色，增加遊戲的沉浸感和個性化體驗。", "**電影特效製作：** 藝術家可以使用AdaHuman快速創建逼真的3D數字替身，簡化特效製作流程，降低製作成本。"], "pitch": "AdaHuman解決了當前3D人體模型生成技術在細節和動畫能力方面的瓶頸。我們的技術可以從單張照片生成高保真、可動畫化的3D人體模型，這具有巨大的商業潛力。試想一下，從電商的虛擬試衣間到遊戲的角色定制，再到電影特效的數字替身，AdaHuman都能大幅提升用戶體驗，降低製作成本。我們團隊擁有多視圖擴散模型的領先技術和成熟的3D重建算法，在市場競爭中具有明顯優勢。我們正在尋求種子輪投資，以加速產品開發，拓展應用場景，並搶占市場先機。我們的目標是成為3D人體模型生成領域的領導者，為各行各業帶來革命性的變革。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-03T19:12:18.448906"}
{"query": "AI", "id": "2505.24870v1", "url": "http://arxiv.org/abs/2505.24870v1", "title": "GenSpace: Benchmarking Spatially-Aware Image Generation", "summary": "Humans can intuitively compose and arrange scenes in the 3D space for\nphotography. However, can advanced AI image generators plan scenes with similar\n3D spatial awareness when creating images from text or image prompts? We\npresent GenSpace, a novel benchmark and evaluation pipeline to comprehensively\nassess the spatial awareness of current image generation models. Furthermore,\nstandard evaluations using general Vision-Language Models (VLMs) frequently\nfail to capture the detailed spatial errors. To handle this challenge, we\npropose a specialized evaluation pipeline and metric, which reconstructs 3D\nscene geometry using multiple visual foundation models and provides a more\naccurate and human-aligned metric of spatial faithfulness. Our findings show\nthat while AI models create visually appealing images and can follow general\ninstructions, they struggle with specific 3D details like object placement,\nrelationships, and measurements. We summarize three core limitations in the\nspatial perception of current state-of-the-art image generation models: 1)\nObject Perspective Understanding, 2) Egocentric-Allocentric Transformation and\n3) Metric Measurement Adherence, highlighting possible directions for improving\nspatial intelligence in image generation.", "authors": ["Zehan Wang", "Jiayang Xu", "Ziang Zhang", "Tianyu Pan", "Chao Du", "Hengshuang Zhao", "Zhou Zhao"], "published_date": "2025-05-30", "title_zh": "GenSpace：空間感知圖像生成基準測試", "summary_zh": "這篇論文提出一個名為GenSpace的基準測試，用來評估AI圖像生成模型在創建圖像時，是否能像人類一樣具備3D空間感知能力。研究發現，雖然AI生成的圖像在視覺上吸引人，也能遵循一般指令，但在物體放置、關係和測量等具體3D細節方面表現不佳。研究指出了三個核心限制：物體透視理解、以自我為中心的視角轉換，以及度量標準的遵守，並為改善圖像生成中的空間智能指明了方向。", "applications": ["**建築設計輔助：** 設計師可以輸入文字描述，AI生成符合空間規劃和透視比例的建築草圖，節省前期繪製時間並提供靈感。", "**虛擬實境內容生成：** 遊戲開發者或VR內容創作者，能基於文字快速生成符合空間關係的3D場景，加速開發流程並降低成本。", "**電商產品展示：** 用戶可以輸入商品描述和空間大小，AI生成模擬房間擺放效果的圖像，幫助消費者更直觀地了解商品尺寸和搭配效果。"], "pitch": "GenSpace論文揭示了當前AI圖像生成模型在3D空間感知方面的局限性，而這正是下一代圖像生成技術的關鍵突破口。我們將基於這項研究，開發更具空間智能的圖像生成引擎，解決市場痛點，例如建築設計、VR/AR內容創作和電商產品展示等。想像一下，建築師能透過簡單的文字描述快速生成多種符合透視和比例的建築方案；VR開發者能夠更快地構建沉浸式的虛擬世界；消費者能夠更直觀地預覽商品在自家空間中的擺放效果。這不僅能大幅提升效率，降低成本，還能帶來全新的用戶體驗。我們相信，具備卓越空間感知能力的圖像生成技術，將在多個行業掀起革命性的變革，帶來巨大的商業價值。", "audio": "audios/2505.24870v1.mp3", "timestamp": "2025-06-03T20:16:01.316163"}
{"query": "Foundation Model", "id": "2505.24874v1", "url": "http://arxiv.org/abs/2505.24874v1", "title": "The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models", "summary": "Neuro-symbolic learning was proposed to address challenges with training\nneural networks for complex reasoning tasks with the added benefits of\ninterpretability, reliability, and efficiency. Neuro-symbolic learning methods\ntraditionally train neural models in conjunction with symbolic programs, but\nthey face significant challenges that limit them to simplistic problems. On the\nother hand, purely-neural foundation models now reach state-of-the-art\nperformance through prompting rather than training, but they are often\nunreliable and lack interpretability. Supplementing foundation models with\nsymbolic programs, which we call neuro-symbolic prompting, provides a way to\nuse these models for complex reasoning tasks. Doing so raises the question:\nWhat role does specialized model training as part of neuro-symbolic learning\nhave in the age of foundation models? To explore this question, we highlight\nthree pitfalls of traditional neuro-symbolic learning with respect to the\ncompute, data, and programs leading to generalization problems. This position\npaper argues that foundation models enable generalizable neuro-symbolic\nsolutions, offering a path towards achieving the original goals of\nneuro-symbolic learning without the downsides of training from scratch.", "authors": ["Adam Stein", "Aaditya Naik", "Neelay Velingker", "Mayur Naik", "Eric Wong"], "published_date": "2025-05-30", "title_zh": "通往廣義神經符號學習的道路應該以基礎模型鋪就", "summary_zh": "神經符號學習旨在解決訓練神經網絡進行複雜推理任務時遇到的挑戰，並帶來可解釋性、可靠性和效率等優點。傳統的神經符號學習方法通常將神經模型與符號程序結合訓練，但它們面臨著嚴峻的挑戰，使其只能解決簡單的問題。另一方面，純神經基礎模型現在通過提示而非訓練達到了最先進的性能，但它們通常不可靠且缺乏可解釋性。我們稱之為「神經符號提示」的方法，即使用符號程序來補充基礎模型，為這些模型用於複雜推理任務提供了一種途徑。 這樣做就引出了一個問題：在基礎模型的時代，作為神經符號學習一部分的專門模型訓練有什麼作用？ 為了探討這個問題，我們重點介紹了傳統神經符號學習在計算、數據和程序方面的三個陷阱，這些陷阱導致了泛化問題。 這篇立場論文認為，基礎模型能夠實現廣義神經符號解決方案，為實現神經符號學習的原始目標提供了一條途徑，而無需從頭開始訓練的缺點。", "applications": ["**智能客服與法律諮詢：** 基於大語言模型的客服機器人，通過神經符號提示整合法律知識庫，能更準確且可解釋地回答複雜的法律問題，例如房產繼承、勞資糾紛等。", "**醫學診斷輔助：** 將醫學知識庫（例如疾病症狀、藥物相互作用）嵌入到大型語言模型中，幫助醫生進行更準確的診斷和治療方案制定。 相比於單純依賴模型判斷，醫生可以追溯模型推理過程，提高信任度。", "**金融風險評估：** 利用神經符號提示，結合經濟指標、公司財務數據等結構化信息與新聞文本等非結構化信息，更全面地評估企業或個人的信用風險。 此方法可解釋風險評估的原因，方便進行風險管理。"], "pitch": "我們正在打造下一代AI推理引擎，它結合了大型語言模型的強大能力和符號邏輯的嚴謹性。傳統的神經符號學習方法受限於訓練數據和計算資源，難以應用於複雜場景。而我們的「神經符號提示」技術，充分利用了現有基礎模型的優勢，只需少量甚至無需訓練，就能讓AI在醫療、金融、法律等領域實現可解釋、可靠的推理。想像一下，一個能夠分析複雜的金融交易，並清晰解釋其潛在風險的AI系統，或者一個能夠根據患者病歷和醫學知識，提供個性化治療建議的AI醫生。這就是我們的願景。 我們相信，這項技術將顛覆傳統的AI應用，為各行各業帶來巨大的效率提升和價值創造。尋求種子輪融資，用於擴大研發團隊，並將技術應用於特定垂直領域的商業化驗證。", "audio": "audios/2505.24874v1.mp3", "timestamp": "2025-06-03T20:16:39.496896"}
{"query": "Diffusion Model", "id": "2505.24877v1", "url": "http://arxiv.org/abs/2505.24877v1", "title": "AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion", "summary": "Existing methods for image-to-3D avatar generation struggle to produce highly\ndetailed, animation-ready avatars suitable for real-world applications. We\nintroduce AdaHuman, a novel framework that generates high-fidelity animatable\n3D avatars from a single in-the-wild image. AdaHuman incorporates two key\ninnovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes\nconsistent multi-view images in arbitrary poses alongside corresponding 3D\nGaussian Splats (3DGS) reconstruction at each diffusion step; (2) A\ncompositional 3DGS refinement module that enhances the details of local body\nparts through image-to-image refinement and seamlessly integrates them using a\nnovel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These\ncomponents allow AdaHuman to generate highly realistic standardized A-pose\navatars with minimal self-occlusion, enabling rigging and animation with any\ninput motion. Extensive evaluation on public benchmarks and in-the-wild images\ndemonstrates that AdaHuman significantly outperforms state-of-the-art methods\nin both avatar reconstruction and reposing. Code and models will be publicly\navailable for research purposes.", "authors": ["Yangyi Huang", "Ye Yuan", "Xueting Li", "Jan Kautz", "Umar Iqbal"], "published_date": "2025-05-30", "title_zh": "AdaHuman：具備可動畫細節的三維人體生成，採用組合式多視角擴散", "summary_zh": "AdaHuman 是一個創新的框架，它能夠從單張真實環境照片生成高逼真度、可動畫的三維人體模型。它結合了兩個關鍵創新：(1) 一個姿勢條件的3D聯合擴散模型，可在任意姿勢下合成一致的多視角圖像，並在每個擴散步驟進行對應的3D高斯潑濺(3DGS)重建；(2) 一個組合式的3DGS精煉模組，透過圖像到圖像的精煉來增強局部身體部位的細節，並使用一種新穎的、感知裁剪的相機光線圖將它們無縫整合，產生一個具有凝聚力且細節豐富的3D人體模型。 AdaHuman 能夠生成高度真實、標準A字站姿的人體模型，並具有最小的自我遮擋，從而可以使用任何輸入運動進行綁定和動畫。 大量在公開基準和真實環境圖像上的評估表明，AdaHuman 在人體模型重建和重新擺姿勢方面顯著優於最先進的方法。", "applications": ["**虛擬試衣間：** 顧客可以上傳一張照片，立即生成自己的3D模型，並試穿各種線上服飾，預覽實際穿著效果，減少退貨率。", "**遊戲角色客製化：** 玩家可以上傳自拍照，快速創建一個與自己高度相似的遊戲角色，增加遊戲的沉浸感和個性化體驗。", "**遠程醫療/健身指導：** 病患/使用者可以上傳照片，醫生/教練利用生成的3D模型進行遠程評估身體狀況，提供個人化的復健或健身計畫。"], "pitch": "AdaHuman 解決了傳統3D人體模型生成技術在細節和動畫方面的瓶頸，為虛擬人物創造帶來革命性的突破。其基於單張照片生成高保真、可動畫模型的技術，降低了3D建模的門檻和成本，開闢了廣闊的商業應用前景。想像一下，虛擬試衣間、遊戲角色客製化、遠程醫療等領域都將因此獲得質的飛躍。其潛在商業價值不僅體現在 B2C 端的用戶體驗提升，更在於 B2B 端，例如為服裝品牌、遊戲開發商、醫療機構等提供高效且個性化的 3D 人體模型解決方案，具有極高的市場潛力和投資回報。", "audio": "audios/2505.24877v1.mp3", "timestamp": "2025-06-03T20:17:07.889536"}
