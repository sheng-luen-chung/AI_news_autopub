import re
import arxiv
import json
import os
from gtts import gTTS
from datetime import datetime
from dotenv import load_dotenv
import google.generativeai as genai

NEWS_PATH = "docs/news.jsonl"
PROCESSED_IDS_PATH = "processed_ids.txt"
AUDIO_DIR = "docs/audios"
QUERY = ["AI", "Foundation Model", "Diffusion Model"]

# === è¨­å®š Gemini API é‡‘é‘° ===
load_dotenv()
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
model = genai.GenerativeModel("gemini-2.0-flash")

# === è®€å–èˆ‡å„²å­˜å·²è™•ç†çš„ ID ===
def load_processed_ids(path=PROCESSED_IDS_PATH):
    if not os.path.exists(path):
        return set()
    with open(path, "r") as f:
        return set(line.strip() for line in f.readlines())

def save_processed_ids(ids, path=PROCESSED_IDS_PATH):
    with open(path, "w") as f:
        f.write("\n".join(sorted(ids)))

# === æŠ“å– arXiv è«–æ–‡æ‘˜è¦ ===
def fetch_ai_papers(query, processed_ids, max_results=50):
    client = arxiv.Client()
    search = arxiv.Search(
        query=f'"{query}"',
        max_results=max_results,
        sort_by=arxiv.SortCriterion.SubmittedDate
    )
    papers = []
    new_ids = set()

    for result in client.results(search):
        arxiv_id = result.get_short_id()
        if arxiv_id in processed_ids:
            continue

        papers.append({
            "query": query,
            "id": arxiv_id,
            "url": result.entry_id,
            "title": result.title,
            "summary": result.summary,
            "authors": [author.name for author in result.authors],
            "published_date": result.published.strftime("%Y-%m-%d"),
        })

        processed_ids.add(arxiv_id)
        new_ids.add(arxiv_id)
        break  # æ¯å€‹ query åªå–ä¸€ç¯‡

    return papers, new_ids

# === Gemini ç”Ÿæˆä¸­æ–‡æ‘˜è¦èˆ‡ pitch ===
def summarize_to_chinese(title, summary):
    prompt = (
        f"è«‹å°‡ä»¥ä¸‹arXivè«–æ–‡æ¨™é¡Œèˆ‡æ‘˜è¦ç¿»è­¯æˆç¹é«”ä¸­æ–‡ï¼Œä¸¦å®Œæˆä»¥ä¸‹ä¸‰å€‹ä»»å‹™ï¼š\n"
        f"1. æ¿ƒç¸®ç‚ºé©åˆæ”¶è½çš„ç°¡æ˜æ‘˜è¦ã€‚\n"
        f"2. è¨­æƒ³ä¸‰å€‹ç”Ÿæ´»åŒ–æ‡‰ç”¨å ´æ™¯ã€‚\n"
        f"3. ç”¨å‰µæŠ•è¦–è§’èªªæ˜æ½›åœ¨å•†æ¥­åƒ¹å€¼ã€‚\n\n"
        f"æ¨™é¡Œï¼š{title}\n"
        f"æ‘˜è¦ï¼š{summary}\n"
        f"è«‹å›å‚³ JSON æ ¼å¼ï¼š\n"
        f'{{"title_zh": "...", "summary_zh": "...", "applications": ["...", "...", "..."], "pitch": "..."}}'
    )

    try:
        response = model.generate_content(prompt)
        text = response.text.strip()
        text = re.sub(r"^```json|^```|```$", "", text, flags=re.MULTILINE).strip()
        return json.loads(text)
    except Exception as e:
        print("âš ï¸ ç„¡æ³•è§£æ Gemini å›å‚³çš„ JSONï¼Œè·³éè©²ç­†è³‡æ–™ã€‚")
        print("ğŸ”¹ åŸå§‹å…§å®¹ï¼š\n", response.text)
        return None

# === å°‡æ‘˜è¦è½‰æˆèªéŸ³æª” ===
def save_audio(text, filename):
    tts = gTTS(text, lang='zh-tw')
    tts.save(filename)

# === ä¸»ç¨‹å¼ ===
def main():
    os.makedirs(AUDIO_DIR, exist_ok=True)
    all_processed_ids = load_processed_ids()
    new_processed_ids = set()
    papers = []

    for query in QUERY:
        print(f"æ­£åœ¨æŠ“å– {query} ç›¸é—œæ–‡ç« ...")
        result, new_ids = fetch_ai_papers(query, all_processed_ids)
        if result:
            papers.extend(result)
            new_processed_ids.update(new_ids)
            print(f"âœ… å·²æŠ“å–: {result[0]['title']}")
    print(f"ğŸ“š ç¸½å…±æŠ“å–åˆ° {len(papers)} ç¯‡æ–‡ç« ")

    if len(papers) == 0:
        print("âš ï¸ æ²’æœ‰æŠ“åˆ°ä»»ä½•æ–°æ–‡ç« ")
        return

    count = 0
    for i, paper in enumerate(papers):
        print(f"ğŸ”„ æ­£åœ¨è™•ç†ç¬¬ {i+1} ç¯‡ {paper['title']}")
        result = summarize_to_chinese(paper['title'], paper['summary'])
        if result is None:
            continue  # è·³éæ ¼å¼éŒ¯èª¤çš„å›æ‡‰

        audio_text = (
            f"{result['title_zh']}\n\n"
            f"{result['summary_zh']}\n\n"
            f"é€™é …æŠ€è¡“æœ‰ä¸‰å€‹ç”Ÿæ´»åŒ–çš„æ‡‰ç”¨å ´æ™¯ï¼š\n"
            f"ç¬¬ä¸€ï¼Œ{result['applications'][0]}\n"
            f"ç¬¬äºŒï¼Œ{result['applications'][1]}\n"
            f"ç¬¬ä¸‰ï¼Œ{result['applications'][2]}\n\n"
            f"å‘å‰µæŠ•æ¨éŠ·çš„èªªæ³•ï¼š\n{result['pitch']}"
        )

        audio_filename = f"{paper['id']}.mp3"
        audio_path = f"{AUDIO_DIR}/{audio_filename}"
        save_audio(audio_text, audio_path)

        paper.update({
            "title_zh": result["title_zh"],
            "summary_zh": result["summary_zh"],
            "applications": result["applications"],
            "pitch": result["pitch"],
            "audio": f"audios/{audio_filename}",
            "timestamp": datetime.now().isoformat()
        })

        with open(NEWS_PATH, "a", encoding="utf-8") as f:
            f.write(json.dumps(paper, ensure_ascii=False) + "\n")

        count += 1
        if count >= 5:
            print("ğŸš« å·²é”æ¯æ—¥ä¸Šé™ 5 ç­†ï¼Œåœæ­¢æ›´æ–°")
            break

    save_processed_ids(all_processed_ids.union(new_processed_ids))
    print("âœ… æ›´æ–°å®Œæˆï¼šnews.jsonl å’Œ MP3 éŸ³æª”å·²ç”¢ç”Ÿ")

if __name__ == "__main__":
    main()
